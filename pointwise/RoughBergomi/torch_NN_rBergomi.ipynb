{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "f = gzip.GzipFile(r\"../../Data/rBergomiTrainSet.txt.gz\", \"r\")\n",
    "dat=np.load(f)\n",
    "xx=dat[:,:4]\n",
    "yy=dat[:,4:]\n",
    "strikes=np.array([0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4,1.5 ])\n",
    "maturities=np.array([0.1,0.3,0.6,0.9,1.2,1.5,1.8,2.0 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = yy.reshape(-1,8,11)\n",
    "xx = np.repeat(xx, 8*11,axis=0)\n",
    "\n",
    "def append_and_expand(a,x,y):\n",
    "    # use choose and where !\n",
    "    n = len(x)*len(y)\n",
    "    a_index = np.arange(len(a))%n\n",
    "    \n",
    "    \n",
    "    x_index = a_index//len(y)\n",
    "    y_index = a_index%len(y)\n",
    "    \n",
    "    x_added = np.choose(x_index,x.reshape(-1,1)).reshape(-1,1)\n",
    "    y_added = np.choose(y_index,y.reshape(-1,1)).reshape(-1,1)\n",
    "    \n",
    "    return np.hstack([a,x_added,y_added])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_pointwise=append_and_expand(xx,maturities,strikes)\n",
    "yy_pointwise = yy.reshape(-1)\n",
    "\n",
    "xx = xx_pointwise\n",
    "yy = yy_pointwise.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    xx, yy, test_size=0.15, random_state=42)\n",
    "\n",
    "scale_y=  StandardScaler()\n",
    "\n",
    "def ytransform(y_train,y_test):\n",
    "    return [scale_y.fit_transform(y_train),scale_y.transform(y_test)]\n",
    "\n",
    "def yinversetransform(y):\n",
    "    return scale_y.inverse_transform(y)\n",
    "\n",
    "# Upper and lower bounds used in the training set\n",
    "ub=np.array([0.16,4,-0.1,0.5,2.0,1.5])\n",
    "lb=np.array([0.01,0.3,-0.95,0.025,0.1,0.5])\n",
    "\n",
    "def myscale(x):\n",
    "    return (x - (ub+lb)*0.5)*2/(ub-lb)\n",
    "def myinverse(x):\n",
    "    return x*(ub-lb)*0.5+(ub+lb)*0.5\n",
    "\n",
    "x_train_transform = myscale(x_train)\n",
    "x_test_transform = myscale(x_test)\n",
    "[y_train_transform,y_test_transform] = ytransform(y_train,y_test)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"device is {device}\")\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(x_train_transform).to(device=device),\n",
    "                                               torch.from_numpy(y_train_transform).to(device=device))\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.from_numpy(x_test_transform).to(device=device),\n",
    "                                              torch.from_numpy(y_test_transform).to(device=device))\n",
    "\n",
    "\n",
    "train_data = (torch.from_numpy(x_train_transform).to(device=device),torch.from_numpy(y_train_transform).to(device=device))\n",
    "test_data = (torch.from_numpy(x_test_transform).to(device=device),torch.from_numpy(y_test_transform).to(device=device))\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(train_dataset,batch_size =64,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')  # Add the parent directory to the Python path\n",
    "\n",
    "from torch_NN.nn import ResNN_pricing\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "hyperparas = {'input_dim':6,'hidden_dim':32,'hidden_nums':20,'output_dim':1,'block_layer_nums':2}\n",
    "\n",
    "model = ResNN_pricing(hyperparas=hyperparas).to(device=device,dtype=torch.float64)\n",
    "\n",
    "loss_MSE = nn.MSELoss()\n",
    "optim_Adam = torch.optim.Adam(model.parameters(),lr= 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Epoch: 0----------------------------------\n",
      "Batch: 0,train loss is: 39.29756704228721\n",
      "test loss is 38.151167433797696\n",
      "Batch: 100,train loss is: 0.5843639881753468\n",
      "test loss is 0.842929599332749\n",
      "Batch: 200,train loss is: 0.5139097652656766\n",
      "test loss is 0.5533844549454616\n",
      "Batch: 300,train loss is: 0.33245057524485067\n",
      "test loss is 0.39787144608994124\n",
      "Batch: 400,train loss is: 0.2517176686485404\n",
      "test loss is 0.30462911331256803\n",
      "Batch: 500,train loss is: 0.15948836229594548\n",
      "test loss is 0.24707808222130523\n",
      "Batch: 600,train loss is: 0.18326424653655704\n",
      "test loss is 0.2042683893634944\n",
      "Batch: 700,train loss is: 0.13854767127035805\n",
      "test loss is 0.17489374900916344\n",
      "Batch: 800,train loss is: 0.24754608838131598\n",
      "test loss is 0.15244884814755724\n",
      "Batch: 900,train loss is: 0.13333990608123086\n",
      "test loss is 0.13547747333558702\n",
      "Batch: 1000,train loss is: 0.13805892667338932\n",
      "test loss is 0.122157739067549\n",
      "Batch: 1100,train loss is: 0.060020524032344286\n",
      "test loss is 0.11211852469420187\n",
      "Batch: 1200,train loss is: 0.11977025543487968\n",
      "test loss is 0.10279737193953366\n",
      "Batch: 1300,train loss is: 0.10025701860070743\n",
      "test loss is 0.0950092910516527\n",
      "Batch: 1400,train loss is: 0.06586897144721125\n",
      "test loss is 0.08824193052261993\n",
      "Batch: 1500,train loss is: 0.13277649509982364\n",
      "test loss is 0.08314352950222836\n",
      "Batch: 1600,train loss is: 0.08926455079862339\n",
      "test loss is 0.07816766830403352\n",
      "Batch: 1700,train loss is: 0.05246025770213057\n",
      "test loss is 0.07258891068855897\n",
      "Batch: 1800,train loss is: 0.07901551074510683\n",
      "test loss is 0.06765178075665196\n",
      "Batch: 1900,train loss is: 0.04591573989220381\n",
      "test loss is 0.0642467518594539\n",
      "Batch: 2000,train loss is: 0.05931716367468746\n",
      "test loss is 0.06140379313073587\n",
      "Batch: 2100,train loss is: 0.04077196914252533\n",
      "test loss is 0.058033541798004504\n",
      "Batch: 2200,train loss is: 0.07000051398790644\n",
      "test loss is 0.05320393299997225\n",
      "Batch: 2300,train loss is: 0.025346963602989922\n",
      "test loss is 0.050466777390528704\n",
      "Batch: 2400,train loss is: 0.0633887139194332\n",
      "test loss is 0.047237975132147246\n",
      "Batch: 2500,train loss is: 0.05365970316563884\n",
      "test loss is 0.04527146712076503\n",
      "Batch: 2600,train loss is: 0.023313850009297435\n",
      "test loss is 0.04342747345487417\n",
      "Batch: 2700,train loss is: 0.05081497391972541\n",
      "test loss is 0.039747350521639815\n",
      "Batch: 2800,train loss is: 0.057088666481538236\n",
      "test loss is 0.038179375801639714\n",
      "Batch: 2900,train loss is: 0.03200358813495131\n",
      "test loss is 0.03629990874118294\n",
      "Batch: 3000,train loss is: 0.035576744446556674\n",
      "test loss is 0.033817692033030415\n",
      "Batch: 3100,train loss is: 0.03487870511501113\n",
      "test loss is 0.03283428111346102\n",
      "Batch: 3200,train loss is: 0.025794623758594686\n",
      "test loss is 0.031762026321044357\n",
      "Batch: 3300,train loss is: 0.0311621449888766\n",
      "test loss is 0.029551085676430878\n",
      "Batch: 3400,train loss is: 0.022848930698506235\n",
      "test loss is 0.027896694004817067\n",
      "Batch: 3500,train loss is: 0.015641738380958073\n",
      "test loss is 0.026490227498550425\n",
      "Batch: 3600,train loss is: 0.033756798422726136\n",
      "test loss is 0.02560988056823703\n",
      "Batch: 3700,train loss is: 0.03134724282720107\n",
      "test loss is 0.024516905475254973\n",
      "Batch: 3800,train loss is: 0.02121841914586847\n",
      "test loss is 0.023716325524788882\n",
      "Batch: 3900,train loss is: 0.02366821693157682\n",
      "test loss is 0.02287706047603364\n",
      "Batch: 4000,train loss is: 0.016438327349515582\n",
      "test loss is 0.02310380810501679\n",
      "Batch: 4100,train loss is: 0.017115191628119523\n",
      "test loss is 0.02142754684901967\n",
      "Batch: 4200,train loss is: 0.026771948694130573\n",
      "test loss is 0.01961900362943975\n",
      "Batch: 4300,train loss is: 0.010772352946735314\n",
      "test loss is 0.01922362252147706\n",
      "Batch: 4400,train loss is: 0.016888760114707153\n",
      "test loss is 0.018392445844813636\n",
      "Batch: 4500,train loss is: 0.014111501544065746\n",
      "test loss is 0.017982914650433456\n",
      "Batch: 4600,train loss is: 0.015981522647501013\n",
      "test loss is 0.017244084275634317\n",
      "Batch: 4700,train loss is: 0.009380908455309478\n",
      "test loss is 0.0173117517458242\n",
      "Batch: 4800,train loss is: 0.0208472964176303\n",
      "test loss is 0.01755887481533392\n",
      "Batch: 4900,train loss is: 0.01848872849176874\n",
      "test loss is 0.016903922552609627\n",
      "Batch: 5000,train loss is: 0.02849704601634952\n",
      "test loss is 0.015598408474335539\n",
      "Batch: 5100,train loss is: 0.01599358321636809\n",
      "test loss is 0.015448063009788553\n",
      "Batch: 5200,train loss is: 0.010734984830592712\n",
      "test loss is 0.01658510147405663\n",
      "Batch: 5300,train loss is: 0.02550321080387488\n",
      "test loss is 0.014327139998019696\n",
      "Batch: 5400,train loss is: 0.013336603615238116\n",
      "test loss is 0.014219954263638536\n",
      "Batch: 5500,train loss is: 0.01437061883816813\n",
      "test loss is 0.01331721231667693\n",
      "Batch: 5600,train loss is: 0.015616655123001857\n",
      "test loss is 0.012804719548956968\n",
      "Batch: 5700,train loss is: 0.010657182436716161\n",
      "test loss is 0.012723156649740417\n",
      "Batch: 5800,train loss is: 0.014016589738190914\n",
      "test loss is 0.012340599391690223\n",
      "Batch: 5900,train loss is: 0.012103911214676775\n",
      "test loss is 0.012184574945221642\n",
      "Batch: 6000,train loss is: 0.013506451778085423\n",
      "test loss is 0.011771811326028432\n",
      "Batch: 6100,train loss is: 0.0113056309108283\n",
      "test loss is 0.012270246530237615\n",
      "Batch: 6200,train loss is: 0.010415962412990249\n",
      "test loss is 0.011935808262546968\n",
      "Batch: 6300,train loss is: 0.009689316877397547\n",
      "test loss is 0.011363493125451648\n",
      "Batch: 6400,train loss is: 0.010738666299597459\n",
      "test loss is 0.010668055797467938\n",
      "Batch: 6500,train loss is: 0.018567065591244165\n",
      "test loss is 0.011828060902915787\n",
      "Batch: 6600,train loss is: 0.01660747591600781\n",
      "test loss is 0.011333820159616327\n",
      "Batch: 6700,train loss is: 0.009336993323364642\n",
      "test loss is 0.014039985669585264\n",
      "Batch: 6800,train loss is: 0.009767560735406338\n",
      "test loss is 0.01128822763144878\n",
      "Batch: 6900,train loss is: 0.005705746207872989\n",
      "test loss is 0.010612125505504633\n",
      "Batch: 7000,train loss is: 0.009028605265057218\n",
      "test loss is 0.01217605362109891\n",
      "Batch: 7100,train loss is: 0.005571605469222172\n",
      "test loss is 0.01070731215348106\n",
      "Batch: 7200,train loss is: 0.009035523195299159\n",
      "test loss is 0.009779418912474303\n",
      "Batch: 7300,train loss is: 0.007727137045196276\n",
      "test loss is 0.00951714768724289\n",
      "Batch: 7400,train loss is: 0.011114959746362056\n",
      "test loss is 0.009635057197807948\n",
      "Batch: 7500,train loss is: 0.009955112152793686\n",
      "test loss is 0.008979301180419619\n",
      "Batch: 7600,train loss is: 0.018622810603520584\n",
      "test loss is 0.010217200809885963\n",
      "Batch: 7700,train loss is: 0.015888531853526323\n",
      "test loss is 0.008839117470519868\n",
      "Batch: 7800,train loss is: 0.005066033239138739\n",
      "test loss is 0.008549282222902571\n",
      "Batch: 7900,train loss is: 0.009302310322187073\n",
      "test loss is 0.008503763285241371\n",
      "Batch: 8000,train loss is: 0.006794680080238438\n",
      "test loss is 0.008479069918017734\n",
      "Batch: 8100,train loss is: 0.005755940759766844\n",
      "test loss is 0.008818017963916335\n",
      "Batch: 8200,train loss is: 0.005312755246215599\n",
      "test loss is 0.00801806496768319\n",
      "Batch: 8300,train loss is: 0.010309916429692333\n",
      "test loss is 0.007634511321863778\n",
      "Batch: 8400,train loss is: 0.012823977042449114\n",
      "test loss is 0.008134919526146492\n",
      "Batch: 8500,train loss is: 0.005979759976216565\n",
      "test loss is 0.007910316995421909\n",
      "Batch: 8600,train loss is: 0.007271886208698058\n",
      "test loss is 0.007323575010578644\n",
      "Batch: 8700,train loss is: 0.006513194541610788\n",
      "test loss is 0.007408454534510094\n",
      "Batch: 8800,train loss is: 0.008314748274628848\n",
      "test loss is 0.007745366331649255\n",
      "Batch: 8900,train loss is: 0.009853019386335229\n",
      "test loss is 0.007031398908640166\n",
      "Batch: 9000,train loss is: 0.006304617674754683\n",
      "test loss is 0.008426163451622561\n",
      "Batch: 9100,train loss is: 0.004617385053181261\n",
      "test loss is 0.008210707867104833\n",
      "Batch: 9200,train loss is: 0.009440593549628448\n",
      "test loss is 0.0075752248932951785\n",
      "Batch: 9300,train loss is: 0.008894194871018977\n",
      "test loss is 0.0070915194504406675\n",
      "Batch: 9400,train loss is: 0.00782026757934372\n",
      "test loss is 0.006451767479651928\n",
      "Batch: 9500,train loss is: 0.0057555298001624\n",
      "test loss is 0.006538011615095201\n",
      "Batch: 9600,train loss is: 0.005972817866777459\n",
      "test loss is 0.007151821349087636\n",
      "Batch: 9700,train loss is: 0.008007511927543605\n",
      "test loss is 0.0064676762675438355\n",
      "Batch: 9800,train loss is: 0.011601614335350485\n",
      "test loss is 0.006153988646235253\n",
      "Batch: 9900,train loss is: 0.004826343726167481\n",
      "test loss is 0.006682016917631608\n",
      "Batch: 10000,train loss is: 0.005615942691347707\n",
      "test loss is 0.006830936039587365\n",
      "Batch: 10100,train loss is: 0.0034425760515579443\n",
      "test loss is 0.005910105888403178\n",
      "Batch: 10200,train loss is: 0.0057225762974065595\n",
      "test loss is 0.00741377226339733\n",
      "Batch: 10300,train loss is: 0.0046735369699516405\n",
      "test loss is 0.0069650352883709665\n",
      "Batch: 10400,train loss is: 0.006170565559035697\n",
      "test loss is 0.007924077008298152\n",
      "Batch: 10500,train loss is: 0.003977867673246751\n",
      "test loss is 0.005903258712021357\n",
      "Batch: 10600,train loss is: 0.011529421028604808\n",
      "test loss is 0.005820610210692457\n",
      "Batch: 10700,train loss is: 0.005161391052111466\n",
      "test loss is 0.0066487125808916605\n",
      "Batch: 10800,train loss is: 0.0069784361713287136\n",
      "test loss is 0.006626317379293027\n",
      "Batch: 10900,train loss is: 0.005161126718489029\n",
      "test loss is 0.006869472940415246\n",
      "Batch: 11000,train loss is: 0.0053449461499394686\n",
      "test loss is 0.0054943063839991944\n",
      "Batch: 11100,train loss is: 0.007072988324790317\n",
      "test loss is 0.006930148408197781\n",
      "Batch: 11200,train loss is: 0.004506372185299295\n",
      "test loss is 0.005530869660856987\n",
      "Batch: 11300,train loss is: 0.004155111244851854\n",
      "test loss is 0.005540703503075123\n",
      "Batch: 11400,train loss is: 0.005908371645419347\n",
      "test loss is 0.006071784832983455\n",
      "Batch: 11500,train loss is: 0.005271933292817649\n",
      "test loss is 0.005230017850423539\n",
      "Batch: 11600,train loss is: 0.00312385734709443\n",
      "test loss is 0.005090681081460203\n",
      "Batch: 11700,train loss is: 0.006028947941432013\n",
      "test loss is 0.006304659577369621\n",
      "Batch: 11800,train loss is: 0.008734826218832821\n",
      "test loss is 0.005660095378356447\n",
      "Batch: 11900,train loss is: 0.004977977382495705\n",
      "test loss is 0.006102137496147342\n",
      "Batch: 12000,train loss is: 0.00325165746055234\n",
      "test loss is 0.005220889120977567\n",
      "Batch: 12100,train loss is: 0.009940947430993998\n",
      "test loss is 0.005168858288281559\n",
      "Batch: 12200,train loss is: 0.00332558834291671\n",
      "test loss is 0.006266971895199737\n",
      "Batch: 12300,train loss is: 0.006107740545823234\n",
      "test loss is 0.0051156409970310604\n",
      "Batch: 12400,train loss is: 0.008689420104194663\n",
      "test loss is 0.006926825254441992\n",
      "Batch: 12500,train loss is: 0.006447795824126681\n",
      "test loss is 0.0051576223431117505\n",
      "Batch: 12600,train loss is: 0.007739464783506963\n",
      "test loss is 0.004862209244056924\n",
      "Batch: 12700,train loss is: 0.00311677731559868\n",
      "test loss is 0.0051082314019189005\n",
      "Batch: 12800,train loss is: 0.00517044158180998\n",
      "test loss is 0.004561875565965612\n",
      "Batch: 12900,train loss is: 0.006521439888635488\n",
      "test loss is 0.005324688724402495\n",
      "Batch: 13000,train loss is: 0.004837075291618601\n",
      "test loss is 0.00493533201838424\n",
      "Batch: 13100,train loss is: 0.003353708110009558\n",
      "test loss is 0.006227327900602051\n",
      "Batch: 13200,train loss is: 0.005307063022677916\n",
      "test loss is 0.00444211197745871\n",
      "Batch: 13300,train loss is: 0.0032941235155241225\n",
      "test loss is 0.004577135895837931\n",
      "Batch: 13400,train loss is: 0.005625772137444701\n",
      "test loss is 0.005377674277000561\n",
      "Batch: 13500,train loss is: 0.004115958278199734\n",
      "test loss is 0.0041919510731935426\n",
      "Batch: 13600,train loss is: 0.00353243113008359\n",
      "test loss is 0.005450428639570391\n",
      "Batch: 13700,train loss is: 0.004521932576633125\n",
      "test loss is 0.00441191769521795\n",
      "Batch: 13800,train loss is: 0.010885051730823707\n",
      "test loss is 0.005302972181119175\n",
      "Batch: 13900,train loss is: 0.008218783225133312\n",
      "test loss is 0.004978941491286301\n",
      "Batch: 14000,train loss is: 0.00848554483831809\n",
      "test loss is 0.004249447402232332\n",
      "Batch: 14100,train loss is: 0.005078100099972621\n",
      "test loss is 0.004835958483860416\n",
      "Batch: 14200,train loss is: 0.003374636040106924\n",
      "test loss is 0.004276987840911922\n",
      "Batch: 14300,train loss is: 0.004066189116703358\n",
      "test loss is 0.005038853768830748\n",
      "Batch: 14400,train loss is: 0.003964038222916175\n",
      "test loss is 0.004086295698909536\n",
      "Batch: 14500,train loss is: 0.004679571412473979\n",
      "test loss is 0.004061699049004375\n",
      "Batch: 14600,train loss is: 0.004554333771115594\n",
      "test loss is 0.004245025661894613\n",
      "Batch: 14700,train loss is: 0.0024681961881926236\n",
      "test loss is 0.0040999582978774015\n",
      "Batch: 14800,train loss is: 0.0039396202781172215\n",
      "test loss is 0.00523559367837986\n",
      "Batch: 14900,train loss is: 0.005475304045288373\n",
      "test loss is 0.004975033275482657\n",
      "Batch: 15000,train loss is: 0.004283923664573804\n",
      "test loss is 0.004194645006687808\n",
      "Batch: 15100,train loss is: 0.004290703234529694\n",
      "test loss is 0.004369753433764278\n",
      "Batch: 15200,train loss is: 0.0028228298253329125\n",
      "test loss is 0.003897045938799391\n",
      "Batch: 15300,train loss is: 0.004493220006018397\n",
      "test loss is 0.004173786496840559\n",
      "Batch: 15400,train loss is: 0.0038658582862169063\n",
      "test loss is 0.0071438013662218575\n",
      "Batch: 15500,train loss is: 0.007025973113495952\n",
      "test loss is 0.003907086410140923\n",
      "Batch: 15600,train loss is: 0.0034370341455330325\n",
      "test loss is 0.0035571404688767103\n",
      "Batch: 15700,train loss is: 0.009128369944556205\n",
      "test loss is 0.007424966960970103\n",
      "Batch: 15800,train loss is: 0.003259073417089399\n",
      "test loss is 0.003961437349784855\n",
      "Batch: 15900,train loss is: 0.003275925762449099\n",
      "test loss is 0.004975211424954681\n",
      "Batch: 16000,train loss is: 0.004155936770647044\n",
      "test loss is 0.003933442693330073\n",
      "Batch: 16100,train loss is: 0.004829279093219976\n",
      "test loss is 0.005865630302352202\n",
      "Batch: 16200,train loss is: 0.003417377546616888\n",
      "test loss is 0.0036446697369895463\n",
      "Batch: 16300,train loss is: 0.002810190707121712\n",
      "test loss is 0.004117815508741873\n",
      "Batch: 16400,train loss is: 0.0066088396781049195\n",
      "test loss is 0.003823530077982053\n",
      "Batch: 16500,train loss is: 0.003959634132294955\n",
      "test loss is 0.003753327573524626\n",
      "Batch: 16600,train loss is: 0.003978409891694613\n",
      "test loss is 0.003841742310413111\n",
      "Batch: 16700,train loss is: 0.005557069700036679\n",
      "test loss is 0.0034465777692827258\n",
      "Batch: 16800,train loss is: 0.008458116082666067\n",
      "test loss is 0.0034098813686190732\n",
      "Batch: 16900,train loss is: 0.003743305329438596\n",
      "test loss is 0.004489951918450353\n",
      "Batch: 17000,train loss is: 0.0029660039645428814\n",
      "test loss is 0.0036667975580199122\n",
      "Batch: 17100,train loss is: 0.002901361728758829\n",
      "test loss is 0.003342214725813245\n",
      "Batch: 17200,train loss is: 0.004539457027108766\n",
      "test loss is 0.0032323726265153486\n",
      "Batch: 17300,train loss is: 0.0025405759596076076\n",
      "test loss is 0.004129788315155395\n",
      "Batch: 17400,train loss is: 0.003216385752709811\n",
      "test loss is 0.003310827159125481\n",
      "Batch: 17500,train loss is: 0.008926011256384293\n",
      "test loss is 0.0034057633785851945\n",
      "Batch: 17600,train loss is: 0.0038933817144266615\n",
      "test loss is 0.0035037214664218478\n",
      "Batch: 17700,train loss is: 0.002366224733951747\n",
      "test loss is 0.0034313695011146204\n",
      "Batch: 17800,train loss is: 0.0016822252612932582\n",
      "test loss is 0.004140819417480865\n",
      "Batch: 17900,train loss is: 0.006989308427085556\n",
      "test loss is 0.003264724573199972\n",
      "Batch: 18000,train loss is: 0.0025645057004725813\n",
      "test loss is 0.0032775556805576646\n",
      "Batch: 18100,train loss is: 0.004451214800557331\n",
      "test loss is 0.0035895953786682813\n",
      "Batch: 18200,train loss is: 0.0036450786728058253\n",
      "test loss is 0.003178628563961696\n",
      "Batch: 18300,train loss is: 0.003572213133698499\n",
      "test loss is 0.006323154486544631\n",
      "Batch: 18400,train loss is: 0.002993384415548011\n",
      "test loss is 0.003935042596473194\n",
      "Batch: 18500,train loss is: 0.00273059405039354\n",
      "test loss is 0.0036577016419241624\n",
      "Batch: 18600,train loss is: 0.001940743081910722\n",
      "test loss is 0.003034218358115718\n",
      "Batch: 18700,train loss is: 0.0027613421402528492\n",
      "test loss is 0.003384858473589174\n",
      "Batch: 18800,train loss is: 0.003959375235312047\n",
      "test loss is 0.0029475395979552585\n",
      "Batch: 18900,train loss is: 0.003277880276530574\n",
      "test loss is 0.003741789989618231\n",
      "Batch: 19000,train loss is: 0.0033037378755861823\n",
      "test loss is 0.003634659282660109\n",
      "Batch: 19100,train loss is: 0.005558304377992016\n",
      "test loss is 0.003103596747745947\n",
      "Batch: 19200,train loss is: 0.0031576606320906026\n",
      "test loss is 0.003068118334614158\n",
      "Batch: 19300,train loss is: 0.002750803238276817\n",
      "test loss is 0.003351065787248258\n",
      "Batch: 19400,train loss is: 0.0020213130307939506\n",
      "test loss is 0.003130867880612005\n",
      "Batch: 19500,train loss is: 0.0033136837319856175\n",
      "test loss is 0.003212919810486983\n",
      "Batch: 19600,train loss is: 0.0020164572939715686\n",
      "test loss is 0.0029055102634398104\n",
      "Batch: 19700,train loss is: 0.002564642555850081\n",
      "test loss is 0.003404057396746992\n",
      "Batch: 19800,train loss is: 0.0026162407560133003\n",
      "test loss is 0.003720466489526448\n",
      "Batch: 19900,train loss is: 0.0020156515693733157\n",
      "test loss is 0.0030115844386876603\n",
      "Batch: 20000,train loss is: 0.0029018192748022447\n",
      "test loss is 0.0029509854923370393\n",
      "Batch: 20100,train loss is: 0.0029985340305795066\n",
      "test loss is 0.0036307737451803453\n",
      "Batch: 20200,train loss is: 0.002680859684733627\n",
      "test loss is 0.0028249671612803127\n",
      "Batch: 20300,train loss is: 0.0027961994192913947\n",
      "test loss is 0.0029572967768403654\n",
      "Batch: 20400,train loss is: 0.01134624623066325\n",
      "test loss is 0.003004807142942227\n",
      "Batch: 20500,train loss is: 0.002536123388079134\n",
      "test loss is 0.002846664168649894\n",
      "Batch: 20600,train loss is: 0.0031953291363732497\n",
      "test loss is 0.0034954655375678332\n",
      "Batch: 20700,train loss is: 0.004074689948185514\n",
      "test loss is 0.003912732378078532\n",
      "Batch: 20800,train loss is: 0.003959290046913338\n",
      "test loss is 0.0035775624674355194\n",
      "Batch: 20900,train loss is: 0.0018034820559616595\n",
      "test loss is 0.0028903106922098494\n",
      "Batch: 21000,train loss is: 0.002706238090890615\n",
      "test loss is 0.0026940013669954996\n",
      "Batch: 21100,train loss is: 0.003486584854409479\n",
      "test loss is 0.0031377047289982882\n",
      "Batch: 21200,train loss is: 0.002410917255869846\n",
      "test loss is 0.004079037705077241\n",
      "Batch: 21300,train loss is: 0.003235096927090594\n",
      "test loss is 0.0029243856628482115\n",
      "Batch: 21400,train loss is: 0.0045076822071177716\n",
      "test loss is 0.0029554999145717455\n",
      "Batch: 21500,train loss is: 0.002904119197441514\n",
      "test loss is 0.0028903245602083785\n",
      "Batch: 21600,train loss is: 0.002682398363543603\n",
      "test loss is 0.003451487903303354\n",
      "Batch: 21700,train loss is: 0.0016190317213051657\n",
      "test loss is 0.0026551231295360808\n",
      "Batch: 21800,train loss is: 0.0036237485244742795\n",
      "test loss is 0.0025304041183506127\n",
      "Batch: 21900,train loss is: 0.0023392201963291902\n",
      "test loss is 0.0029424004368085455\n",
      "Batch: 22000,train loss is: 0.003914220779084897\n",
      "test loss is 0.002852649789112887\n",
      "Batch: 22100,train loss is: 0.0031961072449945248\n",
      "test loss is 0.002702731429954467\n",
      "Batch: 22200,train loss is: 0.003067139563384248\n",
      "test loss is 0.00328435414030522\n",
      "Batch: 22300,train loss is: 0.0024163914490377136\n",
      "test loss is 0.0027899230903484277\n",
      "Batch: 22400,train loss is: 0.002257983584242133\n",
      "test loss is 0.0032094564111111532\n",
      "Batch: 22500,train loss is: 0.0017638592450303927\n",
      "test loss is 0.0025802635297313007\n",
      "Batch: 22600,train loss is: 0.0023312601938856254\n",
      "test loss is 0.0029110973696299986\n",
      "Batch: 22700,train loss is: 0.003280877595992911\n",
      "test loss is 0.0037510783760779792\n",
      "Batch: 22800,train loss is: 0.002748802382170643\n",
      "test loss is 0.0038137756719199035\n",
      "Batch: 22900,train loss is: 0.0020148196403386408\n",
      "test loss is 0.00264175235872281\n",
      "Batch: 23000,train loss is: 0.002982793736320497\n",
      "test loss is 0.002792884399038519\n",
      "Batch: 23100,train loss is: 0.003852068863625048\n",
      "test loss is 0.0034539309539069903\n",
      "Batch: 23200,train loss is: 0.007415223096073636\n",
      "test loss is 0.0028011966969391936\n",
      "Batch: 23300,train loss is: 0.0014148849346024748\n",
      "test loss is 0.002514189404465322\n",
      "Batch: 23400,train loss is: 0.0033697014751358227\n",
      "test loss is 0.0025689201144941075\n",
      "Batch: 23500,train loss is: 0.0027777104957901373\n",
      "test loss is 0.002759807532418725\n",
      "Batch: 23600,train loss is: 0.0033264779422445888\n",
      "test loss is 0.002997856811467374\n",
      "Batch: 23700,train loss is: 0.0021243624320917186\n",
      "test loss is 0.002426749992320548\n",
      "Batch: 23800,train loss is: 0.0018475811908604499\n",
      "test loss is 0.0038017938838928486\n",
      "Batch: 23900,train loss is: 0.0031039268990354554\n",
      "test loss is 0.002540396528893751\n",
      "Batch: 24000,train loss is: 0.0025660624559122065\n",
      "test loss is 0.003282965726472469\n",
      "Batch: 24100,train loss is: 0.004833581746809418\n",
      "test loss is 0.0030454325371604986\n",
      "Batch: 24200,train loss is: 0.003329171710112118\n",
      "test loss is 0.003469601544312478\n",
      "Batch: 24300,train loss is: 0.0020955345111132425\n",
      "test loss is 0.002646661447035586\n",
      "Batch: 24400,train loss is: 0.0038223948283210357\n",
      "test loss is 0.0034037158746017923\n",
      "Batch: 24500,train loss is: 0.002079336067947815\n",
      "test loss is 0.002786482438914927\n",
      "Batch: 24600,train loss is: 0.0017789399338587813\n",
      "test loss is 0.0038352264760403874\n",
      "Batch: 24700,train loss is: 0.002706256817057575\n",
      "test loss is 0.002331907228589649\n",
      "Batch: 24800,train loss is: 0.0012769064884241778\n",
      "test loss is 0.0025543757177076886\n",
      "Batch: 24900,train loss is: 0.0022874621154253096\n",
      "test loss is 0.0023066049292772587\n",
      "Batch: 25000,train loss is: 0.002432930151276812\n",
      "test loss is 0.002595827675227324\n",
      "Batch: 25100,train loss is: 0.002202926869885426\n",
      "test loss is 0.002691576132364881\n",
      "Batch: 25200,train loss is: 0.004029714347780931\n",
      "test loss is 0.005179903148058064\n",
      "Batch: 25300,train loss is: 0.0021545378770877694\n",
      "test loss is 0.002380497487654419\n",
      "Batch: 25400,train loss is: 0.00340659854994535\n",
      "test loss is 0.0029901607849219837\n",
      "Batch: 25500,train loss is: 0.0022398251148129803\n",
      "test loss is 0.0029045495091748674\n",
      "Batch: 25600,train loss is: 0.0019639262534779144\n",
      "test loss is 0.0037334795047686306\n",
      "Batch: 25700,train loss is: 0.003158040079690531\n",
      "test loss is 0.0026067560802717884\n",
      "Batch: 25800,train loss is: 0.0023606419245173356\n",
      "test loss is 0.0030950967111742647\n",
      "Batch: 25900,train loss is: 0.0020297616426539736\n",
      "test loss is 0.0031661410871322705\n",
      "Batch: 26000,train loss is: 0.001799759359182189\n",
      "test loss is 0.0024716808696648284\n",
      "Batch: 26100,train loss is: 0.003050495145933089\n",
      "test loss is 0.002865163323423028\n",
      "Batch: 26200,train loss is: 0.0028298894091766925\n",
      "test loss is 0.002395156910926169\n",
      "Batch: 26300,train loss is: 0.0038688945828500776\n",
      "test loss is 0.00277574374444051\n",
      "Batch: 26400,train loss is: 0.0028243650384347776\n",
      "test loss is 0.0023080572667102837\n",
      "Batch: 26500,train loss is: 0.003668230365280155\n",
      "test loss is 0.0026509867665420023\n",
      "Batch: 26600,train loss is: 0.002319343003296283\n",
      "test loss is 0.002496549916203608\n",
      "Batch: 26700,train loss is: 0.0035221037184985753\n",
      "test loss is 0.002877572889807259\n",
      "Batch: 26800,train loss is: 0.002414951364878539\n",
      "test loss is 0.0022521230380687317\n",
      "Batch: 26900,train loss is: 0.0019519860071990016\n",
      "test loss is 0.002431795706864428\n",
      "Batch: 27000,train loss is: 0.0013816676898857397\n",
      "test loss is 0.002108178007632837\n",
      "Batch: 27100,train loss is: 0.003407952757610664\n",
      "test loss is 0.0033452228629440093\n",
      "Batch: 27200,train loss is: 0.0035499245590060206\n",
      "test loss is 0.002657944823147392\n",
      "Batch: 27300,train loss is: 0.0025991657413724726\n",
      "test loss is 0.00370624754657614\n",
      "Batch: 27400,train loss is: 0.002164797410329135\n",
      "test loss is 0.005206429248016118\n",
      "Batch: 27500,train loss is: 0.0013668865125367937\n",
      "test loss is 0.002346476569792562\n",
      "Batch: 27600,train loss is: 0.0014195850904899683\n",
      "test loss is 0.0023039469612559605\n",
      "Batch: 27700,train loss is: 0.003795940133119105\n",
      "test loss is 0.0035428524674273308\n",
      "Batch: 27800,train loss is: 0.002674937771310935\n",
      "test loss is 0.002073565054323856\n",
      "Batch: 27900,train loss is: 0.004184498039242916\n",
      "test loss is 0.0022511797875909545\n",
      "Batch: 28000,train loss is: 0.0022371859908750912\n",
      "test loss is 0.002050802702431529\n",
      "Batch: 28100,train loss is: 0.0024884868598915987\n",
      "test loss is 0.0023357224310077843\n",
      "Batch: 28200,train loss is: 0.003443739466794343\n",
      "test loss is 0.0021384583242771743\n",
      "Batch: 28300,train loss is: 0.0017883416835194946\n",
      "test loss is 0.0022611672313190044\n",
      "Batch: 28400,train loss is: 0.0018233438743820312\n",
      "test loss is 0.0028462565022637897\n",
      "Batch: 28500,train loss is: 0.0025930893151355455\n",
      "test loss is 0.0019936793132233695\n",
      "Batch: 28600,train loss is: 0.0031526060885116516\n",
      "test loss is 0.0025144629088504405\n",
      "Batch: 28700,train loss is: 0.0030842483113469052\n",
      "test loss is 0.0020571311711817\n",
      "Batch: 28800,train loss is: 0.002824393749267999\n",
      "test loss is 0.0021947891287391985\n",
      "Batch: 28900,train loss is: 0.001713181614591643\n",
      "test loss is 0.0029794743963083756\n",
      "Batch: 29000,train loss is: 0.002209993329853322\n",
      "test loss is 0.0023826431612344492\n",
      "Batch: 29100,train loss is: 0.002832664866385239\n",
      "test loss is 0.0021707849178338435\n",
      "Batch: 29200,train loss is: 0.0016122803051633612\n",
      "test loss is 0.0019565598864630337\n",
      "Batch: 29300,train loss is: 0.0017607100921757607\n",
      "test loss is 0.0031370962871050826\n",
      "Batch: 29400,train loss is: 0.0027086546958626566\n",
      "test loss is 0.0019982791635881196\n",
      "Batch: 29500,train loss is: 0.0034280642159111685\n",
      "test loss is 0.0022579759962996515\n",
      "Batch: 29600,train loss is: 0.0015970982179334442\n",
      "test loss is 0.0030677147524387715\n",
      "Batch: 29700,train loss is: 0.0015890342703037891\n",
      "test loss is 0.0020496286631788963\n",
      "Batch: 29800,train loss is: 0.0038787904610558246\n",
      "test loss is 0.002427639643581463\n",
      "Batch: 29900,train loss is: 0.0016400847910579548\n",
      "test loss is 0.0018794941277308426\n",
      "Batch: 30000,train loss is: 0.0025601410409912266\n",
      "test loss is 0.002081240194814618\n",
      "Batch: 30100,train loss is: 0.0018764076662900419\n",
      "test loss is 0.0025447703060154426\n",
      "Batch: 30200,train loss is: 0.0013423321713382133\n",
      "test loss is 0.001934139275186986\n",
      "Batch: 30300,train loss is: 0.002259446483131405\n",
      "test loss is 0.0019590340873439993\n",
      "Batch: 30400,train loss is: 0.002604439210602583\n",
      "test loss is 0.002148597537364746\n",
      "Batch: 30500,train loss is: 0.002915637579660975\n",
      "test loss is 0.0020365802444689243\n",
      "Batch: 30600,train loss is: 0.0026512675707444453\n",
      "test loss is 0.0020702722266472697\n",
      "Batch: 30700,train loss is: 0.002972905763635929\n",
      "test loss is 0.0025838879210281695\n",
      "Batch: 30800,train loss is: 0.0026956583353436657\n",
      "test loss is 0.002615047533045448\n",
      "Batch: 30900,train loss is: 0.003000219764440878\n",
      "test loss is 0.0028949039278110046\n",
      "Batch: 31000,train loss is: 0.0015494014406877174\n",
      "test loss is 0.0024069704381458\n",
      "Batch: 31100,train loss is: 0.003466748227065025\n",
      "test loss is 0.0020474211339185752\n",
      "Batch: 31200,train loss is: 0.0011138474525393325\n",
      "test loss is 0.002062868466581295\n",
      "Batch: 31300,train loss is: 0.0024302315099162513\n",
      "test loss is 0.002719283156238147\n",
      "Batch: 31400,train loss is: 0.001891118154836547\n",
      "test loss is 0.0019860923381909298\n",
      "Batch: 31500,train loss is: 0.0023117414207277096\n",
      "test loss is 0.001851509398848302\n",
      "Batch: 31600,train loss is: 0.0022245924764131334\n",
      "test loss is 0.0022826801912006296\n",
      "Batch: 31700,train loss is: 0.001955095195628727\n",
      "test loss is 0.0018483692947312653\n",
      "Batch: 31800,train loss is: 0.0019898171553993428\n",
      "test loss is 0.0028162435062817712\n",
      "Batch: 31900,train loss is: 0.001964182953624509\n",
      "test loss is 0.0027830842523034905\n",
      "Batch: 32000,train loss is: 0.0028587881633001196\n",
      "test loss is 0.001950692225501172\n",
      "Batch: 32100,train loss is: 0.0023814123594807475\n",
      "test loss is 0.0018514134092507898\n",
      "Batch: 32200,train loss is: 0.0015557921386224716\n",
      "test loss is 0.0018617240570206002\n",
      "Batch: 32300,train loss is: 0.002349022353854733\n",
      "test loss is 0.0031677384525219295\n",
      "Batch: 32400,train loss is: 0.0027555349172020474\n",
      "test loss is 0.0022194086206556397\n",
      "Batch: 32500,train loss is: 0.004226265117900832\n",
      "test loss is 0.0022008314230534524\n",
      "Batch: 32600,train loss is: 0.0024523042165961375\n",
      "test loss is 0.0020930142697007667\n",
      "Batch: 32700,train loss is: 0.0021981580277316056\n",
      "test loss is 0.0018718616098515157\n",
      "Batch: 32800,train loss is: 0.0015858076481143363\n",
      "test loss is 0.002017316276383929\n",
      "Batch: 32900,train loss is: 0.0013968329917443104\n",
      "test loss is 0.0018907053932760267\n",
      "Batch: 33000,train loss is: 0.003486517155197124\n",
      "test loss is 0.0018648632095923823\n",
      "Batch: 33100,train loss is: 0.0029624891587715337\n",
      "test loss is 0.0019013582303783606\n",
      "Batch: 33200,train loss is: 0.0017761536214865642\n",
      "test loss is 0.001927937393368658\n",
      "Batch: 33300,train loss is: 0.0035339955999012344\n",
      "test loss is 0.003023081159020242\n",
      "Batch: 33400,train loss is: 0.0023436055962936203\n",
      "test loss is 0.0025452851018583165\n",
      "Batch: 33500,train loss is: 0.0012758641589657072\n",
      "test loss is 0.0018133129218630827\n",
      "Batch: 33600,train loss is: 0.002078964710008918\n",
      "test loss is 0.002503361797486625\n",
      "Batch: 33700,train loss is: 0.001986285544759266\n",
      "test loss is 0.0022345231701977847\n",
      "Batch: 33800,train loss is: 0.0023883916081190743\n",
      "test loss is 0.0024866079894634934\n",
      "Batch: 33900,train loss is: 0.0029541724237274573\n",
      "test loss is 0.0019405355406748738\n",
      "Batch: 34000,train loss is: 0.0022859146120279596\n",
      "test loss is 0.0025019868801522487\n",
      "Batch: 34100,train loss is: 0.003796146032153372\n",
      "test loss is 0.0032563048169994423\n",
      "Batch: 34200,train loss is: 0.0025262015380672536\n",
      "test loss is 0.0017596088682100368\n",
      "Batch: 34300,train loss is: 0.001970725263355924\n",
      "test loss is 0.0021841324606390535\n",
      "Batch: 34400,train loss is: 0.001459186589400585\n",
      "test loss is 0.0018635538763644834\n",
      "Batch: 34500,train loss is: 0.001371934380879563\n",
      "test loss is 0.001780561673702526\n",
      "Batch: 34600,train loss is: 0.0022372371731759124\n",
      "test loss is 0.001868834302566889\n",
      "Batch: 34700,train loss is: 0.0017931850674141724\n",
      "test loss is 0.0023521839082576356\n",
      "Batch: 34800,train loss is: 0.0016090096226609486\n",
      "test loss is 0.003160698001451389\n",
      "Batch: 34900,train loss is: 0.001985055571852699\n",
      "test loss is 0.0019538391156474215\n",
      "Batch: 35000,train loss is: 0.002050041434201555\n",
      "test loss is 0.001757395400066883\n",
      "Batch: 35100,train loss is: 0.0016172291784413203\n",
      "test loss is 0.0017286707557460847\n",
      "Batch: 35200,train loss is: 0.0026663295963680143\n",
      "test loss is 0.002000446305120295\n",
      "Batch: 35300,train loss is: 0.0019456614268512943\n",
      "test loss is 0.002106257067731207\n",
      "Batch: 35400,train loss is: 0.0017428429411624664\n",
      "test loss is 0.0017893420796015395\n",
      "Batch: 35500,train loss is: 0.0016990632144743497\n",
      "test loss is 0.0016383523924616808\n",
      "Batch: 35600,train loss is: 0.001626113320457318\n",
      "test loss is 0.0016839247157095245\n",
      "Batch: 35700,train loss is: 0.0015717345760686017\n",
      "test loss is 0.0029279604713381923\n",
      "Batch: 35800,train loss is: 0.0016276935316924374\n",
      "test loss is 0.0016622181006735362\n",
      "Batch: 35900,train loss is: 0.001757207169423307\n",
      "test loss is 0.00172025521676236\n",
      "Batch: 36000,train loss is: 0.0020742134345176425\n",
      "test loss is 0.0017029425407527209\n",
      "Batch: 36100,train loss is: 0.0029853594070428343\n",
      "test loss is 0.002030303197025563\n",
      "Batch: 36200,train loss is: 0.001879627454474786\n",
      "test loss is 0.001672994223616894\n",
      "Batch: 36300,train loss is: 0.0016917252081481706\n",
      "test loss is 0.002305140670609255\n",
      "Batch: 36400,train loss is: 0.0013752411043088408\n",
      "test loss is 0.0016656269975348153\n",
      "Batch: 36500,train loss is: 0.0017929924266364887\n",
      "test loss is 0.002347867988974536\n",
      "Batch: 36600,train loss is: 0.0011814047655230977\n",
      "test loss is 0.0017114123941066569\n",
      "Batch: 36700,train loss is: 0.0016203202372042896\n",
      "test loss is 0.0020775896507634044\n",
      "Batch: 36800,train loss is: 0.0014503329948632347\n",
      "test loss is 0.0017017766633182468\n",
      "Batch: 36900,train loss is: 0.001496944561112343\n",
      "test loss is 0.001899343847422935\n",
      "Batch: 37000,train loss is: 0.0021587940001844214\n",
      "test loss is 0.0016496567861632632\n",
      "Batch: 37100,train loss is: 0.0017064270498117816\n",
      "test loss is 0.001690321612206936\n",
      "Batch: 37200,train loss is: 0.001681284886855882\n",
      "test loss is 0.0019042063093883876\n",
      "Batch: 37300,train loss is: 0.0018104118535278205\n",
      "test loss is 0.00164519338302165\n",
      "Batch: 37400,train loss is: 0.0025852753194311583\n",
      "test loss is 0.002566183158531356\n",
      "Batch: 37500,train loss is: 0.0016866110566037423\n",
      "test loss is 0.0023134679032673915\n",
      "Batch: 37600,train loss is: 0.002091830532967437\n",
      "test loss is 0.0019259699701023906\n",
      "Batch: 37700,train loss is: 0.002448024672940766\n",
      "test loss is 0.0018523451441028289\n",
      "Batch: 37800,train loss is: 0.0015418664156514185\n",
      "test loss is 0.0016237534102827775\n",
      "Batch: 37900,train loss is: 0.0027765031820526694\n",
      "test loss is 0.0016415888966977584\n",
      "Batch: 38000,train loss is: 0.002361561226570242\n",
      "test loss is 0.0016866377767053322\n",
      "Batch: 38100,train loss is: 0.0015280598643900866\n",
      "test loss is 0.0017085648776416725\n",
      "Batch: 38200,train loss is: 0.0016210603537002539\n",
      "test loss is 0.002059670480779623\n",
      "Batch: 38300,train loss is: 0.0007891688561565878\n",
      "test loss is 0.0016297768291830789\n",
      "Batch: 38400,train loss is: 0.001198059311871069\n",
      "test loss is 0.002936501213047053\n",
      "Batch: 38500,train loss is: 0.001246356372538192\n",
      "test loss is 0.0017137089900297398\n",
      "Batch: 38600,train loss is: 0.001721910296644352\n",
      "test loss is 0.0021652726206576427\n",
      "Batch: 38700,train loss is: 0.0013523454980827014\n",
      "test loss is 0.0016146255644657404\n",
      "Batch: 38800,train loss is: 0.0013301089227380452\n",
      "test loss is 0.0026098819204767054\n",
      "Batch: 38900,train loss is: 0.0014093036195729825\n",
      "test loss is 0.002075441918986253\n",
      "Batch: 39000,train loss is: 0.0020012499821561135\n",
      "test loss is 0.0016252963160705812\n",
      "Batch: 39100,train loss is: 0.003679617493661996\n",
      "test loss is 0.001816141893897607\n",
      "Batch: 39200,train loss is: 0.00144439405854005\n",
      "test loss is 0.0016001872136739014\n",
      "Batch: 39300,train loss is: 0.001784092266883322\n",
      "test loss is 0.0016361963595267457\n",
      "Batch: 39400,train loss is: 0.0018279561817786498\n",
      "test loss is 0.0022292314485586316\n",
      "Batch: 39500,train loss is: 0.0018258002550553681\n",
      "test loss is 0.003199453162092213\n",
      "Batch: 39600,train loss is: 0.002574104518553012\n",
      "test loss is 0.002165573502224649\n",
      "Batch: 39700,train loss is: 0.0016383422863290665\n",
      "test loss is 0.0016407213497826045\n",
      "Batch: 39800,train loss is: 0.002247909642817904\n",
      "test loss is 0.001520528896113155\n",
      "Batch: 39900,train loss is: 0.0013401188598908967\n",
      "test loss is 0.001457662073120937\n",
      "Batch: 40000,train loss is: 0.0020176367983965396\n",
      "test loss is 0.0017074725273623483\n",
      "Batch: 40100,train loss is: 0.001670026343308257\n",
      "test loss is 0.0021649504718957574\n",
      "Batch: 40200,train loss is: 0.0011431858974179808\n",
      "test loss is 0.0016998350306950485\n",
      "Batch: 40300,train loss is: 0.0022554643506931295\n",
      "test loss is 0.001642277724526731\n",
      "Batch: 40400,train loss is: 0.0014971623092299724\n",
      "test loss is 0.0018439341333931567\n",
      "Batch: 40500,train loss is: 0.0013444792772156055\n",
      "test loss is 0.0017183578954726112\n",
      "Batch: 40600,train loss is: 0.001294622481842169\n",
      "test loss is 0.0016230008733114389\n",
      "Batch: 40700,train loss is: 0.0012885354889412882\n",
      "test loss is 0.0015052672854663025\n",
      "Batch: 40800,train loss is: 0.004455631623863766\n",
      "test loss is 0.003346434928314079\n",
      "Batch: 40900,train loss is: 0.001313846498675512\n",
      "test loss is 0.001429441136218589\n",
      "Batch: 41000,train loss is: 0.001504105381737961\n",
      "test loss is 0.0015346901333632634\n",
      "Batch: 41100,train loss is: 0.001265769694432475\n",
      "test loss is 0.001535415955758988\n",
      "Batch: 41200,train loss is: 0.0014052107296871495\n",
      "test loss is 0.0015699895319088773\n",
      "Batch: 41300,train loss is: 0.0017854667841064615\n",
      "test loss is 0.0014640644155070172\n",
      "Batch: 41400,train loss is: 0.0014410175268777551\n",
      "test loss is 0.0024250484669098276\n",
      "Batch: 41500,train loss is: 0.00233742394084702\n",
      "test loss is 0.001740994989424024\n",
      "Batch: 41600,train loss is: 0.0013901300428969113\n",
      "test loss is 0.0016017053009918316\n",
      "Batch: 41700,train loss is: 0.0020275304476661377\n",
      "test loss is 0.001910528212939589\n",
      "Batch: 41800,train loss is: 0.0025047893904903535\n",
      "test loss is 0.0023621236613218196\n",
      "Batch: 41900,train loss is: 0.0020438191713073517\n",
      "test loss is 0.0017506382508970842\n",
      "Batch: 42000,train loss is: 0.001707209884693028\n",
      "test loss is 0.0018479853847434917\n",
      "Batch: 42100,train loss is: 0.001357524304661878\n",
      "test loss is 0.001534077914723356\n",
      "Batch: 42200,train loss is: 0.0012016472676902638\n",
      "test loss is 0.0015670631374870797\n",
      "Batch: 42300,train loss is: 0.002019628590429874\n",
      "test loss is 0.0014540014757783368\n",
      "Batch: 42400,train loss is: 0.002042254205473581\n",
      "test loss is 0.0018757611396432654\n",
      "Batch: 42500,train loss is: 0.0019085038219973215\n",
      "test loss is 0.0017517455312205648\n",
      "Batch: 42600,train loss is: 0.0013981365028065422\n",
      "test loss is 0.0015089976657331737\n",
      "Batch: 42700,train loss is: 0.0010259329489080914\n",
      "test loss is 0.0022036772762822877\n",
      "Batch: 42800,train loss is: 0.001481705655818846\n",
      "test loss is 0.0018369668312903547\n",
      "Batch: 42900,train loss is: 0.0019216987762999617\n",
      "test loss is 0.0017053242568069492\n",
      "Batch: 43000,train loss is: 0.002963343129921677\n",
      "test loss is 0.0017220650826269704\n",
      "Batch: 43100,train loss is: 0.0016290993498545442\n",
      "test loss is 0.0021536422102456454\n",
      "Batch: 43200,train loss is: 0.0013598376019444201\n",
      "test loss is 0.0014871788722406714\n",
      "Batch: 43300,train loss is: 0.001745319305491976\n",
      "test loss is 0.0014725312427898204\n",
      "Batch: 43400,train loss is: 0.0013541332323008083\n",
      "test loss is 0.001529139876108733\n",
      "Batch: 43500,train loss is: 0.0029098469456657586\n",
      "test loss is 0.0017089967321619022\n",
      "Batch: 43600,train loss is: 0.001262220103764069\n",
      "test loss is 0.0019254574991639608\n",
      "Batch: 43700,train loss is: 0.001929055602476504\n",
      "test loss is 0.0014622230244378614\n",
      "Batch: 43800,train loss is: 0.00217936290169436\n",
      "test loss is 0.001612364378705131\n",
      "Batch: 43900,train loss is: 0.002617011744590107\n",
      "test loss is 0.0030253370915006897\n",
      "Batch: 44000,train loss is: 0.00125707320652841\n",
      "test loss is 0.0016845068786812893\n",
      "Batch: 44100,train loss is: 0.001623234413894406\n",
      "test loss is 0.0016285516581040899\n",
      "Batch: 44200,train loss is: 0.0013545286911417938\n",
      "test loss is 0.0015793984400105925\n",
      "Batch: 44300,train loss is: 0.002819957904856899\n",
      "test loss is 0.0030672180752061248\n",
      "Batch: 44400,train loss is: 0.0019119558643075183\n",
      "test loss is 0.001517843832478898\n",
      "Batch: 44500,train loss is: 0.0011697861942407666\n",
      "test loss is 0.0014538714533097695\n",
      "Batch: 44600,train loss is: 0.0024294939690960296\n",
      "test loss is 0.001907550744542313\n",
      "Batch: 44700,train loss is: 0.0013872118975084013\n",
      "test loss is 0.0015309602286045089\n",
      "Batch: 44800,train loss is: 0.0013091418752186903\n",
      "test loss is 0.0019002664309531567\n",
      "Batch: 44900,train loss is: 0.0009404626777426398\n",
      "test loss is 0.0014569247942763247\n",
      "Batch: 45000,train loss is: 0.0021066891252136963\n",
      "test loss is 0.0016715934627951515\n",
      "Batch: 45100,train loss is: 0.0027298377429686447\n",
      "test loss is 0.002109219652076237\n",
      "Batch: 45200,train loss is: 0.0020152290481033515\n",
      "test loss is 0.001836740084470598\n",
      "Batch: 45300,train loss is: 0.0009213357819121235\n",
      "test loss is 0.0020003999780998792\n",
      "Batch: 45400,train loss is: 0.0017825828820121378\n",
      "test loss is 0.0016477654775102018\n",
      "Batch: 45500,train loss is: 0.001120874883233271\n",
      "test loss is 0.0018020032229338601\n",
      "Batch: 45600,train loss is: 0.00125772206972507\n",
      "test loss is 0.0014873607894737109\n",
      "Batch: 45700,train loss is: 0.0011299486242708021\n",
      "test loss is 0.0015422972849041437\n",
      "Batch: 45800,train loss is: 0.0016937018617873716\n",
      "test loss is 0.0021279992032795076\n",
      "Batch: 45900,train loss is: 0.0016201063822183464\n",
      "test loss is 0.001567111635812697\n",
      "Batch: 46000,train loss is: 0.0008998858627632953\n",
      "test loss is 0.001376040441064638\n",
      "Batch: 46100,train loss is: 0.0017961807110610621\n",
      "test loss is 0.0013224949876737698\n",
      "Batch: 46200,train loss is: 0.0010239004771315395\n",
      "test loss is 0.0018158970760659898\n",
      "Batch: 46300,train loss is: 0.0013606672688657529\n",
      "test loss is 0.0014470233393531157\n",
      "Batch: 46400,train loss is: 0.0011179980401617566\n",
      "test loss is 0.002136016949567492\n",
      "Batch: 46500,train loss is: 0.0010814097915983806\n",
      "test loss is 0.0017042733978721435\n",
      "Batch: 46600,train loss is: 0.0013361777818586204\n",
      "test loss is 0.0013931107156180764\n",
      "Batch: 46700,train loss is: 0.0011240395246216058\n",
      "test loss is 0.0013373581159943962\n",
      "-----------------------Epoch: 1----------------------------------\n",
      "Batch: 0,train loss is: 0.0010648666689473804\n",
      "test loss is 0.0014675506712237888\n",
      "Batch: 100,train loss is: 0.0022313416779989933\n",
      "test loss is 0.0022206824187481653\n",
      "Batch: 200,train loss is: 0.0031117578519919797\n",
      "test loss is 0.001977667147149443\n",
      "Batch: 300,train loss is: 0.0029761627697504037\n",
      "test loss is 0.0019097800848344574\n",
      "Batch: 400,train loss is: 0.0013597235025809159\n",
      "test loss is 0.0020301944606055077\n",
      "Batch: 500,train loss is: 0.001025974098783431\n",
      "test loss is 0.0019096383516891517\n",
      "Batch: 600,train loss is: 0.0008619499703885814\n",
      "test loss is 0.0020076557482287096\n",
      "Batch: 700,train loss is: 0.0018153744333503719\n",
      "test loss is 0.00173071820505662\n",
      "Batch: 800,train loss is: 0.0013835177109417951\n",
      "test loss is 0.001458278013956116\n",
      "Batch: 900,train loss is: 0.0013678610011773314\n",
      "test loss is 0.001629655500825643\n",
      "Batch: 1000,train loss is: 0.002023142644456807\n",
      "test loss is 0.001373436952412744\n",
      "Batch: 1100,train loss is: 0.0024623614291890096\n",
      "test loss is 0.0017876069655457286\n",
      "Batch: 1200,train loss is: 0.0014290053859379215\n",
      "test loss is 0.0013440686429514262\n",
      "Batch: 1300,train loss is: 0.0012046976629887872\n",
      "test loss is 0.001381928111242105\n",
      "Batch: 1400,train loss is: 0.003615960062654919\n",
      "test loss is 0.0015519149644225559\n",
      "Batch: 1500,train loss is: 0.0015326805938782747\n",
      "test loss is 0.0015737002018487696\n",
      "Batch: 1600,train loss is: 0.0016606192217144167\n",
      "test loss is 0.0013383564929987071\n",
      "Batch: 1700,train loss is: 0.0012483904434876687\n",
      "test loss is 0.0017364662956875904\n",
      "Batch: 1800,train loss is: 0.00085549501903171\n",
      "test loss is 0.0017959733001738623\n",
      "Batch: 1900,train loss is: 0.0028584216336963357\n",
      "test loss is 0.0017879586518699086\n",
      "Batch: 2000,train loss is: 0.0026614701589387758\n",
      "test loss is 0.0012863737160215803\n",
      "Batch: 2100,train loss is: 0.0009164349199231268\n",
      "test loss is 0.001647048984992984\n",
      "Batch: 2200,train loss is: 0.0014759819733521262\n",
      "test loss is 0.001469853112417654\n",
      "Batch: 2300,train loss is: 0.001812111652202111\n",
      "test loss is 0.0014188330245521936\n",
      "Batch: 2400,train loss is: 0.0013203148549811292\n",
      "test loss is 0.0015157185258815836\n",
      "Batch: 2500,train loss is: 0.001891402795826779\n",
      "test loss is 0.0013562282087132382\n",
      "Batch: 2600,train loss is: 0.0012204231167500185\n",
      "test loss is 0.001238827859335746\n",
      "Batch: 2700,train loss is: 0.001982735889668663\n",
      "test loss is 0.002158077120718258\n",
      "Batch: 2800,train loss is: 0.0015117652107113641\n",
      "test loss is 0.0021400747143156783\n",
      "Batch: 2900,train loss is: 0.0010279052299455396\n",
      "test loss is 0.0018771809071202275\n",
      "Batch: 3000,train loss is: 0.005151008709710313\n",
      "test loss is 0.0015283785972783912\n",
      "Batch: 3100,train loss is: 0.001642984597365291\n",
      "test loss is 0.001342799445576817\n",
      "Batch: 3200,train loss is: 0.0014530780799725412\n",
      "test loss is 0.0015552330331812996\n",
      "Batch: 3300,train loss is: 0.0016357184693813786\n",
      "test loss is 0.0012697914126366138\n",
      "Batch: 3400,train loss is: 0.0015429221142714482\n",
      "test loss is 0.001397474725685059\n",
      "Batch: 3500,train loss is: 0.0028410644427477915\n",
      "test loss is 0.0015398477938183591\n",
      "Batch: 3600,train loss is: 0.0014123787547916618\n",
      "test loss is 0.0017190689419051249\n",
      "Batch: 3700,train loss is: 0.0025919152891869983\n",
      "test loss is 0.0015222368823913507\n",
      "Batch: 3800,train loss is: 0.0009507393447003117\n",
      "test loss is 0.0013153943734541848\n",
      "Batch: 3900,train loss is: 0.0007383590828943182\n",
      "test loss is 0.0014557382401797587\n",
      "Batch: 4000,train loss is: 0.001132888164178606\n",
      "test loss is 0.0021090468063814237\n",
      "Batch: 4100,train loss is: 0.0011427238613440798\n",
      "test loss is 0.0012783399035215194\n",
      "Batch: 4200,train loss is: 0.0012021101561731063\n",
      "test loss is 0.0013368714261067253\n",
      "Batch: 4300,train loss is: 0.0007179068563059549\n",
      "test loss is 0.0013118282795120892\n",
      "Batch: 4400,train loss is: 0.0011895699800427515\n",
      "test loss is 0.0021890348520106685\n",
      "Batch: 4500,train loss is: 0.001698137841981926\n",
      "test loss is 0.0017703542738817812\n",
      "Batch: 4600,train loss is: 0.0015201947535909547\n",
      "test loss is 0.0022143688169838187\n",
      "Batch: 4700,train loss is: 0.0017711469130472404\n",
      "test loss is 0.0018275461559827749\n",
      "Batch: 4800,train loss is: 0.0015131673969028875\n",
      "test loss is 0.0015626178565822531\n",
      "Batch: 4900,train loss is: 0.001824344934801556\n",
      "test loss is 0.0012077247472377414\n",
      "Batch: 5000,train loss is: 0.0016674210714361659\n",
      "test loss is 0.001263908174386008\n",
      "Batch: 5100,train loss is: 0.0019237690549675094\n",
      "test loss is 0.0012385336777859931\n",
      "Batch: 5200,train loss is: 0.0009797588947731961\n",
      "test loss is 0.001682901117811064\n",
      "Batch: 5300,train loss is: 0.0013307861781330543\n",
      "test loss is 0.001423776930614775\n",
      "Batch: 5400,train loss is: 0.0007642069953841348\n",
      "test loss is 0.0012838092627612176\n",
      "Batch: 5500,train loss is: 0.0009775835246649545\n",
      "test loss is 0.0011854601924836956\n",
      "Batch: 5600,train loss is: 0.0011835558784355263\n",
      "test loss is 0.0012072090816318795\n",
      "Batch: 5700,train loss is: 0.0019989054998667266\n",
      "test loss is 0.001457028009775637\n",
      "Batch: 5800,train loss is: 0.0010120312962070338\n",
      "test loss is 0.0018023099255899606\n",
      "Batch: 5900,train loss is: 0.0019938125490089166\n",
      "test loss is 0.0013979762513180983\n",
      "Batch: 6000,train loss is: 0.0013759158820116218\n",
      "test loss is 0.0014729582941183767\n",
      "Batch: 6100,train loss is: 0.0010718295253399988\n",
      "test loss is 0.0012292225314075578\n",
      "Batch: 6200,train loss is: 0.001022357381227774\n",
      "test loss is 0.0014520342444295181\n",
      "Batch: 6300,train loss is: 0.001182063419996163\n",
      "test loss is 0.001293594326545688\n",
      "Batch: 6400,train loss is: 0.0012677355054556004\n",
      "test loss is 0.001553741262918513\n",
      "Batch: 6500,train loss is: 0.0009430540260914362\n",
      "test loss is 0.0012324107132291667\n",
      "Batch: 6600,train loss is: 0.0013366447401835096\n",
      "test loss is 0.001397464624912433\n",
      "Batch: 6700,train loss is: 0.002126224316708443\n",
      "test loss is 0.0016363868646604414\n",
      "Batch: 6800,train loss is: 0.001390631064443381\n",
      "test loss is 0.0014807611830940394\n",
      "Batch: 6900,train loss is: 0.001401796000881177\n",
      "test loss is 0.0014150007400975798\n",
      "Batch: 7000,train loss is: 0.0012675940722298683\n",
      "test loss is 0.0012292665748517373\n",
      "Batch: 7100,train loss is: 0.0015631468169744235\n",
      "test loss is 0.0013980342765291295\n",
      "Batch: 7200,train loss is: 0.0013622036659452592\n",
      "test loss is 0.0012836169307361464\n",
      "Batch: 7300,train loss is: 0.0008317750666791175\n",
      "test loss is 0.0014327689099632196\n",
      "Batch: 7400,train loss is: 0.0010517595826666085\n",
      "test loss is 0.001138630428231998\n",
      "Batch: 7500,train loss is: 0.0014723438076974195\n",
      "test loss is 0.0012270175696240321\n",
      "Batch: 7600,train loss is: 0.000987776445258573\n",
      "test loss is 0.0011481020752805518\n",
      "Batch: 7700,train loss is: 0.0032901053257152063\n",
      "test loss is 0.0011582740936507006\n",
      "Batch: 7800,train loss is: 0.001114863995596008\n",
      "test loss is 0.0012066846325001254\n",
      "Batch: 7900,train loss is: 0.0012214347381812754\n",
      "test loss is 0.0012574852932684304\n",
      "Batch: 8000,train loss is: 0.0009337909307962874\n",
      "test loss is 0.001242329395805566\n",
      "Batch: 8100,train loss is: 0.002758201508520256\n",
      "test loss is 0.0011523839261675124\n",
      "Batch: 8200,train loss is: 0.0013060244071810823\n",
      "test loss is 0.0016534202431476423\n",
      "Batch: 8300,train loss is: 0.0007694271804087426\n",
      "test loss is 0.0013481435481466782\n",
      "Batch: 8400,train loss is: 0.0010389415297329867\n",
      "test loss is 0.001242089057864865\n",
      "Batch: 8500,train loss is: 0.00324842452574134\n",
      "test loss is 0.0013501596077841902\n",
      "Batch: 8600,train loss is: 0.0010937601131562691\n",
      "test loss is 0.001284114294341187\n",
      "Batch: 8700,train loss is: 0.0017563115110403244\n",
      "test loss is 0.00248917836672922\n",
      "Batch: 8800,train loss is: 0.001185262716963218\n",
      "test loss is 0.0016731449471136604\n",
      "Batch: 8900,train loss is: 0.0016936333012079888\n",
      "test loss is 0.001705317660479563\n",
      "Batch: 9000,train loss is: 0.0012658798926917596\n",
      "test loss is 0.0013042918998083409\n",
      "Batch: 9100,train loss is: 0.001200970750617612\n",
      "test loss is 0.0012708226519317853\n",
      "Batch: 9200,train loss is: 0.0012554066603544444\n",
      "test loss is 0.0012590763853326948\n",
      "Batch: 9300,train loss is: 0.0018000874221783037\n",
      "test loss is 0.0013099509373533868\n",
      "Batch: 9400,train loss is: 0.0013878531145500366\n",
      "test loss is 0.0014116734621302915\n",
      "Batch: 9500,train loss is: 0.0009549956989789269\n",
      "test loss is 0.0011963652702407322\n",
      "Batch: 9600,train loss is: 0.0007040256949864264\n",
      "test loss is 0.0011468707309901316\n",
      "Batch: 9700,train loss is: 0.0010260132348997772\n",
      "test loss is 0.001332792285426558\n",
      "Batch: 9800,train loss is: 0.0019448308583176444\n",
      "test loss is 0.001260914372369069\n",
      "Batch: 9900,train loss is: 0.0013958341782834556\n",
      "test loss is 0.0012086435217588275\n",
      "Batch: 10000,train loss is: 0.0017629768964787013\n",
      "test loss is 0.0012226437119534\n",
      "Batch: 10100,train loss is: 0.0010233819748045351\n",
      "test loss is 0.001187078506474227\n",
      "Batch: 10200,train loss is: 0.0009095603150820757\n",
      "test loss is 0.0016302256252512842\n",
      "Batch: 10300,train loss is: 0.0011348377314581415\n",
      "test loss is 0.001228939629703951\n",
      "Batch: 10400,train loss is: 0.006459676420534189\n",
      "test loss is 0.0014230201789662664\n",
      "Batch: 10500,train loss is: 0.00139999202599754\n",
      "test loss is 0.0012316502482591508\n",
      "Batch: 10600,train loss is: 0.000976832203133337\n",
      "test loss is 0.00115626268380214\n",
      "Batch: 10700,train loss is: 0.0014413181843784709\n",
      "test loss is 0.0012177515118674668\n",
      "Batch: 10800,train loss is: 0.0018234877420893175\n",
      "test loss is 0.0011314884246129541\n",
      "Batch: 10900,train loss is: 0.0009596083682019434\n",
      "test loss is 0.0015190207743307315\n",
      "Batch: 11000,train loss is: 0.0012454507810512729\n",
      "test loss is 0.001537563708353201\n",
      "Batch: 11100,train loss is: 0.0010215552023815693\n",
      "test loss is 0.0017216911555446941\n",
      "Batch: 11200,train loss is: 0.001702166799403116\n",
      "test loss is 0.001430413118037623\n",
      "Batch: 11300,train loss is: 0.0013896465362324435\n",
      "test loss is 0.0014222402087331564\n",
      "Batch: 11400,train loss is: 0.0009684236654665403\n",
      "test loss is 0.0010803356002271666\n",
      "Batch: 11500,train loss is: 0.0019599946352850035\n",
      "test loss is 0.0011553220969194255\n",
      "Batch: 11600,train loss is: 0.0014817999754129564\n",
      "test loss is 0.0015068132698260312\n",
      "Batch: 11700,train loss is: 0.0015603789656957695\n",
      "test loss is 0.0016238309535929816\n",
      "Batch: 11800,train loss is: 0.001097753688620448\n",
      "test loss is 0.0013075751754679872\n",
      "Batch: 11900,train loss is: 0.001254464296239989\n",
      "test loss is 0.0013368519522832148\n",
      "Batch: 12000,train loss is: 0.0011127063002265335\n",
      "test loss is 0.0011629128826390045\n",
      "Batch: 12100,train loss is: 0.0016771818713054623\n",
      "test loss is 0.0013396747466502307\n",
      "Batch: 12200,train loss is: 0.0014863710151836354\n",
      "test loss is 0.0010889309324135657\n",
      "Batch: 12300,train loss is: 0.0015059382249062215\n",
      "test loss is 0.0012575365868371456\n",
      "Batch: 12400,train loss is: 0.0008664491194647391\n",
      "test loss is 0.0016054737887987076\n",
      "Batch: 12500,train loss is: 0.0013051356838044913\n",
      "test loss is 0.0015803331734067778\n",
      "Batch: 12600,train loss is: 0.0010355572146890717\n",
      "test loss is 0.0012406174152649876\n",
      "Batch: 12700,train loss is: 0.001499537499070806\n",
      "test loss is 0.0014614201891273283\n",
      "Batch: 12800,train loss is: 0.0016880654096819744\n",
      "test loss is 0.0011749309824488129\n",
      "Batch: 12900,train loss is: 0.0009954808721017467\n",
      "test loss is 0.0012082087814019018\n",
      "Batch: 13000,train loss is: 0.001836190969905509\n",
      "test loss is 0.001969339097521003\n",
      "Batch: 13100,train loss is: 0.0009455733344487361\n",
      "test loss is 0.0012612468255001506\n",
      "Batch: 13200,train loss is: 0.001012772385048268\n",
      "test loss is 0.001117783342625478\n",
      "Batch: 13300,train loss is: 0.0013325200553235968\n",
      "test loss is 0.001209843386768032\n",
      "Batch: 13400,train loss is: 0.0015313473029392752\n",
      "test loss is 0.00191046365933069\n",
      "Batch: 13500,train loss is: 0.0009352660716854986\n",
      "test loss is 0.0024178311690163943\n",
      "Batch: 13600,train loss is: 0.0011078043189232446\n",
      "test loss is 0.0011684708814144668\n",
      "Batch: 13700,train loss is: 0.001105473068972023\n",
      "test loss is 0.0012051387030935004\n",
      "Batch: 13800,train loss is: 0.0008681107459251633\n",
      "test loss is 0.0014359476675509394\n",
      "Batch: 13900,train loss is: 0.0007686983351776317\n",
      "test loss is 0.0017353874826319677\n",
      "Batch: 14000,train loss is: 0.0018969227464217672\n",
      "test loss is 0.001299800404908719\n",
      "Batch: 14100,train loss is: 0.001087161244311088\n",
      "test loss is 0.0010297054310514555\n",
      "Batch: 14200,train loss is: 0.0016424708792414531\n",
      "test loss is 0.0012232076561003416\n",
      "Batch: 14300,train loss is: 0.0014146558865045576\n",
      "test loss is 0.001152748405033597\n",
      "Batch: 14400,train loss is: 0.0012187431458178487\n",
      "test loss is 0.0011335594361283625\n",
      "Batch: 14500,train loss is: 0.0008207959345580742\n",
      "test loss is 0.001057987759216051\n",
      "Batch: 14600,train loss is: 0.0013976272520519598\n",
      "test loss is 0.001403286518606914\n",
      "Batch: 14700,train loss is: 0.0017676362781611063\n",
      "test loss is 0.0015475684323339506\n",
      "Batch: 14800,train loss is: 0.001262699786498984\n",
      "test loss is 0.001440763344108004\n",
      "Batch: 14900,train loss is: 0.0023862015361651038\n",
      "test loss is 0.0016602249294711926\n",
      "Batch: 15000,train loss is: 0.0017640261715587491\n",
      "test loss is 0.0011995882630591536\n",
      "Batch: 15100,train loss is: 0.0010150278287617825\n",
      "test loss is 0.0010870961413893916\n",
      "Batch: 15200,train loss is: 0.001037170662497517\n",
      "test loss is 0.0011206487088312913\n",
      "Batch: 15300,train loss is: 0.0007905862879479001\n",
      "test loss is 0.0010831581711777528\n",
      "Batch: 15400,train loss is: 0.001173954883234153\n",
      "test loss is 0.0012193175872698032\n",
      "Batch: 15500,train loss is: 0.0009264786076717992\n",
      "test loss is 0.0012926099909987012\n",
      "Batch: 15600,train loss is: 0.0013505726142067694\n",
      "test loss is 0.0011422491365135827\n",
      "Batch: 15700,train loss is: 0.0015181238464496228\n",
      "test loss is 0.0010781058867247652\n",
      "Batch: 15800,train loss is: 0.001135786787507307\n",
      "test loss is 0.0010633226416849474\n",
      "Batch: 15900,train loss is: 0.0011098558234211428\n",
      "test loss is 0.0027325456555374957\n",
      "Batch: 16000,train loss is: 0.0009433605564147158\n",
      "test loss is 0.0018680289400074023\n",
      "Batch: 16100,train loss is: 0.0014992171559357879\n",
      "test loss is 0.0016268920023743863\n",
      "Batch: 16200,train loss is: 0.0012307508306588846\n",
      "test loss is 0.0010784933002457827\n",
      "Batch: 16300,train loss is: 0.0008474478947196114\n",
      "test loss is 0.0013476978839780875\n",
      "Batch: 16400,train loss is: 0.0012100539941088352\n",
      "test loss is 0.0021938571969147666\n",
      "Batch: 16500,train loss is: 0.0013802355598016305\n",
      "test loss is 0.0012549868789559816\n",
      "Batch: 16600,train loss is: 0.0008711975250188281\n",
      "test loss is 0.0010594748023064384\n",
      "Batch: 16700,train loss is: 0.0009141509850945853\n",
      "test loss is 0.0011594824258619326\n",
      "Batch: 16800,train loss is: 0.0008689711971603483\n",
      "test loss is 0.001406486772792778\n",
      "Batch: 16900,train loss is: 0.0008394074112369962\n",
      "test loss is 0.0013633544582385977\n",
      "Batch: 17000,train loss is: 0.0012733189766372566\n",
      "test loss is 0.0014356747916485146\n",
      "Batch: 17100,train loss is: 0.001227671201275592\n",
      "test loss is 0.0011695746206050147\n",
      "Batch: 17200,train loss is: 0.0006999725669711758\n",
      "test loss is 0.001102709083767209\n",
      "Batch: 17300,train loss is: 0.0010115879678906316\n",
      "test loss is 0.0012218978835490267\n",
      "Batch: 17400,train loss is: 0.0010924594438415582\n",
      "test loss is 0.0014102287591980696\n",
      "Batch: 17500,train loss is: 0.001418953267314718\n",
      "test loss is 0.0013800373503357938\n",
      "Batch: 17600,train loss is: 0.0014229633852184606\n",
      "test loss is 0.0012650773947150162\n",
      "Batch: 17700,train loss is: 0.0008912096751884472\n",
      "test loss is 0.0013940179240188686\n",
      "Batch: 17800,train loss is: 0.0021254403555080862\n",
      "test loss is 0.0012431617430582929\n",
      "Batch: 17900,train loss is: 0.0015328888182226772\n",
      "test loss is 0.0016032884907294454\n",
      "Batch: 18000,train loss is: 0.0007927382669504427\n",
      "test loss is 0.0011202851070298211\n",
      "Batch: 18100,train loss is: 0.0015564243955653234\n",
      "test loss is 0.0012545373583471996\n",
      "Batch: 18200,train loss is: 0.0011093254664878932\n",
      "test loss is 0.0010631475319677593\n",
      "Batch: 18300,train loss is: 0.0008254181960850703\n",
      "test loss is 0.0010699378628961198\n",
      "Batch: 18400,train loss is: 0.00044201684679711956\n",
      "test loss is 0.0012708328794171306\n",
      "Batch: 18500,train loss is: 0.0009845688830399592\n",
      "test loss is 0.0017730557860931668\n",
      "Batch: 18600,train loss is: 0.0009141530920852524\n",
      "test loss is 0.0010772595878829566\n",
      "Batch: 18700,train loss is: 0.0012038204264509566\n",
      "test loss is 0.0012299767527524568\n",
      "Batch: 18800,train loss is: 0.0018136007504822595\n",
      "test loss is 0.0016919511344151428\n",
      "Batch: 18900,train loss is: 0.0011300594112576908\n",
      "test loss is 0.0014438353225211932\n",
      "Batch: 19000,train loss is: 0.003455741891221605\n",
      "test loss is 0.001380933359130809\n",
      "Batch: 19100,train loss is: 0.0007732503450365857\n",
      "test loss is 0.0011572738893791833\n",
      "Batch: 19200,train loss is: 0.0021305235707929472\n",
      "test loss is 0.0013115151554223352\n",
      "Batch: 19300,train loss is: 0.0009988439901565925\n",
      "test loss is 0.001188207705635061\n",
      "Batch: 19400,train loss is: 0.0006398967073511588\n",
      "test loss is 0.0012028447478383917\n",
      "Batch: 19500,train loss is: 0.0017745440508248763\n",
      "test loss is 0.0014148805280874007\n",
      "Batch: 19600,train loss is: 0.0011188266452244445\n",
      "test loss is 0.0013961948629214785\n",
      "Batch: 19700,train loss is: 0.0014528156568933523\n",
      "test loss is 0.0010777818669224471\n",
      "Batch: 19800,train loss is: 0.0018032308689477747\n",
      "test loss is 0.0013811547342293881\n",
      "Batch: 19900,train loss is: 0.0011117116325127423\n",
      "test loss is 0.0010181313325453038\n",
      "Batch: 20000,train loss is: 0.0012967772186988431\n",
      "test loss is 0.0012106778929399811\n",
      "Batch: 20100,train loss is: 0.0008891577371479135\n",
      "test loss is 0.0012211564261586504\n",
      "Batch: 20200,train loss is: 0.0014625663231218906\n",
      "test loss is 0.0012683081950593126\n",
      "Batch: 20300,train loss is: 0.0009336215562483832\n",
      "test loss is 0.0019229381659020418\n",
      "Batch: 20400,train loss is: 0.0007966727564418266\n",
      "test loss is 0.0011962695835470655\n",
      "Batch: 20500,train loss is: 0.0006551264458102125\n",
      "test loss is 0.0014863148504035313\n",
      "Batch: 20600,train loss is: 0.0009239101521215159\n",
      "test loss is 0.0010459700376419803\n",
      "Batch: 20700,train loss is: 0.0009186547069121586\n",
      "test loss is 0.0012989325215133944\n",
      "Batch: 20800,train loss is: 0.000977649249739123\n",
      "test loss is 0.001282402671648368\n",
      "Batch: 20900,train loss is: 0.0006765516276896412\n",
      "test loss is 0.0011955673926653339\n",
      "Batch: 21000,train loss is: 0.0010335155096697106\n",
      "test loss is 0.0014811199193402813\n",
      "Batch: 21100,train loss is: 0.0012572644318613454\n",
      "test loss is 0.0011463368300439658\n",
      "Batch: 21200,train loss is: 0.0007969688943048716\n",
      "test loss is 0.0012486410314002576\n",
      "Batch: 21300,train loss is: 0.003177854642019805\n",
      "test loss is 0.001256359367064423\n",
      "Batch: 21400,train loss is: 0.0012175687932946646\n",
      "test loss is 0.001130776603635061\n",
      "Batch: 21500,train loss is: 0.0013435721926063854\n",
      "test loss is 0.0012269525982857392\n",
      "Batch: 21600,train loss is: 0.0015434175207788398\n",
      "test loss is 0.0011864910545153194\n",
      "Batch: 21700,train loss is: 0.0019797233952552748\n",
      "test loss is 0.0014811564789315775\n",
      "Batch: 21800,train loss is: 0.0012350110345506053\n",
      "test loss is 0.0012597309844663046\n",
      "Batch: 21900,train loss is: 0.0009532824700841748\n",
      "test loss is 0.0009733517007196682\n",
      "Batch: 22000,train loss is: 0.0006226053657814892\n",
      "test loss is 0.0016245507507473206\n",
      "Batch: 22100,train loss is: 0.001210371673763591\n",
      "test loss is 0.0010400791593726316\n",
      "Batch: 22200,train loss is: 0.0011515011244566259\n",
      "test loss is 0.0009888848362091002\n",
      "Batch: 22300,train loss is: 0.0010929052278484958\n",
      "test loss is 0.0018997962975609022\n",
      "Batch: 22400,train loss is: 0.0007322469599616733\n",
      "test loss is 0.000991808222000395\n",
      "Batch: 22500,train loss is: 0.000821650719166156\n",
      "test loss is 0.0011630053290491225\n",
      "Batch: 22600,train loss is: 0.0006443870206040075\n",
      "test loss is 0.0010536545700870815\n",
      "Batch: 22700,train loss is: 0.0009132583636484155\n",
      "test loss is 0.0009397904105917507\n",
      "Batch: 22800,train loss is: 0.0018903582249706556\n",
      "test loss is 0.001229579082567882\n",
      "Batch: 22900,train loss is: 0.001306183565878153\n",
      "test loss is 0.0010595258860021982\n",
      "Batch: 23000,train loss is: 0.0010658587740527694\n",
      "test loss is 0.0010276062759513682\n",
      "Batch: 23100,train loss is: 0.0009928460722688724\n",
      "test loss is 0.0009618488683992533\n",
      "Batch: 23200,train loss is: 0.0011292643543869391\n",
      "test loss is 0.001217434713239673\n",
      "Batch: 23300,train loss is: 0.0010965825648069831\n",
      "test loss is 0.0010287113413842905\n",
      "Batch: 23400,train loss is: 0.001132711773717497\n",
      "test loss is 0.001157222320213834\n",
      "Batch: 23500,train loss is: 0.0015366458450567985\n",
      "test loss is 0.0010331603077525136\n",
      "Batch: 23600,train loss is: 0.00186434667052187\n",
      "test loss is 0.0011940531589317164\n",
      "Batch: 23700,train loss is: 0.001470473714751851\n",
      "test loss is 0.0012397540518349936\n",
      "Batch: 23800,train loss is: 0.0009393337040840658\n",
      "test loss is 0.0010482538436962683\n",
      "Batch: 23900,train loss is: 0.0009049443282389679\n",
      "test loss is 0.0011513217386326493\n",
      "Batch: 24000,train loss is: 0.0013288962589991975\n",
      "test loss is 0.0012015489727302737\n",
      "Batch: 24100,train loss is: 0.0008837268965157273\n",
      "test loss is 0.0014282655673121866\n",
      "Batch: 24200,train loss is: 0.00125620829696737\n",
      "test loss is 0.001106646110177604\n",
      "Batch: 24300,train loss is: 0.0009404908294459073\n",
      "test loss is 0.000978155470756303\n",
      "Batch: 24400,train loss is: 0.001224536568852182\n",
      "test loss is 0.0013883916318728651\n",
      "Batch: 24500,train loss is: 0.0007437478984898713\n",
      "test loss is 0.0011332112424474177\n",
      "Batch: 24600,train loss is: 0.0010589860633440401\n",
      "test loss is 0.0013159458537977183\n",
      "Batch: 24700,train loss is: 0.0019031375485748902\n",
      "test loss is 0.0011150117801263174\n",
      "Batch: 24800,train loss is: 0.000801249453056718\n",
      "test loss is 0.001030833211641215\n",
      "Batch: 24900,train loss is: 0.0016754795957426326\n",
      "test loss is 0.001059550149311934\n",
      "Batch: 25000,train loss is: 0.0011997640010794318\n",
      "test loss is 0.0013744719228407432\n",
      "Batch: 25100,train loss is: 0.0011116498657788656\n",
      "test loss is 0.0012375923838361988\n",
      "Batch: 25200,train loss is: 0.001759902992979107\n",
      "test loss is 0.0009505855497904325\n",
      "Batch: 25300,train loss is: 0.0011804582015249574\n",
      "test loss is 0.001073744331359587\n",
      "Batch: 25400,train loss is: 0.0013686191787425795\n",
      "test loss is 0.001049996114029649\n",
      "Batch: 25500,train loss is: 0.0009029861729097507\n",
      "test loss is 0.0010857483037482148\n",
      "Batch: 25600,train loss is: 0.0009938337804142538\n",
      "test loss is 0.0009187150650874619\n",
      "Batch: 25700,train loss is: 0.000908753204470328\n",
      "test loss is 0.0009146116233522485\n",
      "Batch: 25800,train loss is: 0.0010174911926883199\n",
      "test loss is 0.0012443197590270999\n",
      "Batch: 25900,train loss is: 0.0011230455815301082\n",
      "test loss is 0.00122475428170024\n",
      "Batch: 26000,train loss is: 0.0008371795808347352\n",
      "test loss is 0.0010375748920696535\n",
      "Batch: 26100,train loss is: 0.0017879489895717639\n",
      "test loss is 0.0016092532459786738\n",
      "Batch: 26200,train loss is: 0.0006972817908863982\n",
      "test loss is 0.0017274663908815886\n",
      "Batch: 26300,train loss is: 0.0013045398991098668\n",
      "test loss is 0.0010448904374646339\n",
      "Batch: 26400,train loss is: 0.0006457162577196925\n",
      "test loss is 0.0014007532014015558\n",
      "Batch: 26500,train loss is: 0.0015487196713929578\n",
      "test loss is 0.0010483598466556959\n",
      "Batch: 26600,train loss is: 0.0011878240395406586\n",
      "test loss is 0.0014990880768347668\n",
      "Batch: 26700,train loss is: 0.0013774228247455497\n",
      "test loss is 0.0011623549669893406\n",
      "Batch: 26800,train loss is: 0.001064319655497388\n",
      "test loss is 0.0010046910042557826\n",
      "Batch: 26900,train loss is: 0.0008434760513015597\n",
      "test loss is 0.0010489427701915255\n",
      "Batch: 27000,train loss is: 0.0008575399364122195\n",
      "test loss is 0.0011138390362074341\n",
      "Batch: 27100,train loss is: 0.0009556264074375039\n",
      "test loss is 0.0010020659035727252\n",
      "Batch: 27200,train loss is: 0.0021702236768911826\n",
      "test loss is 0.0010376100731149121\n",
      "Batch: 27300,train loss is: 0.0007308478541550276\n",
      "test loss is 0.0009995448773756624\n",
      "Batch: 27400,train loss is: 0.0008457761227148155\n",
      "test loss is 0.0011653748226096092\n",
      "Batch: 27500,train loss is: 0.0020238334129512917\n",
      "test loss is 0.0009994868600843201\n",
      "Batch: 27600,train loss is: 0.001290714222937449\n",
      "test loss is 0.0009741275975072237\n",
      "Batch: 27700,train loss is: 0.0018239701609565405\n",
      "test loss is 0.0020836578105605935\n",
      "Batch: 27800,train loss is: 0.0026566863641994346\n",
      "test loss is 0.0012191061428240015\n",
      "Batch: 27900,train loss is: 0.0006787530184328474\n",
      "test loss is 0.000993482065210356\n",
      "Batch: 28000,train loss is: 0.0010680914788612724\n",
      "test loss is 0.0022643578036543248\n",
      "Batch: 28100,train loss is: 0.001037931396789205\n",
      "test loss is 0.001097505893693602\n",
      "Batch: 28200,train loss is: 0.0007780044089787243\n",
      "test loss is 0.0009192788638930564\n",
      "Batch: 28300,train loss is: 0.0008872487562145585\n",
      "test loss is 0.001573024220938693\n",
      "Batch: 28400,train loss is: 0.0008667051348600664\n",
      "test loss is 0.000941580506158072\n",
      "Batch: 28500,train loss is: 0.001799693175764408\n",
      "test loss is 0.0009555841622497198\n",
      "Batch: 28600,train loss is: 0.0007250596688928057\n",
      "test loss is 0.0009341987454743033\n",
      "Batch: 28700,train loss is: 0.0009781948864789928\n",
      "test loss is 0.0008770901712255133\n",
      "Batch: 28800,train loss is: 0.0008903356262609998\n",
      "test loss is 0.0010225559378722955\n",
      "Batch: 28900,train loss is: 0.0008679833371235246\n",
      "test loss is 0.0009192800038546641\n",
      "Batch: 29000,train loss is: 0.0009963262364347746\n",
      "test loss is 0.0014080566413405347\n",
      "Batch: 29100,train loss is: 0.0014120619932287823\n",
      "test loss is 0.001166902145948552\n",
      "Batch: 29200,train loss is: 0.0009530398415702868\n",
      "test loss is 0.001207165071207948\n",
      "Batch: 29300,train loss is: 0.001729443607818175\n",
      "test loss is 0.000928650472451661\n",
      "Batch: 29400,train loss is: 0.0010027999518169616\n",
      "test loss is 0.0010950906038812846\n",
      "Batch: 29500,train loss is: 0.0007557588454815578\n",
      "test loss is 0.001045738534589731\n",
      "Batch: 29600,train loss is: 0.001031287276060582\n",
      "test loss is 0.0012442631644612684\n",
      "Batch: 29700,train loss is: 0.0011660892921160047\n",
      "test loss is 0.0012113481371524685\n",
      "Batch: 29800,train loss is: 0.0006160123393283647\n",
      "test loss is 0.0015331758359016289\n",
      "Batch: 29900,train loss is: 0.0012523489764535755\n",
      "test loss is 0.0010523317814574223\n",
      "Batch: 30000,train loss is: 0.0012460762947363675\n",
      "test loss is 0.000870525706385971\n",
      "Batch: 30100,train loss is: 0.0010182935571104018\n",
      "test loss is 0.0013386741835318637\n",
      "Batch: 30200,train loss is: 0.000797761823719268\n",
      "test loss is 0.0010164432235991213\n",
      "Batch: 30300,train loss is: 0.0014128738597848547\n",
      "test loss is 0.0014896246999593135\n",
      "Batch: 30400,train loss is: 0.0014255138977468979\n",
      "test loss is 0.0009702686616925864\n",
      "Batch: 30500,train loss is: 0.0012557543530370917\n",
      "test loss is 0.0009263072298479691\n",
      "Batch: 30600,train loss is: 0.0007826592846208156\n",
      "test loss is 0.00115150245790093\n",
      "Batch: 30700,train loss is: 0.0008562583249457376\n",
      "test loss is 0.0009869713780291415\n",
      "Batch: 30800,train loss is: 0.0009609054413144273\n",
      "test loss is 0.0009823866063181622\n",
      "Batch: 30900,train loss is: 0.0021747824446224926\n",
      "test loss is 0.0008669728514631503\n",
      "Batch: 31000,train loss is: 0.0011978644458637178\n",
      "test loss is 0.0008624315127374053\n",
      "Batch: 31100,train loss is: 0.002542377371833662\n",
      "test loss is 0.0013564398346362025\n",
      "Batch: 31200,train loss is: 0.0009883208605113675\n",
      "test loss is 0.0011230341690098649\n",
      "Batch: 31300,train loss is: 0.0008590617697568905\n",
      "test loss is 0.0009928634397789262\n",
      "Batch: 31400,train loss is: 0.001176137519010947\n",
      "test loss is 0.001368449068406048\n",
      "Batch: 31500,train loss is: 0.004335052682247911\n",
      "test loss is 0.0017706903366311397\n",
      "Batch: 31600,train loss is: 0.0005874283245285223\n",
      "test loss is 0.0014795722226286217\n",
      "Batch: 31700,train loss is: 0.001267321111358021\n",
      "test loss is 0.001335767392306461\n",
      "Batch: 31800,train loss is: 0.0013524919229366218\n",
      "test loss is 0.001078122293510401\n",
      "Batch: 31900,train loss is: 0.0009798041647453009\n",
      "test loss is 0.0009419871709748856\n",
      "Batch: 32000,train loss is: 0.0005789808415027805\n",
      "test loss is 0.0011933829275738068\n",
      "Batch: 32100,train loss is: 0.00107678978065507\n",
      "test loss is 0.0008580621933756051\n",
      "Batch: 32200,train loss is: 0.0012233836256824835\n",
      "test loss is 0.000956588808610267\n",
      "Batch: 32300,train loss is: 0.0014220093985987787\n",
      "test loss is 0.0009356171086036116\n",
      "Batch: 32400,train loss is: 0.001250018283934575\n",
      "test loss is 0.0022476615508930506\n",
      "Batch: 32500,train loss is: 0.0020773401320646465\n",
      "test loss is 0.000925797793583665\n",
      "Batch: 32600,train loss is: 0.0008079666324613843\n",
      "test loss is 0.0013952231216206035\n",
      "Batch: 32700,train loss is: 0.0009572100265727081\n",
      "test loss is 0.001425984619335582\n",
      "Batch: 32800,train loss is: 0.0013312817420182443\n",
      "test loss is 0.0014823786406670085\n",
      "Batch: 32900,train loss is: 0.0006084428319150535\n",
      "test loss is 0.0010346829222504534\n",
      "Batch: 33000,train loss is: 0.0007851160800931021\n",
      "test loss is 0.0008914460681210305\n",
      "Batch: 33100,train loss is: 0.0008285200296072816\n",
      "test loss is 0.000846385297059891\n",
      "Batch: 33200,train loss is: 0.0014144239739048705\n",
      "test loss is 0.000978009656000999\n",
      "Batch: 33300,train loss is: 0.0013715908106650986\n",
      "test loss is 0.0009632407276041588\n",
      "Batch: 33400,train loss is: 0.0009734096364625246\n",
      "test loss is 0.0009848496151031716\n",
      "Batch: 33500,train loss is: 0.0008877376076965942\n",
      "test loss is 0.0009204431353714446\n",
      "Batch: 33600,train loss is: 0.0009939228694442561\n",
      "test loss is 0.0009596839397078494\n",
      "Batch: 33700,train loss is: 0.001155217881731657\n",
      "test loss is 0.000879461441431934\n",
      "Batch: 33800,train loss is: 0.001050710689957753\n",
      "test loss is 0.0010212291402131115\n",
      "Batch: 33900,train loss is: 0.0010510307780658389\n",
      "test loss is 0.0011607292024816492\n",
      "Batch: 34000,train loss is: 0.001023020593569616\n",
      "test loss is 0.0015651667223026333\n",
      "Batch: 34100,train loss is: 0.0010921711928651185\n",
      "test loss is 0.0008420733634469408\n",
      "Batch: 34200,train loss is: 0.0012661581938234\n",
      "test loss is 0.0013136226170596176\n",
      "Batch: 34300,train loss is: 0.0007561522463050984\n",
      "test loss is 0.0009354126775070873\n",
      "Batch: 34400,train loss is: 0.0007454208873899265\n",
      "test loss is 0.0008797916903044589\n",
      "Batch: 34500,train loss is: 0.0007539306853352104\n",
      "test loss is 0.0010333423671674104\n",
      "Batch: 34600,train loss is: 0.0015954099405861364\n",
      "test loss is 0.0009533976693810116\n",
      "Batch: 34700,train loss is: 0.0013712662821379357\n",
      "test loss is 0.0009698359791889193\n",
      "Batch: 34800,train loss is: 0.0007156279677219582\n",
      "test loss is 0.0009627787975857199\n",
      "Batch: 34900,train loss is: 0.0011269300228672285\n",
      "test loss is 0.0010181642621099657\n",
      "Batch: 35000,train loss is: 0.0008795511850707578\n",
      "test loss is 0.0008855581686237972\n",
      "Batch: 35100,train loss is: 0.000960182398666767\n",
      "test loss is 0.0008944828357243882\n",
      "Batch: 35200,train loss is: 0.0009111172071609634\n",
      "test loss is 0.0011064370916486457\n",
      "Batch: 35300,train loss is: 0.00133667707178098\n",
      "test loss is 0.001034036687227532\n",
      "Batch: 35400,train loss is: 0.0006233963473013898\n",
      "test loss is 0.0011663833620569571\n",
      "Batch: 35500,train loss is: 0.0009844805154656602\n",
      "test loss is 0.0009986023350023584\n",
      "Batch: 35600,train loss is: 0.0007506767659485059\n",
      "test loss is 0.001303220053301815\n",
      "Batch: 35700,train loss is: 0.000981205890888735\n",
      "test loss is 0.0010033761115607303\n",
      "Batch: 35800,train loss is: 0.0012028063285293785\n",
      "test loss is 0.000899257300765791\n",
      "Batch: 35900,train loss is: 0.0007912533422510522\n",
      "test loss is 0.0008790847255212478\n",
      "Batch: 36000,train loss is: 0.0012325417027880254\n",
      "test loss is 0.0008962003554966683\n",
      "Batch: 36100,train loss is: 0.0008577785272042351\n",
      "test loss is 0.0009386009638962116\n",
      "Batch: 36200,train loss is: 0.0013102736538691884\n",
      "test loss is 0.0012813888002954285\n",
      "Batch: 36300,train loss is: 0.0006606965134037836\n",
      "test loss is 0.0012375081166638427\n",
      "Batch: 36400,train loss is: 0.0025073724443340955\n",
      "test loss is 0.001520641454397077\n",
      "Batch: 36500,train loss is: 0.0010441516940736895\n",
      "test loss is 0.0009891850004926026\n",
      "Batch: 36600,train loss is: 0.0009500009233000777\n",
      "test loss is 0.0008534058751187627\n",
      "Batch: 36700,train loss is: 0.0009924908123351357\n",
      "test loss is 0.0013897120302278371\n",
      "Batch: 36800,train loss is: 0.001204153984002285\n",
      "test loss is 0.0008372934153515958\n",
      "Batch: 36900,train loss is: 0.0004073590890278933\n",
      "test loss is 0.000904277931468313\n",
      "Batch: 37000,train loss is: 0.00324142982284355\n",
      "test loss is 0.0011965880856737953\n",
      "Batch: 37100,train loss is: 0.0009257220827148322\n",
      "test loss is 0.0009374073250615316\n",
      "Batch: 37200,train loss is: 0.0006607376040839079\n",
      "test loss is 0.0008030317847015776\n",
      "Batch: 37300,train loss is: 0.0009955258371479882\n",
      "test loss is 0.0011663523357802572\n",
      "Batch: 37400,train loss is: 0.0007585733053063447\n",
      "test loss is 0.0008464503601632089\n",
      "Batch: 37500,train loss is: 0.0010622740312339204\n",
      "test loss is 0.0009064550354917331\n",
      "Batch: 37600,train loss is: 0.0010408646178392696\n",
      "test loss is 0.0011288447343915712\n",
      "Batch: 37700,train loss is: 0.0007294205022859598\n",
      "test loss is 0.0009282874402697128\n",
      "Batch: 37800,train loss is: 0.0006581856851777494\n",
      "test loss is 0.000889133576437847\n",
      "Batch: 37900,train loss is: 0.0012520869293632764\n",
      "test loss is 0.0008061157918789605\n",
      "Batch: 38000,train loss is: 0.0010743063387378036\n",
      "test loss is 0.0009710863756298741\n",
      "Batch: 38100,train loss is: 0.000646890675796455\n",
      "test loss is 0.0009234169718279861\n",
      "Batch: 38200,train loss is: 0.000773328621148384\n",
      "test loss is 0.0010149040073515816\n",
      "Batch: 38300,train loss is: 0.0008669492920578465\n",
      "test loss is 0.0007865284038217061\n",
      "Batch: 38400,train loss is: 0.0012231610531339334\n",
      "test loss is 0.0009201293549554071\n",
      "Batch: 38500,train loss is: 0.0013931728749856662\n",
      "test loss is 0.0009511045915361798\n",
      "Batch: 38600,train loss is: 0.0015019069304107661\n",
      "test loss is 0.0010884542677551867\n",
      "Batch: 38700,train loss is: 0.0011359123962147598\n",
      "test loss is 0.0008503816128818029\n",
      "Batch: 38800,train loss is: 0.0007055424125536217\n",
      "test loss is 0.0008507708997579522\n",
      "Batch: 38900,train loss is: 0.0013562537935974644\n",
      "test loss is 0.0012564614888371448\n",
      "Batch: 39000,train loss is: 0.000638024459766694\n",
      "test loss is 0.0008048930479684934\n",
      "Batch: 39100,train loss is: 0.0013090822959983801\n",
      "test loss is 0.000938399526563655\n",
      "Batch: 39200,train loss is: 0.0009678425456983593\n",
      "test loss is 0.0008402754132243902\n",
      "Batch: 39300,train loss is: 0.0009797042542880213\n",
      "test loss is 0.0011198273001827303\n",
      "Batch: 39400,train loss is: 0.000760281500983557\n",
      "test loss is 0.0013016230852004348\n",
      "Batch: 39500,train loss is: 0.0006246243303565933\n",
      "test loss is 0.0011288379627573918\n",
      "Batch: 39600,train loss is: 0.0008518990363361731\n",
      "test loss is 0.0008869469515809981\n",
      "Batch: 39700,train loss is: 0.0014249271736873122\n",
      "test loss is 0.0008291518248349854\n",
      "Batch: 39800,train loss is: 0.0010911853301769023\n",
      "test loss is 0.0010626894334479122\n",
      "Batch: 39900,train loss is: 0.0009370652501888789\n",
      "test loss is 0.0010033623897264617\n",
      "Batch: 40000,train loss is: 0.0009843771312240922\n",
      "test loss is 0.0014317560701857714\n",
      "Batch: 40100,train loss is: 0.0006237613412537756\n",
      "test loss is 0.0010852841355618724\n",
      "Batch: 40200,train loss is: 0.00079079032543429\n",
      "test loss is 0.001202442530112049\n",
      "Batch: 40300,train loss is: 0.0010036257268482977\n",
      "test loss is 0.00084721757848608\n",
      "Batch: 40400,train loss is: 0.0007799258356304634\n",
      "test loss is 0.0011206628487437477\n",
      "Batch: 40500,train loss is: 0.0011255532507579054\n",
      "test loss is 0.0008498283015608059\n",
      "Batch: 40600,train loss is: 0.000987313677605527\n",
      "test loss is 0.0007875199706505623\n",
      "Batch: 40700,train loss is: 0.0006655800582585105\n",
      "test loss is 0.001024875849877159\n",
      "Batch: 40800,train loss is: 0.0007510404864192309\n",
      "test loss is 0.000982443457324613\n",
      "Batch: 40900,train loss is: 0.0009389137102330681\n",
      "test loss is 0.0009117764169032781\n",
      "Batch: 41000,train loss is: 0.0009382350919523814\n",
      "test loss is 0.0008371585118684861\n",
      "Batch: 41100,train loss is: 0.0010494051191764528\n",
      "test loss is 0.001055129316345729\n",
      "Batch: 41200,train loss is: 0.0010343878526437107\n",
      "test loss is 0.0008602410545350715\n",
      "Batch: 41300,train loss is: 0.002075296682937999\n",
      "test loss is 0.0009533070663780782\n",
      "Batch: 41400,train loss is: 0.0008237951090955514\n",
      "test loss is 0.000862144454844149\n",
      "Batch: 41500,train loss is: 0.0006965895028689371\n",
      "test loss is 0.0009904528234819406\n",
      "Batch: 41600,train loss is: 0.000808385091035911\n",
      "test loss is 0.0008712210877658389\n",
      "Batch: 41700,train loss is: 0.0006960194042687861\n",
      "test loss is 0.0008325686304528429\n",
      "Batch: 41800,train loss is: 0.0007226374220122005\n",
      "test loss is 0.0009029399590092263\n",
      "Batch: 41900,train loss is: 0.0008460883481072308\n",
      "test loss is 0.0009129086857838543\n",
      "Batch: 42000,train loss is: 0.0013952792801775501\n",
      "test loss is 0.0010667456290847372\n",
      "Batch: 42100,train loss is: 0.0006050336708261858\n",
      "test loss is 0.0007938321075191934\n",
      "Batch: 42200,train loss is: 0.0007948653158769915\n",
      "test loss is 0.0013248605809285513\n",
      "Batch: 42300,train loss is: 0.0012008860020212373\n",
      "test loss is 0.0009536585919197296\n",
      "Batch: 42400,train loss is: 0.0004977820295476998\n",
      "test loss is 0.0009918977098070235\n",
      "Batch: 42500,train loss is: 0.001366649631917054\n",
      "test loss is 0.0009613914552369654\n",
      "Batch: 42600,train loss is: 0.0007261871076710811\n",
      "test loss is 0.0009616367347990588\n",
      "Batch: 42700,train loss is: 0.0008651395805136828\n",
      "test loss is 0.0008137694789785539\n",
      "Batch: 42800,train loss is: 0.002241361792134953\n",
      "test loss is 0.0009479396093244695\n",
      "Batch: 42900,train loss is: 0.0011891462909617134\n",
      "test loss is 0.0007946316922306947\n",
      "Batch: 43000,train loss is: 0.0005450799216864819\n",
      "test loss is 0.0010808747486977402\n",
      "Batch: 43100,train loss is: 0.001146639407414174\n",
      "test loss is 0.0010006739049771185\n",
      "Batch: 43200,train loss is: 0.0026471630198842628\n",
      "test loss is 0.0009353651688368585\n",
      "Batch: 43300,train loss is: 0.0008856794042133397\n",
      "test loss is 0.0008581256764471009\n",
      "Batch: 43400,train loss is: 0.0009529336782484192\n",
      "test loss is 0.0012792316561579722\n",
      "Batch: 43500,train loss is: 0.0006368908218150909\n",
      "test loss is 0.001004715008868962\n",
      "Batch: 43600,train loss is: 0.0010813824677150877\n",
      "test loss is 0.0008215546841505965\n",
      "Batch: 43700,train loss is: 0.0009581346893659117\n",
      "test loss is 0.0007569750389554541\n",
      "Batch: 43800,train loss is: 0.0008949513109935858\n",
      "test loss is 0.0011361874113082486\n",
      "Batch: 43900,train loss is: 0.0011616904811351317\n",
      "test loss is 0.0008198851893339258\n",
      "Batch: 44000,train loss is: 0.0007758628548096298\n",
      "test loss is 0.0007839845636044311\n",
      "Batch: 44100,train loss is: 0.000891892394219104\n",
      "test loss is 0.0010142560692746523\n",
      "Batch: 44200,train loss is: 0.0012319257800890349\n",
      "test loss is 0.0007751273399607973\n",
      "Batch: 44300,train loss is: 0.0009711655710464308\n",
      "test loss is 0.0008636496427359131\n",
      "Batch: 44400,train loss is: 0.0012811597979407415\n",
      "test loss is 0.0008409286090511345\n",
      "Batch: 44500,train loss is: 0.0010498207114698708\n",
      "test loss is 0.0008854223296389342\n",
      "Batch: 44600,train loss is: 0.0007028130809937903\n",
      "test loss is 0.0010353650245254634\n",
      "Batch: 44700,train loss is: 0.000804024470497651\n",
      "test loss is 0.0010872404769589191\n",
      "Batch: 44800,train loss is: 0.0008627504097120196\n",
      "test loss is 0.0008061463344330044\n",
      "Batch: 44900,train loss is: 0.0007089035823888085\n",
      "test loss is 0.0007639225833155445\n",
      "Batch: 45000,train loss is: 0.0007350046616665311\n",
      "test loss is 0.0010791436109491217\n",
      "Batch: 45100,train loss is: 0.0009353586092976942\n",
      "test loss is 0.0010848135824860134\n",
      "Batch: 45200,train loss is: 0.0007704912917726131\n",
      "test loss is 0.001130980927488321\n",
      "Batch: 45300,train loss is: 0.0013782138378704185\n",
      "test loss is 0.0009467375245324113\n",
      "Batch: 45400,train loss is: 0.0007079035230300966\n",
      "test loss is 0.0009922597197755613\n",
      "Batch: 45500,train loss is: 0.0006757554874970694\n",
      "test loss is 0.0009290024113456401\n",
      "Batch: 45600,train loss is: 0.0007652248021858632\n",
      "test loss is 0.0008734597665377665\n",
      "Batch: 45700,train loss is: 0.0011967486760268283\n",
      "test loss is 0.0008679555488270992\n",
      "Batch: 45800,train loss is: 0.0008339054862325922\n",
      "test loss is 0.0008005912947396892\n",
      "Batch: 45900,train loss is: 0.001099978849606279\n",
      "test loss is 0.001086277707442243\n",
      "Batch: 46000,train loss is: 0.0007622537658945007\n",
      "test loss is 0.0008178011089122272\n",
      "Batch: 46100,train loss is: 0.0008019262848345923\n",
      "test loss is 0.0008758062599820615\n",
      "Batch: 46200,train loss is: 0.0008151359070754704\n",
      "test loss is 0.0007846407166196263\n",
      "Batch: 46300,train loss is: 0.0010738369535753891\n",
      "test loss is 0.0007638923696480988\n",
      "Batch: 46400,train loss is: 0.0010345044172073202\n",
      "test loss is 0.0010701742151759687\n",
      "Batch: 46500,train loss is: 0.0010079137902657205\n",
      "test loss is 0.0009693342388210032\n",
      "Batch: 46600,train loss is: 0.0005954083013822787\n",
      "test loss is 0.0008005000185859108\n",
      "Batch: 46700,train loss is: 0.0011459720123963002\n",
      "test loss is 0.000832765168344419\n",
      "-----------------------Epoch: 2----------------------------------\n",
      "Batch: 0,train loss is: 0.000907692361449994\n",
      "test loss is 0.001100562823707614\n",
      "Batch: 100,train loss is: 0.0014045227362210117\n",
      "test loss is 0.0009645189564736227\n",
      "Batch: 200,train loss is: 0.0005062865789755332\n",
      "test loss is 0.0007441390791142606\n",
      "Batch: 300,train loss is: 0.001124759683334286\n",
      "test loss is 0.0007483517158101235\n",
      "Batch: 400,train loss is: 0.0009700704206418293\n",
      "test loss is 0.0008315272460588564\n",
      "Batch: 500,train loss is: 0.0009053380312815708\n",
      "test loss is 0.0020396294441596056\n",
      "Batch: 600,train loss is: 0.0006489113585550352\n",
      "test loss is 0.0009084310590509382\n",
      "Batch: 700,train loss is: 0.0011510674331970845\n",
      "test loss is 0.0007482515808008319\n",
      "Batch: 800,train loss is: 0.0009898126929624553\n",
      "test loss is 0.000827079076237407\n",
      "Batch: 900,train loss is: 0.0006758029771658412\n",
      "test loss is 0.0007414671870587808\n",
      "Batch: 1000,train loss is: 0.0014210851747544827\n",
      "test loss is 0.0008848288363379586\n",
      "Batch: 1100,train loss is: 0.0006943498255251984\n",
      "test loss is 0.0008354283218484483\n",
      "Batch: 1200,train loss is: 0.0006618317028619846\n",
      "test loss is 0.0007373283668620428\n",
      "Batch: 1300,train loss is: 0.001256246496060412\n",
      "test loss is 0.0010562549946749459\n",
      "Batch: 1400,train loss is: 0.0007729851707183712\n",
      "test loss is 0.0007784467316923312\n",
      "Batch: 1500,train loss is: 0.000967857918524692\n",
      "test loss is 0.0010471533556366316\n",
      "Batch: 1600,train loss is: 0.0007984641009844749\n",
      "test loss is 0.0008222436026966259\n",
      "Batch: 1700,train loss is: 0.0007521893358602687\n",
      "test loss is 0.0011749683337924192\n",
      "Batch: 1800,train loss is: 0.0009363377851641586\n",
      "test loss is 0.0011287335282819909\n",
      "Batch: 1900,train loss is: 0.0009389952816686127\n",
      "test loss is 0.0008808638896516329\n",
      "Batch: 2000,train loss is: 0.0018923382551958815\n",
      "test loss is 0.0007906839922435489\n",
      "Batch: 2100,train loss is: 0.0012708079786029583\n",
      "test loss is 0.0013586429972876912\n",
      "Batch: 2200,train loss is: 0.0007833939439081848\n",
      "test loss is 0.0007323454157700547\n",
      "Batch: 2300,train loss is: 0.0008008249194047952\n",
      "test loss is 0.0009989820667521802\n",
      "Batch: 2400,train loss is: 0.0008945103008195492\n",
      "test loss is 0.0013886498408432289\n",
      "Batch: 2500,train loss is: 0.001530670264418649\n",
      "test loss is 0.000954760598118448\n",
      "Batch: 2600,train loss is: 0.0007576609025574567\n",
      "test loss is 0.0008136705573105855\n",
      "Batch: 2700,train loss is: 0.00039947882472634764\n",
      "test loss is 0.0007350027678624382\n",
      "Batch: 2800,train loss is: 0.0012100599605874257\n",
      "test loss is 0.0010416136293075564\n",
      "Batch: 2900,train loss is: 0.0015117833437931715\n",
      "test loss is 0.0009405370566228187\n",
      "Batch: 3000,train loss is: 0.000810852345844501\n",
      "test loss is 0.0007677857597637017\n",
      "Batch: 3100,train loss is: 0.0005020223465178174\n",
      "test loss is 0.0009943617180703925\n",
      "Batch: 3200,train loss is: 0.0009567293509576584\n",
      "test loss is 0.0009270597700811941\n",
      "Batch: 3300,train loss is: 0.000822706415842677\n",
      "test loss is 0.0007788475018214761\n",
      "Batch: 3400,train loss is: 0.0009260404142887116\n",
      "test loss is 0.0008955132653271424\n",
      "Batch: 3500,train loss is: 0.0007959247648659781\n",
      "test loss is 0.0009252835367937263\n",
      "Batch: 3600,train loss is: 0.0006592401634491939\n",
      "test loss is 0.0008002360600149495\n",
      "Batch: 3700,train loss is: 0.0011778354486640021\n",
      "test loss is 0.0009125079851881131\n",
      "Batch: 3800,train loss is: 0.0007143769539529219\n",
      "test loss is 0.0010749405809472618\n",
      "Batch: 3900,train loss is: 0.0011139747852658873\n",
      "test loss is 0.0008725050553793575\n",
      "Batch: 4000,train loss is: 0.0009021186079049175\n",
      "test loss is 0.0007352069493700011\n",
      "Batch: 4100,train loss is: 0.0008849792373021593\n",
      "test loss is 0.0007440083205760719\n",
      "Batch: 4200,train loss is: 0.0008922468349944709\n",
      "test loss is 0.00082142950630369\n",
      "Batch: 4300,train loss is: 0.0007178388013526326\n",
      "test loss is 0.0007194653184690073\n",
      "Batch: 4400,train loss is: 0.001008172595282762\n",
      "test loss is 0.0008201933251192279\n",
      "Batch: 4500,train loss is: 0.0008655843115713227\n",
      "test loss is 0.0007945498143027742\n",
      "Batch: 4600,train loss is: 0.0008220147476655597\n",
      "test loss is 0.0009054238050828178\n",
      "Batch: 4700,train loss is: 0.001311426120367186\n",
      "test loss is 0.0009356741998575901\n",
      "Batch: 4800,train loss is: 0.0005802781396354948\n",
      "test loss is 0.0007452403112838499\n",
      "Batch: 4900,train loss is: 0.0009172815622022125\n",
      "test loss is 0.0007344935661470175\n",
      "Batch: 5000,train loss is: 0.0007323515560073138\n",
      "test loss is 0.0017931228715448637\n",
      "Batch: 5100,train loss is: 0.001314618214319507\n",
      "test loss is 0.0007982316067518954\n",
      "Batch: 5200,train loss is: 0.0005255969721173029\n",
      "test loss is 0.0008054491488757948\n",
      "Batch: 5300,train loss is: 0.0008068628638189513\n",
      "test loss is 0.0008317374555554845\n",
      "Batch: 5400,train loss is: 0.0014688423928933658\n",
      "test loss is 0.0007778095972186284\n",
      "Batch: 5500,train loss is: 0.0007093217808512758\n",
      "test loss is 0.0008518241802033611\n",
      "Batch: 5600,train loss is: 0.000756392251951636\n",
      "test loss is 0.0009212857168279902\n",
      "Batch: 5700,train loss is: 0.0006241757223849367\n",
      "test loss is 0.0009704057660411945\n",
      "Batch: 5800,train loss is: 0.000937313039570695\n",
      "test loss is 0.0009532802487356052\n",
      "Batch: 5900,train loss is: 0.001750390619615085\n",
      "test loss is 0.0014734945730599993\n",
      "Batch: 6000,train loss is: 0.0009547270942559843\n",
      "test loss is 0.0007741539580489273\n",
      "Batch: 6100,train loss is: 0.0006976394800322634\n",
      "test loss is 0.0012065647239422124\n",
      "Batch: 6200,train loss is: 0.0005540669810142672\n",
      "test loss is 0.0008029160665726532\n",
      "Batch: 6300,train loss is: 0.0007373903050847794\n",
      "test loss is 0.0008702586691369344\n",
      "Batch: 6400,train loss is: 0.0006395384355485714\n",
      "test loss is 0.0009349361735607487\n",
      "Batch: 6500,train loss is: 0.0006722901795340199\n",
      "test loss is 0.0007608839822338655\n",
      "Batch: 6600,train loss is: 0.0010890372500417323\n",
      "test loss is 0.0008358430184783865\n",
      "Batch: 6700,train loss is: 0.0005519410586010839\n",
      "test loss is 0.0007727621300410171\n",
      "Batch: 6800,train loss is: 0.0011175178997484643\n",
      "test loss is 0.000728267032439539\n",
      "Batch: 6900,train loss is: 0.0006910077106827837\n",
      "test loss is 0.0007402651373684963\n",
      "Batch: 7000,train loss is: 0.0017778959956176528\n",
      "test loss is 0.0007826344874142394\n",
      "Batch: 7100,train loss is: 0.0009816852769809307\n",
      "test loss is 0.0009314629179227985\n",
      "Batch: 7200,train loss is: 0.0008908134760223123\n",
      "test loss is 0.0011238545025132672\n",
      "Batch: 7300,train loss is: 0.0006390505238737685\n",
      "test loss is 0.0008165188468406504\n",
      "Batch: 7400,train loss is: 0.0007227842266654253\n",
      "test loss is 0.0008372798973354554\n",
      "Batch: 7500,train loss is: 0.0012055115170638915\n",
      "test loss is 0.0008326274596928565\n",
      "Batch: 7600,train loss is: 0.0007171638091051813\n",
      "test loss is 0.0007726155166315203\n",
      "Batch: 7700,train loss is: 0.0008130844329977992\n",
      "test loss is 0.000984891170329921\n",
      "Batch: 7800,train loss is: 0.0006314547411416248\n",
      "test loss is 0.0007215120500475128\n",
      "Batch: 7900,train loss is: 0.0006919595991196254\n",
      "test loss is 0.0008854781174872812\n",
      "Batch: 8000,train loss is: 0.0007466710224445391\n",
      "test loss is 0.0007406844494586626\n",
      "Batch: 8100,train loss is: 0.0009521842247117267\n",
      "test loss is 0.0007241543700136014\n",
      "Batch: 8200,train loss is: 0.0007436409052635044\n",
      "test loss is 0.0013318582309696703\n",
      "Batch: 8300,train loss is: 0.0010850197168262722\n",
      "test loss is 0.0007883489317109696\n",
      "Batch: 8400,train loss is: 0.001719495397869892\n",
      "test loss is 0.0010489373051489346\n",
      "Batch: 8500,train loss is: 0.0008493072332412953\n",
      "test loss is 0.0007258346691473785\n",
      "Batch: 8600,train loss is: 0.0005654890835222117\n",
      "test loss is 0.0007625197454894458\n",
      "Batch: 8700,train loss is: 0.0005611999817988023\n",
      "test loss is 0.0008589394192008075\n",
      "Batch: 8800,train loss is: 0.0007977282289178041\n",
      "test loss is 0.001519308894479596\n",
      "Batch: 8900,train loss is: 0.0007696764268903015\n",
      "test loss is 0.0007096057485507477\n",
      "Batch: 9000,train loss is: 0.0006098309333772431\n",
      "test loss is 0.0006628274626679009\n",
      "Batch: 9100,train loss is: 0.0008481395597697626\n",
      "test loss is 0.0008945797341297393\n",
      "Batch: 9200,train loss is: 0.0008631584993174903\n",
      "test loss is 0.0008759365782228946\n",
      "Batch: 9300,train loss is: 0.0005884600751706172\n",
      "test loss is 0.0007352505875358262\n",
      "Batch: 9400,train loss is: 0.0005559506248169243\n",
      "test loss is 0.0007385311051770232\n",
      "Batch: 9500,train loss is: 0.0006322089825913967\n",
      "test loss is 0.0008950848343448784\n",
      "Batch: 9600,train loss is: 0.0009441557319974626\n",
      "test loss is 0.000895710670842229\n",
      "Batch: 9700,train loss is: 0.0008484671609521565\n",
      "test loss is 0.0007442422410184656\n",
      "Batch: 9800,train loss is: 0.0012417957442404503\n",
      "test loss is 0.000750674573263377\n",
      "Batch: 9900,train loss is: 0.0010762313819424058\n",
      "test loss is 0.000760412542994363\n",
      "Batch: 10000,train loss is: 0.0007214478762332413\n",
      "test loss is 0.0008284024795017003\n",
      "Batch: 10100,train loss is: 0.000903574028885783\n",
      "test loss is 0.0006875853986242336\n",
      "Batch: 10200,train loss is: 0.0005000635946658504\n",
      "test loss is 0.0009725452161224249\n",
      "Batch: 10300,train loss is: 0.0008975915809087087\n",
      "test loss is 0.0007355148976308291\n",
      "Batch: 10400,train loss is: 0.000745888605301156\n",
      "test loss is 0.0014437880253883565\n",
      "Batch: 10500,train loss is: 0.0008382599298643822\n",
      "test loss is 0.001031955473492233\n",
      "Batch: 10600,train loss is: 0.0014713247576749163\n",
      "test loss is 0.0013999953829911835\n",
      "Batch: 10700,train loss is: 0.0006058617804979266\n",
      "test loss is 0.0006768846422434287\n",
      "Batch: 10800,train loss is: 0.0007115860119826554\n",
      "test loss is 0.0011867905536297502\n",
      "Batch: 10900,train loss is: 0.0013455475151837987\n",
      "test loss is 0.001312963759536301\n",
      "Batch: 11000,train loss is: 0.0007430949116839375\n",
      "test loss is 0.0010689549533394\n",
      "Batch: 11100,train loss is: 0.0007015456265892203\n",
      "test loss is 0.0010629712315222998\n",
      "Batch: 11200,train loss is: 0.0007239285625762475\n",
      "test loss is 0.0007099363527345022\n",
      "Batch: 11300,train loss is: 0.0006592378495281128\n",
      "test loss is 0.0007984467825157658\n",
      "Batch: 11400,train loss is: 0.0005715396164360604\n",
      "test loss is 0.0007554928896439396\n",
      "Batch: 11500,train loss is: 0.0005241779450852787\n",
      "test loss is 0.0007765396662740223\n",
      "Batch: 11600,train loss is: 0.0007485719373214889\n",
      "test loss is 0.0007416606848277778\n",
      "Batch: 11700,train loss is: 0.0010779278069604707\n",
      "test loss is 0.00077369520999721\n",
      "Batch: 11800,train loss is: 0.0005879164640091839\n",
      "test loss is 0.0008018569131429631\n",
      "Batch: 11900,train loss is: 0.001030523793212551\n",
      "test loss is 0.0012307937202926885\n",
      "Batch: 12000,train loss is: 0.0017233045197102234\n",
      "test loss is 0.0009302107948807361\n",
      "Batch: 12100,train loss is: 0.00046205478543057555\n",
      "test loss is 0.000953454369965523\n",
      "Batch: 12200,train loss is: 0.0007367289810578658\n",
      "test loss is 0.00072117724683471\n",
      "Batch: 12300,train loss is: 0.0008368490324919718\n",
      "test loss is 0.0009960713808894555\n",
      "Batch: 12400,train loss is: 0.0012298670163327224\n",
      "test loss is 0.0008693675890738825\n",
      "Batch: 12500,train loss is: 0.0006097287106955829\n",
      "test loss is 0.0007549402076802131\n",
      "Batch: 12600,train loss is: 0.0007368528056119652\n",
      "test loss is 0.0007836908966531233\n",
      "Batch: 12700,train loss is: 0.0004176577194819079\n",
      "test loss is 0.0009399849624017298\n",
      "Batch: 12800,train loss is: 0.0007774359835869297\n",
      "test loss is 0.0012199693515360874\n",
      "Batch: 12900,train loss is: 0.0010683926840099156\n",
      "test loss is 0.000910895959440663\n",
      "Batch: 13000,train loss is: 0.001006527565876443\n",
      "test loss is 0.0007548768654757734\n",
      "Batch: 13100,train loss is: 0.0011335665308583784\n",
      "test loss is 0.0008127501216210954\n",
      "Batch: 13200,train loss is: 0.0004945022148937312\n",
      "test loss is 0.0007771507796847456\n",
      "Batch: 13300,train loss is: 0.000899751723587535\n",
      "test loss is 0.001290103332127909\n",
      "Batch: 13400,train loss is: 0.0009130957820693292\n",
      "test loss is 0.0006885493345971607\n",
      "Batch: 13500,train loss is: 0.00059307595139686\n",
      "test loss is 0.000783499561718573\n",
      "Batch: 13600,train loss is: 0.000978173721692744\n",
      "test loss is 0.0009108527832464705\n",
      "Batch: 13700,train loss is: 0.0013386626510584975\n",
      "test loss is 0.000692267057220804\n",
      "Batch: 13800,train loss is: 0.001014048063981341\n",
      "test loss is 0.0008326174504481013\n",
      "Batch: 13900,train loss is: 0.0012472353242984227\n",
      "test loss is 0.0009672066052462265\n",
      "Batch: 14000,train loss is: 0.0016427232391888612\n",
      "test loss is 0.0008212955260510177\n",
      "Batch: 14100,train loss is: 0.0008955458204486435\n",
      "test loss is 0.0007582454939039315\n",
      "Batch: 14200,train loss is: 0.0005592233873453146\n",
      "test loss is 0.0007824127380948736\n",
      "Batch: 14300,train loss is: 0.0007447606903795022\n",
      "test loss is 0.0007412927962252287\n",
      "Batch: 14400,train loss is: 0.00057367388520097\n",
      "test loss is 0.0010653127341796007\n",
      "Batch: 14500,train loss is: 0.0012150374614875122\n",
      "test loss is 0.0008942669828396193\n",
      "Batch: 14600,train loss is: 0.0005390884731065801\n",
      "test loss is 0.0007562202309517026\n",
      "Batch: 14700,train loss is: 0.0006193972272029835\n",
      "test loss is 0.0007351699028325078\n",
      "Batch: 14800,train loss is: 0.0012104123163723674\n",
      "test loss is 0.0007631511696430289\n",
      "Batch: 14900,train loss is: 0.0009680097920060194\n",
      "test loss is 0.0016382489787285367\n",
      "Batch: 15000,train loss is: 0.00044818083031288736\n",
      "test loss is 0.0007454149078149384\n",
      "Batch: 15100,train loss is: 0.0007809978364438966\n",
      "test loss is 0.0008414538346387042\n",
      "Batch: 15200,train loss is: 0.0007813210584275903\n",
      "test loss is 0.0007799488693888775\n",
      "Batch: 15300,train loss is: 0.0004924156020535923\n",
      "test loss is 0.0010280336337637713\n",
      "Batch: 15400,train loss is: 0.00115213211647121\n",
      "test loss is 0.0012290204691457615\n",
      "Batch: 15500,train loss is: 0.0009366982428911982\n",
      "test loss is 0.0008570895229382761\n",
      "Batch: 15600,train loss is: 0.0010051730525468421\n",
      "test loss is 0.0009636161248007741\n",
      "Batch: 15700,train loss is: 0.0007064295149517529\n",
      "test loss is 0.0007700353599334748\n",
      "Batch: 15800,train loss is: 0.0006674207147575335\n",
      "test loss is 0.000982721505210754\n",
      "Batch: 15900,train loss is: 0.0007507007177135073\n",
      "test loss is 0.0007366069710441534\n",
      "Batch: 16000,train loss is: 0.0010610757075555234\n",
      "test loss is 0.0007421498818181364\n",
      "Batch: 16100,train loss is: 0.00036802776568852966\n",
      "test loss is 0.0007896088135828383\n",
      "Batch: 16200,train loss is: 0.0006748152280915923\n",
      "test loss is 0.0007562986926714678\n",
      "Batch: 16300,train loss is: 0.0009605709040975002\n",
      "test loss is 0.0010282945304280443\n",
      "Batch: 16400,train loss is: 0.000699216748740215\n",
      "test loss is 0.0007588681777558249\n",
      "Batch: 16500,train loss is: 0.0009392113060265478\n",
      "test loss is 0.0010310581372595488\n",
      "Batch: 16600,train loss is: 0.0007494998777518442\n",
      "test loss is 0.0006795543336064197\n",
      "Batch: 16700,train loss is: 0.0021908410485275416\n",
      "test loss is 0.0014682702200195733\n",
      "Batch: 16800,train loss is: 0.0006961850627876201\n",
      "test loss is 0.0007385756034495395\n",
      "Batch: 16900,train loss is: 0.00132255803059209\n",
      "test loss is 0.0007470701533276232\n",
      "Batch: 17000,train loss is: 0.001093962889040096\n",
      "test loss is 0.0009903738949966756\n",
      "Batch: 17100,train loss is: 0.0008041612613710081\n",
      "test loss is 0.001044530513985429\n",
      "Batch: 17200,train loss is: 0.000579539400012567\n",
      "test loss is 0.0007152906498480154\n",
      "Batch: 17300,train loss is: 0.000575502884652735\n",
      "test loss is 0.0007675032929282959\n",
      "Batch: 17400,train loss is: 0.0007235420073732426\n",
      "test loss is 0.00167485570133897\n",
      "Batch: 17500,train loss is: 0.0008703582822171478\n",
      "test loss is 0.0007118125624720203\n",
      "Batch: 17600,train loss is: 0.0008777345297000881\n",
      "test loss is 0.0012085771130502852\n",
      "Batch: 17700,train loss is: 0.001326587505151649\n",
      "test loss is 0.000876265527338753\n",
      "Batch: 17800,train loss is: 0.0011091353734025335\n",
      "test loss is 0.0017395624867541516\n",
      "Batch: 17900,train loss is: 0.0004622512602158807\n",
      "test loss is 0.0007252593346411398\n",
      "Batch: 18000,train loss is: 0.0016633229223790233\n",
      "test loss is 0.0007005522953664157\n",
      "Batch: 18100,train loss is: 0.0005579912085481337\n",
      "test loss is 0.0013186122324280187\n",
      "Batch: 18200,train loss is: 0.0006724286964742209\n",
      "test loss is 0.0006814588894315477\n",
      "Batch: 18300,train loss is: 0.0008718395096193234\n",
      "test loss is 0.0009699842544829332\n",
      "Batch: 18400,train loss is: 0.0008172807369705535\n",
      "test loss is 0.0008080065025389451\n",
      "Batch: 18500,train loss is: 0.0006610246212069983\n",
      "test loss is 0.0010448557673355013\n",
      "Batch: 18600,train loss is: 0.0012352187691746885\n",
      "test loss is 0.0009877717726114272\n",
      "Batch: 18700,train loss is: 0.0008628196491461995\n",
      "test loss is 0.0007180905856084958\n",
      "Batch: 18800,train loss is: 0.000615338247330771\n",
      "test loss is 0.0006405640831577747\n",
      "Batch: 18900,train loss is: 0.0013587368984296388\n",
      "test loss is 0.0006862202932015239\n",
      "Batch: 19000,train loss is: 0.0007053341238944272\n",
      "test loss is 0.001413012072198071\n",
      "Batch: 19100,train loss is: 0.0008405831168077554\n",
      "test loss is 0.0007174615224687488\n",
      "Batch: 19200,train loss is: 0.0005719990238058028\n",
      "test loss is 0.0009512657166555971\n",
      "Batch: 19300,train loss is: 0.0008703322275773766\n",
      "test loss is 0.0007643189097116502\n",
      "Batch: 19400,train loss is: 0.0008924180931785953\n",
      "test loss is 0.0006940877948823021\n",
      "Batch: 19500,train loss is: 0.0010537646277794268\n",
      "test loss is 0.0006945735757593189\n",
      "Batch: 19600,train loss is: 0.0010874210246143178\n",
      "test loss is 0.0006304661354384068\n",
      "Batch: 19700,train loss is: 0.0008094384494213651\n",
      "test loss is 0.000776329989752606\n",
      "Batch: 19800,train loss is: 0.0008490170439203851\n",
      "test loss is 0.0009296757698196337\n",
      "Batch: 19900,train loss is: 0.001492372183104817\n",
      "test loss is 0.0010369156428627123\n",
      "Batch: 20000,train loss is: 0.0010501910530739582\n",
      "test loss is 0.000780054370688504\n",
      "Batch: 20100,train loss is: 0.0007350305723017173\n",
      "test loss is 0.0007033687334820975\n",
      "Batch: 20200,train loss is: 0.0008342987829828896\n",
      "test loss is 0.000759720507843216\n",
      "Batch: 20300,train loss is: 0.0007363574863126541\n",
      "test loss is 0.0007089383329769077\n",
      "Batch: 20400,train loss is: 0.0005956873852426271\n",
      "test loss is 0.0006965041944897084\n",
      "Batch: 20500,train loss is: 0.0008821178017449743\n",
      "test loss is 0.001089273538718496\n",
      "Batch: 20600,train loss is: 0.0006532353061801024\n",
      "test loss is 0.0009350994458800807\n",
      "Batch: 20700,train loss is: 0.0006582242666820602\n",
      "test loss is 0.0006219012873514922\n",
      "Batch: 20800,train loss is: 0.0006694536820048206\n",
      "test loss is 0.0007057796579652772\n",
      "Batch: 20900,train loss is: 0.0015394749763475604\n",
      "test loss is 0.0007140107729559429\n",
      "Batch: 21000,train loss is: 0.0005946956226425564\n",
      "test loss is 0.0006835748839538561\n",
      "Batch: 21100,train loss is: 0.000991741585079443\n",
      "test loss is 0.0006846321216335911\n",
      "Batch: 21200,train loss is: 0.0005992803660282691\n",
      "test loss is 0.0007412933782045402\n",
      "Batch: 21300,train loss is: 0.0010735404797858326\n",
      "test loss is 0.0006825854019264038\n",
      "Batch: 21400,train loss is: 0.0008814451691331118\n",
      "test loss is 0.0006797011087784551\n",
      "Batch: 21500,train loss is: 0.0008030093340013888\n",
      "test loss is 0.0006395367085374988\n",
      "Batch: 21600,train loss is: 0.0008874624832356961\n",
      "test loss is 0.000679559210069307\n",
      "Batch: 21700,train loss is: 0.0006263153587293414\n",
      "test loss is 0.0007040937077646236\n",
      "Batch: 21800,train loss is: 0.0009821130368498807\n",
      "test loss is 0.0006279635715058466\n",
      "Batch: 21900,train loss is: 0.0008110466967062575\n",
      "test loss is 0.0007823296657296143\n",
      "Batch: 22000,train loss is: 0.0006235812321659178\n",
      "test loss is 0.0007971730793433914\n",
      "Batch: 22100,train loss is: 0.0005632167399545294\n",
      "test loss is 0.0007777141142795984\n",
      "Batch: 22200,train loss is: 0.0010703937947508962\n",
      "test loss is 0.0006444694498449633\n",
      "Batch: 22300,train loss is: 0.0007387726889865707\n",
      "test loss is 0.0009113177032856354\n",
      "Batch: 22400,train loss is: 0.0008946855396868452\n",
      "test loss is 0.0014500601178700986\n",
      "Batch: 22500,train loss is: 0.0006891700184047367\n",
      "test loss is 0.0007811610926469185\n",
      "Batch: 22600,train loss is: 0.0007812331883177159\n",
      "test loss is 0.0007867471377307157\n",
      "Batch: 22700,train loss is: 0.0010357522815802756\n",
      "test loss is 0.000716208453094109\n",
      "Batch: 22800,train loss is: 0.00037863528309064435\n",
      "test loss is 0.0006932576117724417\n",
      "Batch: 22900,train loss is: 0.0010540176121249928\n",
      "test loss is 0.0007651709471086305\n",
      "Batch: 23000,train loss is: 0.0006028480221035725\n",
      "test loss is 0.0010602380051547395\n",
      "Batch: 23100,train loss is: 0.0006732586009727792\n",
      "test loss is 0.0007904934316630496\n",
      "Batch: 23200,train loss is: 0.0014609957142573808\n",
      "test loss is 0.0008543010962984951\n",
      "Batch: 23300,train loss is: 0.0007454490008557469\n",
      "test loss is 0.0007512049401267208\n",
      "Batch: 23400,train loss is: 0.0007734084218900472\n",
      "test loss is 0.0009455088485168674\n",
      "Batch: 23500,train loss is: 0.000693023968527636\n",
      "test loss is 0.0007449210224806783\n",
      "Batch: 23600,train loss is: 0.000596407246256643\n",
      "test loss is 0.0006731888301847895\n",
      "Batch: 23700,train loss is: 0.0006994055183019916\n",
      "test loss is 0.0006654292355735653\n",
      "Batch: 23800,train loss is: 0.0003875450975384582\n",
      "test loss is 0.000639876326700943\n",
      "Batch: 23900,train loss is: 0.0005491643340963792\n",
      "test loss is 0.0010510636342358154\n",
      "Batch: 24000,train loss is: 0.0002357862958742974\n",
      "test loss is 0.0008387134013308862\n",
      "Batch: 24100,train loss is: 0.001090379173452029\n",
      "test loss is 0.0007062883219778475\n",
      "Batch: 24200,train loss is: 0.0007709044975575068\n",
      "test loss is 0.0008210285029697611\n",
      "Batch: 24300,train loss is: 0.0005126512969471875\n",
      "test loss is 0.0008556250158460503\n",
      "Batch: 24400,train loss is: 0.0005099933703593103\n",
      "test loss is 0.000929305413800543\n",
      "Batch: 24500,train loss is: 0.0006225620349207166\n",
      "test loss is 0.0005936368283484557\n",
      "Batch: 24600,train loss is: 0.0007013805768247658\n",
      "test loss is 0.0006873549154672829\n",
      "Batch: 24700,train loss is: 0.0006347692401585156\n",
      "test loss is 0.0006947921244385974\n",
      "Batch: 24800,train loss is: 0.0007805935615437633\n",
      "test loss is 0.0006994388294080296\n",
      "Batch: 24900,train loss is: 0.00048304559467377055\n",
      "test loss is 0.0008632464857650121\n",
      "Batch: 25000,train loss is: 0.0008675342388874871\n",
      "test loss is 0.0007101930338802171\n",
      "Batch: 25100,train loss is: 0.000772331563360081\n",
      "test loss is 0.000675553614314159\n",
      "Batch: 25200,train loss is: 0.0011425426203217348\n",
      "test loss is 0.0006800041093300742\n",
      "Batch: 25300,train loss is: 0.0005368213741263148\n",
      "test loss is 0.0007309302519477647\n",
      "Batch: 25400,train loss is: 0.0006835867744214178\n",
      "test loss is 0.0010161185896469178\n",
      "Batch: 25500,train loss is: 0.0005571191211217921\n",
      "test loss is 0.0008540401217264622\n",
      "Batch: 25600,train loss is: 0.0013141682629107892\n",
      "test loss is 0.0007150778120507825\n",
      "Batch: 25700,train loss is: 0.001038535242992136\n",
      "test loss is 0.000791665635398858\n",
      "Batch: 25800,train loss is: 0.0006080147999746885\n",
      "test loss is 0.0006693993279277265\n",
      "Batch: 25900,train loss is: 0.0006250913205094394\n",
      "test loss is 0.0008540993678284474\n",
      "Batch: 26000,train loss is: 0.000564164208030536\n",
      "test loss is 0.0006235283382114722\n",
      "Batch: 26100,train loss is: 0.0006288907039052129\n",
      "test loss is 0.00064206478842123\n",
      "Batch: 26200,train loss is: 0.0016324880444838503\n",
      "test loss is 0.0007674552942867989\n",
      "Batch: 26300,train loss is: 0.0004513490527424736\n",
      "test loss is 0.001176006249573459\n",
      "Batch: 26400,train loss is: 0.0007282996812387229\n",
      "test loss is 0.0006218497166684294\n",
      "Batch: 26500,train loss is: 0.0008598834898629922\n",
      "test loss is 0.0007358354301993859\n",
      "Batch: 26600,train loss is: 0.0005111275481029711\n",
      "test loss is 0.0006178108658861567\n",
      "Batch: 26700,train loss is: 0.0006077216264196982\n",
      "test loss is 0.000712346483924351\n",
      "Batch: 26800,train loss is: 0.0009639474349828348\n",
      "test loss is 0.0023269470007317317\n",
      "Batch: 26900,train loss is: 0.0008957987750351493\n",
      "test loss is 0.00075274444479741\n",
      "Batch: 27000,train loss is: 0.0009087861041102725\n",
      "test loss is 0.0007885775594943009\n",
      "Batch: 27100,train loss is: 0.0006503354997125438\n",
      "test loss is 0.0009244333791109471\n",
      "Batch: 27200,train loss is: 0.0008355129152018202\n",
      "test loss is 0.0011363499183944885\n",
      "Batch: 27300,train loss is: 0.0013299006845825542\n",
      "test loss is 0.0009048405882972566\n",
      "Batch: 27400,train loss is: 0.0008440916006963752\n",
      "test loss is 0.000678762124190587\n",
      "Batch: 27500,train loss is: 0.0005196637019874008\n",
      "test loss is 0.001158485431806461\n",
      "Batch: 27600,train loss is: 0.0005230596834796908\n",
      "test loss is 0.000784753247176325\n",
      "Batch: 27700,train loss is: 0.0007736342601298196\n",
      "test loss is 0.0008978012698496559\n",
      "Batch: 27800,train loss is: 0.000737559318018099\n",
      "test loss is 0.0005948662423109854\n",
      "Batch: 27900,train loss is: 0.0004685861717282939\n",
      "test loss is 0.0008238470335935845\n",
      "Batch: 28000,train loss is: 0.0006243078471790826\n",
      "test loss is 0.0009482985211013355\n",
      "Batch: 28100,train loss is: 0.0008964612622037011\n",
      "test loss is 0.0007328709253808148\n",
      "Batch: 28200,train loss is: 0.0005548252161261926\n",
      "test loss is 0.000626872196096252\n",
      "Batch: 28300,train loss is: 0.0014968954970634375\n",
      "test loss is 0.0007431568743106795\n",
      "Batch: 28400,train loss is: 0.000709965980292746\n",
      "test loss is 0.0008134306490670078\n",
      "Batch: 28500,train loss is: 0.0006242714469057013\n",
      "test loss is 0.0007184541992241951\n",
      "Batch: 28600,train loss is: 0.0007641218354026732\n",
      "test loss is 0.0006419237145285665\n",
      "Batch: 28700,train loss is: 0.0008245854218214867\n",
      "test loss is 0.0007041793352418131\n",
      "Batch: 28800,train loss is: 0.0008744308834258447\n",
      "test loss is 0.0006974433925656583\n",
      "Batch: 28900,train loss is: 0.0020332456032248923\n",
      "test loss is 0.0010540966765099473\n",
      "Batch: 29000,train loss is: 0.0007265133296024591\n",
      "test loss is 0.0007000211458558749\n",
      "Batch: 29100,train loss is: 0.0008106901299985056\n",
      "test loss is 0.0007655131387130428\n",
      "Batch: 29200,train loss is: 0.0006971051594151173\n",
      "test loss is 0.0007433133491184542\n",
      "Batch: 29300,train loss is: 0.0004407227840991652\n",
      "test loss is 0.0005923725743159964\n",
      "Batch: 29400,train loss is: 0.0007668062660289415\n",
      "test loss is 0.0006502496545906362\n",
      "Batch: 29500,train loss is: 0.000575781301925264\n",
      "test loss is 0.0006325662172136955\n",
      "Batch: 29600,train loss is: 0.0006053769199924659\n",
      "test loss is 0.0007076528990047233\n",
      "Batch: 29700,train loss is: 0.0004828516976139772\n",
      "test loss is 0.0006022290234370934\n",
      "Batch: 29800,train loss is: 0.0012523967415240512\n",
      "test loss is 0.0007872699622776549\n",
      "Batch: 29900,train loss is: 0.000676763777858747\n",
      "test loss is 0.0008057822490423549\n",
      "Batch: 30000,train loss is: 0.0004372138965812888\n",
      "test loss is 0.00068311488112986\n",
      "Batch: 30100,train loss is: 0.0007535108703127578\n",
      "test loss is 0.0007092601389692487\n",
      "Batch: 30200,train loss is: 0.0009862924521318824\n",
      "test loss is 0.0005781456609542787\n",
      "Batch: 30300,train loss is: 0.0005331388740664955\n",
      "test loss is 0.0007895186609799638\n",
      "Batch: 30400,train loss is: 0.000576325205676333\n",
      "test loss is 0.0007101053290368584\n",
      "Batch: 30500,train loss is: 0.0007802675055161317\n",
      "test loss is 0.0010624569386839143\n",
      "Batch: 30600,train loss is: 0.0006041200232941728\n",
      "test loss is 0.0008430824815790469\n",
      "Batch: 30700,train loss is: 0.0003401469914405339\n",
      "test loss is 0.0005773079127702809\n",
      "Batch: 30800,train loss is: 0.001651159706811172\n",
      "test loss is 0.0006935682772276349\n",
      "Batch: 30900,train loss is: 0.00048001760527334166\n",
      "test loss is 0.0006726322498881679\n",
      "Batch: 31000,train loss is: 0.00043576832260670676\n",
      "test loss is 0.0007462930438244932\n",
      "Batch: 31100,train loss is: 0.0010078028577121421\n",
      "test loss is 0.00110452011892895\n",
      "Batch: 31200,train loss is: 0.00103097377616713\n",
      "test loss is 0.0005936550080105733\n",
      "Batch: 31300,train loss is: 0.0012293556054152855\n",
      "test loss is 0.0006008742566673103\n",
      "Batch: 31400,train loss is: 0.0006373477747568616\n",
      "test loss is 0.0005894870078176133\n",
      "Batch: 31500,train loss is: 0.0007859985345699966\n",
      "test loss is 0.0006840454883239038\n",
      "Batch: 31600,train loss is: 0.000545165835281428\n",
      "test loss is 0.000642551650535549\n",
      "Batch: 31700,train loss is: 0.0014374020260682196\n",
      "test loss is 0.0007479905554291962\n",
      "Batch: 31800,train loss is: 0.00041364954827740084\n",
      "test loss is 0.0005727315903611206\n",
      "Batch: 31900,train loss is: 0.0008964189312679944\n",
      "test loss is 0.0010033994080575223\n",
      "Batch: 32000,train loss is: 0.001168087197850785\n",
      "test loss is 0.0007257202082472513\n",
      "Batch: 32100,train loss is: 0.0011416803917272712\n",
      "test loss is 0.0013178567578970555\n",
      "Batch: 32200,train loss is: 0.0012744002133224333\n",
      "test loss is 0.0007867649583535098\n",
      "Batch: 32300,train loss is: 0.0005688207083592728\n",
      "test loss is 0.0007118848256068606\n",
      "Batch: 32400,train loss is: 0.0004154412376871168\n",
      "test loss is 0.0006583572107963281\n",
      "Batch: 32500,train loss is: 0.0008949865603900252\n",
      "test loss is 0.000790553292626203\n",
      "Batch: 32600,train loss is: 0.0006052523937571375\n",
      "test loss is 0.0006139064337559606\n",
      "Batch: 32700,train loss is: 0.0007744810240360202\n",
      "test loss is 0.0007869515661506735\n",
      "Batch: 32800,train loss is: 0.000777865363121411\n",
      "test loss is 0.0007772583124091921\n",
      "Batch: 32900,train loss is: 0.0011582494583614368\n",
      "test loss is 0.0009306716430146647\n",
      "Batch: 33000,train loss is: 0.0006223581119917652\n",
      "test loss is 0.0006628907820957671\n",
      "Batch: 33100,train loss is: 0.0005340765078068917\n",
      "test loss is 0.0006217428065797203\n",
      "Batch: 33200,train loss is: 0.00041642649804421347\n",
      "test loss is 0.0006449835980354349\n",
      "Batch: 33300,train loss is: 0.00043984618636994983\n",
      "test loss is 0.0006092726456879142\n",
      "Batch: 33400,train loss is: 0.000369895160606831\n",
      "test loss is 0.000624561271400404\n",
      "Batch: 33500,train loss is: 0.0006887999074648985\n",
      "test loss is 0.0006632994630473501\n",
      "Batch: 33600,train loss is: 0.0007323950550633424\n",
      "test loss is 0.0006027324936907485\n",
      "Batch: 33700,train loss is: 0.0006311541737843878\n",
      "test loss is 0.0010796121686949866\n",
      "Batch: 33800,train loss is: 0.0011696299500810877\n",
      "test loss is 0.000778305264255784\n",
      "Batch: 33900,train loss is: 0.0007314707037275694\n",
      "test loss is 0.000884703054048097\n",
      "Batch: 34000,train loss is: 0.0004105125172384106\n",
      "test loss is 0.0006087156512004308\n",
      "Batch: 34100,train loss is: 0.0005107260773750827\n",
      "test loss is 0.0007224895545927737\n",
      "Batch: 34200,train loss is: 0.0003695988969874737\n",
      "test loss is 0.0005852020988347217\n",
      "Batch: 34300,train loss is: 0.0007800330365739846\n",
      "test loss is 0.0009948018981621488\n",
      "Batch: 34400,train loss is: 0.0008882681368622767\n",
      "test loss is 0.0009821219318221627\n",
      "Batch: 34500,train loss is: 0.0005510466147503532\n",
      "test loss is 0.0008102809833701466\n",
      "Batch: 34600,train loss is: 0.0004911018015890468\n",
      "test loss is 0.0006357390509282698\n",
      "Batch: 34700,train loss is: 0.0009512187382962905\n",
      "test loss is 0.0006668264763620313\n",
      "Batch: 34800,train loss is: 0.0009874981745019652\n",
      "test loss is 0.0007926149187891114\n",
      "Batch: 34900,train loss is: 0.0006862019794851124\n",
      "test loss is 0.0006771221999673063\n",
      "Batch: 35000,train loss is: 0.0008205928837222848\n",
      "test loss is 0.0009306073157403009\n",
      "Batch: 35100,train loss is: 0.0005633306915494255\n",
      "test loss is 0.0006222445719998407\n",
      "Batch: 35200,train loss is: 0.0006388648153356445\n",
      "test loss is 0.000658605344371945\n",
      "Batch: 35300,train loss is: 0.0008582535410435958\n",
      "test loss is 0.0005967446356251331\n",
      "Batch: 35400,train loss is: 0.0006354087050853479\n",
      "test loss is 0.0005984144954467872\n",
      "Batch: 35500,train loss is: 0.0004946517154794924\n",
      "test loss is 0.0006032712385669165\n",
      "Batch: 35600,train loss is: 0.0009762859080339544\n",
      "test loss is 0.0006012397629969815\n",
      "Batch: 35700,train loss is: 0.0009404719393677506\n",
      "test loss is 0.0008743037781253147\n",
      "Batch: 35800,train loss is: 0.00056746620102154\n",
      "test loss is 0.0006835743534272826\n",
      "Batch: 35900,train loss is: 0.0008814743078425179\n",
      "test loss is 0.0006104650802766676\n",
      "Batch: 36000,train loss is: 0.0005756723707684265\n",
      "test loss is 0.0006427618276269291\n",
      "Batch: 36100,train loss is: 0.0009342591600319477\n",
      "test loss is 0.0005508087435458248\n",
      "Batch: 36200,train loss is: 0.0009119383745281906\n",
      "test loss is 0.0006347059515887798\n",
      "Batch: 36300,train loss is: 0.0004796043262485364\n",
      "test loss is 0.0007480882665778956\n",
      "Batch: 36400,train loss is: 0.0003940618938529949\n",
      "test loss is 0.0005905737504148838\n",
      "Batch: 36500,train loss is: 0.000525516066324967\n",
      "test loss is 0.0006194726618086624\n",
      "Batch: 36600,train loss is: 0.00042276264896440807\n",
      "test loss is 0.0007361213103652868\n",
      "Batch: 36700,train loss is: 0.0005683279489134556\n",
      "test loss is 0.0005728452741712697\n",
      "Batch: 36800,train loss is: 0.000651003259181651\n",
      "test loss is 0.0008330556749082814\n",
      "Batch: 36900,train loss is: 0.0009856952760309043\n",
      "test loss is 0.0018921769187650779\n",
      "Batch: 37000,train loss is: 0.0009547550412571818\n",
      "test loss is 0.0006165370973683006\n",
      "Batch: 37100,train loss is: 0.0007530168785087012\n",
      "test loss is 0.000699681927122501\n",
      "Batch: 37200,train loss is: 0.0005089897888826533\n",
      "test loss is 0.0006682523258740242\n",
      "Batch: 37300,train loss is: 0.000932061701550045\n",
      "test loss is 0.0007033234869012691\n",
      "Batch: 37400,train loss is: 0.0006105162965025236\n",
      "test loss is 0.0009355619706432305\n",
      "Batch: 37500,train loss is: 0.0004989591813344698\n",
      "test loss is 0.0005841905199903867\n",
      "Batch: 37600,train loss is: 0.0004594853497224933\n",
      "test loss is 0.0008438118929822728\n",
      "Batch: 37700,train loss is: 0.000995178386697941\n",
      "test loss is 0.0007424910015392334\n",
      "Batch: 37800,train loss is: 0.0004774873305595745\n",
      "test loss is 0.0006484602139106936\n",
      "Batch: 37900,train loss is: 0.0006449564584796162\n",
      "test loss is 0.0006352627803996636\n",
      "Batch: 38000,train loss is: 0.000538454577441659\n",
      "test loss is 0.0013746569658724958\n",
      "Batch: 38100,train loss is: 0.0007457165759683601\n",
      "test loss is 0.0013534714635662813\n",
      "Batch: 38200,train loss is: 0.0006304976760232988\n",
      "test loss is 0.0007815805068942186\n",
      "Batch: 38300,train loss is: 0.0008425155455163376\n",
      "test loss is 0.0006171722213364433\n",
      "Batch: 38400,train loss is: 0.0003603490337961967\n",
      "test loss is 0.0006480116390468639\n",
      "Batch: 38500,train loss is: 0.00352277298197355\n",
      "test loss is 0.0008997525829453316\n",
      "Batch: 38600,train loss is: 0.0012755916990788187\n",
      "test loss is 0.0008462373367963061\n",
      "Batch: 38700,train loss is: 0.0005253052941854323\n",
      "test loss is 0.0005641331575857709\n",
      "Batch: 38800,train loss is: 0.0004698419990892992\n",
      "test loss is 0.0007681117338675074\n",
      "Batch: 38900,train loss is: 0.00045751172857833516\n",
      "test loss is 0.0006756220639520972\n",
      "Batch: 39000,train loss is: 0.0006566960963613388\n",
      "test loss is 0.0006461900850687419\n",
      "Batch: 39100,train loss is: 0.0008223614421274532\n",
      "test loss is 0.0006201688750269933\n",
      "Batch: 39200,train loss is: 0.0007774371274390827\n",
      "test loss is 0.0006137536955222554\n",
      "Batch: 39300,train loss is: 0.0004351630601862655\n",
      "test loss is 0.0006841892029600026\n",
      "Batch: 39400,train loss is: 0.0009256368225917999\n",
      "test loss is 0.0005368465860693355\n",
      "Batch: 39500,train loss is: 0.0005013748045865805\n",
      "test loss is 0.0011850496840294946\n",
      "Batch: 39600,train loss is: 0.0006562653790624529\n",
      "test loss is 0.0006937013089320667\n",
      "Batch: 39700,train loss is: 0.00044233901198555267\n",
      "test loss is 0.0006710675169333014\n",
      "Batch: 39800,train loss is: 0.001573204101031425\n",
      "test loss is 0.0008596920513632648\n",
      "Batch: 39900,train loss is: 0.0006931556206387919\n",
      "test loss is 0.0005888597863905095\n",
      "Batch: 40000,train loss is: 0.001486517694326811\n",
      "test loss is 0.000632948445623878\n",
      "Batch: 40100,train loss is: 0.0005347275302141264\n",
      "test loss is 0.0005681069133712382\n",
      "Batch: 40200,train loss is: 0.000545007520982838\n",
      "test loss is 0.0006385132499316866\n",
      "Batch: 40300,train loss is: 0.0005489139728031395\n",
      "test loss is 0.0008966197674150792\n",
      "Batch: 40400,train loss is: 0.0009689060833359182\n",
      "test loss is 0.0008382563358004064\n",
      "Batch: 40500,train loss is: 0.0009680248384215764\n",
      "test loss is 0.0006372764989233117\n",
      "Batch: 40600,train loss is: 0.0005721155250468157\n",
      "test loss is 0.0006641789995390306\n",
      "Batch: 40700,train loss is: 0.000439324232299769\n",
      "test loss is 0.0006730043446122443\n",
      "Batch: 40800,train loss is: 0.0005900247038727764\n",
      "test loss is 0.0005426079929150042\n",
      "Batch: 40900,train loss is: 0.0007203639215247326\n",
      "test loss is 0.0007361072883932833\n",
      "Batch: 41000,train loss is: 0.0005819522395055285\n",
      "test loss is 0.0007213243973553782\n",
      "Batch: 41100,train loss is: 0.0007209953495954315\n",
      "test loss is 0.0006112264737142784\n",
      "Batch: 41200,train loss is: 0.0016978586444609573\n",
      "test loss is 0.001101904769946119\n",
      "Batch: 41300,train loss is: 0.0008261202359531033\n",
      "test loss is 0.0011498752230393713\n",
      "Batch: 41400,train loss is: 0.00043154090011443763\n",
      "test loss is 0.0007239083223444134\n",
      "Batch: 41500,train loss is: 0.001070559307264077\n",
      "test loss is 0.0006547699436485686\n",
      "Batch: 41600,train loss is: 0.0006820914010894846\n",
      "test loss is 0.0006110300610162082\n",
      "Batch: 41700,train loss is: 0.0005596184024708668\n",
      "test loss is 0.0007357184677516762\n",
      "Batch: 41800,train loss is: 0.0005838985015112868\n",
      "test loss is 0.0007873563561012485\n",
      "Batch: 41900,train loss is: 0.0005844050943056777\n",
      "test loss is 0.0006009145540243542\n",
      "Batch: 42000,train loss is: 0.0006547108263277438\n",
      "test loss is 0.0005681613804821548\n",
      "Batch: 42100,train loss is: 0.0005243153605079771\n",
      "test loss is 0.0008294455852210909\n",
      "Batch: 42200,train loss is: 0.0005220014083482426\n",
      "test loss is 0.0005554254406097598\n",
      "Batch: 42300,train loss is: 0.001339720413801075\n",
      "test loss is 0.0005907303385688845\n",
      "Batch: 42400,train loss is: 0.0011799099819694596\n",
      "test loss is 0.0006378992257880249\n",
      "Batch: 42500,train loss is: 0.0006135199334116376\n",
      "test loss is 0.0006299530958216751\n",
      "Batch: 42600,train loss is: 0.0005459383722852527\n",
      "test loss is 0.0007303209151443813\n",
      "Batch: 42700,train loss is: 0.0008166441751548427\n",
      "test loss is 0.0006375042761266737\n",
      "Batch: 42800,train loss is: 0.0005090204808632296\n",
      "test loss is 0.0008936590705453227\n",
      "Batch: 42900,train loss is: 0.0005426690118541137\n",
      "test loss is 0.0007557932445866909\n",
      "Batch: 43000,train loss is: 0.00046316654638501806\n",
      "test loss is 0.0007863784415277315\n",
      "Batch: 43100,train loss is: 0.0008300614616606029\n",
      "test loss is 0.0005915030540550622\n",
      "Batch: 43200,train loss is: 0.0004390642570022777\n",
      "test loss is 0.0005543355801021625\n",
      "Batch: 43300,train loss is: 0.00044465819727206203\n",
      "test loss is 0.0007582319157855231\n",
      "Batch: 43400,train loss is: 0.0005852028491553877\n",
      "test loss is 0.0005650243278387298\n",
      "Batch: 43500,train loss is: 0.0006641445444615341\n",
      "test loss is 0.0006104415591692398\n",
      "Batch: 43600,train loss is: 0.000794698529624728\n",
      "test loss is 0.0012851979738609616\n",
      "Batch: 43700,train loss is: 0.0006281820605534604\n",
      "test loss is 0.0005571261672025793\n",
      "Batch: 43800,train loss is: 0.0008723260464268923\n",
      "test loss is 0.0008310895618551125\n",
      "Batch: 43900,train loss is: 0.0008340553624483519\n",
      "test loss is 0.0006396837105426986\n",
      "Batch: 44000,train loss is: 0.0007069119320961976\n",
      "test loss is 0.0006150402347593272\n",
      "Batch: 44100,train loss is: 0.0009387002361814723\n",
      "test loss is 0.0005808604801976235\n",
      "Batch: 44200,train loss is: 0.0003511073105181431\n",
      "test loss is 0.0006859008414198297\n",
      "Batch: 44300,train loss is: 0.0012328218246389119\n",
      "test loss is 0.0008443176586005704\n",
      "Batch: 44400,train loss is: 0.0014645340307310704\n",
      "test loss is 0.0006974217484430702\n",
      "Batch: 44500,train loss is: 0.0004628546580063901\n",
      "test loss is 0.0005378783849280569\n",
      "Batch: 44600,train loss is: 0.0007789606206146333\n",
      "test loss is 0.0007426591597566592\n",
      "Batch: 44700,train loss is: 0.0005331168718399244\n",
      "test loss is 0.0005866907412381812\n",
      "Batch: 44800,train loss is: 0.0011081860764964977\n",
      "test loss is 0.000663882848046276\n",
      "Batch: 44900,train loss is: 0.0005362162759170288\n",
      "test loss is 0.0005798831678635459\n",
      "Batch: 45000,train loss is: 0.0007057090645918138\n",
      "test loss is 0.0005876638527503661\n",
      "Batch: 45100,train loss is: 0.0005790745141439381\n",
      "test loss is 0.0007457599807873883\n",
      "Batch: 45200,train loss is: 0.0006092364857780109\n",
      "test loss is 0.0008605483366866424\n",
      "Batch: 45300,train loss is: 0.0006569325192455318\n",
      "test loss is 0.0006724354408456181\n",
      "Batch: 45400,train loss is: 0.0005564013225384801\n",
      "test loss is 0.0006293561654044459\n",
      "Batch: 45500,train loss is: 0.0004454206566894033\n",
      "test loss is 0.0006844536021987163\n",
      "Batch: 45600,train loss is: 0.0006621516120658791\n",
      "test loss is 0.000815692211588083\n",
      "Batch: 45700,train loss is: 0.0006717538573206326\n",
      "test loss is 0.0006353769781826584\n",
      "Batch: 45800,train loss is: 0.0006569299724342629\n",
      "test loss is 0.0006457158865300302\n",
      "Batch: 45900,train loss is: 0.0006903213834544001\n",
      "test loss is 0.0005941072197237735\n",
      "Batch: 46000,train loss is: 0.0007280774841456701\n",
      "test loss is 0.0007975796018835935\n",
      "Batch: 46100,train loss is: 0.0005686864669226033\n",
      "test loss is 0.000674398838123592\n",
      "Batch: 46200,train loss is: 0.0009886100304702511\n",
      "test loss is 0.0009936370857103983\n",
      "Batch: 46300,train loss is: 0.0007596706502058081\n",
      "test loss is 0.0006213245233653881\n",
      "Batch: 46400,train loss is: 0.0008233581774632349\n",
      "test loss is 0.0007402675327268985\n",
      "Batch: 46500,train loss is: 0.0006772517959378972\n",
      "test loss is 0.0008012299253853472\n",
      "Batch: 46600,train loss is: 0.0005367044298618766\n",
      "test loss is 0.0007305348862835942\n",
      "Batch: 46700,train loss is: 0.0005693381444569631\n",
      "test loss is 0.0005617864699423954\n",
      "-----------------------Epoch: 3----------------------------------\n",
      "Batch: 0,train loss is: 0.000884843682309887\n",
      "test loss is 0.0006385136851895323\n",
      "Batch: 100,train loss is: 0.0006093113063056023\n",
      "test loss is 0.0007550680327078756\n",
      "Batch: 200,train loss is: 0.0004403136425799773\n",
      "test loss is 0.0009340884755782267\n",
      "Batch: 300,train loss is: 0.0005326713457340501\n",
      "test loss is 0.0006921029694139607\n",
      "Batch: 400,train loss is: 0.0008596154292646401\n",
      "test loss is 0.0006777772340652662\n",
      "Batch: 500,train loss is: 0.0008638375717268554\n",
      "test loss is 0.0007001501880185754\n",
      "Batch: 600,train loss is: 0.0004562095115697557\n",
      "test loss is 0.0012034757681259557\n",
      "Batch: 700,train loss is: 0.0007764328417227784\n",
      "test loss is 0.0007369352905110878\n",
      "Batch: 800,train loss is: 0.000861028849585927\n",
      "test loss is 0.0009903068004435789\n",
      "Batch: 900,train loss is: 0.0005124399805998658\n",
      "test loss is 0.0006943170105420209\n",
      "Batch: 1000,train loss is: 0.0008970764640420158\n",
      "test loss is 0.0006175370479930889\n",
      "Batch: 1100,train loss is: 0.0003866931045725012\n",
      "test loss is 0.0005563130000458587\n",
      "Batch: 1200,train loss is: 0.0005708204630785601\n",
      "test loss is 0.0005877415079862025\n",
      "Batch: 1300,train loss is: 0.0004918684385125797\n",
      "test loss is 0.0005866987612536159\n",
      "Batch: 1400,train loss is: 0.0005110270370419017\n",
      "test loss is 0.0005752844654213672\n",
      "Batch: 1500,train loss is: 0.00147542098991979\n",
      "test loss is 0.0008562299864146571\n",
      "Batch: 1600,train loss is: 0.0006988589414054693\n",
      "test loss is 0.0005907198648748018\n",
      "Batch: 1700,train loss is: 0.0004921334335605173\n",
      "test loss is 0.000796891212322933\n",
      "Batch: 1800,train loss is: 0.0009753118337762416\n",
      "test loss is 0.0005975569643919271\n",
      "Batch: 1900,train loss is: 0.00041778286533108566\n",
      "test loss is 0.0007404886392456844\n",
      "Batch: 2000,train loss is: 0.000643163637751338\n",
      "test loss is 0.0007664296272885215\n",
      "Batch: 2100,train loss is: 0.0004949276246657587\n",
      "test loss is 0.0007191885471647508\n",
      "Batch: 2200,train loss is: 0.0006530625074458793\n",
      "test loss is 0.0008997450095147271\n",
      "Batch: 2300,train loss is: 0.0005017768704279344\n",
      "test loss is 0.0005569389096635655\n",
      "Batch: 2400,train loss is: 0.0006819270710736692\n",
      "test loss is 0.0006019292793578936\n",
      "Batch: 2500,train loss is: 0.00093757310129725\n",
      "test loss is 0.0007058096337520637\n",
      "Batch: 2600,train loss is: 0.0003291258040294218\n",
      "test loss is 0.0005556067071425878\n",
      "Batch: 2700,train loss is: 0.0006064744631239334\n",
      "test loss is 0.0005927433268073064\n",
      "Batch: 2800,train loss is: 0.000590112966016331\n",
      "test loss is 0.0009402468708719359\n",
      "Batch: 2900,train loss is: 0.0005042903301655108\n",
      "test loss is 0.0005516406704801748\n",
      "Batch: 3000,train loss is: 0.0004425701123927996\n",
      "test loss is 0.0007477085914386957\n",
      "Batch: 3100,train loss is: 0.0009150985710946029\n",
      "test loss is 0.0006795123540488607\n",
      "Batch: 3200,train loss is: 0.0007278842014113514\n",
      "test loss is 0.0006398324006739671\n",
      "Batch: 3300,train loss is: 0.000486067242995375\n",
      "test loss is 0.0011269257880344903\n",
      "Batch: 3400,train loss is: 0.0007724385190580479\n",
      "test loss is 0.0005977166920963889\n",
      "Batch: 3500,train loss is: 0.0006601860142751404\n",
      "test loss is 0.0006573535671692807\n",
      "Batch: 3600,train loss is: 0.0004541671398953745\n",
      "test loss is 0.0005865748786038972\n",
      "Batch: 3700,train loss is: 0.0007585163789736119\n",
      "test loss is 0.0010837049938197695\n",
      "Batch: 3800,train loss is: 0.0005736090311676002\n",
      "test loss is 0.0006557999225346872\n",
      "Batch: 3900,train loss is: 0.0005068465560553682\n",
      "test loss is 0.000582794260943923\n",
      "Batch: 4000,train loss is: 0.0007719066477020456\n",
      "test loss is 0.0010483313076217577\n",
      "Batch: 4100,train loss is: 0.0005068387887811833\n",
      "test loss is 0.0005517498280459419\n",
      "Batch: 4200,train loss is: 0.0014593467607964309\n",
      "test loss is 0.0006745482030394045\n",
      "Batch: 4300,train loss is: 0.000780175263140475\n",
      "test loss is 0.0007940689370817388\n",
      "Batch: 4400,train loss is: 0.000511809313333148\n",
      "test loss is 0.000773511982358152\n",
      "Batch: 4500,train loss is: 0.0007927733675045242\n",
      "test loss is 0.000530808347276956\n",
      "Batch: 4600,train loss is: 0.0005441661157592127\n",
      "test loss is 0.0006149353278690748\n",
      "Batch: 4700,train loss is: 0.000500319760772664\n",
      "test loss is 0.0005392548059731921\n",
      "Batch: 4800,train loss is: 0.000660602100531495\n",
      "test loss is 0.0006276881358347276\n",
      "Batch: 4900,train loss is: 0.0005950449120609414\n",
      "test loss is 0.0007919436278360679\n",
      "Batch: 5000,train loss is: 0.0008587373105671875\n",
      "test loss is 0.0011239638079417393\n",
      "Batch: 5100,train loss is: 0.0009449850544486244\n",
      "test loss is 0.000591072386093282\n",
      "Batch: 5200,train loss is: 0.0011337509270353545\n",
      "test loss is 0.0007946998409698645\n",
      "Batch: 5300,train loss is: 0.00040797284374248635\n",
      "test loss is 0.0010287246307520443\n",
      "Batch: 5400,train loss is: 0.0011150314182689677\n",
      "test loss is 0.0006588513694907569\n",
      "Batch: 5500,train loss is: 0.0010477459967470006\n",
      "test loss is 0.0007174484802902671\n",
      "Batch: 5600,train loss is: 0.00043421320027111265\n",
      "test loss is 0.0005737359791778741\n",
      "Batch: 5700,train loss is: 0.0007817018635184094\n",
      "test loss is 0.0006748747451659976\n",
      "Batch: 5800,train loss is: 0.00039081453010238876\n",
      "test loss is 0.0005189977294055072\n",
      "Batch: 5900,train loss is: 0.0008055417430699409\n",
      "test loss is 0.0007738200346480686\n",
      "Batch: 6000,train loss is: 0.0004889706963078198\n",
      "test loss is 0.0006415570312894346\n",
      "Batch: 6100,train loss is: 0.0008384150194662749\n",
      "test loss is 0.0007049182106801695\n",
      "Batch: 6200,train loss is: 0.0009449779506325581\n",
      "test loss is 0.0006763562442740283\n",
      "Batch: 6300,train loss is: 0.0004020468555430325\n",
      "test loss is 0.0005358579153890538\n",
      "Batch: 6400,train loss is: 0.001096667475465276\n",
      "test loss is 0.0007411070881621934\n",
      "Batch: 6500,train loss is: 0.000489642908398659\n",
      "test loss is 0.0007070396880865572\n",
      "Batch: 6600,train loss is: 0.0004130218387406694\n",
      "test loss is 0.0005923121757691127\n",
      "Batch: 6700,train loss is: 0.0003755969073090398\n",
      "test loss is 0.000590502549596104\n",
      "Batch: 6800,train loss is: 0.0005017021760733223\n",
      "test loss is 0.0005754357264806658\n",
      "Batch: 6900,train loss is: 0.0012259497372929427\n",
      "test loss is 0.00098698177114332\n",
      "Batch: 7000,train loss is: 0.0005199468529316229\n",
      "test loss is 0.0006092331843352964\n",
      "Batch: 7100,train loss is: 0.0014275491169944681\n",
      "test loss is 0.0008104897181890654\n",
      "Batch: 7200,train loss is: 0.0007899823215715468\n",
      "test loss is 0.0010026480416866644\n",
      "Batch: 7300,train loss is: 0.0007298691592034904\n",
      "test loss is 0.0007273520407628798\n",
      "Batch: 7400,train loss is: 0.0006929031646684069\n",
      "test loss is 0.0005348700183457777\n",
      "Batch: 7500,train loss is: 0.0007794276632011015\n",
      "test loss is 0.0005596136741808121\n",
      "Batch: 7600,train loss is: 0.0008085911851245277\n",
      "test loss is 0.0005481415032852937\n",
      "Batch: 7700,train loss is: 0.0005194247407105712\n",
      "test loss is 0.0005294027008140671\n",
      "Batch: 7800,train loss is: 0.0005456730252648221\n",
      "test loss is 0.0007337637387756039\n",
      "Batch: 7900,train loss is: 0.0006622663709994242\n",
      "test loss is 0.0006113273562339252\n",
      "Batch: 8000,train loss is: 0.000456084642260656\n",
      "test loss is 0.0007414704353204735\n",
      "Batch: 8100,train loss is: 0.0008613859531131469\n",
      "test loss is 0.0006428446725222721\n",
      "Batch: 8200,train loss is: 0.000562525268472307\n",
      "test loss is 0.0006092513373397626\n",
      "Batch: 8300,train loss is: 0.0005690678876663893\n",
      "test loss is 0.0005814969261201193\n",
      "Batch: 8400,train loss is: 0.0004356572753030102\n",
      "test loss is 0.0005306626592288219\n",
      "Batch: 8500,train loss is: 0.0007328535240749695\n",
      "test loss is 0.0006164246245471209\n",
      "Batch: 8600,train loss is: 0.0006746060948758673\n",
      "test loss is 0.0006421580109755288\n",
      "Batch: 8700,train loss is: 0.0007785667328778918\n",
      "test loss is 0.0007764512364311201\n",
      "Batch: 8800,train loss is: 0.00025158738275393125\n",
      "test loss is 0.0006016286826305606\n",
      "Batch: 8900,train loss is: 0.0006332797583357332\n",
      "test loss is 0.0009698829070273797\n",
      "Batch: 9000,train loss is: 0.0008198087148118887\n",
      "test loss is 0.000561927594558849\n",
      "Batch: 9100,train loss is: 0.0007383151811327953\n",
      "test loss is 0.0006509292152357495\n",
      "Batch: 9200,train loss is: 0.0005156318441018153\n",
      "test loss is 0.0005856394073045642\n",
      "Batch: 9300,train loss is: 0.0007391440182169344\n",
      "test loss is 0.0006783645011788596\n",
      "Batch: 9400,train loss is: 0.0007660738707014798\n",
      "test loss is 0.0015429112676709033\n",
      "Batch: 9500,train loss is: 0.0006467190074793759\n",
      "test loss is 0.0004966174087211709\n",
      "Batch: 9600,train loss is: 0.0005477104535645882\n",
      "test loss is 0.0007395507774770213\n",
      "Batch: 9700,train loss is: 0.0007517033447288954\n",
      "test loss is 0.0006565810470738964\n",
      "Batch: 9800,train loss is: 0.0018299238609790348\n",
      "test loss is 0.000639586834439668\n",
      "Batch: 9900,train loss is: 0.00043080109067408453\n",
      "test loss is 0.0005063972949943334\n",
      "Batch: 10000,train loss is: 0.0007139645978231214\n",
      "test loss is 0.000512084733409159\n",
      "Batch: 10100,train loss is: 0.0005838193035858232\n",
      "test loss is 0.0006301708695257121\n",
      "Batch: 10200,train loss is: 0.0007485675883236587\n",
      "test loss is 0.0007264620570032031\n",
      "Batch: 10300,train loss is: 0.0009688577170035472\n",
      "test loss is 0.0005420796757309882\n",
      "Batch: 10400,train loss is: 0.00048228286701192233\n",
      "test loss is 0.0005530231262634079\n",
      "Batch: 10500,train loss is: 0.0006177531138322422\n",
      "test loss is 0.0005388084716432218\n",
      "Batch: 10600,train loss is: 0.0005854392778365423\n",
      "test loss is 0.0006617825924375032\n",
      "Batch: 10700,train loss is: 0.0008841492562130812\n",
      "test loss is 0.000559170210609457\n",
      "Batch: 10800,train loss is: 0.0005638861514042348\n",
      "test loss is 0.0006502818800298661\n",
      "Batch: 10900,train loss is: 0.0003975659561397554\n",
      "test loss is 0.0009096381999225977\n",
      "Batch: 11000,train loss is: 0.0007231369357375407\n",
      "test loss is 0.0005893742135138076\n",
      "Batch: 11100,train loss is: 0.00045901398462076473\n",
      "test loss is 0.0005206179339429885\n",
      "Batch: 11200,train loss is: 0.0006246951660964783\n",
      "test loss is 0.0005683250549608812\n",
      "Batch: 11300,train loss is: 0.0004971572238823222\n",
      "test loss is 0.0006345635016560438\n",
      "Batch: 11400,train loss is: 0.0006906276936971851\n",
      "test loss is 0.0008954926314217074\n",
      "Batch: 11500,train loss is: 0.0007854821525315428\n",
      "test loss is 0.0006176490543073931\n",
      "Batch: 11600,train loss is: 0.00038837919628055973\n",
      "test loss is 0.0005364922037918411\n",
      "Batch: 11700,train loss is: 0.00064690933298889\n",
      "test loss is 0.0005742827260071929\n",
      "Batch: 11800,train loss is: 0.000622574128010821\n",
      "test loss is 0.0006016230672164016\n",
      "Batch: 11900,train loss is: 0.0006020106845451607\n",
      "test loss is 0.0005367556153184479\n",
      "Batch: 12000,train loss is: 0.0005712417667326092\n",
      "test loss is 0.0005532750061802311\n",
      "Batch: 12100,train loss is: 0.0008030099842560251\n",
      "test loss is 0.0005577018181135901\n",
      "Batch: 12200,train loss is: 0.00039931332208630336\n",
      "test loss is 0.0005974887478438479\n",
      "Batch: 12300,train loss is: 0.0008456988474454337\n",
      "test loss is 0.0005858980247553663\n",
      "Batch: 12400,train loss is: 0.0006269358691528938\n",
      "test loss is 0.0005894638806677192\n",
      "Batch: 12500,train loss is: 0.0005724992367677371\n",
      "test loss is 0.0009373822659150552\n",
      "Batch: 12600,train loss is: 0.001444651520462191\n",
      "test loss is 0.000670726119294267\n",
      "Batch: 12700,train loss is: 0.000652832389052408\n",
      "test loss is 0.0007429155378907454\n",
      "Batch: 12800,train loss is: 0.002430632639205698\n",
      "test loss is 0.0005913851802366215\n",
      "Batch: 12900,train loss is: 0.00034877565266042566\n",
      "test loss is 0.0008105623840834046\n",
      "Batch: 13000,train loss is: 0.000769532161011054\n",
      "test loss is 0.0005540955237413164\n",
      "Batch: 13100,train loss is: 0.00043255831648481317\n",
      "test loss is 0.0006007646873487362\n",
      "Batch: 13200,train loss is: 0.0007791711131334411\n",
      "test loss is 0.0005287977491791304\n",
      "Batch: 13300,train loss is: 0.0004302858201846111\n",
      "test loss is 0.0008302162453689215\n",
      "Batch: 13400,train loss is: 0.0007143148880786642\n",
      "test loss is 0.0006443830980841705\n",
      "Batch: 13500,train loss is: 0.0005952748222198467\n",
      "test loss is 0.0006046624351988616\n",
      "Batch: 13600,train loss is: 0.000754874928144023\n",
      "test loss is 0.000561921068501388\n",
      "Batch: 13700,train loss is: 0.0010361328033404002\n",
      "test loss is 0.0007381178989343347\n",
      "Batch: 13800,train loss is: 0.0005748173164922511\n",
      "test loss is 0.0005928959595831755\n",
      "Batch: 13900,train loss is: 0.00039863817482649534\n",
      "test loss is 0.0005787462237042619\n",
      "Batch: 14000,train loss is: 0.000694321111142487\n",
      "test loss is 0.0008364903557197997\n",
      "Batch: 14100,train loss is: 0.0006222210423793429\n",
      "test loss is 0.000597762074073309\n",
      "Batch: 14200,train loss is: 0.000620479472894713\n",
      "test loss is 0.0005545933033707528\n",
      "Batch: 14300,train loss is: 0.0004951388606274427\n",
      "test loss is 0.0007216319295742913\n",
      "Batch: 14400,train loss is: 0.0007852749616598686\n",
      "test loss is 0.0009590297505353112\n",
      "Batch: 14500,train loss is: 0.0005454406347560456\n",
      "test loss is 0.0006140144908959749\n",
      "Batch: 14600,train loss is: 0.0008356933856040699\n",
      "test loss is 0.0005543739002695316\n",
      "Batch: 14700,train loss is: 0.0005262579765481481\n",
      "test loss is 0.0006555418192790433\n",
      "Batch: 14800,train loss is: 0.0005910846390591869\n",
      "test loss is 0.0005675292328536523\n",
      "Batch: 14900,train loss is: 0.0008009724516161727\n",
      "test loss is 0.0011213055590320504\n",
      "Batch: 15000,train loss is: 0.0008197258129760739\n",
      "test loss is 0.0006082134580099907\n",
      "Batch: 15100,train loss is: 0.0005185458794126993\n",
      "test loss is 0.0004914375902940007\n",
      "Batch: 15200,train loss is: 0.00043369823454407437\n",
      "test loss is 0.0005436880622646279\n",
      "Batch: 15300,train loss is: 0.00033612159643614276\n",
      "test loss is 0.0006093605841690341\n",
      "Batch: 15400,train loss is: 0.0005655838095367853\n",
      "test loss is 0.0006181029634774828\n",
      "Batch: 15500,train loss is: 0.0003783287174383694\n",
      "test loss is 0.0005377225254173844\n",
      "Batch: 15600,train loss is: 0.000239034677991369\n",
      "test loss is 0.0007843273615404609\n",
      "Batch: 15700,train loss is: 0.0007161243550418161\n",
      "test loss is 0.0006369535294345627\n",
      "Batch: 15800,train loss is: 0.0008467827512575185\n",
      "test loss is 0.0007233702837322385\n",
      "Batch: 15900,train loss is: 0.0008739466986509149\n",
      "test loss is 0.0006201502678480275\n",
      "Batch: 16000,train loss is: 0.0004543238015437163\n",
      "test loss is 0.0005259466653912672\n",
      "Batch: 16100,train loss is: 0.0005875640038955619\n",
      "test loss is 0.0005159572001514639\n",
      "Batch: 16200,train loss is: 0.0005527217314310139\n",
      "test loss is 0.0006418041959884457\n",
      "Batch: 16300,train loss is: 0.0008043904685268121\n",
      "test loss is 0.0008743767662086516\n",
      "Batch: 16400,train loss is: 0.0009458186866418733\n",
      "test loss is 0.0007265704454660149\n",
      "Batch: 16500,train loss is: 0.0006159519037637369\n",
      "test loss is 0.000543229599435556\n",
      "Batch: 16600,train loss is: 0.00037373021362865384\n",
      "test loss is 0.0005416487223447819\n",
      "Batch: 16700,train loss is: 0.0005287034233458184\n",
      "test loss is 0.0007213012283437592\n",
      "Batch: 16800,train loss is: 0.000563226762664965\n",
      "test loss is 0.0006207005382353168\n",
      "Batch: 16900,train loss is: 0.0005947514432033\n",
      "test loss is 0.0008330827850207074\n",
      "Batch: 17000,train loss is: 0.0006332668235321346\n",
      "test loss is 0.0005743718077036883\n",
      "Batch: 17100,train loss is: 0.000414747451113635\n",
      "test loss is 0.0005971113537225804\n",
      "Batch: 17200,train loss is: 0.0010940196713138798\n",
      "test loss is 0.0005424342968198141\n",
      "Batch: 17300,train loss is: 0.0003698936741978517\n",
      "test loss is 0.0006473177289597548\n",
      "Batch: 17400,train loss is: 0.0008811920437077224\n",
      "test loss is 0.0007774998688843961\n",
      "Batch: 17500,train loss is: 0.0004785554530566776\n",
      "test loss is 0.000564430348354952\n",
      "Batch: 17600,train loss is: 0.0015858507898556555\n",
      "test loss is 0.0005177604406765237\n",
      "Batch: 17700,train loss is: 0.0005854912563351003\n",
      "test loss is 0.0005720339460678876\n",
      "Batch: 17800,train loss is: 0.0005417186125633322\n",
      "test loss is 0.0006756697368066247\n",
      "Batch: 17900,train loss is: 0.000915724748100005\n",
      "test loss is 0.000558895631672756\n",
      "Batch: 18000,train loss is: 0.000554596560026919\n",
      "test loss is 0.0006139565195085002\n",
      "Batch: 18100,train loss is: 0.00055912266022137\n",
      "test loss is 0.0005977717037233803\n",
      "Batch: 18200,train loss is: 0.00041963824411640424\n",
      "test loss is 0.0005456057770128122\n",
      "Batch: 18300,train loss is: 0.000783594951565576\n",
      "test loss is 0.0005700246951249189\n",
      "Batch: 18400,train loss is: 0.0005882755340745773\n",
      "test loss is 0.0007110395915730962\n",
      "Batch: 18500,train loss is: 0.0009224069756824322\n",
      "test loss is 0.0006225965904674537\n",
      "Batch: 18600,train loss is: 0.0004971496687911201\n",
      "test loss is 0.0005334885915919532\n",
      "Batch: 18700,train loss is: 0.00041151423482414323\n",
      "test loss is 0.0005807418361318111\n",
      "Batch: 18800,train loss is: 0.0006572468882058888\n",
      "test loss is 0.0006002657570111285\n",
      "Batch: 18900,train loss is: 0.0007857861754573439\n",
      "test loss is 0.0010104509297043564\n",
      "Batch: 19000,train loss is: 0.000778437738294123\n",
      "test loss is 0.0005407732415413292\n",
      "Batch: 19100,train loss is: 0.00044907454422307816\n",
      "test loss is 0.0005757596222375507\n",
      "Batch: 19200,train loss is: 0.00043121788418483864\n",
      "test loss is 0.00048039832998057514\n",
      "Batch: 19300,train loss is: 0.00046722414518867534\n",
      "test loss is 0.0006204597496060242\n",
      "Batch: 19400,train loss is: 0.0010107991182527034\n",
      "test loss is 0.0005586061020355537\n",
      "Batch: 19500,train loss is: 0.0008157752475486562\n",
      "test loss is 0.000790970724450246\n",
      "Batch: 19600,train loss is: 0.0007989179601325313\n",
      "test loss is 0.0010819775217185208\n",
      "Batch: 19700,train loss is: 0.0007120157557932581\n",
      "test loss is 0.0006991926902675047\n",
      "Batch: 19800,train loss is: 0.0007595098945728652\n",
      "test loss is 0.0006499197830085064\n",
      "Batch: 19900,train loss is: 0.0003801642732894759\n",
      "test loss is 0.0006182040312460971\n",
      "Batch: 20000,train loss is: 0.0005413562655639697\n",
      "test loss is 0.0005363103519334997\n",
      "Batch: 20100,train loss is: 0.0007700954327272761\n",
      "test loss is 0.0005057456978818072\n",
      "Batch: 20200,train loss is: 0.00036259407174897025\n",
      "test loss is 0.0005697051609540435\n",
      "Batch: 20300,train loss is: 0.00045729190739269914\n",
      "test loss is 0.0005576166750672862\n",
      "Batch: 20400,train loss is: 0.0002468120981846018\n",
      "test loss is 0.000575734815348738\n",
      "Batch: 20500,train loss is: 0.0005322396187566413\n",
      "test loss is 0.00074840511247501\n",
      "Batch: 20600,train loss is: 0.0005608015170226416\n",
      "test loss is 0.0006793666666489156\n",
      "Batch: 20700,train loss is: 0.0003859885089288882\n",
      "test loss is 0.0005682851350785605\n",
      "Batch: 20800,train loss is: 0.0005440843717008628\n",
      "test loss is 0.000503611102242503\n",
      "Batch: 20900,train loss is: 0.0006468291703377038\n",
      "test loss is 0.0010599528135799836\n",
      "Batch: 21000,train loss is: 0.0012066080713997484\n",
      "test loss is 0.000558196179643444\n",
      "Batch: 21100,train loss is: 0.0006207342254879058\n",
      "test loss is 0.00099761443963022\n",
      "Batch: 21200,train loss is: 0.00048443527355638595\n",
      "test loss is 0.0005900860134070139\n",
      "Batch: 21300,train loss is: 0.00036639482544271443\n",
      "test loss is 0.0005653681006555936\n",
      "Batch: 21400,train loss is: 0.0005348532953341901\n",
      "test loss is 0.0007410594610742441\n",
      "Batch: 21500,train loss is: 0.0007564794023362177\n",
      "test loss is 0.0007447980699232684\n",
      "Batch: 21600,train loss is: 0.00046693780807925103\n",
      "test loss is 0.0006631199287320479\n",
      "Batch: 21700,train loss is: 0.0003469242121328828\n",
      "test loss is 0.0005375609559895792\n",
      "Batch: 21800,train loss is: 0.00047144870917804416\n",
      "test loss is 0.0004995016264301765\n",
      "Batch: 21900,train loss is: 0.0009838712716042988\n",
      "test loss is 0.0012609053814877243\n",
      "Batch: 22000,train loss is: 0.0006950810108351782\n",
      "test loss is 0.0005023244898010761\n",
      "Batch: 22100,train loss is: 0.0004915353627541206\n",
      "test loss is 0.0006635001236650993\n",
      "Batch: 22200,train loss is: 0.0011098323443067297\n",
      "test loss is 0.0009955533492243638\n",
      "Batch: 22300,train loss is: 0.00040155502955203806\n",
      "test loss is 0.0004726705152179955\n",
      "Batch: 22400,train loss is: 0.0005403774139334208\n",
      "test loss is 0.0005193622108813927\n",
      "Batch: 22500,train loss is: 0.0005213024006432048\n",
      "test loss is 0.0005193941014510556\n",
      "Batch: 22600,train loss is: 0.0005300311940304755\n",
      "test loss is 0.0005104367555641556\n",
      "Batch: 22700,train loss is: 0.0007942815738965908\n",
      "test loss is 0.0007039039610055513\n",
      "Batch: 22800,train loss is: 0.0004205098432626026\n",
      "test loss is 0.0006981207510008505\n",
      "Batch: 22900,train loss is: 0.0006817640942269416\n",
      "test loss is 0.000626818674731379\n",
      "Batch: 23000,train loss is: 0.0008377610732576335\n",
      "test loss is 0.0005518487935945678\n",
      "Batch: 23100,train loss is: 0.00048484442771315716\n",
      "test loss is 0.0006287990733319462\n",
      "Batch: 23200,train loss is: 0.0005576003534970216\n",
      "test loss is 0.0005917043260529167\n",
      "Batch: 23300,train loss is: 0.00043497393604887677\n",
      "test loss is 0.0005186208557528978\n",
      "Batch: 23400,train loss is: 0.00041751700356501765\n",
      "test loss is 0.0007806580787132139\n",
      "Batch: 23500,train loss is: 0.000693072916987779\n",
      "test loss is 0.0005312716885024353\n",
      "Batch: 23600,train loss is: 0.00047827830705460435\n",
      "test loss is 0.0005776889853297781\n",
      "Batch: 23700,train loss is: 0.0005909322280002944\n",
      "test loss is 0.0007518796399520215\n",
      "Batch: 23800,train loss is: 0.0005487086958792238\n",
      "test loss is 0.0006014927659235496\n",
      "Batch: 23900,train loss is: 0.0005008949647486369\n",
      "test loss is 0.0005927106184999728\n",
      "Batch: 24000,train loss is: 0.0020094523322214753\n",
      "test loss is 0.0008510491925543426\n",
      "Batch: 24100,train loss is: 0.0005641372538514966\n",
      "test loss is 0.0009691331566624063\n",
      "Batch: 24200,train loss is: 0.0007916028391118403\n",
      "test loss is 0.00044883426468088496\n",
      "Batch: 24300,train loss is: 0.0004764134399552175\n",
      "test loss is 0.0005052211840373608\n",
      "Batch: 24400,train loss is: 0.0006495210754344406\n",
      "test loss is 0.0006023540949815654\n",
      "Batch: 24500,train loss is: 0.0005945491124394262\n",
      "test loss is 0.0013113184895461119\n",
      "Batch: 24600,train loss is: 0.0007174038669179386\n",
      "test loss is 0.000928507076093218\n",
      "Batch: 24700,train loss is: 0.00042996497753491734\n",
      "test loss is 0.0016668707035493954\n",
      "Batch: 24800,train loss is: 0.0005759319783945574\n",
      "test loss is 0.001017219162478079\n",
      "Batch: 24900,train loss is: 0.0005730202710764965\n",
      "test loss is 0.000526631985859802\n",
      "Batch: 25000,train loss is: 0.000568109597168569\n",
      "test loss is 0.000581400783286405\n",
      "Batch: 25100,train loss is: 0.0008523146078105701\n",
      "test loss is 0.0007829847065586779\n",
      "Batch: 25200,train loss is: 0.0011609087142632835\n",
      "test loss is 0.0005902828247621326\n",
      "Batch: 25300,train loss is: 0.00044992119826532976\n",
      "test loss is 0.000679911364593938\n",
      "Batch: 25400,train loss is: 0.00028849218155969995\n",
      "test loss is 0.0005204317360712928\n",
      "Batch: 25500,train loss is: 0.0008504204489423607\n",
      "test loss is 0.0006345233863458311\n",
      "Batch: 25600,train loss is: 0.00036685824172806206\n",
      "test loss is 0.0007495594246791493\n",
      "Batch: 25700,train loss is: 0.00046806620332914383\n",
      "test loss is 0.0007610284609612081\n",
      "Batch: 25800,train loss is: 0.0008505246181443195\n",
      "test loss is 0.00048299411008376507\n",
      "Batch: 25900,train loss is: 0.000697445005069426\n",
      "test loss is 0.0004726959258852372\n",
      "Batch: 26000,train loss is: 0.00036984335110841796\n",
      "test loss is 0.00047590248906801785\n",
      "Batch: 26100,train loss is: 0.0005615421213223859\n",
      "test loss is 0.0007457123383072938\n",
      "Batch: 26200,train loss is: 0.0010839912868779622\n",
      "test loss is 0.000703863428629581\n",
      "Batch: 26300,train loss is: 0.0004349056385914569\n",
      "test loss is 0.0005198014692131195\n",
      "Batch: 26400,train loss is: 0.0003592742488186545\n",
      "test loss is 0.0005456046932013625\n",
      "Batch: 26500,train loss is: 0.0003637884788891127\n",
      "test loss is 0.0005231629309341389\n",
      "Batch: 26600,train loss is: 0.0005289428482051937\n",
      "test loss is 0.0005126221548194642\n",
      "Batch: 26700,train loss is: 0.00032389474174885594\n",
      "test loss is 0.0005121082664708296\n",
      "Batch: 26800,train loss is: 0.0006397356126282203\n",
      "test loss is 0.0005844024213474185\n",
      "Batch: 26900,train loss is: 0.0004543112757771757\n",
      "test loss is 0.0005018428516952329\n",
      "Batch: 27000,train loss is: 0.0004727036327289506\n",
      "test loss is 0.0004897324647077228\n",
      "Batch: 27100,train loss is: 0.00062288688978297\n",
      "test loss is 0.0011677495427365055\n",
      "Batch: 27200,train loss is: 0.0003079805242782286\n",
      "test loss is 0.0006104594470788423\n",
      "Batch: 27300,train loss is: 0.0003082931452005037\n",
      "test loss is 0.0005249322184301431\n",
      "Batch: 27400,train loss is: 0.0006556939246193233\n",
      "test loss is 0.0006011284616753805\n",
      "Batch: 27500,train loss is: 0.0006384061771171757\n",
      "test loss is 0.0010130281510023799\n",
      "Batch: 27600,train loss is: 0.00041934040103355053\n",
      "test loss is 0.0005662869304671788\n",
      "Batch: 27700,train loss is: 0.00041597861641252194\n",
      "test loss is 0.0005426269944171558\n",
      "Batch: 27800,train loss is: 0.00044247483161757764\n",
      "test loss is 0.0004588867331656763\n",
      "Batch: 27900,train loss is: 0.0005157620220152524\n",
      "test loss is 0.0009052553556657731\n",
      "Batch: 28000,train loss is: 0.0003730532444624127\n",
      "test loss is 0.00048535939457338296\n",
      "Batch: 28100,train loss is: 0.0006261097738159952\n",
      "test loss is 0.0006562811475318348\n",
      "Batch: 28200,train loss is: 0.0002919825885954906\n",
      "test loss is 0.0005451705460144307\n",
      "Batch: 28300,train loss is: 0.0004547243965827271\n",
      "test loss is 0.0007568735474150457\n",
      "Batch: 28400,train loss is: 0.0006488183188873454\n",
      "test loss is 0.00045140410726066425\n",
      "Batch: 28500,train loss is: 0.0003788734996769585\n",
      "test loss is 0.0008086551465204979\n",
      "Batch: 28600,train loss is: 0.0007938093396557904\n",
      "test loss is 0.0006921441980915696\n",
      "Batch: 28700,train loss is: 0.0004989090870711672\n",
      "test loss is 0.0005233726850644861\n",
      "Batch: 28800,train loss is: 0.0007020659815022821\n",
      "test loss is 0.000614364719441875\n",
      "Batch: 28900,train loss is: 0.0005609709117269065\n",
      "test loss is 0.0006091180130239919\n",
      "Batch: 29000,train loss is: 0.00030545472776349046\n",
      "test loss is 0.0010330629982827537\n",
      "Batch: 29100,train loss is: 0.0005538759601582512\n",
      "test loss is 0.0006340387920210452\n",
      "Batch: 29200,train loss is: 0.0004927776555957214\n",
      "test loss is 0.0005155941016144053\n",
      "Batch: 29300,train loss is: 0.000596965691589565\n",
      "test loss is 0.0007114534072830352\n",
      "Batch: 29400,train loss is: 0.0005994471360322002\n",
      "test loss is 0.0009305487163534589\n",
      "Batch: 29500,train loss is: 0.00047729358225473135\n",
      "test loss is 0.0007541080188876049\n",
      "Batch: 29600,train loss is: 0.00044916714771994776\n",
      "test loss is 0.0006120622619103902\n",
      "Batch: 29700,train loss is: 0.00043209230694009457\n",
      "test loss is 0.0005512236582813397\n",
      "Batch: 29800,train loss is: 0.0008617639372101538\n",
      "test loss is 0.0005218540017329807\n",
      "Batch: 29900,train loss is: 0.00036980782772524457\n",
      "test loss is 0.0008432491461028051\n",
      "Batch: 30000,train loss is: 0.0008013995743674849\n",
      "test loss is 0.00049214501966021\n",
      "Batch: 30100,train loss is: 0.00047388036984974185\n",
      "test loss is 0.0005166839647632879\n",
      "Batch: 30200,train loss is: 0.000773459400857254\n",
      "test loss is 0.0004987518609200179\n",
      "Batch: 30300,train loss is: 0.000561116592122394\n",
      "test loss is 0.000848866397709231\n",
      "Batch: 30400,train loss is: 0.0004800125267039633\n",
      "test loss is 0.0008787426694544652\n",
      "Batch: 30500,train loss is: 0.0003762790953924451\n",
      "test loss is 0.0007551519545280525\n",
      "Batch: 30600,train loss is: 0.00036360381596040194\n",
      "test loss is 0.0006923299746741585\n",
      "Batch: 30700,train loss is: 0.0007087683863280815\n",
      "test loss is 0.0006280275555462691\n",
      "Batch: 30800,train loss is: 0.000401108829029206\n",
      "test loss is 0.0005513642401270542\n",
      "Batch: 30900,train loss is: 0.0005313489395405964\n",
      "test loss is 0.0006996839324890408\n",
      "Batch: 31000,train loss is: 0.0009643832709499943\n",
      "test loss is 0.0005043242497001519\n",
      "Batch: 31100,train loss is: 0.0004566614226800057\n",
      "test loss is 0.0005116298165513053\n",
      "Batch: 31200,train loss is: 0.0003809964362067953\n",
      "test loss is 0.0005879885939675538\n",
      "Batch: 31300,train loss is: 0.0007734117830614725\n",
      "test loss is 0.000537327123457452\n",
      "Batch: 31400,train loss is: 0.0003261609468432732\n",
      "test loss is 0.0005368196125008526\n",
      "Batch: 31500,train loss is: 0.0006808899896538449\n",
      "test loss is 0.0008646340110204993\n",
      "Batch: 31600,train loss is: 0.00029249935483877435\n",
      "test loss is 0.0004777178916703956\n",
      "Batch: 31700,train loss is: 0.0008579820581590927\n",
      "test loss is 0.0006280543798323768\n",
      "Batch: 31800,train loss is: 0.0004570902102451483\n",
      "test loss is 0.0005866696734723924\n",
      "Batch: 31900,train loss is: 0.0005056025688095391\n",
      "test loss is 0.0006289009259155768\n",
      "Batch: 32000,train loss is: 0.0004262485886173074\n",
      "test loss is 0.00048287269946976143\n",
      "Batch: 32100,train loss is: 0.0003373969108722884\n",
      "test loss is 0.0004921843691628574\n",
      "Batch: 32200,train loss is: 0.0006449479291833773\n",
      "test loss is 0.0007868625844538419\n",
      "Batch: 32300,train loss is: 0.00047975642446583956\n",
      "test loss is 0.00044567159033058994\n",
      "Batch: 32400,train loss is: 0.00041031698518105043\n",
      "test loss is 0.0005054260328727267\n",
      "Batch: 32500,train loss is: 0.0009925565122889542\n",
      "test loss is 0.0005110406605176836\n",
      "Batch: 32600,train loss is: 0.0003882054038666336\n",
      "test loss is 0.0005966826787270835\n",
      "Batch: 32700,train loss is: 0.0007175126172682821\n",
      "test loss is 0.000495832765617285\n",
      "Batch: 32800,train loss is: 0.0013407486144300352\n",
      "test loss is 0.0005157604008015247\n",
      "Batch: 32900,train loss is: 0.00046884045270256856\n",
      "test loss is 0.0004937153753990013\n",
      "Batch: 33000,train loss is: 0.00038253277570037404\n",
      "test loss is 0.0005260485993739054\n",
      "Batch: 33100,train loss is: 0.0008794901631032374\n",
      "test loss is 0.00047581134118217987\n",
      "Batch: 33200,train loss is: 0.0007484388331008681\n",
      "test loss is 0.0004474477203429853\n",
      "Batch: 33300,train loss is: 0.0006550145514620181\n",
      "test loss is 0.0008048879154543916\n",
      "Batch: 33400,train loss is: 0.0003454832624957585\n",
      "test loss is 0.0005008042495522445\n",
      "Batch: 33500,train loss is: 0.00041643798723573985\n",
      "test loss is 0.0005019035668616472\n",
      "Batch: 33600,train loss is: 0.0003438633301800736\n",
      "test loss is 0.00043880518866947166\n",
      "Batch: 33700,train loss is: 0.0008248424188361816\n",
      "test loss is 0.0005346009942546917\n",
      "Batch: 33800,train loss is: 0.0007731655732166122\n",
      "test loss is 0.0006780243133495434\n",
      "Batch: 33900,train loss is: 0.0015511254040278559\n",
      "test loss is 0.0012033201325453732\n",
      "Batch: 34000,train loss is: 0.0008678661541211542\n",
      "test loss is 0.0006474234318668253\n",
      "Batch: 34100,train loss is: 0.0004332098455601703\n",
      "test loss is 0.00045537497102702044\n",
      "Batch: 34200,train loss is: 0.000541494685922393\n",
      "test loss is 0.0005345056835814852\n",
      "Batch: 34300,train loss is: 0.0005283015637709502\n",
      "test loss is 0.0005185106514949955\n",
      "Batch: 34400,train loss is: 0.0007135092780145711\n",
      "test loss is 0.000695470548983688\n",
      "Batch: 34500,train loss is: 0.0007222470078273773\n",
      "test loss is 0.0006046262107352292\n",
      "Batch: 34600,train loss is: 0.0005353294082769765\n",
      "test loss is 0.0005491184312165792\n",
      "Batch: 34700,train loss is: 0.00039001288702311785\n",
      "test loss is 0.001118326964719252\n",
      "Batch: 34800,train loss is: 0.0006278839921852674\n",
      "test loss is 0.0006659853260456472\n",
      "Batch: 34900,train loss is: 0.0004669209209338223\n",
      "test loss is 0.0005485448611081347\n",
      "Batch: 35000,train loss is: 0.0005583436231327297\n",
      "test loss is 0.0005806003495185949\n",
      "Batch: 35100,train loss is: 0.00040001891495836117\n",
      "test loss is 0.0005663927426290055\n",
      "Batch: 35200,train loss is: 0.0005784829524975575\n",
      "test loss is 0.0005296240098197076\n",
      "Batch: 35300,train loss is: 0.0005785527899829324\n",
      "test loss is 0.0006357954989216398\n",
      "Batch: 35400,train loss is: 0.0003653839851571997\n",
      "test loss is 0.0005011953628743436\n",
      "Batch: 35500,train loss is: 0.0005346244790106537\n",
      "test loss is 0.0005891721899961525\n",
      "Batch: 35600,train loss is: 0.000568090291463672\n",
      "test loss is 0.0005253492772542445\n",
      "Batch: 35700,train loss is: 0.00043059149745213984\n",
      "test loss is 0.0004815573472580883\n",
      "Batch: 35800,train loss is: 0.0002483551534130198\n",
      "test loss is 0.0006602530679178551\n",
      "Batch: 35900,train loss is: 0.0004966272883221599\n",
      "test loss is 0.0004473189711304373\n",
      "Batch: 36000,train loss is: 0.0008776759671394576\n",
      "test loss is 0.0015642467678834017\n",
      "Batch: 36100,train loss is: 0.0009006115940050324\n",
      "test loss is 0.0005086446161717144\n",
      "Batch: 36200,train loss is: 0.0014706738313497608\n",
      "test loss is 0.0006959506338431636\n",
      "Batch: 36300,train loss is: 0.0006835411217215372\n",
      "test loss is 0.0006345565947991797\n",
      "Batch: 36400,train loss is: 0.0004183407106881129\n",
      "test loss is 0.0007004846291443848\n",
      "Batch: 36500,train loss is: 0.0003998732808924479\n",
      "test loss is 0.0004919785321807433\n",
      "Batch: 36600,train loss is: 0.000337896195015784\n",
      "test loss is 0.00047251662636163294\n",
      "Batch: 36700,train loss is: 0.0006632861017925976\n",
      "test loss is 0.0006194607878880044\n",
      "Batch: 36800,train loss is: 0.0005506546444088636\n",
      "test loss is 0.0004934169784345487\n",
      "Batch: 36900,train loss is: 0.0007929970211500187\n",
      "test loss is 0.0004503570502227913\n",
      "Batch: 37000,train loss is: 0.00048525684896416616\n",
      "test loss is 0.0006527215067755251\n",
      "Batch: 37100,train loss is: 0.0007273006893116726\n",
      "test loss is 0.0005692421405733146\n",
      "Batch: 37200,train loss is: 0.0009061962793117404\n",
      "test loss is 0.0004925053677674879\n",
      "Batch: 37300,train loss is: 0.0006636113015582524\n",
      "test loss is 0.0008447176112791081\n",
      "Batch: 37400,train loss is: 0.0008683579932979854\n",
      "test loss is 0.0006158845387528571\n",
      "Batch: 37500,train loss is: 0.00033695529354023444\n",
      "test loss is 0.0004545174639611683\n",
      "Batch: 37600,train loss is: 0.0007891552613552807\n",
      "test loss is 0.0005501247631352412\n",
      "Batch: 37700,train loss is: 0.0005336630611693647\n",
      "test loss is 0.00047391551891366534\n",
      "Batch: 37800,train loss is: 0.0007215531916225056\n",
      "test loss is 0.0005123414552892399\n",
      "Batch: 37900,train loss is: 0.0004654070764528585\n",
      "test loss is 0.0006091310945359935\n",
      "Batch: 38000,train loss is: 0.0007979452286185735\n",
      "test loss is 0.0005961139088329691\n",
      "Batch: 38100,train loss is: 0.0003944078902828865\n",
      "test loss is 0.000589035688027718\n",
      "Batch: 38200,train loss is: 0.0005201450858706197\n",
      "test loss is 0.0006332706429063477\n",
      "Batch: 38300,train loss is: 0.00030727388846526246\n",
      "test loss is 0.0005051247828208498\n",
      "Batch: 38400,train loss is: 0.000548589705979189\n",
      "test loss is 0.00048139710981507375\n",
      "Batch: 38500,train loss is: 0.0008113227269848741\n",
      "test loss is 0.00048395569012192765\n",
      "Batch: 38600,train loss is: 0.0003277969845340168\n",
      "test loss is 0.0006682066052146582\n",
      "Batch: 38700,train loss is: 0.0005604793388948358\n",
      "test loss is 0.0005138020106186689\n",
      "Batch: 38800,train loss is: 0.00041588633949104104\n",
      "test loss is 0.0005359347675014161\n",
      "Batch: 38900,train loss is: 0.00037835595321487105\n",
      "test loss is 0.0006666468851972688\n",
      "Batch: 39000,train loss is: 0.0009098293679756589\n",
      "test loss is 0.0005737050069022763\n",
      "Batch: 39100,train loss is: 0.0010621667642778695\n",
      "test loss is 0.000744562747387402\n",
      "Batch: 39200,train loss is: 0.001076300444637188\n",
      "test loss is 0.0008002361494470201\n",
      "Batch: 39300,train loss is: 0.0005015438025049957\n",
      "test loss is 0.0005214400201228573\n",
      "Batch: 39400,train loss is: 0.000298864903533101\n",
      "test loss is 0.0005603689771067826\n",
      "Batch: 39500,train loss is: 0.0005241814493192153\n",
      "test loss is 0.0004939398504637486\n",
      "Batch: 39600,train loss is: 0.0006700335636417704\n",
      "test loss is 0.000504258092279599\n",
      "Batch: 39700,train loss is: 0.0009502707037213726\n",
      "test loss is 0.00045364143683147655\n",
      "Batch: 39800,train loss is: 0.00035494906062242236\n",
      "test loss is 0.0005346103920332153\n",
      "Batch: 39900,train loss is: 0.0005492196732231351\n",
      "test loss is 0.0007206938088933131\n",
      "Batch: 40000,train loss is: 0.0007148702997307822\n",
      "test loss is 0.0004759380360629151\n",
      "Batch: 40100,train loss is: 0.00040225315211028453\n",
      "test loss is 0.0005807738618592512\n",
      "Batch: 40200,train loss is: 0.0005873166082430261\n",
      "test loss is 0.0006095582879305933\n",
      "Batch: 40300,train loss is: 0.0005242856971786852\n",
      "test loss is 0.0006012708011939608\n",
      "Batch: 40400,train loss is: 0.0006062074516924521\n",
      "test loss is 0.0005157334326336665\n",
      "Batch: 40500,train loss is: 0.00041026620851824375\n",
      "test loss is 0.0013381382975025473\n",
      "Batch: 40600,train loss is: 0.0003061837367522352\n",
      "test loss is 0.0004899354510494032\n",
      "Batch: 40700,train loss is: 0.00037728725852133826\n",
      "test loss is 0.0007708718276869283\n",
      "Batch: 40800,train loss is: 0.0005990211481125663\n",
      "test loss is 0.0007409031642089899\n",
      "Batch: 40900,train loss is: 0.00039169680302200596\n",
      "test loss is 0.00048610946031715313\n",
      "Batch: 41000,train loss is: 0.0004382251797226017\n",
      "test loss is 0.0010738766844750757\n",
      "Batch: 41100,train loss is: 0.0007025910600118321\n",
      "test loss is 0.0005664674214818447\n",
      "Batch: 41200,train loss is: 0.000361765620357399\n",
      "test loss is 0.0006411427810645785\n",
      "Batch: 41300,train loss is: 0.0002820130703679374\n",
      "test loss is 0.0007293046033196942\n",
      "Batch: 41400,train loss is: 0.00044349564569996124\n",
      "test loss is 0.0004374960920372766\n",
      "Batch: 41500,train loss is: 0.0006311928914641207\n",
      "test loss is 0.0005449462582831233\n",
      "Batch: 41600,train loss is: 0.0005997692992235265\n",
      "test loss is 0.0004801544658893906\n",
      "Batch: 41700,train loss is: 0.0005005206755141565\n",
      "test loss is 0.00047453086067144077\n",
      "Batch: 41800,train loss is: 0.001071304576592255\n",
      "test loss is 0.00047480553803464637\n",
      "Batch: 41900,train loss is: 0.0004243242253153299\n",
      "test loss is 0.0004650207553956376\n",
      "Batch: 42000,train loss is: 0.0005949288871660012\n",
      "test loss is 0.0005774607426500986\n",
      "Batch: 42100,train loss is: 0.0006970188087947767\n",
      "test loss is 0.0008650070874293972\n",
      "Batch: 42200,train loss is: 0.0014801333197907538\n",
      "test loss is 0.001913341427341236\n",
      "Batch: 42300,train loss is: 0.0006142898728742282\n",
      "test loss is 0.0006812026814022355\n",
      "Batch: 42400,train loss is: 0.00043996767711630995\n",
      "test loss is 0.0004740429576053289\n",
      "Batch: 42500,train loss is: 0.0007127348595845791\n",
      "test loss is 0.00043967488974258655\n",
      "Batch: 42600,train loss is: 0.0005148076113128557\n",
      "test loss is 0.0007197502291146644\n",
      "Batch: 42700,train loss is: 0.001080842400849062\n",
      "test loss is 0.0005081449265306384\n",
      "Batch: 42800,train loss is: 0.00051656872962851\n",
      "test loss is 0.0006290318188112515\n",
      "Batch: 42900,train loss is: 0.0003484500307193696\n",
      "test loss is 0.0006456523435870977\n",
      "Batch: 43000,train loss is: 0.0006907599347923826\n",
      "test loss is 0.0004784049113442025\n",
      "Batch: 43100,train loss is: 0.0008369409709814528\n",
      "test loss is 0.0005956709678506418\n",
      "Batch: 43200,train loss is: 0.0007407422035216892\n",
      "test loss is 0.00046850992987518915\n",
      "Batch: 43300,train loss is: 0.0005109762134432457\n",
      "test loss is 0.0004865657022748731\n",
      "Batch: 43400,train loss is: 0.0005868519255157719\n",
      "test loss is 0.00048153939982089564\n",
      "Batch: 43500,train loss is: 0.00035546082808330965\n",
      "test loss is 0.0006073709730569338\n",
      "Batch: 43600,train loss is: 0.0006830862089266786\n",
      "test loss is 0.0005521674852254233\n",
      "Batch: 43700,train loss is: 0.0005810244275253023\n",
      "test loss is 0.0006772116906423927\n",
      "Batch: 43800,train loss is: 0.0003789552344327657\n",
      "test loss is 0.0005159809478205136\n",
      "Batch: 43900,train loss is: 0.0003536657624883285\n",
      "test loss is 0.0005036543097656012\n",
      "Batch: 44000,train loss is: 0.0004940681996286598\n",
      "test loss is 0.0005438401118435048\n",
      "Batch: 44100,train loss is: 0.00025755862277623576\n",
      "test loss is 0.00047087278872948304\n",
      "Batch: 44200,train loss is: 0.0004705653612911097\n",
      "test loss is 0.0004879693242359243\n",
      "Batch: 44300,train loss is: 0.0005549681286063401\n",
      "test loss is 0.0004672429409410038\n",
      "Batch: 44400,train loss is: 0.0010217350642569203\n",
      "test loss is 0.000432046275619449\n",
      "Batch: 44500,train loss is: 0.0009009210430443895\n",
      "test loss is 0.0005749288761375774\n",
      "Batch: 44600,train loss is: 0.00039899725235343535\n",
      "test loss is 0.0009297703537496759\n",
      "Batch: 44700,train loss is: 0.0006184772732519867\n",
      "test loss is 0.0006380960653531079\n",
      "Batch: 44800,train loss is: 0.0005876085052289055\n",
      "test loss is 0.0005955173035431199\n",
      "Batch: 44900,train loss is: 0.000367738363021156\n",
      "test loss is 0.0005845588768446604\n",
      "Batch: 45000,train loss is: 0.00041998020787756095\n",
      "test loss is 0.0006199451854057601\n",
      "Batch: 45100,train loss is: 0.0005968447099316638\n",
      "test loss is 0.0004639809901731384\n",
      "Batch: 45200,train loss is: 0.0003617401040197597\n",
      "test loss is 0.000643202531668683\n",
      "Batch: 45300,train loss is: 0.00065585268458201\n",
      "test loss is 0.0006201914250520653\n",
      "Batch: 45400,train loss is: 0.0003193755000689227\n",
      "test loss is 0.0005613616463878732\n",
      "Batch: 45500,train loss is: 0.0006819416283004839\n",
      "test loss is 0.0005100076329135914\n",
      "Batch: 45600,train loss is: 0.00041163703115641384\n",
      "test loss is 0.000887915618019621\n",
      "Batch: 45700,train loss is: 0.0003121112577134284\n",
      "test loss is 0.0004749037706291564\n",
      "Batch: 45800,train loss is: 0.000576068738784996\n",
      "test loss is 0.0005410653614743832\n",
      "Batch: 45900,train loss is: 0.00030358765801584897\n",
      "test loss is 0.00046910088744689875\n",
      "Batch: 46000,train loss is: 0.0006832434243726924\n",
      "test loss is 0.0005541983972520069\n",
      "Batch: 46100,train loss is: 0.0004908727666898064\n",
      "test loss is 0.0005312144123040663\n",
      "Batch: 46200,train loss is: 0.000775933714031619\n",
      "test loss is 0.0004915778598048414\n",
      "Batch: 46300,train loss is: 0.0004813813054154807\n",
      "test loss is 0.0006752028847476345\n",
      "Batch: 46400,train loss is: 0.0044842408248990055\n",
      "test loss is 0.0006098956347270007\n",
      "Batch: 46500,train loss is: 0.000541405162781532\n",
      "test loss is 0.000661796525511226\n",
      "Batch: 46600,train loss is: 0.0006511701406037322\n",
      "test loss is 0.0007686595765342318\n",
      "Batch: 46700,train loss is: 0.0005248419244511186\n",
      "test loss is 0.0004945136096025066\n",
      "-----------------------Epoch: 4----------------------------------\n",
      "Batch: 0,train loss is: 0.0003907466819029579\n",
      "test loss is 0.0005609607005983126\n",
      "Batch: 100,train loss is: 0.0005958185150188187\n",
      "test loss is 0.0005157852152002301\n",
      "Batch: 200,train loss is: 0.000449560107631078\n",
      "test loss is 0.0005011711048057852\n",
      "Batch: 300,train loss is: 0.0002679381692652819\n",
      "test loss is 0.0005631062634113146\n",
      "Batch: 400,train loss is: 0.0004647750933174927\n",
      "test loss is 0.000674030348465181\n",
      "Batch: 500,train loss is: 0.0005820613859619627\n",
      "test loss is 0.0005541272829721391\n",
      "Batch: 600,train loss is: 0.00029886221117329315\n",
      "test loss is 0.0004546046600330122\n",
      "Batch: 700,train loss is: 0.0005387185500275155\n",
      "test loss is 0.00045787514334387583\n",
      "Batch: 800,train loss is: 0.00044150698075307525\n",
      "test loss is 0.0005109471039748471\n",
      "Batch: 900,train loss is: 0.0005682744726173436\n",
      "test loss is 0.0005104598477123416\n",
      "Batch: 1000,train loss is: 0.0004152876636757325\n",
      "test loss is 0.0006422199245022487\n",
      "Batch: 1100,train loss is: 0.000433223081506925\n",
      "test loss is 0.0005471782860213058\n",
      "Batch: 1200,train loss is: 0.001734455522554273\n",
      "test loss is 0.0005434555064824276\n",
      "Batch: 1300,train loss is: 0.0006775601664804437\n",
      "test loss is 0.000580748848488828\n",
      "Batch: 1400,train loss is: 0.0005904336662342688\n",
      "test loss is 0.0005121362179277753\n",
      "Batch: 1500,train loss is: 0.0007075814956153867\n",
      "test loss is 0.0006070058815708818\n",
      "Batch: 1600,train loss is: 0.0004278880519433109\n",
      "test loss is 0.00044356229545994383\n",
      "Batch: 1700,train loss is: 0.0005550384312623112\n",
      "test loss is 0.000994369062998959\n",
      "Batch: 1800,train loss is: 0.0004278176506061921\n",
      "test loss is 0.0006708628258563112\n",
      "Batch: 1900,train loss is: 0.0004028508180741005\n",
      "test loss is 0.0004811645834036857\n",
      "Batch: 2000,train loss is: 0.0003621864355575974\n",
      "test loss is 0.0004902215651554232\n",
      "Batch: 2100,train loss is: 0.0003904103906423122\n",
      "test loss is 0.0004607107181599972\n",
      "Batch: 2200,train loss is: 0.0003898884615906944\n",
      "test loss is 0.0004516745305670692\n",
      "Batch: 2300,train loss is: 0.00045940377754729905\n",
      "test loss is 0.0004561769850430403\n",
      "Batch: 2400,train loss is: 0.00028097667008757413\n",
      "test loss is 0.0004287545714198914\n",
      "Batch: 2500,train loss is: 0.0006355952352359236\n",
      "test loss is 0.0007551922037188127\n",
      "Batch: 2600,train loss is: 0.00068586852750622\n",
      "test loss is 0.0005707096335865312\n",
      "Batch: 2700,train loss is: 0.0004618876937110768\n",
      "test loss is 0.0005904795505891921\n",
      "Batch: 2800,train loss is: 0.0008002573987058326\n",
      "test loss is 0.0005935004501672397\n",
      "Batch: 2900,train loss is: 0.000491041069386152\n",
      "test loss is 0.00043025634267562026\n",
      "Batch: 3000,train loss is: 0.0011851093605984037\n",
      "test loss is 0.0006957774331007167\n",
      "Batch: 3100,train loss is: 0.00038231956451658126\n",
      "test loss is 0.0004762973871996541\n",
      "Batch: 3200,train loss is: 0.00040796570733739665\n",
      "test loss is 0.0005954967241693236\n",
      "Batch: 3300,train loss is: 0.0005424983209972831\n",
      "test loss is 0.0005123639286632262\n",
      "Batch: 3400,train loss is: 0.0002324400878264822\n",
      "test loss is 0.0005398712383042937\n",
      "Batch: 3500,train loss is: 0.0006500583341897453\n",
      "test loss is 0.00047723797379154825\n",
      "Batch: 3600,train loss is: 0.0004934534596256301\n",
      "test loss is 0.00045199625991451144\n",
      "Batch: 3700,train loss is: 0.0005902264536628012\n",
      "test loss is 0.000557853638549778\n",
      "Batch: 3800,train loss is: 0.0005348334952379049\n",
      "test loss is 0.0005697760850633299\n",
      "Batch: 3900,train loss is: 0.0005470334844428369\n",
      "test loss is 0.0004496243081563017\n",
      "Batch: 4000,train loss is: 0.0006158219334582032\n",
      "test loss is 0.0004941049465912868\n",
      "Batch: 4100,train loss is: 0.0004977867288445961\n",
      "test loss is 0.0006099093486117806\n",
      "Batch: 4200,train loss is: 0.000467643256712555\n",
      "test loss is 0.00048368372829344324\n",
      "Batch: 4300,train loss is: 0.0003364001249894294\n",
      "test loss is 0.00048363556186854307\n",
      "Batch: 4400,train loss is: 0.0005353826611687658\n",
      "test loss is 0.0005720385332948034\n",
      "Batch: 4500,train loss is: 0.0004974899115042552\n",
      "test loss is 0.0006201725077278973\n",
      "Batch: 4600,train loss is: 0.00032268797076594636\n",
      "test loss is 0.0004838950131986914\n",
      "Batch: 4700,train loss is: 0.00048215658065583485\n",
      "test loss is 0.0006139281241029439\n",
      "Batch: 4800,train loss is: 0.0006467573352066439\n",
      "test loss is 0.0005913159158628735\n",
      "Batch: 4900,train loss is: 0.00043236308110718277\n",
      "test loss is 0.0004863103221375877\n",
      "Batch: 5000,train loss is: 0.000499125296878069\n",
      "test loss is 0.0005388954028171768\n",
      "Batch: 5100,train loss is: 0.000585654301320313\n",
      "test loss is 0.0005288251456636097\n",
      "Batch: 5200,train loss is: 0.0004857425385549528\n",
      "test loss is 0.0004398079475263209\n",
      "Batch: 5300,train loss is: 0.0005349037347570643\n",
      "test loss is 0.0004882067890753001\n",
      "Batch: 5400,train loss is: 0.00035369747565546763\n",
      "test loss is 0.0005181566667390709\n",
      "Batch: 5500,train loss is: 0.001022091486179595\n",
      "test loss is 0.0005182651354735348\n",
      "Batch: 5600,train loss is: 0.0003016187911846686\n",
      "test loss is 0.0004654797596578147\n",
      "Batch: 5700,train loss is: 0.00035900739290638386\n",
      "test loss is 0.0004060721897293392\n",
      "Batch: 5800,train loss is: 0.0005854937617667054\n",
      "test loss is 0.0006032906947196376\n",
      "Batch: 5900,train loss is: 0.00027325222674076904\n",
      "test loss is 0.00041553078183012874\n",
      "Batch: 6000,train loss is: 0.0003023290979635968\n",
      "test loss is 0.0005835850388426295\n",
      "Batch: 6100,train loss is: 0.0004441005748586709\n",
      "test loss is 0.00041285552853326577\n",
      "Batch: 6200,train loss is: 0.0004985020514462677\n",
      "test loss is 0.000764522783170325\n",
      "Batch: 6300,train loss is: 0.0003448546415044585\n",
      "test loss is 0.0004889082334931444\n",
      "Batch: 6400,train loss is: 0.0005179940684223338\n",
      "test loss is 0.000442565597825778\n",
      "Batch: 6500,train loss is: 0.0005710670935227264\n",
      "test loss is 0.000645274309207024\n",
      "Batch: 6600,train loss is: 0.00040384601273737086\n",
      "test loss is 0.0005133006448322412\n",
      "Batch: 6700,train loss is: 0.0006761738051783133\n",
      "test loss is 0.0006103621588824762\n",
      "Batch: 6800,train loss is: 0.0005055962320083088\n",
      "test loss is 0.0004712225787038547\n",
      "Batch: 6900,train loss is: 0.00047152364212476575\n",
      "test loss is 0.000538193641986041\n",
      "Batch: 7000,train loss is: 0.0007516711560861052\n",
      "test loss is 0.0007247308346651309\n",
      "Batch: 7100,train loss is: 0.0005303306698061773\n",
      "test loss is 0.000752072275347619\n",
      "Batch: 7200,train loss is: 0.0003731021585193356\n",
      "test loss is 0.000419452535970147\n",
      "Batch: 7300,train loss is: 0.0003785214850308664\n",
      "test loss is 0.0005205966233411458\n",
      "Batch: 7400,train loss is: 0.0006908657934994947\n",
      "test loss is 0.0004319574982732697\n",
      "Batch: 7500,train loss is: 0.0005948795586807755\n",
      "test loss is 0.0004620032115877816\n",
      "Batch: 7600,train loss is: 0.00040095806881459436\n",
      "test loss is 0.0004905553154637125\n",
      "Batch: 7700,train loss is: 0.0003993148932011422\n",
      "test loss is 0.0005037879388380599\n",
      "Batch: 7800,train loss is: 0.00031086586975209653\n",
      "test loss is 0.0004906282671199215\n",
      "Batch: 7900,train loss is: 0.0005257021398194109\n",
      "test loss is 0.0004733118773564501\n",
      "Batch: 8000,train loss is: 0.0008448708351734592\n",
      "test loss is 0.000462748297604514\n",
      "Batch: 8100,train loss is: 0.0004076100614858533\n",
      "test loss is 0.00045681630362560765\n",
      "Batch: 8200,train loss is: 0.00041003476336171735\n",
      "test loss is 0.0005327085379185569\n",
      "Batch: 8300,train loss is: 0.0004722078217235456\n",
      "test loss is 0.0004836828199945809\n",
      "Batch: 8400,train loss is: 0.0005934551348456876\n",
      "test loss is 0.0008115786352964216\n",
      "Batch: 8500,train loss is: 0.0004747319509680057\n",
      "test loss is 0.0005685104291550878\n",
      "Batch: 8600,train loss is: 0.0003930277110728021\n",
      "test loss is 0.0006911505333340378\n",
      "Batch: 8700,train loss is: 0.000467222768322016\n",
      "test loss is 0.0004255827306116767\n",
      "Batch: 8800,train loss is: 0.000547561164648393\n",
      "test loss is 0.0009312210907708667\n",
      "Batch: 8900,train loss is: 0.0003125019794741877\n",
      "test loss is 0.0006639648464369626\n",
      "Batch: 9000,train loss is: 0.0009005883800083225\n",
      "test loss is 0.0005222949428268046\n",
      "Batch: 9100,train loss is: 0.0008028776969232672\n",
      "test loss is 0.00066594360222244\n",
      "Batch: 9200,train loss is: 0.0005926667532236857\n",
      "test loss is 0.0004236007789497464\n",
      "Batch: 9300,train loss is: 0.0003847100159555386\n",
      "test loss is 0.0005057473681122875\n",
      "Batch: 9400,train loss is: 0.0006112400636203361\n",
      "test loss is 0.000458177713598788\n",
      "Batch: 9500,train loss is: 0.0009213773428810676\n",
      "test loss is 0.0006319313602657723\n",
      "Batch: 9600,train loss is: 0.0006000131274212135\n",
      "test loss is 0.0005081169927955149\n",
      "Batch: 9700,train loss is: 0.0004465965615200108\n",
      "test loss is 0.0006319720868781165\n",
      "Batch: 9800,train loss is: 0.00023419210015625719\n",
      "test loss is 0.00044314669225601076\n",
      "Batch: 9900,train loss is: 0.0005286197822114762\n",
      "test loss is 0.0007674375621367913\n",
      "Batch: 10000,train loss is: 0.0005580588470354209\n",
      "test loss is 0.0005255038949409568\n",
      "Batch: 10100,train loss is: 0.0006397364759607667\n",
      "test loss is 0.0008089390213156653\n",
      "Batch: 10200,train loss is: 0.0005937041044565365\n",
      "test loss is 0.0007015879859782749\n",
      "Batch: 10300,train loss is: 0.0009147925333391428\n",
      "test loss is 0.0005644264371746204\n",
      "Batch: 10400,train loss is: 0.0009351722831011313\n",
      "test loss is 0.00045159192089478615\n",
      "Batch: 10500,train loss is: 0.0002874664456463771\n",
      "test loss is 0.00047080138290953066\n",
      "Batch: 10600,train loss is: 0.00039347443605145834\n",
      "test loss is 0.00045826358828192636\n",
      "Batch: 10700,train loss is: 0.0004703118413267544\n",
      "test loss is 0.0004168588627076149\n",
      "Batch: 10800,train loss is: 0.00039871345153499676\n",
      "test loss is 0.0004993358158553536\n",
      "Batch: 10900,train loss is: 0.00044668388008632827\n",
      "test loss is 0.0004434955186077578\n",
      "Batch: 11000,train loss is: 0.0003598587448068826\n",
      "test loss is 0.00044373538829689004\n",
      "Batch: 11100,train loss is: 0.00043731456530790363\n",
      "test loss is 0.0007358909830827074\n",
      "Batch: 11200,train loss is: 0.000516254018263166\n",
      "test loss is 0.0007371180404145117\n",
      "Batch: 11300,train loss is: 0.00028682918475308344\n",
      "test loss is 0.0003962242632422121\n",
      "Batch: 11400,train loss is: 0.0004548619880355887\n",
      "test loss is 0.0005681015585682079\n",
      "Batch: 11500,train loss is: 0.00035224509955576234\n",
      "test loss is 0.0005735954231085159\n",
      "Batch: 11600,train loss is: 0.000583370251892171\n",
      "test loss is 0.0006000783347912856\n",
      "Batch: 11700,train loss is: 0.000643352510159155\n",
      "test loss is 0.0004956030746172571\n",
      "Batch: 11800,train loss is: 0.0005780282374865464\n",
      "test loss is 0.0007215454513797249\n",
      "Batch: 11900,train loss is: 0.00041990521643061186\n",
      "test loss is 0.0006675727223366935\n",
      "Batch: 12000,train loss is: 0.0005335045892191824\n",
      "test loss is 0.0007262164185482198\n",
      "Batch: 12100,train loss is: 0.000346213879699934\n",
      "test loss is 0.00041056523448906786\n",
      "Batch: 12200,train loss is: 0.0006152200466419145\n",
      "test loss is 0.00042734289657549423\n",
      "Batch: 12300,train loss is: 0.0004999100603903816\n",
      "test loss is 0.0004446252416269082\n",
      "Batch: 12400,train loss is: 0.0003745585935645131\n",
      "test loss is 0.0004183622591595194\n",
      "Batch: 12500,train loss is: 0.0006583297625916726\n",
      "test loss is 0.0006579952811685407\n",
      "Batch: 12600,train loss is: 0.000362638144222429\n",
      "test loss is 0.0005379583571844804\n",
      "Batch: 12700,train loss is: 0.00024244082405868312\n",
      "test loss is 0.0008069494776675522\n",
      "Batch: 12800,train loss is: 0.00035830520121341177\n",
      "test loss is 0.0005866033352856569\n",
      "Batch: 12900,train loss is: 0.0013685195372299076\n",
      "test loss is 0.0006167912831734545\n",
      "Batch: 13000,train loss is: 0.00047662330532323633\n",
      "test loss is 0.0007853933808319803\n",
      "Batch: 13100,train loss is: 0.0003429209024148539\n",
      "test loss is 0.000751077949281854\n",
      "Batch: 13200,train loss is: 0.0007842793826166993\n",
      "test loss is 0.0005061449826696473\n",
      "Batch: 13300,train loss is: 0.000545932034375918\n",
      "test loss is 0.00042611542190273375\n",
      "Batch: 13400,train loss is: 0.0004377246591697042\n",
      "test loss is 0.0005021881473438428\n",
      "Batch: 13500,train loss is: 0.0007298376208884801\n",
      "test loss is 0.0012673923975975363\n",
      "Batch: 13600,train loss is: 0.0002986543072585829\n",
      "test loss is 0.00079064249517867\n",
      "Batch: 13700,train loss is: 0.0005123622697193966\n",
      "test loss is 0.0004999714395072738\n",
      "Batch: 13800,train loss is: 0.0005834701245221844\n",
      "test loss is 0.0005086308477390398\n",
      "Batch: 13900,train loss is: 0.0003844695878341485\n",
      "test loss is 0.0004818969203077374\n",
      "Batch: 14000,train loss is: 0.0005447696462342719\n",
      "test loss is 0.0006536833589700859\n",
      "Batch: 14100,train loss is: 0.0005678653673926145\n",
      "test loss is 0.0007480369146966217\n",
      "Batch: 14200,train loss is: 0.00033849116028219963\n",
      "test loss is 0.0006462616141018003\n",
      "Batch: 14300,train loss is: 0.00046107182175105367\n",
      "test loss is 0.0005119888945996268\n",
      "Batch: 14400,train loss is: 0.000357384561785891\n",
      "test loss is 0.0003922471667573169\n",
      "Batch: 14500,train loss is: 0.0006378664160655559\n",
      "test loss is 0.0004917771040280462\n",
      "Batch: 14600,train loss is: 0.0008111017079271228\n",
      "test loss is 0.0004262074602544273\n",
      "Batch: 14700,train loss is: 0.00037228823618788134\n",
      "test loss is 0.00044182198517769535\n",
      "Batch: 14800,train loss is: 0.0004742904729791404\n",
      "test loss is 0.000521364796885107\n",
      "Batch: 14900,train loss is: 0.00040393902928913977\n",
      "test loss is 0.0004902927088729041\n",
      "Batch: 15000,train loss is: 0.0008536989659668114\n",
      "test loss is 0.000564948276381937\n",
      "Batch: 15100,train loss is: 0.00023604199858638322\n",
      "test loss is 0.0004505503764317794\n",
      "Batch: 15200,train loss is: 0.0006200428250954688\n",
      "test loss is 0.0005039827558824567\n",
      "Batch: 15300,train loss is: 0.0005589420069235652\n",
      "test loss is 0.00041334031217967273\n",
      "Batch: 15400,train loss is: 0.0003709339334964523\n",
      "test loss is 0.0005732609717424777\n",
      "Batch: 15500,train loss is: 0.00043661532702113934\n",
      "test loss is 0.0005730221896898259\n",
      "Batch: 15600,train loss is: 0.0004286395111572669\n",
      "test loss is 0.0004596199870712583\n",
      "Batch: 15700,train loss is: 0.0003205013174920958\n",
      "test loss is 0.00045512168240232487\n",
      "Batch: 15800,train loss is: 0.0006146712064869287\n",
      "test loss is 0.0004590409414703733\n",
      "Batch: 15900,train loss is: 0.0004192844878347475\n",
      "test loss is 0.0004515146661641072\n",
      "Batch: 16000,train loss is: 0.0005821099311512922\n",
      "test loss is 0.00044557950610332893\n",
      "Batch: 16100,train loss is: 0.00031470628583110346\n",
      "test loss is 0.0005330306305820387\n",
      "Batch: 16200,train loss is: 0.0005038623713991234\n",
      "test loss is 0.0004288488551087377\n",
      "Batch: 16300,train loss is: 0.00037747375294178506\n",
      "test loss is 0.0004619497235987083\n",
      "Batch: 16400,train loss is: 0.0003741468793415426\n",
      "test loss is 0.0004369167439721742\n",
      "Batch: 16500,train loss is: 0.0007800419421220397\n",
      "test loss is 0.0004121851865705493\n",
      "Batch: 16600,train loss is: 0.0007511016882803714\n",
      "test loss is 0.0015514160812006089\n",
      "Batch: 16700,train loss is: 0.00034604544840040135\n",
      "test loss is 0.0006689050062073439\n",
      "Batch: 16800,train loss is: 0.000393514484776699\n",
      "test loss is 0.0005341651231643943\n",
      "Batch: 16900,train loss is: 0.0010713117362489918\n",
      "test loss is 0.0004327463910460447\n",
      "Batch: 17000,train loss is: 0.0008875113585330079\n",
      "test loss is 0.0005162675158650862\n",
      "Batch: 17100,train loss is: 0.0004002151581988436\n",
      "test loss is 0.000617080315269952\n",
      "Batch: 17200,train loss is: 0.0003550450908457465\n",
      "test loss is 0.0005096753425315184\n",
      "Batch: 17300,train loss is: 0.0004903107101559129\n",
      "test loss is 0.0004794083404529316\n",
      "Batch: 17400,train loss is: 0.00045666073571839767\n",
      "test loss is 0.0004331587898105229\n",
      "Batch: 17500,train loss is: 0.0006293473205286762\n",
      "test loss is 0.0004068771742012312\n",
      "Batch: 17600,train loss is: 0.0003121279557087919\n",
      "test loss is 0.00059258264337951\n",
      "Batch: 17700,train loss is: 0.0005469107334937032\n",
      "test loss is 0.00044190993244898395\n",
      "Batch: 17800,train loss is: 0.00036886688543032105\n",
      "test loss is 0.0004967463651070983\n",
      "Batch: 17900,train loss is: 0.0003348248791714913\n",
      "test loss is 0.0004220872784392313\n",
      "Batch: 18000,train loss is: 0.0005714369812550005\n",
      "test loss is 0.0004894374386168625\n",
      "Batch: 18100,train loss is: 0.0004890576804209226\n",
      "test loss is 0.000713796275534785\n",
      "Batch: 18200,train loss is: 0.0007364337901332228\n",
      "test loss is 0.0008510745704519868\n",
      "Batch: 18300,train loss is: 0.0008173227404382863\n",
      "test loss is 0.00041635847434386326\n",
      "Batch: 18400,train loss is: 0.00035644779811323454\n",
      "test loss is 0.0005536885585388693\n",
      "Batch: 18500,train loss is: 0.00033961959519484067\n",
      "test loss is 0.0009831368183327517\n",
      "Batch: 18600,train loss is: 0.00024660862613935146\n",
      "test loss is 0.00042255464672548296\n",
      "Batch: 18700,train loss is: 0.00041575795929808536\n",
      "test loss is 0.00043304569386881664\n",
      "Batch: 18800,train loss is: 0.0003478555817174444\n",
      "test loss is 0.0004695795069617789\n",
      "Batch: 18900,train loss is: 0.00041249443578341035\n",
      "test loss is 0.0005593958006625602\n",
      "Batch: 19000,train loss is: 0.0003925829440161615\n",
      "test loss is 0.00044878719035108187\n",
      "Batch: 19100,train loss is: 0.0007551270320565619\n",
      "test loss is 0.0006182840458246435\n",
      "Batch: 19200,train loss is: 0.00038467663605497017\n",
      "test loss is 0.000417188418769879\n",
      "Batch: 19300,train loss is: 0.0005693216615007466\n",
      "test loss is 0.0005642587027455979\n",
      "Batch: 19400,train loss is: 0.0004774416211253558\n",
      "test loss is 0.0005648267518284985\n",
      "Batch: 19500,train loss is: 0.0008768780640379407\n",
      "test loss is 0.0004926012549333203\n",
      "Batch: 19600,train loss is: 0.0005764750816569741\n",
      "test loss is 0.0006034221260597536\n",
      "Batch: 19700,train loss is: 0.0005758860259578774\n",
      "test loss is 0.00043588761660732454\n",
      "Batch: 19800,train loss is: 0.0004694788456830735\n",
      "test loss is 0.0004897264709181896\n",
      "Batch: 19900,train loss is: 0.0004977600105346189\n",
      "test loss is 0.0004467779533481199\n",
      "Batch: 20000,train loss is: 0.000383166938562853\n",
      "test loss is 0.0004342645827835923\n",
      "Batch: 20100,train loss is: 0.00039667069276885883\n",
      "test loss is 0.0006382333523272788\n",
      "Batch: 20200,train loss is: 0.00024537519121321273\n",
      "test loss is 0.0005851694054724372\n",
      "Batch: 20300,train loss is: 0.000669618247866805\n",
      "test loss is 0.00039192750428271853\n",
      "Batch: 20400,train loss is: 0.00028433638659824316\n",
      "test loss is 0.000797068158639047\n",
      "Batch: 20500,train loss is: 0.0006925428102995737\n",
      "test loss is 0.0006945039672356959\n",
      "Batch: 20600,train loss is: 0.000523203420494528\n",
      "test loss is 0.0009424052148744146\n",
      "Batch: 20700,train loss is: 0.0005229233709433344\n",
      "test loss is 0.0004827484559489526\n",
      "Batch: 20800,train loss is: 0.0003314612807324748\n",
      "test loss is 0.00039654831748224377\n",
      "Batch: 20900,train loss is: 0.00046890352111680726\n",
      "test loss is 0.00041359673119318246\n",
      "Batch: 21000,train loss is: 0.0005045031711007494\n",
      "test loss is 0.0004748367117166754\n",
      "Batch: 21100,train loss is: 0.0005878011981263057\n",
      "test loss is 0.0006729793159349759\n",
      "Batch: 21200,train loss is: 0.0006613317581923721\n",
      "test loss is 0.000616949293318014\n",
      "Batch: 21300,train loss is: 0.00031833170304197797\n",
      "test loss is 0.0004430980444603468\n",
      "Batch: 21400,train loss is: 0.0004813390379404814\n",
      "test loss is 0.0005046586330693668\n",
      "Batch: 21500,train loss is: 0.00029363003493647284\n",
      "test loss is 0.000501664145386716\n",
      "Batch: 21600,train loss is: 0.0005354412786445556\n",
      "test loss is 0.00042244092070566053\n",
      "Batch: 21700,train loss is: 0.0003895104362709056\n",
      "test loss is 0.00048326332685181194\n",
      "Batch: 21800,train loss is: 0.0004404980543592122\n",
      "test loss is 0.00040544959663116515\n",
      "Batch: 21900,train loss is: 0.0003459273624765284\n",
      "test loss is 0.00042029320030968715\n",
      "Batch: 22000,train loss is: 0.0003913333923098067\n",
      "test loss is 0.00038070430708651387\n",
      "Batch: 22100,train loss is: 0.00046171555836133435\n",
      "test loss is 0.0006324781052981987\n",
      "Batch: 22200,train loss is: 0.0007480736767553424\n",
      "test loss is 0.0007301908899497411\n",
      "Batch: 22300,train loss is: 0.000515427408905319\n",
      "test loss is 0.0007216916749898062\n",
      "Batch: 22400,train loss is: 0.0009748905181371549\n",
      "test loss is 0.0006832215933800657\n",
      "Batch: 22500,train loss is: 0.0006218078436492178\n",
      "test loss is 0.0007305527458813672\n",
      "Batch: 22600,train loss is: 0.0008646896808799228\n",
      "test loss is 0.0009292167154264522\n",
      "Batch: 22700,train loss is: 0.00048058845422171417\n",
      "test loss is 0.0005102306790523197\n",
      "Batch: 22800,train loss is: 0.0007084077055121327\n",
      "test loss is 0.000483000401064528\n",
      "Batch: 22900,train loss is: 0.0005794098008081419\n",
      "test loss is 0.0004422796775831705\n",
      "Batch: 23000,train loss is: 0.0003022278272683644\n",
      "test loss is 0.00039231274907193373\n",
      "Batch: 23100,train loss is: 0.0006031412928631737\n",
      "test loss is 0.0006026112777129066\n",
      "Batch: 23200,train loss is: 0.0006509171788236171\n",
      "test loss is 0.0006333811933567397\n",
      "Batch: 23300,train loss is: 0.0004874672709381286\n",
      "test loss is 0.0004021229267544586\n",
      "Batch: 23400,train loss is: 0.0003627693812788655\n",
      "test loss is 0.0004275128464409876\n",
      "Batch: 23500,train loss is: 0.0004422557333897638\n",
      "test loss is 0.0004044762144550426\n",
      "Batch: 23600,train loss is: 0.0003646890460972527\n",
      "test loss is 0.000382116358457122\n",
      "Batch: 23700,train loss is: 0.0006075759211337468\n",
      "test loss is 0.00046345346667913155\n",
      "Batch: 23800,train loss is: 0.00034824974358508697\n",
      "test loss is 0.0005567839049272124\n",
      "Batch: 23900,train loss is: 0.0001830939485552347\n",
      "test loss is 0.00042916394443922034\n",
      "Batch: 24000,train loss is: 0.0003997302380048841\n",
      "test loss is 0.00041188235577880786\n",
      "Batch: 24100,train loss is: 0.00036962266191240055\n",
      "test loss is 0.0005745000639856236\n",
      "Batch: 24200,train loss is: 0.000393597941728544\n",
      "test loss is 0.00041933906843292666\n",
      "Batch: 24300,train loss is: 0.00046833633931665645\n",
      "test loss is 0.00045138451570477797\n",
      "Batch: 24400,train loss is: 0.0003011835749416154\n",
      "test loss is 0.0004522989960118229\n",
      "Batch: 24500,train loss is: 0.000580886264331086\n",
      "test loss is 0.00047453681121898344\n",
      "Batch: 24600,train loss is: 0.0003794499251838434\n",
      "test loss is 0.000498207259183029\n",
      "Batch: 24700,train loss is: 0.0003823750611005512\n",
      "test loss is 0.00043945432748061113\n",
      "Batch: 24800,train loss is: 0.0002849508493894075\n",
      "test loss is 0.0005677134998822378\n",
      "Batch: 24900,train loss is: 0.0006643842621290386\n",
      "test loss is 0.00041639268252886987\n",
      "Batch: 25000,train loss is: 0.0006441503393829324\n",
      "test loss is 0.00040773378251853485\n",
      "Batch: 25100,train loss is: 0.00045077314202221215\n",
      "test loss is 0.0006539808938009768\n",
      "Batch: 25200,train loss is: 0.0004853441417228025\n",
      "test loss is 0.00040871960561531136\n",
      "Batch: 25300,train loss is: 0.0005322828953123045\n",
      "test loss is 0.0004494010465180522\n",
      "Batch: 25400,train loss is: 0.000580674247033915\n",
      "test loss is 0.0004735299853744497\n",
      "Batch: 25500,train loss is: 0.0006558201357431629\n",
      "test loss is 0.00046512687497086074\n",
      "Batch: 25600,train loss is: 0.00036791396362622996\n",
      "test loss is 0.0003881793910507773\n",
      "Batch: 25700,train loss is: 0.0008882519193940711\n",
      "test loss is 0.0004267659864388184\n",
      "Batch: 25800,train loss is: 0.0006288193976545356\n",
      "test loss is 0.00044562463039914065\n",
      "Batch: 25900,train loss is: 0.0005673493682144604\n",
      "test loss is 0.000505057524362536\n",
      "Batch: 26000,train loss is: 0.0009122086236063578\n",
      "test loss is 0.00060588709441791\n",
      "Batch: 26100,train loss is: 0.0006747208732213742\n",
      "test loss is 0.0005447424614399785\n",
      "Batch: 26200,train loss is: 0.00042922866008281103\n",
      "test loss is 0.00046580575953558706\n",
      "Batch: 26300,train loss is: 0.0006004568441197565\n",
      "test loss is 0.0005112860609988636\n",
      "Batch: 26400,train loss is: 0.0003891881881762468\n",
      "test loss is 0.00042427538231422097\n",
      "Batch: 26500,train loss is: 0.0003644164230792539\n",
      "test loss is 0.0003654219244264604\n",
      "Batch: 26600,train loss is: 0.00038329981244721636\n",
      "test loss is 0.0004410461970504201\n",
      "Batch: 26700,train loss is: 0.00030707143049905645\n",
      "test loss is 0.0005271676756261747\n",
      "Batch: 26800,train loss is: 0.0006895358089058025\n",
      "test loss is 0.0005403143218268725\n",
      "Batch: 26900,train loss is: 0.0005724068106443787\n",
      "test loss is 0.0004930437545446085\n",
      "Batch: 27000,train loss is: 0.000786292137790838\n",
      "test loss is 0.0006036967793130609\n",
      "Batch: 27100,train loss is: 0.0005041537970276302\n",
      "test loss is 0.000490712844830073\n",
      "Batch: 27200,train loss is: 0.0008748564546031032\n",
      "test loss is 0.0005291705571244253\n",
      "Batch: 27300,train loss is: 0.0008625889919737419\n",
      "test loss is 0.0005493160394475144\n",
      "Batch: 27400,train loss is: 0.0004182462103839823\n",
      "test loss is 0.00040749455184293945\n",
      "Batch: 27500,train loss is: 0.00035552080420859627\n",
      "test loss is 0.0004641245605285129\n",
      "Batch: 27600,train loss is: 0.0003096384874281507\n",
      "test loss is 0.00048551455223930896\n",
      "Batch: 27700,train loss is: 0.0006594427714171295\n",
      "test loss is 0.0004164883548746261\n",
      "Batch: 27800,train loss is: 0.00035692423295666356\n",
      "test loss is 0.0005095251920016519\n",
      "Batch: 27900,train loss is: 0.00032317590715801736\n",
      "test loss is 0.0004922848347817878\n",
      "Batch: 28000,train loss is: 0.00041624919934288654\n",
      "test loss is 0.0004511468803734155\n",
      "Batch: 28100,train loss is: 0.0006041239726551403\n",
      "test loss is 0.0004938974721154686\n",
      "Batch: 28200,train loss is: 0.00047514592878491167\n",
      "test loss is 0.0005551281945380791\n",
      "Batch: 28300,train loss is: 0.0003983765299519587\n",
      "test loss is 0.0004988612997037088\n",
      "Batch: 28400,train loss is: 0.00039419779921416663\n",
      "test loss is 0.00047501473920618883\n",
      "Batch: 28500,train loss is: 0.0004371327394460711\n",
      "test loss is 0.0005108444883674644\n",
      "Batch: 28600,train loss is: 0.000478607031679214\n",
      "test loss is 0.000887815694602134\n",
      "Batch: 28700,train loss is: 0.0003935976818179093\n",
      "test loss is 0.0004211167731883333\n",
      "Batch: 28800,train loss is: 0.0007147292660442052\n",
      "test loss is 0.000573341364427852\n",
      "Batch: 28900,train loss is: 0.000348896223596751\n",
      "test loss is 0.0004215619597063015\n",
      "Batch: 29000,train loss is: 0.00035675890065630495\n",
      "test loss is 0.0006658516083972766\n",
      "Batch: 29100,train loss is: 0.0003686472392280065\n",
      "test loss is 0.0005345092275607491\n",
      "Batch: 29200,train loss is: 0.0004422363169805166\n",
      "test loss is 0.0004123240050124799\n",
      "Batch: 29300,train loss is: 0.00031893655506423006\n",
      "test loss is 0.0006275458648994891\n",
      "Batch: 29400,train loss is: 0.0004143004250213533\n",
      "test loss is 0.0005852689252525766\n",
      "Batch: 29500,train loss is: 0.00043601193454598274\n",
      "test loss is 0.0005082963211026127\n",
      "Batch: 29600,train loss is: 0.0004827519224192828\n",
      "test loss is 0.0005334094866179665\n",
      "Batch: 29700,train loss is: 0.00047293917845449914\n",
      "test loss is 0.00041920630242979273\n",
      "Batch: 29800,train loss is: 0.00043095890521196165\n",
      "test loss is 0.0004384416621431155\n",
      "Batch: 29900,train loss is: 0.00029142450755472495\n",
      "test loss is 0.0005525345335982227\n",
      "Batch: 30000,train loss is: 0.0004444675864660355\n",
      "test loss is 0.0004889714074218435\n",
      "Batch: 30100,train loss is: 0.00097803963792425\n",
      "test loss is 0.0004567133711262816\n",
      "Batch: 30200,train loss is: 0.0007570539869604853\n",
      "test loss is 0.0004887483561512733\n",
      "Batch: 30300,train loss is: 0.0002939837813505217\n",
      "test loss is 0.0005313419349890907\n",
      "Batch: 30400,train loss is: 0.0023263183694225307\n",
      "test loss is 0.00045615341547273504\n",
      "Batch: 30500,train loss is: 0.00032426389222216234\n",
      "test loss is 0.0005041345765100956\n",
      "Batch: 30600,train loss is: 0.0003365140688491008\n",
      "test loss is 0.0005496992525678168\n",
      "Batch: 30700,train loss is: 0.0007679443172416292\n",
      "test loss is 0.0006674382168386572\n",
      "Batch: 30800,train loss is: 0.0006327769042297514\n",
      "test loss is 0.0005246891883427115\n",
      "Batch: 30900,train loss is: 0.00033833607371120864\n",
      "test loss is 0.0004500561625723637\n",
      "Batch: 31000,train loss is: 0.0007572020077192643\n",
      "test loss is 0.0013256547873491534\n",
      "Batch: 31100,train loss is: 0.0008922371879964068\n",
      "test loss is 0.0005541092847144431\n",
      "Batch: 31200,train loss is: 0.0006062220229182019\n",
      "test loss is 0.0004176510739426553\n",
      "Batch: 31300,train loss is: 0.0004023315747133834\n",
      "test loss is 0.00060129632610199\n",
      "Batch: 31400,train loss is: 0.000492909728575016\n",
      "test loss is 0.00042969859986970133\n",
      "Batch: 31500,train loss is: 0.0004888073879975897\n",
      "test loss is 0.00038550685777034047\n",
      "Batch: 31600,train loss is: 0.00049488249814964\n",
      "test loss is 0.0006386316424003292\n",
      "Batch: 31700,train loss is: 0.0003779641839881528\n",
      "test loss is 0.0005064640821010795\n",
      "Batch: 31800,train loss is: 0.0005020290229605952\n",
      "test loss is 0.00046093899415306655\n",
      "Batch: 31900,train loss is: 0.00035744435488920304\n",
      "test loss is 0.00038263635939755235\n",
      "Batch: 32000,train loss is: 0.0002402348956588252\n",
      "test loss is 0.000505921880783659\n",
      "Batch: 32100,train loss is: 0.000343540055238338\n",
      "test loss is 0.00045599714876286756\n",
      "Batch: 32200,train loss is: 0.0006208415350013728\n",
      "test loss is 0.0005906477251758665\n",
      "Batch: 32300,train loss is: 0.0003841270364485222\n",
      "test loss is 0.0007129532924058155\n",
      "Batch: 32400,train loss is: 0.0004483624850361177\n",
      "test loss is 0.0004829611353542631\n",
      "Batch: 32500,train loss is: 0.00036731124128359385\n",
      "test loss is 0.0003995871939566322\n",
      "Batch: 32600,train loss is: 0.00038666064152027434\n",
      "test loss is 0.00039774174036488844\n",
      "Batch: 32700,train loss is: 0.00035112316262802623\n",
      "test loss is 0.00037463036575003907\n",
      "Batch: 32800,train loss is: 0.00044152608806978977\n",
      "test loss is 0.0005870463758160113\n",
      "Batch: 32900,train loss is: 0.0003627691431454849\n",
      "test loss is 0.00036163264908337686\n",
      "Batch: 33000,train loss is: 0.0005417826381641923\n",
      "test loss is 0.0004010593159089922\n",
      "Batch: 33100,train loss is: 0.0003223919815546461\n",
      "test loss is 0.0005014838225489848\n",
      "Batch: 33200,train loss is: 0.0005448850873382438\n",
      "test loss is 0.0004506994158642597\n",
      "Batch: 33300,train loss is: 0.0006508571818164819\n",
      "test loss is 0.0006242024600495876\n",
      "Batch: 33400,train loss is: 0.0002449171193318616\n",
      "test loss is 0.00040706473761784767\n",
      "Batch: 33500,train loss is: 0.0004461615466238821\n",
      "test loss is 0.0004548523529994509\n",
      "Batch: 33600,train loss is: 0.000310967810456658\n",
      "test loss is 0.00045133465554955855\n",
      "Batch: 33700,train loss is: 0.0008758860793838066\n",
      "test loss is 0.0006063325380365812\n",
      "Batch: 33800,train loss is: 0.00030161872150287006\n",
      "test loss is 0.00043568619833980364\n",
      "Batch: 33900,train loss is: 0.000831026701340747\n",
      "test loss is 0.00040593874296067774\n",
      "Batch: 34000,train loss is: 0.0003747169117290323\n",
      "test loss is 0.0004536575897723764\n",
      "Batch: 34100,train loss is: 0.0005720145853407171\n",
      "test loss is 0.00038231082489745185\n",
      "Batch: 34200,train loss is: 0.0006029392101308253\n",
      "test loss is 0.0004975527131969016\n",
      "Batch: 34300,train loss is: 0.00047575432412958076\n",
      "test loss is 0.0004324682687183622\n",
      "Batch: 34400,train loss is: 0.00045883803890197066\n",
      "test loss is 0.00045817279811873307\n",
      "Batch: 34500,train loss is: 0.000508298803883672\n",
      "test loss is 0.0004622571200961002\n",
      "Batch: 34600,train loss is: 0.001001813451207224\n",
      "test loss is 0.00047298693387256495\n",
      "Batch: 34700,train loss is: 0.00026891373154994694\n",
      "test loss is 0.0003852994267757565\n",
      "Batch: 34800,train loss is: 0.0006304231284463512\n",
      "test loss is 0.0004086008358769463\n",
      "Batch: 34900,train loss is: 0.0004681322950583072\n",
      "test loss is 0.0005544509137027308\n",
      "Batch: 35000,train loss is: 0.0004290022084344216\n",
      "test loss is 0.00040069179172947013\n",
      "Batch: 35100,train loss is: 0.0009200114018133596\n",
      "test loss is 0.0006888745545882616\n",
      "Batch: 35200,train loss is: 0.00041503350948850855\n",
      "test loss is 0.0004750600025722244\n",
      "Batch: 35300,train loss is: 0.000270856758587281\n",
      "test loss is 0.00048282129746441484\n",
      "Batch: 35400,train loss is: 0.00033812512513968604\n",
      "test loss is 0.0003964267560188614\n",
      "Batch: 35500,train loss is: 0.0005422387051269484\n",
      "test loss is 0.0005280234383373122\n",
      "Batch: 35600,train loss is: 0.0009777509350660705\n",
      "test loss is 0.0004945193570753102\n",
      "Batch: 35700,train loss is: 0.0002925628821746116\n",
      "test loss is 0.00039515787421741584\n",
      "Batch: 35800,train loss is: 0.00040220275592702274\n",
      "test loss is 0.00041231073431682814\n",
      "Batch: 35900,train loss is: 0.0003563846765179652\n",
      "test loss is 0.0004614442019452033\n",
      "Batch: 36000,train loss is: 0.00039685146216511937\n",
      "test loss is 0.00044053238777526864\n",
      "Batch: 36100,train loss is: 0.0003101521964100789\n",
      "test loss is 0.0006022329229278228\n",
      "Batch: 36200,train loss is: 0.0004050757171357585\n",
      "test loss is 0.00041334813601488383\n",
      "Batch: 36300,train loss is: 0.00036988278091611447\n",
      "test loss is 0.0004577769665367346\n",
      "Batch: 36400,train loss is: 0.0009090150066164914\n",
      "test loss is 0.0004290069383757148\n",
      "Batch: 36500,train loss is: 0.000535259706545555\n",
      "test loss is 0.0004257014035474056\n",
      "Batch: 36600,train loss is: 0.0004792014612482678\n",
      "test loss is 0.00041528564822363145\n",
      "Batch: 36700,train loss is: 0.0004987210050189917\n",
      "test loss is 0.00045942701259393717\n",
      "Batch: 36800,train loss is: 0.0005384198182304216\n",
      "test loss is 0.0004592457673063694\n",
      "Batch: 36900,train loss is: 0.0004289406651902056\n",
      "test loss is 0.000384186869595802\n",
      "Batch: 37000,train loss is: 0.0004953905574085789\n",
      "test loss is 0.00040830545924617465\n",
      "Batch: 37100,train loss is: 0.0004133459973932576\n",
      "test loss is 0.0005355900004316302\n",
      "Batch: 37200,train loss is: 0.0003431857152300318\n",
      "test loss is 0.00039258469663860343\n",
      "Batch: 37300,train loss is: 0.00036555863565036935\n",
      "test loss is 0.0006723783549797345\n",
      "Batch: 37400,train loss is: 0.0004704980414005448\n",
      "test loss is 0.00045809355879411896\n",
      "Batch: 37500,train loss is: 0.0008177675430520285\n",
      "test loss is 0.0005629353325491323\n",
      "Batch: 37600,train loss is: 0.0005807713960976509\n",
      "test loss is 0.00045691251611530703\n",
      "Batch: 37700,train loss is: 0.0003244093578113382\n",
      "test loss is 0.00045660206648246576\n",
      "Batch: 37800,train loss is: 0.0008622625827885732\n",
      "test loss is 0.000583012100945298\n",
      "Batch: 37900,train loss is: 0.0005808729581485366\n",
      "test loss is 0.00037441414912555354\n",
      "Batch: 38000,train loss is: 0.00033487043506050365\n",
      "test loss is 0.0005163716181617938\n",
      "Batch: 38100,train loss is: 0.00045751997151159966\n",
      "test loss is 0.0005914775031875077\n",
      "Batch: 38200,train loss is: 0.0003920481116738984\n",
      "test loss is 0.00047843219953852756\n",
      "Batch: 38300,train loss is: 0.0008016279962438345\n",
      "test loss is 0.001074015928096672\n",
      "Batch: 38400,train loss is: 0.00042851321521187257\n",
      "test loss is 0.0003851000484024546\n",
      "Batch: 38500,train loss is: 0.00029620810555536356\n",
      "test loss is 0.0007356159302035183\n",
      "Batch: 38600,train loss is: 0.00037781451297692653\n",
      "test loss is 0.00039466815557360276\n",
      "Batch: 38700,train loss is: 0.0006621522120720575\n",
      "test loss is 0.00039869090663790277\n",
      "Batch: 38800,train loss is: 0.0004919585669335406\n",
      "test loss is 0.00044843664703796593\n",
      "Batch: 38900,train loss is: 0.0004096141527687886\n",
      "test loss is 0.0006349203499118781\n",
      "Batch: 39000,train loss is: 0.0003091483240728987\n",
      "test loss is 0.0004473193408353749\n",
      "Batch: 39100,train loss is: 0.0003142585469622225\n",
      "test loss is 0.0005588964142216148\n",
      "Batch: 39200,train loss is: 0.00048041884086493924\n",
      "test loss is 0.00048230074145275543\n",
      "Batch: 39300,train loss is: 0.00044441216587308115\n",
      "test loss is 0.0004651759111134535\n",
      "Batch: 39400,train loss is: 0.00028861385257844476\n",
      "test loss is 0.0004214266707952673\n",
      "Batch: 39500,train loss is: 0.0003440783058741619\n",
      "test loss is 0.00039012502699369554\n",
      "Batch: 39600,train loss is: 0.0010608672648434563\n",
      "test loss is 0.0005021353201930853\n",
      "Batch: 39700,train loss is: 0.0005316746118983714\n",
      "test loss is 0.0005461457756366317\n",
      "Batch: 39800,train loss is: 0.0004880533433232386\n",
      "test loss is 0.0005275959025007702\n",
      "Batch: 39900,train loss is: 0.0005721389622252351\n",
      "test loss is 0.00043048222472701043\n",
      "Batch: 40000,train loss is: 0.00024927495936779805\n",
      "test loss is 0.0004952701241758155\n",
      "Batch: 40100,train loss is: 0.00042062556287845597\n",
      "test loss is 0.0004594184500385958\n",
      "Batch: 40200,train loss is: 0.00046018581507717475\n",
      "test loss is 0.00036043444092916083\n",
      "Batch: 40300,train loss is: 0.00036263009020539086\n",
      "test loss is 0.00037961074567246963\n",
      "Batch: 40400,train loss is: 0.0004088792090602306\n",
      "test loss is 0.0003700694095708335\n",
      "Batch: 40500,train loss is: 0.00024820336027543536\n",
      "test loss is 0.0004437250217557024\n",
      "Batch: 40600,train loss is: 0.0003994836682724131\n",
      "test loss is 0.00044752337739232975\n",
      "Batch: 40700,train loss is: 0.0004297250670798308\n",
      "test loss is 0.00042316354962812383\n",
      "Batch: 40800,train loss is: 0.00042129930774318584\n",
      "test loss is 0.0004907697874412091\n",
      "Batch: 40900,train loss is: 0.0008910320148834466\n",
      "test loss is 0.00046100776943075916\n",
      "Batch: 41000,train loss is: 0.0002942235494491761\n",
      "test loss is 0.0004145824763021008\n",
      "Batch: 41100,train loss is: 0.00046599773967009346\n",
      "test loss is 0.00042889009698087577\n",
      "Batch: 41200,train loss is: 0.0005954135323919126\n",
      "test loss is 0.0006384901145251107\n",
      "Batch: 41300,train loss is: 0.00036293086619041\n",
      "test loss is 0.00042011979901811875\n",
      "Batch: 41400,train loss is: 0.0009579554249254125\n",
      "test loss is 0.0006617679003777715\n",
      "Batch: 41500,train loss is: 0.000372700704159068\n",
      "test loss is 0.00048905129311743\n",
      "Batch: 41600,train loss is: 0.00029948538645405815\n",
      "test loss is 0.0004118962897795938\n",
      "Batch: 41700,train loss is: 0.0003952698910520435\n",
      "test loss is 0.00043222245206043245\n",
      "Batch: 41800,train loss is: 0.00024944449348232416\n",
      "test loss is 0.00039738739682120087\n",
      "Batch: 41900,train loss is: 0.0006542995156345307\n",
      "test loss is 0.0004471987344482662\n",
      "Batch: 42000,train loss is: 0.00029721963931927497\n",
      "test loss is 0.00046390793112781076\n",
      "Batch: 42100,train loss is: 0.00035477728335696877\n",
      "test loss is 0.0006925334247221343\n",
      "Batch: 42200,train loss is: 0.000574022648762386\n",
      "test loss is 0.0004736817773460898\n",
      "Batch: 42300,train loss is: 0.001019732678707762\n",
      "test loss is 0.0009752530765180141\n",
      "Batch: 42400,train loss is: 0.0004679733935077912\n",
      "test loss is 0.00048805683790064487\n",
      "Batch: 42500,train loss is: 0.00037181623217352706\n",
      "test loss is 0.00040475718360578666\n",
      "Batch: 42600,train loss is: 0.0002802628965835924\n",
      "test loss is 0.0005711673445756467\n",
      "Batch: 42700,train loss is: 0.0008878140978572217\n",
      "test loss is 0.0004980650971946778\n",
      "Batch: 42800,train loss is: 0.0005342246485465776\n",
      "test loss is 0.0005368368741309258\n",
      "Batch: 42900,train loss is: 0.0005043921859061212\n",
      "test loss is 0.0005528280703134502\n",
      "Batch: 43000,train loss is: 0.0003228184577523668\n",
      "test loss is 0.0005268323221558782\n",
      "Batch: 43100,train loss is: 0.0003162934016583095\n",
      "test loss is 0.0007370516008182771\n",
      "Batch: 43200,train loss is: 0.00046899442803048934\n",
      "test loss is 0.00037044153693197626\n",
      "Batch: 43300,train loss is: 0.00044249241925993174\n",
      "test loss is 0.0004185066919897647\n",
      "Batch: 43400,train loss is: 0.0004919837679127313\n",
      "test loss is 0.0004189408568905701\n",
      "Batch: 43500,train loss is: 0.0004902426308482106\n",
      "test loss is 0.00046678386846945105\n",
      "Batch: 43600,train loss is: 0.0003424459541805104\n",
      "test loss is 0.0003588476591990746\n",
      "Batch: 43700,train loss is: 0.0008118673852627747\n",
      "test loss is 0.0004144361255145089\n",
      "Batch: 43800,train loss is: 0.00028980793047516095\n",
      "test loss is 0.0003794155394054528\n",
      "Batch: 43900,train loss is: 0.000355314656905408\n",
      "test loss is 0.00044346389784159757\n",
      "Batch: 44000,train loss is: 0.0006035072973223893\n",
      "test loss is 0.0004751032316442225\n",
      "Batch: 44100,train loss is: 0.0004181641633940698\n",
      "test loss is 0.0004194875293038037\n",
      "Batch: 44200,train loss is: 0.0003285648831678977\n",
      "test loss is 0.0004803937852800625\n",
      "Batch: 44300,train loss is: 0.0003648378894422609\n",
      "test loss is 0.0005917255718349483\n",
      "Batch: 44400,train loss is: 0.0002514725736808645\n",
      "test loss is 0.00040820510857999783\n",
      "Batch: 44500,train loss is: 0.0003699437293265351\n",
      "test loss is 0.0005189377393767232\n",
      "Batch: 44600,train loss is: 0.00029823241508547646\n",
      "test loss is 0.00043483851306983304\n",
      "Batch: 44700,train loss is: 0.0002273017935487705\n",
      "test loss is 0.00038989188585568493\n",
      "Batch: 44800,train loss is: 0.00023476144822589528\n",
      "test loss is 0.00043184666950091434\n",
      "Batch: 44900,train loss is: 0.0003205544155170588\n",
      "test loss is 0.0006203232882043145\n",
      "Batch: 45000,train loss is: 0.00028732513788776436\n",
      "test loss is 0.00037084251186371214\n",
      "Batch: 45100,train loss is: 0.0003997904062952545\n",
      "test loss is 0.00046205953229360764\n",
      "Batch: 45200,train loss is: 0.0008742048407089719\n",
      "test loss is 0.0007285543351615269\n",
      "Batch: 45300,train loss is: 0.00047517323560818694\n",
      "test loss is 0.000651431680651772\n",
      "Batch: 45400,train loss is: 0.0002537825390107216\n",
      "test loss is 0.0004949820120267067\n",
      "Batch: 45500,train loss is: 0.0005252247180223808\n",
      "test loss is 0.0004207141837243784\n",
      "Batch: 45600,train loss is: 0.00035478852762384867\n",
      "test loss is 0.0004956608487777045\n",
      "Batch: 45700,train loss is: 0.0004907585901034535\n",
      "test loss is 0.00040067023166560645\n",
      "Batch: 45800,train loss is: 0.0004057887818131533\n",
      "test loss is 0.0007404010431595601\n",
      "Batch: 45900,train loss is: 0.0006626577202677764\n",
      "test loss is 0.0006110781857661064\n",
      "Batch: 46000,train loss is: 0.00024343694067643687\n",
      "test loss is 0.0008560251027194385\n",
      "Batch: 46100,train loss is: 0.00034545620691032227\n",
      "test loss is 0.0005200364165543596\n",
      "Batch: 46200,train loss is: 0.0005899513734545672\n",
      "test loss is 0.00048361602510179805\n",
      "Batch: 46300,train loss is: 0.00026926879919762737\n",
      "test loss is 0.00040889115063401724\n",
      "Batch: 46400,train loss is: 0.0003276275748287515\n",
      "test loss is 0.0003702881816702603\n",
      "Batch: 46500,train loss is: 0.0015398941729542134\n",
      "test loss is 0.0005452848684719753\n",
      "Batch: 46600,train loss is: 0.0004559424391766734\n",
      "test loss is 0.0005031867260753592\n",
      "Batch: 46700,train loss is: 0.0003758446468097315\n",
      "test loss is 0.0003737212162476253\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch_NN.train import train_model\n",
    "\n",
    "train_model(loss_MSE,optim_Adam,model,data_loader,train_data,test_data,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'../../Data/YU/rflatBergomi_pointwise.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "initial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
