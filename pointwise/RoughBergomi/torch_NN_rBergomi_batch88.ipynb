{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "f = gzip.GzipFile(r\"../../Data/rBergomiTrainSet.txt.gz\", \"r\")\n",
    "dat=np.load(f)\n",
    "xx=dat[:,:4]\n",
    "yy=dat[:,4:]\n",
    "strikes=np.array([0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4,1.5 ])\n",
    "maturities=np.array([0.1,0.3,0.6,0.9,1.2,1.5,1.8,2.0 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    xx, yy, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_and_expand(a,x,y):\n",
    "    # use choose and where !\n",
    "    n = len(x)*len(y)\n",
    "    a_index = np.arange(len(a))%n\n",
    "    \n",
    "    \n",
    "    x_index = a_index//len(y)\n",
    "    y_index = a_index%len(y)\n",
    "    \n",
    "    x_added = np.choose(x_index,x.reshape(-1,1)).reshape(-1,1)\n",
    "    y_added = np.choose(y_index,y.reshape(-1,1)).reshape(-1,1)\n",
    "    \n",
    "    return np.hstack([a,x_added,y_added])\n",
    "\n",
    "y_train,y_test = y_train.reshape(-1,8,11),y_test.reshape(-1,8,11)\n",
    "x_train,x_test = np.repeat(x_train, 8*11,axis=0),np.repeat(x_test, 8*11,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test=append_and_expand(x_train,maturities,strikes),append_and_expand(x_test,maturities,strikes)\n",
    "y_train,y_test = y_train.reshape(-1).reshape(-1,1), y_test.reshape(-1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "scale_y=  StandardScaler()\n",
    "\n",
    "def ytransform(y_train,y_test):\n",
    "    return [scale_y.fit_transform(y_train),scale_y.transform(y_test)]\n",
    "\n",
    "def yinversetransform(y):\n",
    "    return scale_y.inverse_transform(y)\n",
    "\n",
    "# Upper and lower bounds used in the training set\n",
    "ub=np.array([0.16,4,-0.1,0.5,2.0,1.5])\n",
    "lb=np.array([0.01,0.3,-0.95,0.025,0.1,0.5])\n",
    "\n",
    "def myscale(x):\n",
    "    return (x - (ub+lb)*0.5)*2/(ub-lb)\n",
    "def myinverse(x):\n",
    "    return x*(ub-lb)*0.5+(ub+lb)*0.5\n",
    "\n",
    "x_train_transform = myscale(x_train)\n",
    "x_test_transform = myscale(x_test)\n",
    "[y_train_transform,y_test_transform] = ytransform(y_train,y_test)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"device is {device}\")\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(x_train_transform).to(device=device),\n",
    "                                               torch.from_numpy(y_train_transform).to(device=device))\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.from_numpy(x_test_transform).to(device=device),\n",
    "                                              torch.from_numpy(y_test_transform).to(device=device))\n",
    "\n",
    "\n",
    "train_data = (torch.from_numpy(x_train_transform).to(device=device),torch.from_numpy(y_train_transform).to(device=device))\n",
    "test_data = (torch.from_numpy(x_test_transform).to(device=device),torch.from_numpy(y_test_transform).to(device=device))\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(train_dataset,batch_size =88,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')  # Add the parent directory to the Python path\n",
    "\n",
    "from torch_NN.nn import ResNN_pricing\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "hyperparas = {'input_dim':6,'hidden_dim':64,'hidden_nums':10,'output_dim':1,'block_layer_nums':3}\n",
    "\n",
    "model = ResNN_pricing(hyperparas=hyperparas).to(device=device,dtype=torch.float64)\n",
    "\n",
    "loss_MSE = nn.MSELoss()\n",
    "optim_Adam = torch.optim.Adam(model.parameters(),lr= 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Epoch: 0----------------------------------\n",
      "Batch: 0,train loss is: 31.156474178896744\n",
      "test loss is 30.606977827973218\n",
      "Batch: 100,train loss is: 0.7649635856511395\n",
      "test loss is 0.5948405447769134\n",
      "Batch: 200,train loss is: 0.24970110095104248\n",
      "test loss is 0.320732422210515\n",
      "Batch: 300,train loss is: 0.11754152049552177\n",
      "test loss is 0.22375495942034973\n",
      "Batch: 400,train loss is: 0.04698420463815498\n",
      "test loss is 0.16758846367267985\n",
      "Batch: 500,train loss is: 0.035451536867497316\n",
      "test loss is 0.20793402800558755\n",
      "Batch: 600,train loss is: 0.0158520087502169\n",
      "test loss is 0.10674091613938971\n",
      "Batch: 700,train loss is: 0.026357052593868203\n",
      "test loss is 0.09657108887458654\n",
      "Batch: 800,train loss is: 0.048291075913838806\n",
      "test loss is 0.08036734397736131\n",
      "Batch: 900,train loss is: 0.07352593843935042\n",
      "test loss is 0.06753446116296673\n",
      "Batch: 1000,train loss is: 0.05217992400472053\n",
      "test loss is 0.059058582658887984\n",
      "Batch: 1100,train loss is: 0.17038690341984858\n",
      "test loss is 0.0530723319373352\n",
      "Batch: 1200,train loss is: 0.017913090640406897\n",
      "test loss is 0.06011068787710034\n",
      "Batch: 1300,train loss is: 0.03614360810256664\n",
      "test loss is 0.05304154071452525\n",
      "Batch: 1400,train loss is: 0.06683322980855771\n",
      "test loss is 0.046363910323173364\n",
      "Batch: 1500,train loss is: 0.03737299260959339\n",
      "test loss is 0.050992761179766394\n",
      "Batch: 1600,train loss is: 0.0444197618006975\n",
      "test loss is 0.04117171088637593\n",
      "Batch: 1700,train loss is: 0.05756141252972712\n",
      "test loss is 0.04017421663538625\n",
      "Batch: 1800,train loss is: 0.012119728783875027\n",
      "test loss is 0.030779794373076867\n",
      "Batch: 1900,train loss is: 0.025899993224682918\n",
      "test loss is 0.05958931679618457\n",
      "Batch: 2000,train loss is: 0.027398382205988236\n",
      "test loss is 0.03199566121535258\n",
      "Batch: 2100,train loss is: 0.05606905013404145\n",
      "test loss is 0.03157421015898131\n",
      "Batch: 2200,train loss is: 0.033570797670543134\n",
      "test loss is 0.02950106816382797\n",
      "Batch: 2300,train loss is: 0.016870038708781135\n",
      "test loss is 0.023806980665381967\n",
      "Batch: 2400,train loss is: 0.01562246441806287\n",
      "test loss is 0.022142329608736935\n",
      "Batch: 2500,train loss is: 0.036406959192076815\n",
      "test loss is 0.024905921923102932\n",
      "Batch: 2600,train loss is: 0.025504884953280082\n",
      "test loss is 0.02184104591525898\n",
      "Batch: 2700,train loss is: 0.011049762690506525\n",
      "test loss is 0.021538168815002028\n",
      "Batch: 2800,train loss is: 0.006559527542691306\n",
      "test loss is 0.01990682072530181\n",
      "Batch: 2900,train loss is: 0.028094317913337236\n",
      "test loss is 0.02253328410613448\n",
      "Batch: 3000,train loss is: 0.01090260654115624\n",
      "test loss is 0.020875187765497676\n",
      "Batch: 3100,train loss is: 0.017713557877943643\n",
      "test loss is 0.017571128346958818\n",
      "Batch: 3200,train loss is: 0.03375430491135391\n",
      "test loss is 0.02198339419295449\n",
      "Batch: 3300,train loss is: 0.023590262499995126\n",
      "test loss is 0.020953189668841677\n",
      "Batch: 3400,train loss is: 0.008789716302544453\n",
      "test loss is 0.028970573610652993\n",
      "Batch: 3500,train loss is: 0.009938498891128873\n",
      "test loss is 0.01616966533949282\n",
      "Batch: 3600,train loss is: 0.007664871299640293\n",
      "test loss is 0.018285264699827823\n",
      "Batch: 3700,train loss is: 0.00491911266703341\n",
      "test loss is 0.020960541760882037\n",
      "Batch: 3800,train loss is: 0.020819287394128406\n",
      "test loss is 0.018522924106824126\n",
      "Batch: 3900,train loss is: 0.032069112969713064\n",
      "test loss is 0.015679585455476854\n",
      "Batch: 4000,train loss is: 0.010641902559799934\n",
      "test loss is 0.015595103857125816\n",
      "Batch: 4100,train loss is: 0.011231109340994944\n",
      "test loss is 0.019137216672013256\n",
      "Batch: 4200,train loss is: 0.023911819755651143\n",
      "test loss is 0.015319929345052831\n",
      "Batch: 4300,train loss is: 0.014197017841408461\n",
      "test loss is 0.016119199603608608\n",
      "Batch: 4400,train loss is: 0.00703571546311005\n",
      "test loss is 0.011841425428307665\n",
      "Batch: 4500,train loss is: 0.0087731912210464\n",
      "test loss is 0.011530603018850817\n",
      "Batch: 4600,train loss is: 0.022292660169731773\n",
      "test loss is 0.014682092729439914\n",
      "Batch: 4700,train loss is: 0.020540297673756727\n",
      "test loss is 0.01846818966269832\n",
      "Batch: 4800,train loss is: 0.012495001553711175\n",
      "test loss is 0.02739998066307211\n",
      "Batch: 4900,train loss is: 0.026340428188504628\n",
      "test loss is 0.022137493746469613\n",
      "Batch: 5000,train loss is: 0.0044055276493108735\n",
      "test loss is 0.013462289269058774\n",
      "Batch: 5100,train loss is: 0.018499221400960813\n",
      "test loss is 0.04282675446446877\n",
      "Batch: 5200,train loss is: 0.00972154886117521\n",
      "test loss is 0.017237851459107088\n",
      "Batch: 5300,train loss is: 0.018244259701410876\n",
      "test loss is 0.030942063577022935\n",
      "Batch: 5400,train loss is: 0.028303085806896156\n",
      "test loss is 0.010052056098525868\n",
      "Batch: 5500,train loss is: 0.047765973165181094\n",
      "test loss is 0.03804709648148111\n",
      "Batch: 5600,train loss is: 0.007921249013769494\n",
      "test loss is 0.012624518357632007\n",
      "Batch: 5700,train loss is: 0.018162659142561568\n",
      "test loss is 0.018827327681374382\n",
      "Batch: 5800,train loss is: 0.009544188337044998\n",
      "test loss is 0.012381723680896023\n",
      "Batch: 5900,train loss is: 0.09409057074847102\n",
      "test loss is 0.018698602943703\n",
      "Batch: 6000,train loss is: 0.0155409997963797\n",
      "test loss is 0.013362436913080674\n",
      "Batch: 6100,train loss is: 0.02693894097765145\n",
      "test loss is 0.01884107867334959\n",
      "Batch: 6200,train loss is: 0.018075509431729295\n",
      "test loss is 0.015802898557030518\n",
      "Batch: 6300,train loss is: 0.007578475037752589\n",
      "test loss is 0.010654574750281236\n",
      "Batch: 6400,train loss is: 0.006998402120453749\n",
      "test loss is 0.021054677700603385\n",
      "Batch: 6500,train loss is: 0.004666012416899839\n",
      "test loss is 0.016392355836898166\n",
      "Batch: 6600,train loss is: 0.02647957191175936\n",
      "test loss is 0.01509841254032899\n",
      "Batch: 6700,train loss is: 0.003853008000421336\n",
      "test loss is 0.01040508326435135\n",
      "Batch: 6800,train loss is: 0.02611721998833091\n",
      "test loss is 0.009855580082685344\n",
      "Batch: 6900,train loss is: 0.007279598540468524\n",
      "test loss is 0.008037201341844456\n",
      "Batch: 7000,train loss is: 0.009922269016079259\n",
      "test loss is 0.008722771311606292\n",
      "Batch: 7100,train loss is: 0.01988269531966836\n",
      "test loss is 0.037845961118577395\n",
      "Batch: 7200,train loss is: 0.0055212004292226535\n",
      "test loss is 0.010443607978773795\n",
      "Batch: 7300,train loss is: 0.016079101585597406\n",
      "test loss is 0.013783240773568784\n",
      "Batch: 7400,train loss is: 0.006536983399415232\n",
      "test loss is 0.010319520768204022\n",
      "Batch: 7500,train loss is: 0.010395563232830902\n",
      "test loss is 0.008109500228652134\n",
      "Batch: 7600,train loss is: 0.00821117066428431\n",
      "test loss is 0.00756333347413595\n",
      "Batch: 7700,train loss is: 0.023541263713026252\n",
      "test loss is 0.008060420982470144\n",
      "Batch: 7800,train loss is: 0.0025630115198668436\n",
      "test loss is 0.01090329758659176\n",
      "Batch: 7900,train loss is: 0.007307231205491657\n",
      "test loss is 0.01100612631133934\n",
      "Batch: 8000,train loss is: 0.006655127407646533\n",
      "test loss is 0.011704717023011279\n",
      "Batch: 8100,train loss is: 0.059678887046707825\n",
      "test loss is 0.028224019438752534\n",
      "Batch: 8200,train loss is: 0.033145543035592404\n",
      "test loss is 0.020766110448075078\n",
      "Batch: 8300,train loss is: 0.005917820572224349\n",
      "test loss is 0.022798582251174534\n",
      "Batch: 8400,train loss is: 0.01769165196707253\n",
      "test loss is 0.012918878168434319\n",
      "Batch: 8500,train loss is: 0.010193117792655084\n",
      "test loss is 0.00807164187644786\n",
      "Batch: 8600,train loss is: 0.010594032756846868\n",
      "test loss is 0.020812634404054373\n",
      "Batch: 8700,train loss is: 0.005145657901858302\n",
      "test loss is 0.01443398848524416\n",
      "Batch: 8800,train loss is: 0.011401659538558623\n",
      "test loss is 0.011591564986319774\n",
      "Batch: 8900,train loss is: 0.013673892395729376\n",
      "test loss is 0.015880354771694603\n",
      "Batch: 9000,train loss is: 0.002364027741227739\n",
      "test loss is 0.010522449634681845\n",
      "Batch: 9100,train loss is: 0.004160947176898827\n",
      "test loss is 0.01000459031004695\n",
      "Batch: 9200,train loss is: 0.009154247805825128\n",
      "test loss is 0.005929747709636929\n",
      "Batch: 9300,train loss is: 0.005280422656386344\n",
      "test loss is 0.009230373040367902\n",
      "Batch: 9400,train loss is: 0.00402447577883667\n",
      "test loss is 0.008416007375616893\n",
      "Batch: 9500,train loss is: 0.009824721490315888\n",
      "test loss is 0.01383582381647472\n",
      "Batch: 9600,train loss is: 0.010589012159192352\n",
      "test loss is 0.012243125164373748\n",
      "Batch: 9700,train loss is: 0.004994661403299232\n",
      "test loss is 0.011369702029014158\n",
      "Batch: 9800,train loss is: 0.0032742891974992428\n",
      "test loss is 0.006135700884273116\n",
      "Batch: 9900,train loss is: 0.00926963360520774\n",
      "test loss is 0.03120498888439195\n",
      "Batch: 10000,train loss is: 0.0030479582373733037\n",
      "test loss is 0.008882511673345785\n",
      "Batch: 10100,train loss is: 0.019810225363036716\n",
      "test loss is 0.006115990188440655\n",
      "Batch: 10200,train loss is: 0.02472561948725336\n",
      "test loss is 0.016575637552021082\n",
      "Batch: 10300,train loss is: 0.01094902041997922\n",
      "test loss is 0.029627341230537844\n",
      "Batch: 10400,train loss is: 0.008327912156347277\n",
      "test loss is 0.015251135439717756\n",
      "Batch: 10500,train loss is: 0.018164901287231913\n",
      "test loss is 0.007256290238098285\n",
      "Batch: 10600,train loss is: 0.0018869342664407968\n",
      "test loss is 0.0073135975933905925\n",
      "Batch: 10700,train loss is: 0.007450488190022491\n",
      "test loss is 0.01316436581394809\n",
      "Batch: 10800,train loss is: 0.01084664951403848\n",
      "test loss is 0.02944044585665276\n",
      "Batch: 10900,train loss is: 0.004478748144915507\n",
      "test loss is 0.007133393344585405\n",
      "Batch: 11000,train loss is: 0.005168294882148987\n",
      "test loss is 0.006617928803868389\n",
      "Batch: 11100,train loss is: 0.0065984292940166386\n",
      "test loss is 0.006631260307666661\n",
      "Batch: 11200,train loss is: 0.009263049190690103\n",
      "test loss is 0.0076229514436738964\n",
      "Batch: 11300,train loss is: 0.009050816063763642\n",
      "test loss is 0.009743972388556808\n",
      "Batch: 11400,train loss is: 0.004316233856971661\n",
      "test loss is 0.005447442109442207\n",
      "Batch: 11500,train loss is: 0.010522011233355317\n",
      "test loss is 0.011041540130159844\n",
      "Batch: 11600,train loss is: 0.008583765915181509\n",
      "test loss is 0.005671058320278152\n",
      "Batch: 11700,train loss is: 0.003874017333277055\n",
      "test loss is 0.005415790979442466\n",
      "Batch: 11800,train loss is: 0.00521295884597137\n",
      "test loss is 0.006202932831101463\n",
      "Batch: 11900,train loss is: 0.006515567766437432\n",
      "test loss is 0.0055855447540367415\n",
      "Batch: 12000,train loss is: 0.0412627805512843\n",
      "test loss is 0.013794255032336347\n",
      "Batch: 12100,train loss is: 0.052530778396625635\n",
      "test loss is 0.011228233494042283\n",
      "Batch: 12200,train loss is: 0.00321800488042646\n",
      "test loss is 0.007142155376253499\n",
      "Batch: 12300,train loss is: 0.007656511678378393\n",
      "test loss is 0.005971747297457079\n",
      "Batch: 12400,train loss is: 0.0036598277978874896\n",
      "test loss is 0.004727123718368861\n",
      "Batch: 12500,train loss is: 0.006326865869736458\n",
      "test loss is 0.00605029213227916\n",
      "Batch: 12600,train loss is: 0.0036383840352494086\n",
      "test loss is 0.006915838977593272\n",
      "Batch: 12700,train loss is: 0.0023605286023419244\n",
      "test loss is 0.0058567614540077995\n",
      "Batch: 12800,train loss is: 0.0018766945248167362\n",
      "test loss is 0.005994329991778612\n",
      "Batch: 12900,train loss is: 0.006024682249104826\n",
      "test loss is 0.013887534273446314\n",
      "Batch: 13000,train loss is: 0.004188704937646323\n",
      "test loss is 0.006814535871464036\n",
      "Batch: 13100,train loss is: 0.015337521701688159\n",
      "test loss is 0.005183351151803978\n",
      "Batch: 13200,train loss is: 0.0069799051524669165\n",
      "test loss is 0.006733258144876736\n",
      "Batch: 13300,train loss is: 0.0018955703512127304\n",
      "test loss is 0.0061459064194368486\n",
      "Batch: 13400,train loss is: 0.004154779374081251\n",
      "test loss is 0.009582387465245411\n",
      "Batch: 13500,train loss is: 0.005325614505579651\n",
      "test loss is 0.0077833672180226245\n",
      "Batch: 13600,train loss is: 0.0020659380329790073\n",
      "test loss is 0.009231396507573065\n",
      "Batch: 13700,train loss is: 0.006626055549642007\n",
      "test loss is 0.005857240178882332\n",
      "Batch: 13800,train loss is: 0.0021262836127531187\n",
      "test loss is 0.006850735360236062\n",
      "Batch: 13900,train loss is: 0.0026928044834441714\n",
      "test loss is 0.007189260424999206\n",
      "Batch: 14000,train loss is: 0.003526744945395237\n",
      "test loss is 0.008947558650821008\n",
      "Batch: 14100,train loss is: 0.00771702158918475\n",
      "test loss is 0.008675108615681455\n",
      "Batch: 14200,train loss is: 0.003354037492064437\n",
      "test loss is 0.004518569226217141\n",
      "Batch: 14300,train loss is: 0.0014064637435672667\n",
      "test loss is 0.004970181468781219\n",
      "Batch: 14400,train loss is: 0.00252523458207098\n",
      "test loss is 0.005524417934536273\n",
      "Batch: 14500,train loss is: 0.0029617056808545414\n",
      "test loss is 0.004234654974876388\n",
      "Batch: 14600,train loss is: 0.01140562776857349\n",
      "test loss is 0.006642848379540067\n",
      "Batch: 14700,train loss is: 0.017098154540495982\n",
      "test loss is 0.017609329432701763\n",
      "Batch: 14800,train loss is: 0.021045820237186843\n",
      "test loss is 0.013816707560227123\n",
      "Batch: 14900,train loss is: 0.008709737727394357\n",
      "test loss is 0.006420983959040555\n",
      "Batch: 15000,train loss is: 0.004076792959641045\n",
      "test loss is 0.006621235270164401\n",
      "Batch: 15100,train loss is: 0.005075231721965381\n",
      "test loss is 0.005946031497813349\n",
      "Batch: 15200,train loss is: 0.003102673750861639\n",
      "test loss is 0.005108903969637158\n",
      "Batch: 15300,train loss is: 0.011633967247713333\n",
      "test loss is 0.005085733703382739\n",
      "Batch: 15400,train loss is: 0.0020247062148927644\n",
      "test loss is 0.005006664865448185\n",
      "Batch: 15500,train loss is: 0.009780337146988897\n",
      "test loss is 0.006704543253641313\n",
      "Batch: 15600,train loss is: 0.0030129912296641237\n",
      "test loss is 0.006080608500783796\n",
      "Batch: 15700,train loss is: 0.00544231792475319\n",
      "test loss is 0.0075756272919167205\n",
      "Batch: 15800,train loss is: 0.00429964023941917\n",
      "test loss is 0.00690356657215464\n",
      "Batch: 15900,train loss is: 0.010919494373629364\n",
      "test loss is 0.0052972028645052105\n",
      "Batch: 16000,train loss is: 0.00276036841629992\n",
      "test loss is 0.005949840398918971\n",
      "Batch: 16100,train loss is: 0.004742768861006927\n",
      "test loss is 0.004810890953023219\n",
      "Batch: 16200,train loss is: 0.006379952960296294\n",
      "test loss is 0.013551375210254865\n",
      "Batch: 16300,train loss is: 0.004151386346340134\n",
      "test loss is 0.006844395247402152\n",
      "Batch: 16400,train loss is: 0.002214104095880824\n",
      "test loss is 0.003537818063292435\n",
      "Batch: 16500,train loss is: 0.02084055538500229\n",
      "test loss is 0.014090646481759353\n",
      "Batch: 16600,train loss is: 0.002857816973821105\n",
      "test loss is 0.013121336657678867\n",
      "Batch: 16700,train loss is: 0.006455295367397452\n",
      "test loss is 0.005515246387791456\n",
      "Batch: 16800,train loss is: 0.004227389440077011\n",
      "test loss is 0.004322453004765167\n",
      "Batch: 16900,train loss is: 0.007208916596998673\n",
      "test loss is 0.009698700480394914\n",
      "Batch: 17000,train loss is: 0.018589001162493474\n",
      "test loss is 0.021512960069071392\n",
      "Batch: 17100,train loss is: 0.0033069755212873837\n",
      "test loss is 0.007154899648013761\n",
      "Batch: 17200,train loss is: 0.0018297453448532825\n",
      "test loss is 0.0037745307455073823\n",
      "Batch: 17300,train loss is: 0.0018903437288509666\n",
      "test loss is 0.003412735838024636\n",
      "Batch: 17400,train loss is: 0.013119262698629886\n",
      "test loss is 0.012342426978696966\n",
      "Batch: 17500,train loss is: 0.0077566213655410835\n",
      "test loss is 0.005600440621965388\n",
      "Batch: 17600,train loss is: 0.002910426915073033\n",
      "test loss is 0.00580584117082762\n",
      "Batch: 17700,train loss is: 0.002837767832124059\n",
      "test loss is 0.0059713729033905125\n",
      "Batch: 17800,train loss is: 0.010684266521085256\n",
      "test loss is 0.005397031187875517\n",
      "Batch: 17900,train loss is: 0.019730754582411313\n",
      "test loss is 0.00864302045626174\n",
      "Batch: 18000,train loss is: 0.004898367903330514\n",
      "test loss is 0.006008426485288922\n",
      "Batch: 18100,train loss is: 0.0030024387636021417\n",
      "test loss is 0.004838907283399306\n",
      "Batch: 18200,train loss is: 0.002825243393201383\n",
      "test loss is 0.006155338052388627\n",
      "Batch: 18300,train loss is: 0.012814828239328611\n",
      "test loss is 0.0054468764893517005\n",
      "Batch: 18400,train loss is: 0.0016259096772939083\n",
      "test loss is 0.0032400338121736033\n",
      "Batch: 18500,train loss is: 0.007910461657403414\n",
      "test loss is 0.007254485354175628\n",
      "Batch: 18600,train loss is: 0.0038692588653293125\n",
      "test loss is 0.0036843597487535274\n",
      "Batch: 18700,train loss is: 0.011124291679889075\n",
      "test loss is 0.004043269362203828\n",
      "Batch: 18800,train loss is: 0.00447464230205424\n",
      "test loss is 0.0045186653417360785\n",
      "Batch: 18900,train loss is: 0.0023870317009291884\n",
      "test loss is 0.004603249404835098\n",
      "Batch: 19000,train loss is: 0.004470938435474886\n",
      "test loss is 0.004059421054373247\n",
      "Batch: 19100,train loss is: 0.0034346876845117802\n",
      "test loss is 0.004614996486095552\n",
      "Batch: 19200,train loss is: 0.0019630941942274404\n",
      "test loss is 0.004177638802948471\n",
      "Batch: 19300,train loss is: 0.003058890987403528\n",
      "test loss is 0.004988178644452793\n",
      "Batch: 19400,train loss is: 0.0018779684648512927\n",
      "test loss is 0.003125307184146999\n",
      "Batch: 19500,train loss is: 0.0017552587156506115\n",
      "test loss is 0.006889448479625921\n",
      "Batch: 19600,train loss is: 0.002723656628963563\n",
      "test loss is 0.008569770015669735\n",
      "Batch: 19700,train loss is: 0.009226496242840174\n",
      "test loss is 0.005457226252390344\n",
      "Batch: 19800,train loss is: 0.0062335854902077605\n",
      "test loss is 0.003485645737869276\n",
      "Batch: 19900,train loss is: 0.0033663462448391955\n",
      "test loss is 0.007506148561631242\n",
      "Batch: 20000,train loss is: 0.010771869752072541\n",
      "test loss is 0.0056122372090089\n",
      "Batch: 20100,train loss is: 0.005454903280543036\n",
      "test loss is 0.005341410487089025\n",
      "Batch: 20200,train loss is: 0.010572824335679824\n",
      "test loss is 0.00553631161646685\n",
      "Batch: 20300,train loss is: 0.0031422985001816313\n",
      "test loss is 0.00295842806096757\n",
      "Batch: 20400,train loss is: 0.011265014606463558\n",
      "test loss is 0.016569285318817108\n",
      "Batch: 20500,train loss is: 0.0026993417665900185\n",
      "test loss is 0.004159334317940461\n",
      "Batch: 20600,train loss is: 0.004596390933065954\n",
      "test loss is 0.003236690760656225\n",
      "Batch: 20700,train loss is: 0.0018132261777223835\n",
      "test loss is 0.007804360139489535\n",
      "Batch: 20800,train loss is: 0.002118656515529221\n",
      "test loss is 0.0028096730383821355\n",
      "Batch: 20900,train loss is: 0.0018720363926165535\n",
      "test loss is 0.003138990062824211\n",
      "Batch: 21000,train loss is: 0.0027264409933543694\n",
      "test loss is 0.006092118782542357\n",
      "Batch: 21100,train loss is: 0.00156384025119366\n",
      "test loss is 0.003154832686125476\n",
      "Batch: 21200,train loss is: 0.003984465018887676\n",
      "test loss is 0.005225829584947293\n",
      "Batch: 21300,train loss is: 0.002097411929592478\n",
      "test loss is 0.005153074915233047\n",
      "Batch: 21400,train loss is: 0.007280489571653022\n",
      "test loss is 0.0047072651320760875\n",
      "Batch: 21500,train loss is: 0.002720512326337822\n",
      "test loss is 0.003392924701700919\n",
      "Batch: 21600,train loss is: 0.012491880592333804\n",
      "test loss is 0.013271225123982834\n",
      "Batch: 21700,train loss is: 0.007171360808872544\n",
      "test loss is 0.003511780292238475\n",
      "Batch: 21800,train loss is: 0.004933306075004586\n",
      "test loss is 0.0026160925155133794\n",
      "Batch: 21900,train loss is: 0.0049592531723732566\n",
      "test loss is 0.007624797295981197\n",
      "Batch: 22000,train loss is: 0.00798631180815197\n",
      "test loss is 0.0037857650209618173\n",
      "Batch: 22100,train loss is: 0.0028531381982966285\n",
      "test loss is 0.003279075441396744\n",
      "Batch: 22200,train loss is: 0.002549846625190459\n",
      "test loss is 0.004269176475440503\n",
      "Batch: 22300,train loss is: 0.0026432286943158414\n",
      "test loss is 0.004520361152597724\n",
      "Batch: 22400,train loss is: 0.005630799730746197\n",
      "test loss is 0.013303920085629866\n",
      "Batch: 22500,train loss is: 0.007371356110866244\n",
      "test loss is 0.009481067427193055\n",
      "Batch: 22600,train loss is: 0.0033022462116754194\n",
      "test loss is 0.00514973160399284\n",
      "Batch: 22700,train loss is: 0.0035227000122256143\n",
      "test loss is 0.003028998770525554\n",
      "Batch: 22800,train loss is: 0.004098617825581063\n",
      "test loss is 0.003552017357173156\n",
      "Batch: 22900,train loss is: 0.0013650063208784325\n",
      "test loss is 0.006748644144731232\n",
      "Batch: 23000,train loss is: 0.009187990312565343\n",
      "test loss is 0.004073096953847305\n",
      "Batch: 23100,train loss is: 0.0011834263253319207\n",
      "test loss is 0.003777487177022835\n",
      "Batch: 23200,train loss is: 0.0015827282683011652\n",
      "test loss is 0.004837973776972648\n",
      "Batch: 23300,train loss is: 0.0023006027258634745\n",
      "test loss is 0.0038788889284124294\n",
      "Batch: 23400,train loss is: 0.005144944362457421\n",
      "test loss is 0.011353902993553185\n",
      "Batch: 23500,train loss is: 0.006215925771672106\n",
      "test loss is 0.002807546542857676\n",
      "Batch: 23600,train loss is: 0.0029477935787198106\n",
      "test loss is 0.003004305462180581\n",
      "Batch: 23700,train loss is: 0.003330534852569185\n",
      "test loss is 0.0025158287307516457\n",
      "Batch: 23800,train loss is: 0.002894798996030019\n",
      "test loss is 0.006590342921490507\n",
      "Batch: 23900,train loss is: 0.007002432833964413\n",
      "test loss is 0.010349437944504943\n",
      "Batch: 24000,train loss is: 0.004130801648895967\n",
      "test loss is 0.003353118097117996\n",
      "Batch: 24100,train loss is: 0.002962761857993514\n",
      "test loss is 0.0035234968238528883\n",
      "Batch: 24200,train loss is: 0.00264174934624591\n",
      "test loss is 0.00265043558081963\n",
      "Batch: 24300,train loss is: 0.003343324627819444\n",
      "test loss is 0.0038096620663122965\n",
      "Batch: 24400,train loss is: 0.01225159799862961\n",
      "test loss is 0.0037565454829893183\n",
      "Batch: 24500,train loss is: 0.005480434906386173\n",
      "test loss is 0.002707129990969421\n",
      "Batch: 24600,train loss is: 0.002544319777217599\n",
      "test loss is 0.003184902036275543\n",
      "Batch: 24700,train loss is: 0.002042075061975493\n",
      "test loss is 0.0030991044257652586\n",
      "Batch: 24800,train loss is: 0.00444422092731876\n",
      "test loss is 0.004412904304094691\n",
      "Batch: 24900,train loss is: 0.005227397051822668\n",
      "test loss is 0.010170837633890411\n",
      "Batch: 25000,train loss is: 0.011745854274125238\n",
      "test loss is 0.007067370181512944\n",
      "Batch: 25100,train loss is: 0.0022628105061989045\n",
      "test loss is 0.005948802928411831\n",
      "Batch: 25200,train loss is: 0.006007857724118559\n",
      "test loss is 0.004451638215539869\n",
      "Batch: 25300,train loss is: 0.00873650183548995\n",
      "test loss is 0.003094598513312674\n",
      "Batch: 25400,train loss is: 0.0025511019229126413\n",
      "test loss is 0.0032013519585222714\n",
      "Batch: 25500,train loss is: 0.0013321979469206417\n",
      "test loss is 0.001989279919963317\n",
      "Batch: 25600,train loss is: 0.004340380227570958\n",
      "test loss is 0.0023926841316386856\n",
      "Batch: 25700,train loss is: 0.005964471594957385\n",
      "test loss is 0.002472280443567397\n",
      "Batch: 25800,train loss is: 0.0014047566036997435\n",
      "test loss is 0.0030756501673573347\n",
      "Batch: 25900,train loss is: 0.0021197882805391597\n",
      "test loss is 0.004162627387402097\n",
      "Batch: 26000,train loss is: 0.004476872043556586\n",
      "test loss is 0.00904100051483047\n",
      "Batch: 26100,train loss is: 0.002378389161110988\n",
      "test loss is 0.0031979345080325105\n",
      "Batch: 26200,train loss is: 0.0029657459089598853\n",
      "test loss is 0.006561523507282953\n",
      "Batch: 26300,train loss is: 0.002918787106430635\n",
      "test loss is 0.003290841451518602\n",
      "Batch: 26400,train loss is: 0.0020490809090881676\n",
      "test loss is 0.003071005104567455\n",
      "Batch: 26500,train loss is: 0.0025698462242007038\n",
      "test loss is 0.0025644479074498484\n",
      "Batch: 26600,train loss is: 0.003573159882534406\n",
      "test loss is 0.0033462776619227856\n",
      "Batch: 26700,train loss is: 0.0021173363572856425\n",
      "test loss is 0.0028848607730816763\n",
      "Batch: 26800,train loss is: 0.00501176415695947\n",
      "test loss is 0.002432664613304804\n",
      "Batch: 26900,train loss is: 0.0018157347130953493\n",
      "test loss is 0.002447790614538488\n",
      "Batch: 27000,train loss is: 0.0016844071874843566\n",
      "test loss is 0.0033565780828574837\n",
      "Batch: 27100,train loss is: 0.0029315610768056127\n",
      "test loss is 0.004037709590708502\n",
      "Batch: 27200,train loss is: 0.00227119942058501\n",
      "test loss is 0.002868118486004157\n",
      "Batch: 27300,train loss is: 0.0014705577958981813\n",
      "test loss is 0.003181646770086526\n",
      "Batch: 27400,train loss is: 0.008166135285292137\n",
      "test loss is 0.003544085695831565\n",
      "Batch: 27500,train loss is: 0.0018916972045550575\n",
      "test loss is 0.0023224784976115117\n",
      "Batch: 27600,train loss is: 0.004364508913167967\n",
      "test loss is 0.003577883310721141\n",
      "Batch: 27700,train loss is: 0.0042222592958488375\n",
      "test loss is 0.0059102783003424164\n",
      "Batch: 27800,train loss is: 0.002251600903937877\n",
      "test loss is 0.006287552529677218\n",
      "Batch: 27900,train loss is: 0.005484885698339644\n",
      "test loss is 0.0024248232046829507\n",
      "Batch: 28000,train loss is: 0.002199597097936857\n",
      "test loss is 0.002413555202224544\n",
      "Batch: 28100,train loss is: 0.004577778160653962\n",
      "test loss is 0.006115535774567151\n",
      "Batch: 28200,train loss is: 0.001837499217456504\n",
      "test loss is 0.002222652208074686\n",
      "Batch: 28300,train loss is: 0.0027253826072831167\n",
      "test loss is 0.0023095407456588156\n",
      "Batch: 28400,train loss is: 0.013582661609612096\n",
      "test loss is 0.006178677394846189\n",
      "Batch: 28500,train loss is: 0.003003080968350291\n",
      "test loss is 0.0027845517836636783\n",
      "Batch: 28600,train loss is: 0.005443111684778936\n",
      "test loss is 0.0031546323807401327\n",
      "Batch: 28700,train loss is: 0.0016489885137911033\n",
      "test loss is 0.003970156692493255\n",
      "Batch: 28800,train loss is: 0.004967137064803369\n",
      "test loss is 0.0021811056586236435\n",
      "Batch: 28900,train loss is: 0.001710189871183459\n",
      "test loss is 0.00379272260436621\n",
      "Batch: 29000,train loss is: 0.002618468822715044\n",
      "test loss is 0.006638681540968123\n",
      "Batch: 29100,train loss is: 0.003768413460502373\n",
      "test loss is 0.00895614294671142\n",
      "Batch: 29200,train loss is: 0.002558143277131535\n",
      "test loss is 0.0045025280692629855\n",
      "Batch: 29300,train loss is: 0.004291389149987998\n",
      "test loss is 0.0027812390034054404\n",
      "Batch: 29400,train loss is: 0.0015672036645207347\n",
      "test loss is 0.0065284486066358735\n",
      "Batch: 29500,train loss is: 0.0016362326133072265\n",
      "test loss is 0.0041659372136847485\n",
      "Batch: 29600,train loss is: 0.0036237015756446014\n",
      "test loss is 0.002170896183689129\n",
      "Batch: 29700,train loss is: 0.0011996465415140837\n",
      "test loss is 0.004350708446272812\n",
      "Batch: 29800,train loss is: 0.004896785945300654\n",
      "test loss is 0.002163318717346583\n",
      "Batch: 29900,train loss is: 0.006588212237280039\n",
      "test loss is 0.0022461652481143724\n",
      "Batch: 30000,train loss is: 0.0014383681835060985\n",
      "test loss is 0.0018020355581553277\n",
      "Batch: 30100,train loss is: 0.0012202705472013765\n",
      "test loss is 0.004015608546855996\n",
      "Batch: 30200,train loss is: 0.0017102869852471023\n",
      "test loss is 0.002393429631965538\n",
      "Batch: 30300,train loss is: 0.0016577431474030949\n",
      "test loss is 0.001999707321530679\n",
      "Batch: 30400,train loss is: 0.0010360372059443769\n",
      "test loss is 0.00307384885256559\n",
      "Batch: 30500,train loss is: 0.005725652804679753\n",
      "test loss is 0.005315053316448423\n",
      "Batch: 30600,train loss is: 0.0017771159118886975\n",
      "test loss is 0.002853654529183038\n",
      "Batch: 30700,train loss is: 0.002813608255514413\n",
      "test loss is 0.00422567467459017\n",
      "Batch: 30800,train loss is: 0.002751394872065484\n",
      "test loss is 0.0036388449369536427\n",
      "Batch: 30900,train loss is: 0.0031558766194646095\n",
      "test loss is 0.003135648046557462\n",
      "Batch: 31000,train loss is: 0.0019404085090731272\n",
      "test loss is 0.005113124096125808\n",
      "Batch: 31100,train loss is: 0.0011785652528610972\n",
      "test loss is 0.0024397925248892632\n",
      "Batch: 31200,train loss is: 0.0026063288094994447\n",
      "test loss is 0.0021402601739701575\n",
      "Batch: 31300,train loss is: 0.009718982994814128\n",
      "test loss is 0.004137496754656902\n",
      "Batch: 31400,train loss is: 0.014004109858101492\n",
      "test loss is 0.012453864677552835\n",
      "Batch: 31500,train loss is: 0.01201889788831178\n",
      "test loss is 0.0039895026041304295\n",
      "Batch: 31600,train loss is: 0.0022067363514620083\n",
      "test loss is 0.002682220632596967\n",
      "Batch: 31700,train loss is: 0.0013026397718418133\n",
      "test loss is 0.0018819636902254722\n",
      "Batch: 31800,train loss is: 0.013009381382058148\n",
      "test loss is 0.005971512567539775\n",
      "Batch: 31900,train loss is: 0.012925377122218943\n",
      "test loss is 0.008122865232807974\n",
      "Batch: 32000,train loss is: 0.012118295502635\n",
      "test loss is 0.0019432504957148551\n",
      "Batch: 32100,train loss is: 0.004423249730321681\n",
      "test loss is 0.003627067363380378\n",
      "Batch: 32200,train loss is: 0.004920570410867316\n",
      "test loss is 0.003297611630262301\n",
      "Batch: 32300,train loss is: 0.0006057448094255815\n",
      "test loss is 0.002603640302683923\n",
      "Batch: 32400,train loss is: 0.0011110274760215634\n",
      "test loss is 0.0023517256275945933\n",
      "Batch: 32500,train loss is: 0.001520011896276367\n",
      "test loss is 0.003024113146039998\n",
      "Batch: 32600,train loss is: 0.0024181196570528135\n",
      "test loss is 0.0024677955930734177\n",
      "Batch: 32700,train loss is: 0.002224102360488917\n",
      "test loss is 0.0028179590326719746\n",
      "Batch: 32800,train loss is: 0.0028356972932683786\n",
      "test loss is 0.0023718025775510358\n",
      "Batch: 32900,train loss is: 0.0050860956214498305\n",
      "test loss is 0.0028060112280023195\n",
      "Batch: 33000,train loss is: 0.0018159154883324153\n",
      "test loss is 0.003771361825252632\n",
      "Batch: 33100,train loss is: 0.0018435817058460207\n",
      "test loss is 0.004556304039925917\n",
      "Batch: 33200,train loss is: 0.0019475552611822448\n",
      "test loss is 0.0032099230131472168\n",
      "Batch: 33300,train loss is: 0.0037374959119153097\n",
      "test loss is 0.004277517239701734\n",
      "Batch: 33400,train loss is: 0.0015757093844033906\n",
      "test loss is 0.0056825499479011875\n",
      "Batch: 33500,train loss is: 0.0086508798256445\n",
      "test loss is 0.006655287654928159\n",
      "Batch: 33600,train loss is: 0.0007879662521920825\n",
      "test loss is 0.0030050905900570117\n",
      "Batch: 33700,train loss is: 0.003827023872717611\n",
      "test loss is 0.002008511237911077\n",
      "Batch: 33800,train loss is: 0.011372711713177724\n",
      "test loss is 0.004717691527905728\n",
      "Batch: 33900,train loss is: 0.002058948901534211\n",
      "test loss is 0.0018018187509113862\n",
      "-----------------------Epoch: 1----------------------------------\n",
      "Batch: 0,train loss is: 0.002231572789077185\n",
      "test loss is 0.0019855233409594546\n",
      "Batch: 100,train loss is: 0.009645610570185818\n",
      "test loss is 0.002935851793367956\n",
      "Batch: 200,train loss is: 0.003452723374604695\n",
      "test loss is 0.0034545335039561217\n",
      "Batch: 300,train loss is: 0.004720188759642691\n",
      "test loss is 0.005980449829681432\n",
      "Batch: 400,train loss is: 0.0029531795658460746\n",
      "test loss is 0.004077614228397146\n",
      "Batch: 500,train loss is: 0.0008055859998359411\n",
      "test loss is 0.007941739280747038\n",
      "Batch: 600,train loss is: 0.0009049224091087847\n",
      "test loss is 0.0029710924655079593\n",
      "Batch: 700,train loss is: 0.0012684374217467962\n",
      "test loss is 0.00266756215573681\n",
      "Batch: 800,train loss is: 0.0008156387465182924\n",
      "test loss is 0.005019671141767019\n",
      "Batch: 900,train loss is: 0.0020465238953559515\n",
      "test loss is 0.002634780814688434\n",
      "Batch: 1000,train loss is: 0.0022293606646597475\n",
      "test loss is 0.002810599192364365\n",
      "Batch: 1100,train loss is: 0.005759646590071609\n",
      "test loss is 0.0037685995521069956\n",
      "Batch: 1200,train loss is: 0.001046199673305971\n",
      "test loss is 0.00633171859497798\n",
      "Batch: 1300,train loss is: 0.0016197579237262844\n",
      "test loss is 0.0039036151016755435\n",
      "Batch: 1400,train loss is: 0.001181317741212485\n",
      "test loss is 0.001992087571036705\n",
      "Batch: 1500,train loss is: 0.007387194874015889\n",
      "test loss is 0.009870547334279646\n",
      "Batch: 1600,train loss is: 0.0011646670377805023\n",
      "test loss is 0.004024448874325134\n",
      "Batch: 1700,train loss is: 0.0018158003539245926\n",
      "test loss is 0.0020426311461915325\n",
      "Batch: 1800,train loss is: 0.0017540570758652735\n",
      "test loss is 0.0026020408081055104\n",
      "Batch: 1900,train loss is: 0.0006259747449170801\n",
      "test loss is 0.003831544920497118\n",
      "Batch: 2000,train loss is: 0.0013969078391987463\n",
      "test loss is 0.0016762739729102665\n",
      "Batch: 2100,train loss is: 0.0020884097504197544\n",
      "test loss is 0.0015332779832765435\n",
      "Batch: 2200,train loss is: 0.0015642014514285062\n",
      "test loss is 0.0016649858102158219\n",
      "Batch: 2300,train loss is: 0.0008656514933599732\n",
      "test loss is 0.0030234206153402555\n",
      "Batch: 2400,train loss is: 0.0006092974345013631\n",
      "test loss is 0.001884848718537426\n",
      "Batch: 2500,train loss is: 0.0010369498628941356\n",
      "test loss is 0.0030516585228781952\n",
      "Batch: 2600,train loss is: 0.002294652522851002\n",
      "test loss is 0.0026587948388828468\n",
      "Batch: 2700,train loss is: 0.0014603139989886932\n",
      "test loss is 0.0019771995835150802\n",
      "Batch: 2800,train loss is: 0.0008793082075284029\n",
      "test loss is 0.00180842371258442\n",
      "Batch: 2900,train loss is: 0.0035756929822471817\n",
      "test loss is 0.003593420180484566\n",
      "Batch: 3000,train loss is: 0.0016633667049075836\n",
      "test loss is 0.003141359676128006\n",
      "Batch: 3100,train loss is: 0.0025458062611386115\n",
      "test loss is 0.0028972341172667617\n",
      "Batch: 3200,train loss is: 0.004261086954849829\n",
      "test loss is 0.003293729338443702\n",
      "Batch: 3300,train loss is: 0.0028686169513128133\n",
      "test loss is 0.002477132364839081\n",
      "Batch: 3400,train loss is: 0.003143323201686279\n",
      "test loss is 0.0022381682545632004\n",
      "Batch: 3500,train loss is: 0.0008906317938882569\n",
      "test loss is 0.002033800871177832\n",
      "Batch: 3600,train loss is: 0.0015242707594424572\n",
      "test loss is 0.008182104397072976\n",
      "Batch: 3700,train loss is: 0.004626931050805771\n",
      "test loss is 0.004026973107479499\n",
      "Batch: 3800,train loss is: 0.0015951374165610933\n",
      "test loss is 0.0022024479854063188\n",
      "Batch: 3900,train loss is: 0.0037041975747941864\n",
      "test loss is 0.002105655250194309\n",
      "Batch: 4000,train loss is: 0.0006251418199695141\n",
      "test loss is 0.002329885308448409\n",
      "Batch: 4100,train loss is: 0.0013625106459533218\n",
      "test loss is 0.0016567847248448262\n",
      "Batch: 4200,train loss is: 0.0019964998794042353\n",
      "test loss is 0.0017188380851007245\n",
      "Batch: 4300,train loss is: 0.0015197888187908394\n",
      "test loss is 0.002992100627452986\n",
      "Batch: 4400,train loss is: 0.0008179851431439945\n",
      "test loss is 0.0018427045071872951\n",
      "Batch: 4500,train loss is: 0.0011249681660568756\n",
      "test loss is 0.0022671599964859588\n",
      "Batch: 4600,train loss is: 0.002877436109196365\n",
      "test loss is 0.0023687174247783\n",
      "Batch: 4700,train loss is: 0.0037066924237086603\n",
      "test loss is 0.0020458841747597214\n",
      "Batch: 4800,train loss is: 0.004803278090344014\n",
      "test loss is 0.006298305405037649\n",
      "Batch: 4900,train loss is: 0.011066030493321067\n",
      "test loss is 0.003582006826005311\n",
      "Batch: 5000,train loss is: 0.0012438461098823609\n",
      "test loss is 0.0017347661928148548\n",
      "Batch: 5100,train loss is: 0.0015614527095210453\n",
      "test loss is 0.003354152087379425\n",
      "Batch: 5200,train loss is: 0.0008103690112252137\n",
      "test loss is 0.0019448793575411426\n",
      "Batch: 5300,train loss is: 0.009472089545345523\n",
      "test loss is 0.015944035195069896\n",
      "Batch: 5400,train loss is: 0.004346795986541008\n",
      "test loss is 0.0025989930639776174\n",
      "Batch: 5500,train loss is: 0.003767849264293807\n",
      "test loss is 0.0019579861876119356\n",
      "Batch: 5600,train loss is: 0.001683946514131361\n",
      "test loss is 0.0016026910572980158\n",
      "Batch: 5700,train loss is: 0.0020333075567672\n",
      "test loss is 0.002269013483374513\n",
      "Batch: 5800,train loss is: 0.0012966282083996922\n",
      "test loss is 0.0014310474359639207\n",
      "Batch: 5900,train loss is: 0.005083480946657543\n",
      "test loss is 0.00199901074596221\n",
      "Batch: 6000,train loss is: 0.00445188850812116\n",
      "test loss is 0.0019084186075571644\n",
      "Batch: 6100,train loss is: 0.0014544568371788302\n",
      "test loss is 0.0015686435418822958\n",
      "Batch: 6200,train loss is: 0.001348441853199886\n",
      "test loss is 0.003481421265276227\n",
      "Batch: 6300,train loss is: 0.0027553379432486724\n",
      "test loss is 0.004539550068269312\n",
      "Batch: 6400,train loss is: 0.0027726712807221218\n",
      "test loss is 0.0022698817265787634\n",
      "Batch: 6500,train loss is: 0.0010783087455072892\n",
      "test loss is 0.0017044733312154402\n",
      "Batch: 6600,train loss is: 0.004987860386352813\n",
      "test loss is 0.0016831718895010657\n",
      "Batch: 6700,train loss is: 0.0010546380147730056\n",
      "test loss is 0.0023930712997299723\n",
      "Batch: 6800,train loss is: 0.003023307455332644\n",
      "test loss is 0.0018405258105679144\n",
      "Batch: 6900,train loss is: 0.012235684082265875\n",
      "test loss is 0.00323729999818733\n",
      "Batch: 7000,train loss is: 0.006360067968712016\n",
      "test loss is 0.0028674798460495998\n",
      "Batch: 7100,train loss is: 0.006039337900420526\n",
      "test loss is 0.0036842233888331913\n",
      "Batch: 7200,train loss is: 0.0006862853602843188\n",
      "test loss is 0.0013253182413937336\n",
      "Batch: 7300,train loss is: 0.007827230265873548\n",
      "test loss is 0.0022743449242233127\n",
      "Batch: 7400,train loss is: 0.001183758019447146\n",
      "test loss is 0.003912166548963878\n",
      "Batch: 7500,train loss is: 0.0010643636744875069\n",
      "test loss is 0.0018642036602833428\n",
      "Batch: 7600,train loss is: 0.0013697215554300713\n",
      "test loss is 0.002001179073873523\n",
      "Batch: 7700,train loss is: 0.0026720438850450444\n",
      "test loss is 0.0015450914323588822\n",
      "Batch: 7800,train loss is: 0.001134781931580924\n",
      "test loss is 0.0013793592307147576\n",
      "Batch: 7900,train loss is: 0.0018061090528979445\n",
      "test loss is 0.0013741593220054432\n",
      "Batch: 8000,train loss is: 0.0012358761676327429\n",
      "test loss is 0.00283318592026047\n",
      "Batch: 8100,train loss is: 0.007758591688833048\n",
      "test loss is 0.0025425282567557825\n",
      "Batch: 8200,train loss is: 0.0029739268424344807\n",
      "test loss is 0.0016543556949429794\n",
      "Batch: 8300,train loss is: 0.0012375586996986143\n",
      "test loss is 0.004141846895651474\n",
      "Batch: 8400,train loss is: 0.008178257832327887\n",
      "test loss is 0.003451417077240441\n",
      "Batch: 8500,train loss is: 0.006736666939739443\n",
      "test loss is 0.0037979736885621796\n",
      "Batch: 8600,train loss is: 0.0031734516852035656\n",
      "test loss is 0.003625550051883173\n",
      "Batch: 8700,train loss is: 0.002167950523404421\n",
      "test loss is 0.0050210375433908765\n",
      "Batch: 8800,train loss is: 0.003325659079770082\n",
      "test loss is 0.00205895366171965\n",
      "Batch: 8900,train loss is: 0.001066134584090525\n",
      "test loss is 0.0026868467329550765\n",
      "Batch: 9000,train loss is: 0.0016434813380318428\n",
      "test loss is 0.0024473559444023346\n",
      "Batch: 9100,train loss is: 0.002281545701709381\n",
      "test loss is 0.0017546200553059273\n",
      "Batch: 9200,train loss is: 0.007423961266176651\n",
      "test loss is 0.002989711913284689\n",
      "Batch: 9300,train loss is: 0.002261671341977985\n",
      "test loss is 0.0020831514608334037\n",
      "Batch: 9400,train loss is: 0.0010762107170253583\n",
      "test loss is 0.0014286063599116838\n",
      "Batch: 9500,train loss is: 0.0013886247363497349\n",
      "test loss is 0.003171513860354522\n",
      "Batch: 9600,train loss is: 0.0022907975736671513\n",
      "test loss is 0.0022553729763770355\n",
      "Batch: 9700,train loss is: 0.003504536546303051\n",
      "test loss is 0.002333301649983109\n",
      "Batch: 9800,train loss is: 0.0006133871138426322\n",
      "test loss is 0.002206057000511216\n",
      "Batch: 9900,train loss is: 0.0006150757052525605\n",
      "test loss is 0.0024762404864061057\n",
      "Batch: 10000,train loss is: 0.00271710552980802\n",
      "test loss is 0.0017844743925186342\n",
      "Batch: 10100,train loss is: 0.005148200122139107\n",
      "test loss is 0.0024924423087074833\n",
      "Batch: 10200,train loss is: 0.004022505889327533\n",
      "test loss is 0.0013150254840899468\n",
      "Batch: 10300,train loss is: 0.0014009692618226562\n",
      "test loss is 0.003608681200707713\n",
      "Batch: 10400,train loss is: 0.0019364507593848037\n",
      "test loss is 0.0019474184836287575\n",
      "Batch: 10500,train loss is: 0.0034425780462263517\n",
      "test loss is 0.0018350291860453215\n",
      "Batch: 10600,train loss is: 0.000943860655236351\n",
      "test loss is 0.003496322274346262\n",
      "Batch: 10700,train loss is: 0.0013313249616982504\n",
      "test loss is 0.002636503488774406\n",
      "Batch: 10800,train loss is: 0.005665566774791255\n",
      "test loss is 0.027113468644670676\n",
      "Batch: 10900,train loss is: 0.003710450844851348\n",
      "test loss is 0.0023773831152618607\n",
      "Batch: 11000,train loss is: 0.0010013806595950382\n",
      "test loss is 0.0015231634454583969\n",
      "Batch: 11100,train loss is: 0.0019314522744871662\n",
      "test loss is 0.002070105378705858\n",
      "Batch: 11200,train loss is: 0.0013894363922777795\n",
      "test loss is 0.0017429181803942416\n",
      "Batch: 11300,train loss is: 0.0029739625643489306\n",
      "test loss is 0.002540490551030066\n",
      "Batch: 11400,train loss is: 0.00323077324827247\n",
      "test loss is 0.002627149679892263\n",
      "Batch: 11500,train loss is: 0.0004167619231769196\n",
      "test loss is 0.0035625450354762394\n",
      "Batch: 11600,train loss is: 0.0031583910256880884\n",
      "test loss is 0.003359509189265318\n",
      "Batch: 11700,train loss is: 0.0005300815608204931\n",
      "test loss is 0.0014618885422047496\n",
      "Batch: 11800,train loss is: 0.0009318321658490599\n",
      "test loss is 0.0014202507147413144\n",
      "Batch: 11900,train loss is: 0.004089739183373734\n",
      "test loss is 0.0018031124229199694\n",
      "Batch: 12000,train loss is: 0.007260102728907516\n",
      "test loss is 0.0030088884940507182\n",
      "Batch: 12100,train loss is: 0.01337081509994764\n",
      "test loss is 0.0016080331523126044\n",
      "Batch: 12200,train loss is: 0.0013279012248817305\n",
      "test loss is 0.002231705847320985\n",
      "Batch: 12300,train loss is: 0.0005398627161491587\n",
      "test loss is 0.0019526441731452218\n",
      "Batch: 12400,train loss is: 0.0013632569397569558\n",
      "test loss is 0.001272222369044218\n",
      "Batch: 12500,train loss is: 0.004322436751050604\n",
      "test loss is 0.004505776519966437\n",
      "Batch: 12600,train loss is: 0.00055097476770504\n",
      "test loss is 0.002765510857798383\n",
      "Batch: 12700,train loss is: 0.0016683571428301699\n",
      "test loss is 0.0014897007688879386\n",
      "Batch: 12800,train loss is: 0.0027844041122730852\n",
      "test loss is 0.0022488280177076565\n",
      "Batch: 12900,train loss is: 0.002310034329875231\n",
      "test loss is 0.007118228146694071\n",
      "Batch: 13000,train loss is: 0.001405832270738841\n",
      "test loss is 0.0020537092829902496\n",
      "Batch: 13100,train loss is: 0.003243767574677242\n",
      "test loss is 0.004663913963619259\n",
      "Batch: 13200,train loss is: 0.0025473076258629014\n",
      "test loss is 0.002947364356068378\n",
      "Batch: 13300,train loss is: 0.000977118151692366\n",
      "test loss is 0.004839986297706344\n",
      "Batch: 13400,train loss is: 0.002191851474932356\n",
      "test loss is 0.006059938606571152\n",
      "Batch: 13500,train loss is: 0.001163924609621705\n",
      "test loss is 0.002735357121282319\n",
      "Batch: 13600,train loss is: 0.008580504410264573\n",
      "test loss is 0.00947719586333011\n",
      "Batch: 13700,train loss is: 0.001885601715865797\n",
      "test loss is 0.0019790739362556655\n",
      "Batch: 13800,train loss is: 0.002077960474548599\n",
      "test loss is 0.0028425197079291137\n",
      "Batch: 13900,train loss is: 0.000848341394332058\n",
      "test loss is 0.001722509056693159\n",
      "Batch: 14000,train loss is: 0.00227275643515079\n",
      "test loss is 0.0036564257585468443\n",
      "Batch: 14100,train loss is: 0.002304018799319828\n",
      "test loss is 0.002083633412332308\n",
      "Batch: 14200,train loss is: 0.0004975889266202233\n",
      "test loss is 0.0014596338202627452\n",
      "Batch: 14300,train loss is: 0.0010717921760356596\n",
      "test loss is 0.0016996221046268294\n",
      "Batch: 14400,train loss is: 0.0010739341385573354\n",
      "test loss is 0.0011983219666900522\n",
      "Batch: 14500,train loss is: 0.0010687716033228108\n",
      "test loss is 0.0014033047277909746\n",
      "Batch: 14600,train loss is: 0.0052941084134813\n",
      "test loss is 0.0036280733951703408\n",
      "Batch: 14700,train loss is: 0.003152039025775564\n",
      "test loss is 0.010300486704519654\n",
      "Batch: 14800,train loss is: 0.0013085655840385205\n",
      "test loss is 0.003239670210898005\n",
      "Batch: 14900,train loss is: 0.0007598756282782601\n",
      "test loss is 0.0023477204310658893\n",
      "Batch: 15000,train loss is: 0.0025115445636553556\n",
      "test loss is 0.0016794314734671705\n",
      "Batch: 15100,train loss is: 0.0010285539318338642\n",
      "test loss is 0.0012643512398203777\n",
      "Batch: 15200,train loss is: 0.001518230793455816\n",
      "test loss is 0.0012935771624186375\n",
      "Batch: 15300,train loss is: 0.0015409628313296105\n",
      "test loss is 0.0012720795750924773\n",
      "Batch: 15400,train loss is: 0.0020572062693474053\n",
      "test loss is 0.0021991571594266353\n",
      "Batch: 15500,train loss is: 0.0009074220438358314\n",
      "test loss is 0.0012409661327774297\n",
      "Batch: 15600,train loss is: 0.0014925962139361413\n",
      "test loss is 0.002425686161379716\n",
      "Batch: 15700,train loss is: 0.003846362870042605\n",
      "test loss is 0.005829987416942201\n",
      "Batch: 15800,train loss is: 0.002145374552831531\n",
      "test loss is 0.0023665821008118557\n",
      "Batch: 15900,train loss is: 0.0036767980631487855\n",
      "test loss is 0.0018820949878858783\n",
      "Batch: 16000,train loss is: 0.0005287439774272579\n",
      "test loss is 0.0017475581906545858\n",
      "Batch: 16100,train loss is: 0.0017821730280347884\n",
      "test loss is 0.0014204100345711723\n",
      "Batch: 16200,train loss is: 0.0012572763283717169\n",
      "test loss is 0.0016360718914849317\n",
      "Batch: 16300,train loss is: 0.0011906333106018933\n",
      "test loss is 0.0012278569998909637\n",
      "Batch: 16400,train loss is: 0.001826832692604586\n",
      "test loss is 0.001425238186118863\n",
      "Batch: 16500,train loss is: 0.0021415851841990233\n",
      "test loss is 0.0021575074343982074\n",
      "Batch: 16600,train loss is: 0.0007689679492678983\n",
      "test loss is 0.0021259063545051218\n",
      "Batch: 16700,train loss is: 0.0025123247264178866\n",
      "test loss is 0.0013942722599848295\n",
      "Batch: 16800,train loss is: 0.001331696695952353\n",
      "test loss is 0.0016477586401483072\n",
      "Batch: 16900,train loss is: 0.0011702438758380313\n",
      "test loss is 0.00201620776362533\n",
      "Batch: 17000,train loss is: 0.0019111938323513424\n",
      "test loss is 0.002966946990125015\n",
      "Batch: 17100,train loss is: 0.0022264521141089773\n",
      "test loss is 0.0026804937471309556\n",
      "Batch: 17200,train loss is: 0.0005408224790095325\n",
      "test loss is 0.001505585810893869\n",
      "Batch: 17300,train loss is: 0.00082112841448206\n",
      "test loss is 0.001350788483005583\n",
      "Batch: 17400,train loss is: 0.0012408433071459872\n",
      "test loss is 0.0013927481977762822\n",
      "Batch: 17500,train loss is: 0.0016124647245635575\n",
      "test loss is 0.0012264647550360758\n",
      "Batch: 17600,train loss is: 0.0025643787337625624\n",
      "test loss is 0.0033057577999644036\n",
      "Batch: 17700,train loss is: 0.0008275830770292674\n",
      "test loss is 0.002432934570034335\n",
      "Batch: 17800,train loss is: 0.004222279211083684\n",
      "test loss is 0.0035083920547174744\n",
      "Batch: 17900,train loss is: 0.0030978087650416902\n",
      "test loss is 0.001889442851267602\n",
      "Batch: 18000,train loss is: 0.002439995541170078\n",
      "test loss is 0.0023112906112865515\n",
      "Batch: 18100,train loss is: 0.001518224211657983\n",
      "test loss is 0.0018326214401995643\n",
      "Batch: 18200,train loss is: 0.0010430164106080254\n",
      "test loss is 0.003109095444870582\n",
      "Batch: 18300,train loss is: 0.0022696098757588076\n",
      "test loss is 0.0012609782432271913\n",
      "Batch: 18400,train loss is: 0.002426469901269187\n",
      "test loss is 0.001766455007364611\n",
      "Batch: 18500,train loss is: 0.001401189390700788\n",
      "test loss is 0.0021070916143394744\n",
      "Batch: 18600,train loss is: 0.001045170029694331\n",
      "test loss is 0.0013951286259134103\n",
      "Batch: 18700,train loss is: 0.001930559420498696\n",
      "test loss is 0.0016273405679844968\n",
      "Batch: 18800,train loss is: 0.0008500897144247696\n",
      "test loss is 0.0011090043958405383\n",
      "Batch: 18900,train loss is: 0.0010145917853067784\n",
      "test loss is 0.0025915176932689243\n",
      "Batch: 19000,train loss is: 0.0011465472625178078\n",
      "test loss is 0.0012476649228442694\n",
      "Batch: 19100,train loss is: 0.001678320629388604\n",
      "test loss is 0.001398037530070657\n",
      "Batch: 19200,train loss is: 0.0010889231977601955\n",
      "test loss is 0.0014411604029892072\n",
      "Batch: 19300,train loss is: 0.0027393753697627452\n",
      "test loss is 0.003369880515514865\n",
      "Batch: 19400,train loss is: 0.003657690696477546\n",
      "test loss is 0.0035264563448317724\n",
      "Batch: 19500,train loss is: 0.0009183968073527951\n",
      "test loss is 0.0014239878829391084\n",
      "Batch: 19600,train loss is: 0.000782182998241897\n",
      "test loss is 0.0018707408124267817\n",
      "Batch: 19700,train loss is: 0.005602331145401786\n",
      "test loss is 0.002088558962585599\n",
      "Batch: 19800,train loss is: 0.0013216002378920349\n",
      "test loss is 0.0010544192077905194\n",
      "Batch: 19900,train loss is: 0.0008538948760106754\n",
      "test loss is 0.0016658897623459104\n",
      "Batch: 20000,train loss is: 0.002366984186408354\n",
      "test loss is 0.004599330022962284\n",
      "Batch: 20100,train loss is: 0.0016250785511594825\n",
      "test loss is 0.0030911291031467643\n",
      "Batch: 20200,train loss is: 0.006873422720015098\n",
      "test loss is 0.002351167639143057\n",
      "Batch: 20300,train loss is: 0.0007042584030156239\n",
      "test loss is 0.001164265189226081\n",
      "Batch: 20400,train loss is: 0.002219313351414187\n",
      "test loss is 0.00451625079277031\n",
      "Batch: 20500,train loss is: 0.002052699821783307\n",
      "test loss is 0.0027529983437089853\n",
      "Batch: 20600,train loss is: 0.003399162969640165\n",
      "test loss is 0.0015027080546012495\n",
      "Batch: 20700,train loss is: 0.001424742282215856\n",
      "test loss is 0.0016798575018937326\n",
      "Batch: 20800,train loss is: 0.0013676037299659956\n",
      "test loss is 0.0012392550357008779\n",
      "Batch: 20900,train loss is: 0.0011190393254224857\n",
      "test loss is 0.001161640425752547\n",
      "Batch: 21000,train loss is: 0.0030783711710482176\n",
      "test loss is 0.0018423770596703523\n",
      "Batch: 21100,train loss is: 0.0014639936150704484\n",
      "test loss is 0.0027166015126951592\n",
      "Batch: 21200,train loss is: 0.002130377194329225\n",
      "test loss is 0.002657557045169841\n",
      "Batch: 21300,train loss is: 0.0007575399112666539\n",
      "test loss is 0.0030705698482180067\n",
      "Batch: 21400,train loss is: 0.0021723957252724408\n",
      "test loss is 0.0013107006307874571\n",
      "Batch: 21500,train loss is: 0.0007000587038521004\n",
      "test loss is 0.0020295741627455515\n",
      "Batch: 21600,train loss is: 0.0015519071047681691\n",
      "test loss is 0.0027829325765050465\n",
      "Batch: 21700,train loss is: 0.000800259587976372\n",
      "test loss is 0.0012676075315870118\n",
      "Batch: 21800,train loss is: 0.0031807762870171906\n",
      "test loss is 0.0011246479582685332\n",
      "Batch: 21900,train loss is: 0.004001525209004728\n",
      "test loss is 0.0036187590737189186\n",
      "Batch: 22000,train loss is: 0.005571493500762807\n",
      "test loss is 0.0056929674173307136\n",
      "Batch: 22100,train loss is: 0.0006587383634081471\n",
      "test loss is 0.002009644711391668\n",
      "Batch: 22200,train loss is: 0.011137072980448592\n",
      "test loss is 0.003349664654578455\n",
      "Batch: 22300,train loss is: 0.007031616749233723\n",
      "test loss is 0.010311310302697453\n",
      "Batch: 22400,train loss is: 0.00236565582561795\n",
      "test loss is 0.0023644831312196984\n",
      "Batch: 22500,train loss is: 0.0010244865521291176\n",
      "test loss is 0.0012217276471229914\n",
      "Batch: 22600,train loss is: 0.0005424015027855237\n",
      "test loss is 0.0011294792158439866\n",
      "Batch: 22700,train loss is: 0.002145100905532807\n",
      "test loss is 0.0035412963479200415\n",
      "Batch: 22800,train loss is: 0.0008720810240379857\n",
      "test loss is 0.0013770308621152952\n",
      "Batch: 22900,train loss is: 0.0008043583625563409\n",
      "test loss is 0.0013895396490054537\n",
      "Batch: 23000,train loss is: 0.0014779427533732168\n",
      "test loss is 0.0012519542710880895\n",
      "Batch: 23100,train loss is: 0.0005642759269613907\n",
      "test loss is 0.0011108210147374178\n",
      "Batch: 23200,train loss is: 0.0015943929358616534\n",
      "test loss is 0.002218771851379861\n",
      "Batch: 23300,train loss is: 0.0025989850509394426\n",
      "test loss is 0.0027579879457417674\n",
      "Batch: 23400,train loss is: 0.0023136124781203333\n",
      "test loss is 0.002962480047862429\n",
      "Batch: 23500,train loss is: 0.0030421420322123594\n",
      "test loss is 0.0012179429635969807\n",
      "Batch: 23600,train loss is: 0.00040739961762824496\n",
      "test loss is 0.001086156371817856\n",
      "Batch: 23700,train loss is: 0.003556836847436397\n",
      "test loss is 0.0022532419746395375\n",
      "Batch: 23800,train loss is: 0.0007522939492375265\n",
      "test loss is 0.003912174468938496\n",
      "Batch: 23900,train loss is: 0.0036375292881281537\n",
      "test loss is 0.004314594994354218\n",
      "Batch: 24000,train loss is: 0.0024776086682208975\n",
      "test loss is 0.002450749289939598\n",
      "Batch: 24100,train loss is: 0.0022882486787818997\n",
      "test loss is 0.0016139228573934707\n",
      "Batch: 24200,train loss is: 0.0014721773641135532\n",
      "test loss is 0.0011077566948555417\n",
      "Batch: 24300,train loss is: 0.0020513424100666758\n",
      "test loss is 0.001653175444784222\n",
      "Batch: 24400,train loss is: 0.002887255050010523\n",
      "test loss is 0.0015892374355802531\n",
      "Batch: 24500,train loss is: 0.002468073863137635\n",
      "test loss is 0.0020326693537534284\n",
      "Batch: 24600,train loss is: 0.0022683656920020555\n",
      "test loss is 0.0012374268761928709\n",
      "Batch: 24700,train loss is: 0.0016318162965440282\n",
      "test loss is 0.001997771201562393\n",
      "Batch: 24800,train loss is: 0.0025065853519632302\n",
      "test loss is 0.0009261964781888337\n",
      "Batch: 24900,train loss is: 0.000984093860499969\n",
      "test loss is 0.0017265004368802095\n",
      "Batch: 25000,train loss is: 0.0008024104051381373\n",
      "test loss is 0.0010353657267845034\n",
      "Batch: 25100,train loss is: 0.0011158610302841481\n",
      "test loss is 0.00180696286653561\n",
      "Batch: 25200,train loss is: 0.0007138025862522938\n",
      "test loss is 0.0011341013811657522\n",
      "Batch: 25300,train loss is: 0.001987951447802685\n",
      "test loss is 0.001590915503253618\n",
      "Batch: 25400,train loss is: 0.0034461012928351715\n",
      "test loss is 0.003230012662363871\n",
      "Batch: 25500,train loss is: 0.0006927483300690579\n",
      "test loss is 0.0015557730573063917\n",
      "Batch: 25600,train loss is: 0.0008645851218840629\n",
      "test loss is 0.0010143417152645579\n",
      "Batch: 25700,train loss is: 0.0018675409805732587\n",
      "test loss is 0.0008967112067796623\n",
      "Batch: 25800,train loss is: 0.00044312666456651056\n",
      "test loss is 0.0010246288868935682\n",
      "Batch: 25900,train loss is: 0.0012853931319423075\n",
      "test loss is 0.0028850424504714696\n",
      "Batch: 26000,train loss is: 0.002882791268822506\n",
      "test loss is 0.002978621686540511\n",
      "Batch: 26100,train loss is: 0.0016880309771637075\n",
      "test loss is 0.005354735730646985\n",
      "Batch: 26200,train loss is: 0.0019339985027495064\n",
      "test loss is 0.002931635577088165\n",
      "Batch: 26300,train loss is: 0.0007152168030345059\n",
      "test loss is 0.001673306872593306\n",
      "Batch: 26400,train loss is: 0.000775903656530429\n",
      "test loss is 0.0013732899960244855\n",
      "Batch: 26500,train loss is: 0.0006550254946264261\n",
      "test loss is 0.001162616089082017\n",
      "Batch: 26600,train loss is: 0.0011185613994298465\n",
      "test loss is 0.0009608858216154683\n",
      "Batch: 26700,train loss is: 0.0008305937535520502\n",
      "test loss is 0.001388716603708783\n",
      "Batch: 26800,train loss is: 0.0013177227035442542\n",
      "test loss is 0.0011149004043418947\n",
      "Batch: 26900,train loss is: 0.0019256737960741698\n",
      "test loss is 0.0012842733589091475\n",
      "Batch: 27000,train loss is: 0.0011557691635244798\n",
      "test loss is 0.0018015218684024482\n",
      "Batch: 27100,train loss is: 0.001777052071681085\n",
      "test loss is 0.0028808594878551603\n",
      "Batch: 27200,train loss is: 0.0005139527008712135\n",
      "test loss is 0.0009157665891254111\n",
      "Batch: 27300,train loss is: 0.0011911528520851082\n",
      "test loss is 0.0026814673684904585\n",
      "Batch: 27400,train loss is: 0.001121529960635648\n",
      "test loss is 0.0016291210406576381\n",
      "Batch: 27500,train loss is: 0.0007800663401390281\n",
      "test loss is 0.0017663201767531918\n",
      "Batch: 27600,train loss is: 0.0014394420280874514\n",
      "test loss is 0.0028153219884667576\n",
      "Batch: 27700,train loss is: 0.0008758360612588909\n",
      "test loss is 0.0015496002265124665\n",
      "Batch: 27800,train loss is: 0.003755535529448103\n",
      "test loss is 0.003863060144243657\n",
      "Batch: 27900,train loss is: 0.002013412001490374\n",
      "test loss is 0.0013990810272208048\n",
      "Batch: 28000,train loss is: 0.0010117007106098363\n",
      "test loss is 0.0013212314294686958\n",
      "Batch: 28100,train loss is: 0.0028578363133386375\n",
      "test loss is 0.002440976863654937\n",
      "Batch: 28200,train loss is: 0.0006486908591973368\n",
      "test loss is 0.0017611725272088912\n",
      "Batch: 28300,train loss is: 0.00357831841422702\n",
      "test loss is 0.0024357404348182158\n",
      "Batch: 28400,train loss is: 0.0012422716121833434\n",
      "test loss is 0.0013260635839683363\n",
      "Batch: 28500,train loss is: 0.0017063582615770392\n",
      "test loss is 0.0015931876199873699\n",
      "Batch: 28600,train loss is: 0.0017975892367423335\n",
      "test loss is 0.002097066180583852\n",
      "Batch: 28700,train loss is: 0.0011957271280912446\n",
      "test loss is 0.001462593278051423\n",
      "Batch: 28800,train loss is: 0.0031825113133111494\n",
      "test loss is 0.0011494244263909378\n",
      "Batch: 28900,train loss is: 0.00203612089884397\n",
      "test loss is 0.0015422028454058745\n",
      "Batch: 29000,train loss is: 0.0015397180863042361\n",
      "test loss is 0.0010086505579961706\n",
      "Batch: 29100,train loss is: 0.002398872514554424\n",
      "test loss is 0.006137899056705734\n",
      "Batch: 29200,train loss is: 0.003484235726581325\n",
      "test loss is 0.005082639158252577\n",
      "Batch: 29300,train loss is: 0.0036431449785061876\n",
      "test loss is 0.0025870565219553456\n",
      "Batch: 29400,train loss is: 0.0019984429493914756\n",
      "test loss is 0.0017145942621718605\n",
      "Batch: 29500,train loss is: 0.0006524883397088097\n",
      "test loss is 0.001278380271830734\n",
      "Batch: 29600,train loss is: 0.001594027342328223\n",
      "test loss is 0.0009868186112608575\n",
      "Batch: 29700,train loss is: 0.0005650842541970836\n",
      "test loss is 0.001317774697124464\n",
      "Batch: 29800,train loss is: 0.0015606969185771237\n",
      "test loss is 0.0009381483570084863\n",
      "Batch: 29900,train loss is: 0.004099517569731371\n",
      "test loss is 0.0010532326860725565\n",
      "Batch: 30000,train loss is: 0.0008507856116166712\n",
      "test loss is 0.0009866456994758932\n",
      "Batch: 30100,train loss is: 0.0018539212999682043\n",
      "test loss is 0.009134863522393757\n",
      "Batch: 30200,train loss is: 0.000821990269261125\n",
      "test loss is 0.0032092120235144837\n",
      "Batch: 30300,train loss is: 0.0030284648451708333\n",
      "test loss is 0.00476647252694905\n",
      "Batch: 30400,train loss is: 0.002729642555672976\n",
      "test loss is 0.002689645337575936\n",
      "Batch: 30500,train loss is: 0.0017077426246091367\n",
      "test loss is 0.0017729745659096493\n",
      "Batch: 30600,train loss is: 0.0012347519788001758\n",
      "test loss is 0.0010668428314253358\n",
      "Batch: 30700,train loss is: 0.0014698603445042631\n",
      "test loss is 0.0015281465935665926\n",
      "Batch: 30800,train loss is: 0.001292035010741096\n",
      "test loss is 0.0015901551558539658\n",
      "Batch: 30900,train loss is: 0.0014153711902275662\n",
      "test loss is 0.0016288987264373796\n",
      "Batch: 31000,train loss is: 0.0005525469438649518\n",
      "test loss is 0.0012742270207785373\n",
      "Batch: 31100,train loss is: 0.0016035689673606231\n",
      "test loss is 0.002591521951769046\n",
      "Batch: 31200,train loss is: 0.0009494047567063992\n",
      "test loss is 0.0016965130923333154\n",
      "Batch: 31300,train loss is: 0.0019349826125344878\n",
      "test loss is 0.002736152756335036\n",
      "Batch: 31400,train loss is: 0.0009798547150447192\n",
      "test loss is 0.004960798606608231\n",
      "Batch: 31500,train loss is: 0.00534745526117411\n",
      "test loss is 0.0028934419409256612\n",
      "Batch: 31600,train loss is: 0.00048249051440338775\n",
      "test loss is 0.0010851365752296593\n",
      "Batch: 31700,train loss is: 0.0004066589123049892\n",
      "test loss is 0.0009945368345347758\n",
      "Batch: 31800,train loss is: 0.0030030987360044983\n",
      "test loss is 0.0015257979187220459\n",
      "Batch: 31900,train loss is: 0.0006999521542177466\n",
      "test loss is 0.00127026534670865\n",
      "Batch: 32000,train loss is: 0.008665047028339822\n",
      "test loss is 0.0011335052490295575\n",
      "Batch: 32100,train loss is: 0.0010415028655011193\n",
      "test loss is 0.0013548860923726302\n",
      "Batch: 32200,train loss is: 0.0007045036187077674\n",
      "test loss is 0.000939544837903764\n",
      "Batch: 32300,train loss is: 0.0003065976240390499\n",
      "test loss is 0.0011916240740188638\n",
      "Batch: 32400,train loss is: 0.0015022992284092164\n",
      "test loss is 0.0023694448663127755\n",
      "Batch: 32500,train loss is: 0.001521994787741898\n",
      "test loss is 0.0017752001320441454\n",
      "Batch: 32600,train loss is: 0.0018521517366554135\n",
      "test loss is 0.002139439248058803\n",
      "Batch: 32700,train loss is: 0.0008855680230983106\n",
      "test loss is 0.003282621740554384\n",
      "Batch: 32800,train loss is: 0.0008228201004976187\n",
      "test loss is 0.0012154531452715604\n",
      "Batch: 32900,train loss is: 0.0016046117594053666\n",
      "test loss is 0.0014199552120455223\n",
      "Batch: 33000,train loss is: 0.0006049916638789529\n",
      "test loss is 0.002235496379356397\n",
      "Batch: 33100,train loss is: 0.0006930320109307983\n",
      "test loss is 0.0014744043353407715\n",
      "Batch: 33200,train loss is: 0.0007859261283686234\n",
      "test loss is 0.0013494918067134772\n",
      "Batch: 33300,train loss is: 0.001564041541442999\n",
      "test loss is 0.002250709908044804\n",
      "Batch: 33400,train loss is: 0.0004386987921931049\n",
      "test loss is 0.003317318300013458\n",
      "Batch: 33500,train loss is: 0.00547557023224063\n",
      "test loss is 0.0030502538691032422\n",
      "Batch: 33600,train loss is: 0.0008959814465014623\n",
      "test loss is 0.0017511198840747383\n",
      "Batch: 33700,train loss is: 0.0009562025567795837\n",
      "test loss is 0.001108320559652578\n",
      "Batch: 33800,train loss is: 0.002325198110910537\n",
      "test loss is 0.0019204023722361127\n",
      "Batch: 33900,train loss is: 0.0021756881627551074\n",
      "test loss is 0.0009833587400424745\n",
      "-----------------------Epoch: 2----------------------------------\n",
      "Batch: 0,train loss is: 0.0012994607372794652\n",
      "test loss is 0.0011504631789933013\n",
      "Batch: 100,train loss is: 0.0043225263738669905\n",
      "test loss is 0.0016792975555172801\n",
      "Batch: 200,train loss is: 0.0008629247095424779\n",
      "test loss is 0.0016797963755161055\n",
      "Batch: 300,train loss is: 0.0018212706984322203\n",
      "test loss is 0.0022832760769371354\n",
      "Batch: 400,train loss is: 0.0007562007510094042\n",
      "test loss is 0.0014928730930306056\n",
      "Batch: 500,train loss is: 0.0008377908980333165\n",
      "test loss is 0.00257115666008251\n",
      "Batch: 600,train loss is: 0.0005693805897087091\n",
      "test loss is 0.0015733521450785424\n",
      "Batch: 700,train loss is: 0.0007279388734407016\n",
      "test loss is 0.0012411255838792605\n",
      "Batch: 800,train loss is: 0.00042475095657391117\n",
      "test loss is 0.0021314132155530446\n",
      "Batch: 900,train loss is: 0.0005164912977611474\n",
      "test loss is 0.001100551232010413\n",
      "Batch: 1000,train loss is: 0.0026094632999278788\n",
      "test loss is 0.0031081181882127084\n",
      "Batch: 1100,train loss is: 0.004394217989146989\n",
      "test loss is 0.002304340582310399\n",
      "Batch: 1200,train loss is: 0.0005236925890927947\n",
      "test loss is 0.001483528422057954\n",
      "Batch: 1300,train loss is: 0.0013568824896944525\n",
      "test loss is 0.0011539960403891822\n",
      "Batch: 1400,train loss is: 0.00037031729721352153\n",
      "test loss is 0.0010395188294939546\n",
      "Batch: 1500,train loss is: 0.0015178774954013549\n",
      "test loss is 0.0034764214476671928\n",
      "Batch: 1600,train loss is: 0.0037320528424770955\n",
      "test loss is 0.003909576680998876\n",
      "Batch: 1700,train loss is: 0.0017312383486289605\n",
      "test loss is 0.0011185220827809046\n",
      "Batch: 1800,train loss is: 0.0010885687444881751\n",
      "test loss is 0.0011802049843004547\n",
      "Batch: 1900,train loss is: 0.0006296048248910563\n",
      "test loss is 0.004874742933957403\n",
      "Batch: 2000,train loss is: 0.0007792485522476744\n",
      "test loss is 0.0029867013278197007\n",
      "Batch: 2100,train loss is: 0.0004924335623968565\n",
      "test loss is 0.0010030077919006574\n",
      "Batch: 2200,train loss is: 0.001729443182059452\n",
      "test loss is 0.001315236230818872\n",
      "Batch: 2300,train loss is: 0.0004804797196191954\n",
      "test loss is 0.00151230009692755\n",
      "Batch: 2400,train loss is: 0.00036920747845715083\n",
      "test loss is 0.0011128827435980312\n",
      "Batch: 2500,train loss is: 0.002606098784650468\n",
      "test loss is 0.0020113473442903654\n",
      "Batch: 2600,train loss is: 0.0019242354694376015\n",
      "test loss is 0.0017778742151576513\n",
      "Batch: 2700,train loss is: 0.0005569713683139362\n",
      "test loss is 0.0010510190325784675\n",
      "Batch: 2800,train loss is: 0.0006708561776768171\n",
      "test loss is 0.0010573505820198838\n",
      "Batch: 2900,train loss is: 0.0010470973820952305\n",
      "test loss is 0.0009696332155431669\n",
      "Batch: 3000,train loss is: 0.0010335275949365308\n",
      "test loss is 0.000861302060336537\n",
      "Batch: 3100,train loss is: 0.0008041748708705162\n",
      "test loss is 0.0009487082550804477\n",
      "Batch: 3200,train loss is: 0.0008528158748491255\n",
      "test loss is 0.0010447012876295615\n",
      "Batch: 3300,train loss is: 0.0008785356652062224\n",
      "test loss is 0.0012110646108526503\n",
      "Batch: 3400,train loss is: 0.0007669849316326495\n",
      "test loss is 0.0008490363403941862\n",
      "Batch: 3500,train loss is: 0.0007258191571487964\n",
      "test loss is 0.001476992899858066\n",
      "Batch: 3600,train loss is: 0.0007544166140653458\n",
      "test loss is 0.005407011966269541\n",
      "Batch: 3700,train loss is: 0.0038312063834790807\n",
      "test loss is 0.0036481048186887857\n",
      "Batch: 3800,train loss is: 0.00082915864096833\n",
      "test loss is 0.0011551547323140946\n",
      "Batch: 3900,train loss is: 0.0011960713079260262\n",
      "test loss is 0.001043784750987818\n",
      "Batch: 4000,train loss is: 0.0002929898133377461\n",
      "test loss is 0.0012654772018441415\n",
      "Batch: 4100,train loss is: 0.0015055808745036577\n",
      "test loss is 0.0009771600829641945\n",
      "Batch: 4200,train loss is: 0.0007114869524962509\n",
      "test loss is 0.0009072470597669771\n",
      "Batch: 4300,train loss is: 0.0011834388426296704\n",
      "test loss is 0.0021306757506718652\n",
      "Batch: 4400,train loss is: 0.000865067497297094\n",
      "test loss is 0.002144366258632425\n",
      "Batch: 4500,train loss is: 0.0018023891347890362\n",
      "test loss is 0.0016545696031455316\n",
      "Batch: 4600,train loss is: 0.007924698719365414\n",
      "test loss is 0.0048096735931877094\n",
      "Batch: 4700,train loss is: 0.002273495294048298\n",
      "test loss is 0.0013762035004384052\n",
      "Batch: 4800,train loss is: 0.002897162595193778\n",
      "test loss is 0.004705621526005725\n",
      "Batch: 4900,train loss is: 0.002326981292208494\n",
      "test loss is 0.0013049584392893856\n",
      "Batch: 5000,train loss is: 0.0005941370219384957\n",
      "test loss is 0.0008718090496461526\n",
      "Batch: 5100,train loss is: 0.0010458303952747502\n",
      "test loss is 0.0012297699546659317\n",
      "Batch: 5200,train loss is: 0.0006496018407960275\n",
      "test loss is 0.0008970418751298408\n",
      "Batch: 5300,train loss is: 0.0022040201779083388\n",
      "test loss is 0.0035803994962587786\n",
      "Batch: 5400,train loss is: 0.002259551404507558\n",
      "test loss is 0.001263498311276409\n",
      "Batch: 5500,train loss is: 0.0020682046196959973\n",
      "test loss is 0.0011543732552368087\n",
      "Batch: 5600,train loss is: 0.0013675478974213857\n",
      "test loss is 0.0008707444739751948\n",
      "Batch: 5700,train loss is: 0.0010909969156134547\n",
      "test loss is 0.0013811508254278216\n",
      "Batch: 5800,train loss is: 0.0007214907220316875\n",
      "test loss is 0.0008801622313652496\n",
      "Batch: 5900,train loss is: 0.0038398527407859095\n",
      "test loss is 0.0015111829693255387\n",
      "Batch: 6000,train loss is: 0.0026493113473988745\n",
      "test loss is 0.0010700258680570295\n",
      "Batch: 6100,train loss is: 0.0006770585684230693\n",
      "test loss is 0.001100055649438594\n",
      "Batch: 6200,train loss is: 0.0005341236513635882\n",
      "test loss is 0.001181667775606451\n",
      "Batch: 6300,train loss is: 0.0012805625689808863\n",
      "test loss is 0.0018116974673031414\n",
      "Batch: 6400,train loss is: 0.0008163791809751305\n",
      "test loss is 0.0015874839247019016\n",
      "Batch: 6500,train loss is: 0.0009807189143813087\n",
      "test loss is 0.0014023515628637585\n",
      "Batch: 6600,train loss is: 0.0029754983822249414\n",
      "test loss is 0.0011658311826713461\n",
      "Batch: 6700,train loss is: 0.0006207366938128213\n",
      "test loss is 0.001367744108945806\n",
      "Batch: 6800,train loss is: 0.0015186370976108426\n",
      "test loss is 0.0011694115434971938\n",
      "Batch: 6900,train loss is: 0.001043622153675172\n",
      "test loss is 0.0010834673581734727\n",
      "Batch: 7000,train loss is: 0.0006438655150373276\n",
      "test loss is 0.001072154216892742\n",
      "Batch: 7100,train loss is: 0.001117705557983825\n",
      "test loss is 0.0013570763059555214\n",
      "Batch: 7200,train loss is: 0.0005517473878795822\n",
      "test loss is 0.0012592996212459216\n",
      "Batch: 7300,train loss is: 0.00163409763607776\n",
      "test loss is 0.0010674431526943862\n",
      "Batch: 7400,train loss is: 0.00036018395092482345\n",
      "test loss is 0.0022115336602092535\n",
      "Batch: 7500,train loss is: 0.0006365886379044336\n",
      "test loss is 0.0009285693762606189\n",
      "Batch: 7600,train loss is: 0.0006425301594980674\n",
      "test loss is 0.0011043258397976591\n",
      "Batch: 7700,train loss is: 0.0018527830824830295\n",
      "test loss is 0.001102359217423144\n",
      "Batch: 7800,train loss is: 0.0009837127812278351\n",
      "test loss is 0.0009747815810820496\n",
      "Batch: 7900,train loss is: 0.0010304088827769943\n",
      "test loss is 0.0007847529215281062\n",
      "Batch: 8000,train loss is: 0.00046547114528664195\n",
      "test loss is 0.0013958606695675657\n",
      "Batch: 8100,train loss is: 0.0022592537420415043\n",
      "test loss is 0.0015947345078652139\n",
      "Batch: 8200,train loss is: 0.0014818829518524082\n",
      "test loss is 0.0013212336746057601\n",
      "Batch: 8300,train loss is: 0.0005482680141160591\n",
      "test loss is 0.0025749520717412077\n",
      "Batch: 8400,train loss is: 0.006869583389531675\n",
      "test loss is 0.0029731965481624113\n",
      "Batch: 8500,train loss is: 0.0027621538119805087\n",
      "test loss is 0.0021948777100642696\n",
      "Batch: 8600,train loss is: 0.0016887752253376858\n",
      "test loss is 0.001938776039604703\n",
      "Batch: 8700,train loss is: 0.0007012609232907169\n",
      "test loss is 0.0018073864636902826\n",
      "Batch: 8800,train loss is: 0.0018079463813197242\n",
      "test loss is 0.001109021279125588\n",
      "Batch: 8900,train loss is: 0.0007274868866113253\n",
      "test loss is 0.001010175153937297\n",
      "Batch: 9000,train loss is: 0.0014630734586169398\n",
      "test loss is 0.0015113376270871886\n",
      "Batch: 9100,train loss is: 0.0005504146120925611\n",
      "test loss is 0.0009236587615688939\n",
      "Batch: 9200,train loss is: 0.0016555540308872913\n",
      "test loss is 0.0013153958224380393\n",
      "Batch: 9300,train loss is: 0.0027110994055200644\n",
      "test loss is 0.0012781149188011106\n",
      "Batch: 9400,train loss is: 0.0005263852445431347\n",
      "test loss is 0.0008834072528423819\n",
      "Batch: 9500,train loss is: 0.0004301082345979931\n",
      "test loss is 0.0019329980951501473\n",
      "Batch: 9600,train loss is: 0.0018853318146984488\n",
      "test loss is 0.0016422942314245454\n",
      "Batch: 9700,train loss is: 0.0017351231836678944\n",
      "test loss is 0.0011916772751040907\n",
      "Batch: 9800,train loss is: 0.0003398241617628981\n",
      "test loss is 0.0009656304439393903\n",
      "Batch: 9900,train loss is: 0.0014614645106876022\n",
      "test loss is 0.0038448309393553383\n",
      "Batch: 10000,train loss is: 0.0028408288737490264\n",
      "test loss is 0.0020802657365053333\n",
      "Batch: 10100,train loss is: 0.0011499072536798496\n",
      "test loss is 0.0012099649136682014\n",
      "Batch: 10200,train loss is: 0.0014437859975866539\n",
      "test loss is 0.000715795255754558\n",
      "Batch: 10300,train loss is: 0.001492108353089195\n",
      "test loss is 0.0009170887432922992\n",
      "Batch: 10400,train loss is: 0.0008591236460821103\n",
      "test loss is 0.0021840731544430065\n",
      "Batch: 10500,train loss is: 0.0020394347700963718\n",
      "test loss is 0.001439335005272424\n",
      "Batch: 10600,train loss is: 0.0005497203279665453\n",
      "test loss is 0.0014474530379383291\n",
      "Batch: 10700,train loss is: 0.0006117953827175723\n",
      "test loss is 0.0012930673537430977\n",
      "Batch: 10800,train loss is: 0.0015784239706361598\n",
      "test loss is 0.010265866258615306\n",
      "Batch: 10900,train loss is: 0.0010843648564549343\n",
      "test loss is 0.000950920408523302\n",
      "Batch: 11000,train loss is: 0.0007964294134858934\n",
      "test loss is 0.0008533008659856622\n",
      "Batch: 11100,train loss is: 0.001657031427093832\n",
      "test loss is 0.001575650680072985\n",
      "Batch: 11200,train loss is: 0.0007572497158428108\n",
      "test loss is 0.0012680200143563919\n",
      "Batch: 11300,train loss is: 0.002855528962814894\n",
      "test loss is 0.002969477227194813\n",
      "Batch: 11400,train loss is: 0.003992557539124484\n",
      "test loss is 0.0024528373799333615\n",
      "Batch: 11500,train loss is: 0.0010570165047832899\n",
      "test loss is 0.0019848155036417897\n",
      "Batch: 11600,train loss is: 0.0039305999042832844\n",
      "test loss is 0.002448452443849615\n",
      "Batch: 11700,train loss is: 0.00026970812938442024\n",
      "test loss is 0.0011011437444003326\n",
      "Batch: 11800,train loss is: 0.0006079253738364741\n",
      "test loss is 0.0008520323011478825\n",
      "Batch: 11900,train loss is: 0.0013007627548027778\n",
      "test loss is 0.0009722239863144407\n",
      "Batch: 12000,train loss is: 0.0007447221402582116\n",
      "test loss is 0.0008872893455817513\n",
      "Batch: 12100,train loss is: 0.0029051282593400016\n",
      "test loss is 0.0018640497122980346\n",
      "Batch: 12200,train loss is: 0.0016886539804497412\n",
      "test loss is 0.001677023971239931\n",
      "Batch: 12300,train loss is: 0.00047482252055520855\n",
      "test loss is 0.002002884292970496\n",
      "Batch: 12400,train loss is: 0.003867235764698111\n",
      "test loss is 0.0029639890267276546\n",
      "Batch: 12500,train loss is: 0.0019603872862520625\n",
      "test loss is 0.005339168517307317\n",
      "Batch: 12600,train loss is: 0.0006356694313740005\n",
      "test loss is 0.0011437802516877937\n",
      "Batch: 12700,train loss is: 0.0010887446720578445\n",
      "test loss is 0.0009581577007594555\n",
      "Batch: 12800,train loss is: 0.0003299666537543077\n",
      "test loss is 0.0008805326538953254\n",
      "Batch: 12900,train loss is: 0.000448393771165801\n",
      "test loss is 0.0010831406627941763\n",
      "Batch: 13000,train loss is: 0.000332992274749391\n",
      "test loss is 0.0008583687920759789\n",
      "Batch: 13100,train loss is: 0.0010446352986129407\n",
      "test loss is 0.0010453123641839426\n",
      "Batch: 13200,train loss is: 0.0013204085896281598\n",
      "test loss is 0.0016181378635682826\n",
      "Batch: 13300,train loss is: 0.0007929813319651622\n",
      "test loss is 0.0022988192444927196\n",
      "Batch: 13400,train loss is: 0.0010151293457139195\n",
      "test loss is 0.003174718488823686\n",
      "Batch: 13500,train loss is: 0.0009387004062938058\n",
      "test loss is 0.0012029200985647726\n",
      "Batch: 13600,train loss is: 0.004796857885715307\n",
      "test loss is 0.005327407906572033\n",
      "Batch: 13700,train loss is: 0.001047788734342693\n",
      "test loss is 0.0016604400446232868\n",
      "Batch: 13800,train loss is: 0.0007436138742070656\n",
      "test loss is 0.001633979551464314\n",
      "Batch: 13900,train loss is: 0.0006340755806033334\n",
      "test loss is 0.0011063328705191065\n",
      "Batch: 14000,train loss is: 0.0017398982748229445\n",
      "test loss is 0.002597058564221848\n",
      "Batch: 14100,train loss is: 0.0009607024629446539\n",
      "test loss is 0.0015977095042895334\n",
      "Batch: 14200,train loss is: 0.0004455388649351747\n",
      "test loss is 0.0009673388443331381\n",
      "Batch: 14300,train loss is: 0.0007203896386039507\n",
      "test loss is 0.0010253528716454513\n",
      "Batch: 14400,train loss is: 0.0006776821306335184\n",
      "test loss is 0.0007837734122660311\n",
      "Batch: 14500,train loss is: 0.0004448259487585391\n",
      "test loss is 0.00067187944223343\n",
      "Batch: 14600,train loss is: 0.001185811643152739\n",
      "test loss is 0.0016920724272137032\n",
      "Batch: 14700,train loss is: 0.0021265476797045733\n",
      "test loss is 0.0017010231837614933\n",
      "Batch: 14800,train loss is: 0.0009661513305026274\n",
      "test loss is 0.0016212169316466738\n",
      "Batch: 14900,train loss is: 0.0011757552847272133\n",
      "test loss is 0.001211899003940309\n",
      "Batch: 15000,train loss is: 0.0007164967454657787\n",
      "test loss is 0.0008352908493766382\n",
      "Batch: 15100,train loss is: 0.0006694360297502411\n",
      "test loss is 0.001050956597415464\n",
      "Batch: 15200,train loss is: 0.0012207860723067586\n",
      "test loss is 0.0011657950796621565\n",
      "Batch: 15300,train loss is: 0.002124791034270769\n",
      "test loss is 0.0007801822479942505\n",
      "Batch: 15400,train loss is: 0.000524867247501543\n",
      "test loss is 0.0016619207723959763\n",
      "Batch: 15500,train loss is: 0.0007977077401009998\n",
      "test loss is 0.0010523892634361105\n",
      "Batch: 15600,train loss is: 0.00444173389915532\n",
      "test loss is 0.0022755774528653034\n",
      "Batch: 15700,train loss is: 0.0007007577244613766\n",
      "test loss is 0.003001507367404314\n",
      "Batch: 15800,train loss is: 0.0006010924153686713\n",
      "test loss is 0.0014095077527875909\n",
      "Batch: 15900,train loss is: 0.0024756463213858352\n",
      "test loss is 0.001961300096635619\n",
      "Batch: 16000,train loss is: 0.0005135266792111535\n",
      "test loss is 0.0007669082972945065\n",
      "Batch: 16100,train loss is: 0.0019005567578754372\n",
      "test loss is 0.0009942184093165599\n",
      "Batch: 16200,train loss is: 0.0006311837604148464\n",
      "test loss is 0.0010280536401047241\n",
      "Batch: 16300,train loss is: 0.0006896093352247373\n",
      "test loss is 0.0006523843507991171\n",
      "Batch: 16400,train loss is: 0.001233513490499508\n",
      "test loss is 0.0011293540622135776\n",
      "Batch: 16500,train loss is: 0.0012562118627644973\n",
      "test loss is 0.0008684784016297611\n",
      "Batch: 16600,train loss is: 0.0002246086304680789\n",
      "test loss is 0.0009620488315601844\n",
      "Batch: 16700,train loss is: 0.0019070322461303118\n",
      "test loss is 0.0008587103819859318\n",
      "Batch: 16800,train loss is: 0.0005792614828459338\n",
      "test loss is 0.0009423872586680819\n",
      "Batch: 16900,train loss is: 0.001259809468288404\n",
      "test loss is 0.001092496118788733\n",
      "Batch: 17000,train loss is: 0.0017486023754190484\n",
      "test loss is 0.0022552119674564106\n",
      "Batch: 17100,train loss is: 0.001639698580492211\n",
      "test loss is 0.002066449674996151\n",
      "Batch: 17200,train loss is: 0.000463316399027688\n",
      "test loss is 0.0008571805812500133\n",
      "Batch: 17300,train loss is: 0.0005979203975765553\n",
      "test loss is 0.0008645240714341634\n",
      "Batch: 17400,train loss is: 0.0006821003437061039\n",
      "test loss is 0.0008011155971132511\n",
      "Batch: 17500,train loss is: 0.0009006460889041292\n",
      "test loss is 0.0008989617785164643\n",
      "Batch: 17600,train loss is: 0.0015328854604286274\n",
      "test loss is 0.00203357950051632\n",
      "Batch: 17700,train loss is: 0.00047015583994640235\n",
      "test loss is 0.0018309964742511016\n",
      "Batch: 17800,train loss is: 0.002447605785591036\n",
      "test loss is 0.002694533806243693\n",
      "Batch: 17900,train loss is: 0.00249276391957421\n",
      "test loss is 0.0011501054421830927\n",
      "Batch: 18000,train loss is: 0.0021039597344398274\n",
      "test loss is 0.0013564463362202683\n",
      "Batch: 18100,train loss is: 0.0010629281806089376\n",
      "test loss is 0.0011050503528378145\n",
      "Batch: 18200,train loss is: 0.00047422062006207853\n",
      "test loss is 0.0018636269073350397\n",
      "Batch: 18300,train loss is: 0.0013313509101488075\n",
      "test loss is 0.0007606141709206828\n",
      "Batch: 18400,train loss is: 0.0013281274808854195\n",
      "test loss is 0.0012832127688478332\n",
      "Batch: 18500,train loss is: 0.0007780099989760233\n",
      "test loss is 0.001166057978076963\n",
      "Batch: 18600,train loss is: 0.0002834260700098988\n",
      "test loss is 0.000828734546751534\n",
      "Batch: 18700,train loss is: 0.0009895650470738405\n",
      "test loss is 0.0011732891895605748\n",
      "Batch: 18800,train loss is: 0.001118000639257155\n",
      "test loss is 0.001092300541311687\n",
      "Batch: 18900,train loss is: 0.0007111772292543214\n",
      "test loss is 0.0016536430949864044\n",
      "Batch: 19000,train loss is: 0.00061396959462857\n",
      "test loss is 0.0008209494660379729\n",
      "Batch: 19100,train loss is: 0.0008414868521478845\n",
      "test loss is 0.0008934239670240288\n",
      "Batch: 19200,train loss is: 0.0010156792191489167\n",
      "test loss is 0.0009694690422705814\n",
      "Batch: 19300,train loss is: 0.001540897811276348\n",
      "test loss is 0.0016600644255106932\n",
      "Batch: 19400,train loss is: 0.0009351633026870892\n",
      "test loss is 0.0020904354957282474\n",
      "Batch: 19500,train loss is: 0.00038421211573311676\n",
      "test loss is 0.0010357292828722328\n",
      "Batch: 19600,train loss is: 0.0008145725436057852\n",
      "test loss is 0.0013345697155744231\n",
      "Batch: 19700,train loss is: 0.004049193251016463\n",
      "test loss is 0.0015261493577181508\n",
      "Batch: 19800,train loss is: 0.0018790252456116609\n",
      "test loss is 0.0009133170845501122\n",
      "Batch: 19900,train loss is: 0.0005909892277949468\n",
      "test loss is 0.001316184022679501\n",
      "Batch: 20000,train loss is: 0.0014782879138209692\n",
      "test loss is 0.003844028449240175\n",
      "Batch: 20100,train loss is: 0.0024130616242092865\n",
      "test loss is 0.001925730810795734\n",
      "Batch: 20200,train loss is: 0.0039301411885537835\n",
      "test loss is 0.0015081245569803683\n",
      "Batch: 20300,train loss is: 0.0003200363303849531\n",
      "test loss is 0.00075944279506742\n",
      "Batch: 20400,train loss is: 0.0012242868621567594\n",
      "test loss is 0.002864800942458097\n",
      "Batch: 20500,train loss is: 0.0026834790284850684\n",
      "test loss is 0.0034612138292551596\n",
      "Batch: 20600,train loss is: 0.003228110036061988\n",
      "test loss is 0.0013911037165893598\n",
      "Batch: 20700,train loss is: 0.0007986415104339423\n",
      "test loss is 0.0008984680358228244\n",
      "Batch: 20800,train loss is: 0.0013955514077072099\n",
      "test loss is 0.000716117331008424\n",
      "Batch: 20900,train loss is: 0.0004438656803750764\n",
      "test loss is 0.000758018120316953\n",
      "Batch: 21000,train loss is: 0.003084081156380802\n",
      "test loss is 0.0016655512368047062\n",
      "Batch: 21100,train loss is: 0.0009030377394128717\n",
      "test loss is 0.0014183077289375622\n",
      "Batch: 21200,train loss is: 0.001290883774404504\n",
      "test loss is 0.0016425069446146425\n",
      "Batch: 21300,train loss is: 0.000704590244097653\n",
      "test loss is 0.0012667787142344581\n",
      "Batch: 21400,train loss is: 0.0009090601871467755\n",
      "test loss is 0.0008909248467248387\n",
      "Batch: 21500,train loss is: 0.0003470241520188126\n",
      "test loss is 0.0013861414189525218\n",
      "Batch: 21600,train loss is: 0.0007052164480200718\n",
      "test loss is 0.0012875179720197212\n",
      "Batch: 21700,train loss is: 0.0006508813548007304\n",
      "test loss is 0.0008540334352906161\n",
      "Batch: 21800,train loss is: 0.001024968445309361\n",
      "test loss is 0.0006286618411346398\n",
      "Batch: 21900,train loss is: 0.0009203688782662636\n",
      "test loss is 0.0012724932009553975\n",
      "Batch: 22000,train loss is: 0.0055711105658730045\n",
      "test loss is 0.005165439254479174\n",
      "Batch: 22100,train loss is: 0.00044644786007285407\n",
      "test loss is 0.0012806495677880135\n",
      "Batch: 22200,train loss is: 0.00626837312924145\n",
      "test loss is 0.0021043335985901283\n",
      "Batch: 22300,train loss is: 0.005274218374686311\n",
      "test loss is 0.007925856929362811\n",
      "Batch: 22400,train loss is: 0.001571122269082807\n",
      "test loss is 0.001454741441267818\n",
      "Batch: 22500,train loss is: 0.0006293005441043849\n",
      "test loss is 0.0008954348433780789\n",
      "Batch: 22600,train loss is: 0.0004929073060733325\n",
      "test loss is 0.000889601657949546\n",
      "Batch: 22700,train loss is: 0.0012581965739068592\n",
      "test loss is 0.0018353318009931942\n",
      "Batch: 22800,train loss is: 0.0009005758109976753\n",
      "test loss is 0.0009249537372058272\n",
      "Batch: 22900,train loss is: 0.0007687457946406247\n",
      "test loss is 0.0010685584638287359\n",
      "Batch: 23000,train loss is: 0.000785336068560035\n",
      "test loss is 0.0006202354510954701\n",
      "Batch: 23100,train loss is: 0.000266231696993047\n",
      "test loss is 0.0007679759622220151\n",
      "Batch: 23200,train loss is: 0.0016679990744829065\n",
      "test loss is 0.0015081694117030317\n",
      "Batch: 23300,train loss is: 0.00213628903551696\n",
      "test loss is 0.0020629732997216555\n",
      "Batch: 23400,train loss is: 0.001504723956225159\n",
      "test loss is 0.0017986367813938124\n",
      "Batch: 23500,train loss is: 0.0018077153023146975\n",
      "test loss is 0.0007721071406221973\n",
      "Batch: 23600,train loss is: 0.00037349020986664796\n",
      "test loss is 0.0006818679740303531\n",
      "Batch: 23700,train loss is: 0.0022777726723726397\n",
      "test loss is 0.0016990991341252694\n",
      "Batch: 23800,train loss is: 0.0012818814051371531\n",
      "test loss is 0.0015872105515973224\n",
      "Batch: 23900,train loss is: 0.0012947022675101336\n",
      "test loss is 0.0022361212562253224\n",
      "Batch: 24000,train loss is: 0.001407759224998331\n",
      "test loss is 0.0019272193055011968\n",
      "Batch: 24100,train loss is: 0.0013303046501017004\n",
      "test loss is 0.0010608743768163258\n",
      "Batch: 24200,train loss is: 0.0015354669077128158\n",
      "test loss is 0.000722839721343538\n",
      "Batch: 24300,train loss is: 0.0038877891151281943\n",
      "test loss is 0.0016889731327205044\n",
      "Batch: 24400,train loss is: 0.0018449115033809197\n",
      "test loss is 0.0010094415587029537\n",
      "Batch: 24500,train loss is: 0.0018913776851293249\n",
      "test loss is 0.001420964944430918\n",
      "Batch: 24600,train loss is: 0.0014346825643309147\n",
      "test loss is 0.0009534112428026729\n",
      "Batch: 24700,train loss is: 0.0019544838469749085\n",
      "test loss is 0.001978172941600689\n",
      "Batch: 24800,train loss is: 0.0018088513762275192\n",
      "test loss is 0.0006894156594405657\n",
      "Batch: 24900,train loss is: 0.0012165246892640897\n",
      "test loss is 0.0014106764815743607\n",
      "Batch: 25000,train loss is: 0.0005210792811304432\n",
      "test loss is 0.0007963019149388181\n",
      "Batch: 25100,train loss is: 0.0006948429074741801\n",
      "test loss is 0.0009038982835152987\n",
      "Batch: 25200,train loss is: 0.001107701839876164\n",
      "test loss is 0.0008477408311808481\n",
      "Batch: 25300,train loss is: 0.0013022314901770752\n",
      "test loss is 0.0010459622637929206\n",
      "Batch: 25400,train loss is: 0.0018309949591992806\n",
      "test loss is 0.0018171020509063494\n",
      "Batch: 25500,train loss is: 0.0006202530757558012\n",
      "test loss is 0.001441759036048746\n",
      "Batch: 25600,train loss is: 0.0006733267005778126\n",
      "test loss is 0.0007283399957108074\n",
      "Batch: 25700,train loss is: 0.0011244298411358347\n",
      "test loss is 0.0005891322415049697\n",
      "Batch: 25800,train loss is: 0.00039022241014382913\n",
      "test loss is 0.0007270520344454919\n",
      "Batch: 25900,train loss is: 0.00038855249966454635\n",
      "test loss is 0.0018322914783894869\n",
      "Batch: 26000,train loss is: 0.0028074237577784553\n",
      "test loss is 0.004929028695244177\n",
      "Batch: 26100,train loss is: 0.002029065104121535\n",
      "test loss is 0.004516835357105205\n",
      "Batch: 26200,train loss is: 0.0008622493492028357\n",
      "test loss is 0.0017988823895268623\n",
      "Batch: 26300,train loss is: 0.0005347109937811003\n",
      "test loss is 0.0007753252275231059\n",
      "Batch: 26400,train loss is: 0.0003224073101928805\n",
      "test loss is 0.0009159720155227055\n",
      "Batch: 26500,train loss is: 0.0003234704056984234\n",
      "test loss is 0.0006652434915667452\n",
      "Batch: 26600,train loss is: 0.0006747979546835857\n",
      "test loss is 0.0006018628974545539\n",
      "Batch: 26700,train loss is: 0.0006139835862263087\n",
      "test loss is 0.0008062214561749166\n",
      "Batch: 26800,train loss is: 0.0009744813997028869\n",
      "test loss is 0.000732241268424414\n",
      "Batch: 26900,train loss is: 0.0009456874165884312\n",
      "test loss is 0.0008571627616596793\n",
      "Batch: 27000,train loss is: 0.0009318373943528222\n",
      "test loss is 0.001122131473472411\n",
      "Batch: 27100,train loss is: 0.0010757231770822208\n",
      "test loss is 0.0018241219663648904\n",
      "Batch: 27200,train loss is: 0.0005458780025239476\n",
      "test loss is 0.0007528501232898562\n",
      "Batch: 27300,train loss is: 0.0013489359470757818\n",
      "test loss is 0.0021206131606355525\n",
      "Batch: 27400,train loss is: 0.0006171235005503466\n",
      "test loss is 0.001167673201996784\n",
      "Batch: 27500,train loss is: 0.0010114773046623747\n",
      "test loss is 0.001469953815275612\n",
      "Batch: 27600,train loss is: 0.001110265846030985\n",
      "test loss is 0.002294090690127632\n",
      "Batch: 27700,train loss is: 0.0006275255793325654\n",
      "test loss is 0.0009551912652453595\n",
      "Batch: 27800,train loss is: 0.003289189468396687\n",
      "test loss is 0.004068674211883376\n",
      "Batch: 27900,train loss is: 0.0008691321850202965\n",
      "test loss is 0.000957428369149409\n",
      "Batch: 28000,train loss is: 0.00075968859426866\n",
      "test loss is 0.0008206410520162273\n",
      "Batch: 28100,train loss is: 0.001702926149969635\n",
      "test loss is 0.0011931563126452525\n",
      "Batch: 28200,train loss is: 0.00046900875387156026\n",
      "test loss is 0.0017825065283998923\n",
      "Batch: 28300,train loss is: 0.0019507976826664774\n",
      "test loss is 0.0015280618581696224\n",
      "Batch: 28400,train loss is: 0.0007459946667555272\n",
      "test loss is 0.0009582122238799057\n",
      "Batch: 28500,train loss is: 0.0010776597044125519\n",
      "test loss is 0.0007839299343085405\n",
      "Batch: 28600,train loss is: 0.0014750765119294538\n",
      "test loss is 0.0016109582545894474\n",
      "Batch: 28700,train loss is: 0.0009579337703920048\n",
      "test loss is 0.0015057248180867034\n",
      "Batch: 28800,train loss is: 0.0009627748583766394\n",
      "test loss is 0.0007853361725282976\n",
      "Batch: 28900,train loss is: 0.0010205319179038932\n",
      "test loss is 0.001346217717048002\n",
      "Batch: 29000,train loss is: 0.0011968101546685189\n",
      "test loss is 0.0006800185118878813\n",
      "Batch: 29100,train loss is: 0.0004211091763408895\n",
      "test loss is 0.0015947936587479403\n",
      "Batch: 29200,train loss is: 0.0020981598018976104\n",
      "test loss is 0.003580390939459513\n",
      "Batch: 29300,train loss is: 0.0020976645298446685\n",
      "test loss is 0.0015382403405630302\n",
      "Batch: 29400,train loss is: 0.0011353284073000742\n",
      "test loss is 0.001096883529550166\n",
      "Batch: 29500,train loss is: 0.0003909298600326506\n",
      "test loss is 0.000699915574437352\n",
      "Batch: 29600,train loss is: 0.0011507534208136676\n",
      "test loss is 0.0006906102148443188\n",
      "Batch: 29700,train loss is: 0.0011084116930600131\n",
      "test loss is 0.0014098607854841248\n",
      "Batch: 29800,train loss is: 0.001108049073076682\n",
      "test loss is 0.0006643623970897043\n",
      "Batch: 29900,train loss is: 0.002259856313355442\n",
      "test loss is 0.0006540813139851612\n",
      "Batch: 30000,train loss is: 0.0007215742696588659\n",
      "test loss is 0.0007080574352751326\n",
      "Batch: 30100,train loss is: 0.0008700350839627506\n",
      "test loss is 0.005919669071483395\n",
      "Batch: 30200,train loss is: 0.0008998814856162219\n",
      "test loss is 0.0019721260416083394\n",
      "Batch: 30300,train loss is: 0.0030353818752055736\n",
      "test loss is 0.005177799603744226\n",
      "Batch: 30400,train loss is: 0.0042198939413824995\n",
      "test loss is 0.005711284487927952\n",
      "Batch: 30500,train loss is: 0.000991974436304406\n",
      "test loss is 0.001382908905388563\n",
      "Batch: 30600,train loss is: 0.0007934546179679331\n",
      "test loss is 0.0006776980513131242\n",
      "Batch: 30700,train loss is: 0.0007582676673191321\n",
      "test loss is 0.0009015769325231314\n",
      "Batch: 30800,train loss is: 0.00019099954076859035\n",
      "test loss is 0.0007910243694764694\n",
      "Batch: 30900,train loss is: 0.0004848197373265288\n",
      "test loss is 0.0009283516850538214\n",
      "Batch: 31000,train loss is: 0.0003525611945631806\n",
      "test loss is 0.0007628793932517142\n",
      "Batch: 31100,train loss is: 0.0012789230131368087\n",
      "test loss is 0.002231604976559413\n",
      "Batch: 31200,train loss is: 0.00048175911221031147\n",
      "test loss is 0.0008126003598532676\n",
      "Batch: 31300,train loss is: 0.0008861799554870542\n",
      "test loss is 0.0011014029384409443\n",
      "Batch: 31400,train loss is: 0.0006756078366391341\n",
      "test loss is 0.0023061251965525134\n",
      "Batch: 31500,train loss is: 0.001386996924804928\n",
      "test loss is 0.0014986311937211824\n",
      "Batch: 31600,train loss is: 0.0007292579978308852\n",
      "test loss is 0.0008366058955235363\n",
      "Batch: 31700,train loss is: 0.00029368722217154564\n",
      "test loss is 0.0006970835440214077\n",
      "Batch: 31800,train loss is: 0.0010553629948755966\n",
      "test loss is 0.0006306631767497389\n",
      "Batch: 31900,train loss is: 0.00031280711980828446\n",
      "test loss is 0.0009365216917270887\n",
      "Batch: 32000,train loss is: 0.002254287075580371\n",
      "test loss is 0.0005614878892310509\n",
      "Batch: 32100,train loss is: 0.00035514381551411976\n",
      "test loss is 0.0008705974499659208\n",
      "Batch: 32200,train loss is: 0.00039029814283596725\n",
      "test loss is 0.0006488144853309047\n",
      "Batch: 32300,train loss is: 0.0005627109972237613\n",
      "test loss is 0.0008209421942231109\n",
      "Batch: 32400,train loss is: 0.0009924460366033372\n",
      "test loss is 0.0015636233533452802\n",
      "Batch: 32500,train loss is: 0.001651349642550718\n",
      "test loss is 0.0009095411062398406\n",
      "Batch: 32600,train loss is: 0.0004957196731077751\n",
      "test loss is 0.0007299750369154987\n",
      "Batch: 32700,train loss is: 0.0006850234662787998\n",
      "test loss is 0.0016965115811749594\n",
      "Batch: 32800,train loss is: 0.00055059494225148\n",
      "test loss is 0.001224994072672771\n",
      "Batch: 32900,train loss is: 0.0013711440845973414\n",
      "test loss is 0.0010651873258718794\n",
      "Batch: 33000,train loss is: 0.00041452499761388033\n",
      "test loss is 0.0010594741493005545\n",
      "Batch: 33100,train loss is: 0.0003604411657014163\n",
      "test loss is 0.0010619537247115389\n",
      "Batch: 33200,train loss is: 0.0005311277763805872\n",
      "test loss is 0.0007186410868712913\n",
      "Batch: 33300,train loss is: 0.000583040228806458\n",
      "test loss is 0.0009603091196263014\n",
      "Batch: 33400,train loss is: 0.0007130694727139243\n",
      "test loss is 0.001610613162173886\n",
      "Batch: 33500,train loss is: 0.0031111039662727958\n",
      "test loss is 0.002613899957032822\n",
      "Batch: 33600,train loss is: 0.00043613001428953235\n",
      "test loss is 0.001103503156641641\n",
      "Batch: 33700,train loss is: 0.0008959579986536767\n",
      "test loss is 0.0008578723377095279\n",
      "Batch: 33800,train loss is: 0.0007387752589220726\n",
      "test loss is 0.0008277698699595688\n",
      "Batch: 33900,train loss is: 0.0015756182194160329\n",
      "test loss is 0.0006387563904967563\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch_NN.train import train_model\n",
    "\n",
    "train_model(loss_MSE,optim_Adam,model,data_loader,train_data,test_data,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Epoch: 0----------------------------------\n",
      "Batch: 0,train loss is: 0.0007537674559484046\n",
      "test loss is 0.000859589918262441\n",
      "Batch: 100,train loss is: 0.001383027451107782\n",
      "test loss is 0.001168932036503983\n",
      "Batch: 200,train loss is: 0.000681544914988162\n",
      "test loss is 0.0008376153926205831\n",
      "Batch: 300,train loss is: 0.0007164231324842057\n",
      "test loss is 0.0007679996300259924\n",
      "Batch: 400,train loss is: 0.0010438876921341272\n",
      "test loss is 0.0017421162743357635\n",
      "Batch: 500,train loss is: 0.0005488233315217334\n",
      "test loss is 0.0015156034320609874\n",
      "Batch: 600,train loss is: 0.00028338331473464737\n",
      "test loss is 0.0010141185299885297\n",
      "Batch: 700,train loss is: 0.0004750451954956375\n",
      "test loss is 0.000859348743633431\n",
      "Batch: 800,train loss is: 0.0002696539182703794\n",
      "test loss is 0.001160051044463672\n",
      "Batch: 900,train loss is: 0.0005240564114719962\n",
      "test loss is 0.0008272023710494387\n",
      "Batch: 1000,train loss is: 0.002892133280653025\n",
      "test loss is 0.001105641962382108\n",
      "Batch: 1100,train loss is: 0.0020965443966130927\n",
      "test loss is 0.0009478226235762665\n",
      "Batch: 1200,train loss is: 0.001423828943664862\n",
      "test loss is 0.001977540226629387\n",
      "Batch: 1300,train loss is: 0.0009416957504105683\n",
      "test loss is 0.0024130687366293903\n",
      "Batch: 1400,train loss is: 0.00036891440034173266\n",
      "test loss is 0.0008544985021865157\n",
      "Batch: 1500,train loss is: 0.001575225209904351\n",
      "test loss is 0.004976266402481745\n",
      "Batch: 1600,train loss is: 0.0008752929775941393\n",
      "test loss is 0.0017282404429223555\n",
      "Batch: 1700,train loss is: 0.0010191597422537643\n",
      "test loss is 0.0008171529159525864\n",
      "Batch: 1800,train loss is: 0.0010285732766117008\n",
      "test loss is 0.0009654551589919242\n",
      "Batch: 1900,train loss is: 0.00040929675570857083\n",
      "test loss is 0.0025972776966811063\n",
      "Batch: 2000,train loss is: 0.0008447047569163014\n",
      "test loss is 0.0008091646830695992\n",
      "Batch: 2100,train loss is: 0.0005004071329571804\n",
      "test loss is 0.0006808488906604531\n",
      "Batch: 2200,train loss is: 0.0011782204874462748\n",
      "test loss is 0.0007473908720498943\n",
      "Batch: 2300,train loss is: 0.00034948837898220083\n",
      "test loss is 0.0007299108847453267\n",
      "Batch: 2400,train loss is: 0.000377184538394717\n",
      "test loss is 0.0006743088972722289\n",
      "Batch: 2500,train loss is: 0.0007278446691493312\n",
      "test loss is 0.0009274930892061118\n",
      "Batch: 2600,train loss is: 0.0010374889547920944\n",
      "test loss is 0.0008428998726582142\n",
      "Batch: 2700,train loss is: 0.00032823344562564867\n",
      "test loss is 0.0006194137193098766\n",
      "Batch: 2800,train loss is: 0.0006311512136847639\n",
      "test loss is 0.0009176181772815865\n",
      "Batch: 2900,train loss is: 0.0013554425440962731\n",
      "test loss is 0.0007352247139123451\n",
      "Batch: 3000,train loss is: 0.00037457884074706266\n",
      "test loss is 0.0007062033545596855\n",
      "Batch: 3100,train loss is: 0.00042492719319577564\n",
      "test loss is 0.0008273298643163971\n",
      "Batch: 3200,train loss is: 0.0018260207743651306\n",
      "test loss is 0.002709143775207164\n",
      "Batch: 3300,train loss is: 0.00039836948409936024\n",
      "test loss is 0.0005815841623592762\n",
      "Batch: 3400,train loss is: 0.0008384381603395069\n",
      "test loss is 0.0009112639006506473\n",
      "Batch: 3500,train loss is: 0.0007650592495535695\n",
      "test loss is 0.0015505774106227136\n",
      "Batch: 3600,train loss is: 0.00034998194988861863\n",
      "test loss is 0.002048802206141817\n",
      "Batch: 3700,train loss is: 0.0015170361967748937\n",
      "test loss is 0.001843418464744783\n",
      "Batch: 3800,train loss is: 0.0011650029415294753\n",
      "test loss is 0.0010760355072477177\n",
      "Batch: 3900,train loss is: 0.0007104054309836123\n",
      "test loss is 0.0008240088828197977\n",
      "Batch: 4000,train loss is: 0.00024325523031718525\n",
      "test loss is 0.0013629868236430434\n",
      "Batch: 4100,train loss is: 0.00031019295523259787\n",
      "test loss is 0.0008965547447271409\n",
      "Batch: 4200,train loss is: 0.0010930416032447882\n",
      "test loss is 0.0015655799905768542\n",
      "Batch: 4300,train loss is: 0.0006276007746237579\n",
      "test loss is 0.001009359262409832\n",
      "Batch: 4400,train loss is: 0.000418261860455646\n",
      "test loss is 0.001153902482556032\n",
      "Batch: 4500,train loss is: 0.0008157372554786588\n",
      "test loss is 0.0008356674749593483\n",
      "Batch: 4600,train loss is: 0.012067667162106287\n",
      "test loss is 0.00769604275940874\n",
      "Batch: 4700,train loss is: 0.0011010139454104527\n",
      "test loss is 0.0012413160098247798\n",
      "Batch: 4800,train loss is: 0.0016071077136288142\n",
      "test loss is 0.0037854502546513713\n",
      "Batch: 4900,train loss is: 0.0008725217286043844\n",
      "test loss is 0.0007635171922235174\n",
      "Batch: 5000,train loss is: 0.0002679560427412396\n",
      "test loss is 0.000560305754373497\n",
      "Batch: 5100,train loss is: 0.0008931112735804928\n",
      "test loss is 0.0009045653098975137\n",
      "Batch: 5200,train loss is: 0.0010556474783129702\n",
      "test loss is 0.0007195763348656341\n",
      "Batch: 5300,train loss is: 0.001202097452211418\n",
      "test loss is 0.0014372140966312434\n",
      "Batch: 5400,train loss is: 0.0013532535372334736\n",
      "test loss is 0.0008337368448816148\n",
      "Batch: 5500,train loss is: 0.001082548817292326\n",
      "test loss is 0.0008050075819229134\n",
      "Batch: 5600,train loss is: 0.0006674425872319142\n",
      "test loss is 0.0009883257715293113\n",
      "Batch: 5700,train loss is: 0.0006866314182833798\n",
      "test loss is 0.0009889647541560772\n",
      "Batch: 5800,train loss is: 0.000571821261313081\n",
      "test loss is 0.0007511272905124922\n",
      "Batch: 5900,train loss is: 0.0017470292511041253\n",
      "test loss is 0.00110178205329214\n",
      "Batch: 6000,train loss is: 0.001647514510377102\n",
      "test loss is 0.000736790098917689\n",
      "Batch: 6100,train loss is: 0.0003581122663814079\n",
      "test loss is 0.0009993400922585194\n",
      "Batch: 6200,train loss is: 0.0005452856655198681\n",
      "test loss is 0.0005684675714066342\n",
      "Batch: 6300,train loss is: 0.0014880711110209232\n",
      "test loss is 0.00128458125798632\n",
      "Batch: 6400,train loss is: 0.00146253026811057\n",
      "test loss is 0.003033586783688451\n",
      "Batch: 6500,train loss is: 0.0007208694114786521\n",
      "test loss is 0.0011264798990520924\n",
      "Batch: 6600,train loss is: 0.001594966532602273\n",
      "test loss is 0.0005830013008349313\n",
      "Batch: 6700,train loss is: 0.0005522168343054989\n",
      "test loss is 0.001125201413092303\n",
      "Batch: 6800,train loss is: 0.0005861504917357449\n",
      "test loss is 0.0009913129292828651\n",
      "Batch: 6900,train loss is: 0.0037982698793182263\n",
      "test loss is 0.001121086741522727\n",
      "Batch: 7000,train loss is: 0.0004464688423875903\n",
      "test loss is 0.0006436037604906461\n",
      "Batch: 7100,train loss is: 0.0010613635023014726\n",
      "test loss is 0.0015091157607498309\n",
      "Batch: 7200,train loss is: 0.0005845701448823916\n",
      "test loss is 0.0011124335891621643\n",
      "Batch: 7300,train loss is: 0.0007054258440768495\n",
      "test loss is 0.0009475498785323039\n",
      "Batch: 7400,train loss is: 0.0002427309710978819\n",
      "test loss is 0.0014496278450484585\n",
      "Batch: 7500,train loss is: 0.0009327726674079054\n",
      "test loss is 0.000594191502142917\n",
      "Batch: 7600,train loss is: 0.0007481522541611896\n",
      "test loss is 0.0006966387802805838\n",
      "Batch: 7700,train loss is: 0.0011029848698088885\n",
      "test loss is 0.000707562276465788\n",
      "Batch: 7800,train loss is: 0.000817111463473539\n",
      "test loss is 0.0008351099820121007\n",
      "Batch: 7900,train loss is: 0.000787354841692571\n",
      "test loss is 0.0007861017868445559\n",
      "Batch: 8000,train loss is: 0.0002363968094611389\n",
      "test loss is 0.0007357097639945374\n",
      "Batch: 8100,train loss is: 0.002186404799275623\n",
      "test loss is 0.0008421287362995629\n",
      "Batch: 8200,train loss is: 0.001972172009510385\n",
      "test loss is 0.000984184644129091\n",
      "Batch: 8300,train loss is: 0.0009438667882190401\n",
      "test loss is 0.0013315086502389275\n",
      "Batch: 8400,train loss is: 0.0033212915470088957\n",
      "test loss is 0.0020942218531089176\n",
      "Batch: 8500,train loss is: 0.0017257107193425927\n",
      "test loss is 0.001250504369881703\n",
      "Batch: 8600,train loss is: 0.0014720619919181108\n",
      "test loss is 0.0012615641333334096\n",
      "Batch: 8700,train loss is: 0.00028235500850985615\n",
      "test loss is 0.0011363400479772053\n",
      "Batch: 8800,train loss is: 0.0008304778041740581\n",
      "test loss is 0.0008247766025752211\n",
      "Batch: 8900,train loss is: 0.0006115276380258126\n",
      "test loss is 0.0008641271756197499\n",
      "Batch: 9000,train loss is: 0.0018488894012086547\n",
      "test loss is 0.0016302702441693664\n",
      "Batch: 9100,train loss is: 0.00044552422597870074\n",
      "test loss is 0.0006973324265020548\n",
      "Batch: 9200,train loss is: 0.0007302300108331983\n",
      "test loss is 0.0007441308398835061\n",
      "Batch: 9300,train loss is: 0.0014763349797252917\n",
      "test loss is 0.0008359597803518118\n",
      "Batch: 9400,train loss is: 0.000635234352150763\n",
      "test loss is 0.000585454759593704\n",
      "Batch: 9500,train loss is: 0.0002741879302721831\n",
      "test loss is 0.0012659659823570783\n",
      "Batch: 9600,train loss is: 0.0012678573745040418\n",
      "test loss is 0.0009975664059480647\n",
      "Batch: 9700,train loss is: 0.0012121326281575818\n",
      "test loss is 0.001067133445464781\n",
      "Batch: 9800,train loss is: 0.00021656149254981055\n",
      "test loss is 0.000709261393990368\n",
      "Batch: 9900,train loss is: 0.000989938773964965\n",
      "test loss is 0.0022185151079587165\n",
      "Batch: 10000,train loss is: 0.001301356609681463\n",
      "test loss is 0.0017796328721659544\n",
      "Batch: 10100,train loss is: 0.0007981051018118185\n",
      "test loss is 0.0008422995799545289\n",
      "Batch: 10200,train loss is: 0.0009197878043179436\n",
      "test loss is 0.00056596706622257\n",
      "Batch: 10300,train loss is: 0.0005918192770689358\n",
      "test loss is 0.0009142869788730904\n",
      "Batch: 10400,train loss is: 0.00025024454869953616\n",
      "test loss is 0.0008975745995719316\n",
      "Batch: 10500,train loss is: 0.0016142580108969718\n",
      "test loss is 0.0010916881047447692\n",
      "Batch: 10600,train loss is: 0.0005518374689842207\n",
      "test loss is 0.0010010365800563676\n",
      "Batch: 10700,train loss is: 0.0002781200567553371\n",
      "test loss is 0.0007500196809444921\n",
      "Batch: 10800,train loss is: 0.0007442217823774861\n",
      "test loss is 0.006905008266896051\n",
      "Batch: 10900,train loss is: 0.0008365407954152294\n",
      "test loss is 0.0007043202640922446\n",
      "Batch: 11000,train loss is: 0.0007700204671476164\n",
      "test loss is 0.0008715708947870718\n",
      "Batch: 11100,train loss is: 0.001252959343544999\n",
      "test loss is 0.0014509369708575535\n",
      "Batch: 11200,train loss is: 0.0005652809257703831\n",
      "test loss is 0.0012147303524448808\n",
      "Batch: 11300,train loss is: 0.005604643525087315\n",
      "test loss is 0.005380406035957835\n",
      "Batch: 11400,train loss is: 0.004125669530330445\n",
      "test loss is 0.0020309392520006173\n",
      "Batch: 11500,train loss is: 0.001959456585735504\n",
      "test loss is 0.0009845348525365251\n",
      "Batch: 11600,train loss is: 0.004368947621807143\n",
      "test loss is 0.0012513922757168261\n",
      "Batch: 11700,train loss is: 0.00018589608961224442\n",
      "test loss is 0.0006676514737834342\n",
      "Batch: 11800,train loss is: 0.000565654085934086\n",
      "test loss is 0.000676186784987978\n",
      "Batch: 11900,train loss is: 0.0007930509959531345\n",
      "test loss is 0.0007162591207627298\n",
      "Batch: 12000,train loss is: 0.00047138140639890226\n",
      "test loss is 0.000574241129726056\n",
      "Batch: 12100,train loss is: 0.006354785088307368\n",
      "test loss is 0.002635934892266833\n",
      "Batch: 12200,train loss is: 0.0007996767573100876\n",
      "test loss is 0.0015406932228303504\n",
      "Batch: 12300,train loss is: 0.0002792514426737453\n",
      "test loss is 0.0010570617208809298\n",
      "Batch: 12400,train loss is: 0.0017317076808531018\n",
      "test loss is 0.001095794051850837\n",
      "Batch: 12500,train loss is: 0.0013323988713392861\n",
      "test loss is 0.0028913654700692035\n",
      "Batch: 12600,train loss is: 0.00017866041504082834\n",
      "test loss is 0.0006242760349885461\n",
      "Batch: 12700,train loss is: 0.0005548012773390072\n",
      "test loss is 0.0006868848466744015\n",
      "Batch: 12800,train loss is: 0.00022707955564718943\n",
      "test loss is 0.0007551827198309708\n",
      "Batch: 12900,train loss is: 0.0008353572893650293\n",
      "test loss is 0.0015635273605743253\n",
      "Batch: 13000,train loss is: 0.0002442797500799034\n",
      "test loss is 0.0006313516774269377\n",
      "Batch: 13100,train loss is: 0.0008134872130301489\n",
      "test loss is 0.0008974799014371418\n",
      "Batch: 13200,train loss is: 0.0007406935221368044\n",
      "test loss is 0.0009757740044971511\n",
      "Batch: 13300,train loss is: 0.0007632879798104951\n",
      "test loss is 0.002380575597012231\n",
      "Batch: 13400,train loss is: 0.0008741768263152175\n",
      "test loss is 0.003199902425395424\n",
      "Batch: 13500,train loss is: 0.0007969867297802954\n",
      "test loss is 0.0013266531664483199\n",
      "Batch: 13600,train loss is: 0.005408770910439833\n",
      "test loss is 0.007776432264970877\n",
      "Batch: 13700,train loss is: 0.000709506895306822\n",
      "test loss is 0.000891207705056826\n",
      "Batch: 13800,train loss is: 0.001880324810051997\n",
      "test loss is 0.00102361269281682\n",
      "Batch: 13900,train loss is: 0.0003111998747300709\n",
      "test loss is 0.0009387941617932075\n",
      "Batch: 14000,train loss is: 0.0003068285070744806\n",
      "test loss is 0.0014698146758241543\n",
      "Batch: 14100,train loss is: 0.0003886215613115411\n",
      "test loss is 0.0007482027537945676\n",
      "Batch: 14200,train loss is: 0.0002534543186728122\n",
      "test loss is 0.0006401235945641816\n",
      "Batch: 14300,train loss is: 0.0005820288202939263\n",
      "test loss is 0.0008820687455967496\n",
      "Batch: 14400,train loss is: 0.000664750460063367\n",
      "test loss is 0.000634725394575609\n",
      "Batch: 14500,train loss is: 0.0004532257372452481\n",
      "test loss is 0.0004740692455675029\n",
      "Batch: 14600,train loss is: 0.0010145856073127159\n",
      "test loss is 0.0013430274563544817\n",
      "Batch: 14700,train loss is: 0.0017182368950681673\n",
      "test loss is 0.0011930702062412815\n",
      "Batch: 14800,train loss is: 0.0007274966032140382\n",
      "test loss is 0.0013339849820732932\n",
      "Batch: 14900,train loss is: 0.0004730751977341346\n",
      "test loss is 0.0005900453309980206\n",
      "Batch: 15000,train loss is: 0.00043415695382069945\n",
      "test loss is 0.0007612382796436001\n",
      "Batch: 15100,train loss is: 0.000391205953005519\n",
      "test loss is 0.0005856231310580693\n",
      "Batch: 15200,train loss is: 0.0005773151424432004\n",
      "test loss is 0.0008374197516099163\n",
      "Batch: 15300,train loss is: 0.0012141955401520899\n",
      "test loss is 0.0005747840802540022\n",
      "Batch: 15400,train loss is: 0.0005074614343057604\n",
      "test loss is 0.0008869068195660185\n",
      "Batch: 15500,train loss is: 0.0006401058491319627\n",
      "test loss is 0.0007432658285127288\n",
      "Batch: 15600,train loss is: 0.005844409348615473\n",
      "test loss is 0.001687439821471341\n",
      "Batch: 15700,train loss is: 0.0005118006816008686\n",
      "test loss is 0.0020380840785023213\n",
      "Batch: 15800,train loss is: 0.00041635813319379955\n",
      "test loss is 0.001407966603856394\n",
      "Batch: 15900,train loss is: 0.0021825254661679166\n",
      "test loss is 0.001231377258317157\n",
      "Batch: 16000,train loss is: 0.0005125355775884632\n",
      "test loss is 0.0005460474723905694\n",
      "Batch: 16100,train loss is: 0.001974325376965674\n",
      "test loss is 0.0007768009607519917\n",
      "Batch: 16200,train loss is: 0.000509853615813106\n",
      "test loss is 0.0008233609522475523\n",
      "Batch: 16300,train loss is: 0.0003671862888360593\n",
      "test loss is 0.000550632011588102\n",
      "Batch: 16400,train loss is: 0.0009346615525572734\n",
      "test loss is 0.0008606770037960307\n",
      "Batch: 16500,train loss is: 0.000994390405443649\n",
      "test loss is 0.0007073093392422976\n",
      "Batch: 16600,train loss is: 0.0002410112231871664\n",
      "test loss is 0.0007739010847158792\n",
      "Batch: 16700,train loss is: 0.001633276028248888\n",
      "test loss is 0.0007967594163117735\n",
      "Batch: 16800,train loss is: 0.00035769181786115645\n",
      "test loss is 0.0007927464905364107\n",
      "Batch: 16900,train loss is: 0.0025145224851023085\n",
      "test loss is 0.0014278712396170996\n",
      "Batch: 17000,train loss is: 0.0006115704985873539\n",
      "test loss is 0.0006621937978947986\n",
      "Batch: 17100,train loss is: 0.0013690122354657855\n",
      "test loss is 0.0014454758301665615\n",
      "Batch: 17200,train loss is: 0.0003557931152083838\n",
      "test loss is 0.0007334433143167395\n",
      "Batch: 17300,train loss is: 0.0007961733416035571\n",
      "test loss is 0.0010634669183055476\n",
      "Batch: 17400,train loss is: 0.0007167105364710701\n",
      "test loss is 0.0006944955698982505\n",
      "Batch: 17500,train loss is: 0.0006484547617923947\n",
      "test loss is 0.0007084835996729593\n",
      "Batch: 17600,train loss is: 0.0016312001970010448\n",
      "test loss is 0.001877925121195144\n",
      "Batch: 17700,train loss is: 0.000396589002363709\n",
      "test loss is 0.0014674403949996979\n",
      "Batch: 17800,train loss is: 0.00186855766981278\n",
      "test loss is 0.002038446113257255\n",
      "Batch: 17900,train loss is: 0.0008710391305660765\n",
      "test loss is 0.0007089051095288516\n",
      "Batch: 18000,train loss is: 0.0011269596433435168\n",
      "test loss is 0.0007104158020825336\n",
      "Batch: 18100,train loss is: 0.0007889742681307872\n",
      "test loss is 0.0009200442646723683\n",
      "Batch: 18200,train loss is: 0.0003045756118539119\n",
      "test loss is 0.0011181390579069305\n",
      "Batch: 18300,train loss is: 0.002118130044729658\n",
      "test loss is 0.0006327463423695507\n",
      "Batch: 18400,train loss is: 0.0004974025314997805\n",
      "test loss is 0.0007657991660434164\n",
      "Batch: 18500,train loss is: 0.0011797056598702867\n",
      "test loss is 0.0008390657579709064\n",
      "Batch: 18600,train loss is: 0.0003024141716247477\n",
      "test loss is 0.000711217594578427\n",
      "Batch: 18700,train loss is: 0.0002046950252865584\n",
      "test loss is 0.0006682244609192163\n",
      "Batch: 18800,train loss is: 0.0009715642029433274\n",
      "test loss is 0.001254050569366586\n",
      "Batch: 18900,train loss is: 0.0006496399286156501\n",
      "test loss is 0.0011842439999225896\n",
      "Batch: 19000,train loss is: 0.00046289550083660115\n",
      "test loss is 0.0006798308132980873\n",
      "Batch: 19100,train loss is: 0.00032198996637328875\n",
      "test loss is 0.000760162102671357\n",
      "Batch: 19200,train loss is: 0.0007450528402625126\n",
      "test loss is 0.0008464572539708813\n",
      "Batch: 19300,train loss is: 0.0007127250111141104\n",
      "test loss is 0.0008668899018034315\n",
      "Batch: 19400,train loss is: 0.0003920587063333815\n",
      "test loss is 0.0012179565936670017\n",
      "Batch: 19500,train loss is: 0.00032287816429541513\n",
      "test loss is 0.0008141207617615026\n",
      "Batch: 19600,train loss is: 0.0004286145589570047\n",
      "test loss is 0.0009800073368910265\n",
      "Batch: 19700,train loss is: 0.001506780673903956\n",
      "test loss is 0.0006971367490541298\n",
      "Batch: 19800,train loss is: 0.0003836560012326333\n",
      "test loss is 0.0012811201365854979\n",
      "Batch: 19900,train loss is: 0.000734352148099225\n",
      "test loss is 0.0008524413336443527\n",
      "Batch: 20000,train loss is: 0.003951809884655769\n",
      "test loss is 0.00196883782196937\n",
      "Batch: 20100,train loss is: 0.00104590870960065\n",
      "test loss is 0.0030229675911458208\n",
      "Batch: 20200,train loss is: 0.0034688687685289236\n",
      "test loss is 0.0017799025501992617\n",
      "Batch: 20300,train loss is: 0.00022917528432615819\n",
      "test loss is 0.0006874983481972584\n",
      "Batch: 20400,train loss is: 0.0006156172658234199\n",
      "test loss is 0.0020918967940983905\n",
      "Batch: 20500,train loss is: 0.0016296769312666819\n",
      "test loss is 0.0022243160099398463\n",
      "Batch: 20600,train loss is: 0.0023397869614501152\n",
      "test loss is 0.0012310343621965178\n",
      "Batch: 20700,train loss is: 0.00028972401603704793\n",
      "test loss is 0.0005971243853410216\n",
      "Batch: 20800,train loss is: 0.0011926833895507351\n",
      "test loss is 0.0005819882342065974\n",
      "Batch: 20900,train loss is: 0.00042278323840914776\n",
      "test loss is 0.0006371557517735986\n",
      "Batch: 21000,train loss is: 0.0018439180519073087\n",
      "test loss is 0.0010295323660268\n",
      "Batch: 21100,train loss is: 0.0008011036124743721\n",
      "test loss is 0.0009807118995159788\n",
      "Batch: 21200,train loss is: 0.0007620427072270597\n",
      "test loss is 0.0010051334803276384\n",
      "Batch: 21300,train loss is: 0.0007007624171601935\n",
      "test loss is 0.0010708188313745307\n",
      "Batch: 21400,train loss is: 0.0005283863050705956\n",
      "test loss is 0.0006713729316369383\n",
      "Batch: 21500,train loss is: 0.00032769581695957125\n",
      "test loss is 0.0010572595732412718\n",
      "Batch: 21600,train loss is: 0.0005657827886677521\n",
      "test loss is 0.0009185094486068041\n",
      "Batch: 21700,train loss is: 0.0004901672532823507\n",
      "test loss is 0.0007059289720445691\n",
      "Batch: 21800,train loss is: 0.0005146633763166296\n",
      "test loss is 0.000528185772106509\n",
      "Batch: 21900,train loss is: 0.00034457575584076315\n",
      "test loss is 0.0007431313180184442\n",
      "Batch: 22000,train loss is: 0.0023104333379162286\n",
      "test loss is 0.0022693944711795182\n",
      "Batch: 22100,train loss is: 0.0004136457324701066\n",
      "test loss is 0.0009047494580863667\n",
      "Batch: 22200,train loss is: 0.006520770364518829\n",
      "test loss is 0.001791464004186621\n",
      "Batch: 22300,train loss is: 0.003559043342904565\n",
      "test loss is 0.007032829585936631\n",
      "Batch: 22400,train loss is: 0.002502164487165331\n",
      "test loss is 0.0014581873604120655\n",
      "Batch: 22500,train loss is: 0.000704123823467027\n",
      "test loss is 0.0008021905236578762\n",
      "Batch: 22600,train loss is: 0.0014728378922162623\n",
      "test loss is 0.0016445802475700006\n",
      "Batch: 22700,train loss is: 0.0008540310561154039\n",
      "test loss is 0.001671853951874902\n",
      "Batch: 22800,train loss is: 0.0004640926480590522\n",
      "test loss is 0.0005781017250479104\n",
      "Batch: 22900,train loss is: 0.00037255052559279875\n",
      "test loss is 0.0010081297353370141\n",
      "Batch: 23000,train loss is: 0.0009699134879075455\n",
      "test loss is 0.0004967485924083445\n",
      "Batch: 23100,train loss is: 0.00020328345264909424\n",
      "test loss is 0.000473931597083088\n",
      "Batch: 23200,train loss is: 0.0003370570488221442\n",
      "test loss is 0.0006451367455559241\n",
      "Batch: 23300,train loss is: 0.0009560501391016336\n",
      "test loss is 0.001067366780061552\n",
      "Batch: 23400,train loss is: 0.0006921463772192778\n",
      "test loss is 0.000851931875507004\n",
      "Batch: 23500,train loss is: 0.0013604066028571544\n",
      "test loss is 0.000684194679878143\n",
      "Batch: 23600,train loss is: 0.0005354354486335571\n",
      "test loss is 0.0005848868475753317\n",
      "Batch: 23700,train loss is: 0.0007766399500048444\n",
      "test loss is 0.0012723223454643403\n",
      "Batch: 23800,train loss is: 0.0009666372798559494\n",
      "test loss is 0.0007407752336340999\n",
      "Batch: 23900,train loss is: 0.0010262006960734809\n",
      "test loss is 0.0014387150333719989\n",
      "Batch: 24000,train loss is: 0.0015651651958849245\n",
      "test loss is 0.0027357926317352066\n",
      "Batch: 24100,train loss is: 0.0009118110891903462\n",
      "test loss is 0.0010149374759018375\n",
      "Batch: 24200,train loss is: 0.0017646233356126701\n",
      "test loss is 0.0006423816705426126\n",
      "Batch: 24300,train loss is: 0.001923231873513077\n",
      "test loss is 0.0009085530031518499\n",
      "Batch: 24400,train loss is: 0.0005959864491544483\n",
      "test loss is 0.0006654670126944079\n",
      "Batch: 24500,train loss is: 0.002289343943292317\n",
      "test loss is 0.0008887661652113141\n",
      "Batch: 24600,train loss is: 0.0017930794889396261\n",
      "test loss is 0.0008014568585420276\n",
      "Batch: 24700,train loss is: 0.0012030800434230451\n",
      "test loss is 0.0014425921823302355\n",
      "Batch: 24800,train loss is: 0.001363401558681945\n",
      "test loss is 0.0006202126285093195\n",
      "Batch: 24900,train loss is: 0.00045700281517602556\n",
      "test loss is 0.0007437623021278591\n",
      "Batch: 25000,train loss is: 0.00043531975123453756\n",
      "test loss is 0.0005290009600410766\n",
      "Batch: 25100,train loss is: 0.0006241207913568021\n",
      "test loss is 0.00090617003379759\n",
      "Batch: 25200,train loss is: 0.0008946125545288139\n",
      "test loss is 0.0013864488087814953\n",
      "Batch: 25300,train loss is: 0.0014667202373552627\n",
      "test loss is 0.0009961926719365512\n",
      "Batch: 25400,train loss is: 0.0011239241987748574\n",
      "test loss is 0.0011331450108232896\n",
      "Batch: 25500,train loss is: 0.00041959752444005374\n",
      "test loss is 0.0006307673117588476\n",
      "Batch: 25600,train loss is: 0.0010283101532024904\n",
      "test loss is 0.0005903526814444681\n",
      "Batch: 25700,train loss is: 0.0019161162607239316\n",
      "test loss is 0.0006069455716099054\n",
      "Batch: 25800,train loss is: 0.0016874173247757981\n",
      "test loss is 0.0011624882587336837\n",
      "Batch: 25900,train loss is: 0.0002750968859867282\n",
      "test loss is 0.0007255639374545474\n",
      "Batch: 26000,train loss is: 0.0023012114581756646\n",
      "test loss is 0.0012249541484509207\n",
      "Batch: 26100,train loss is: 0.0005726466534586224\n",
      "test loss is 0.0018382735733412042\n",
      "Batch: 26200,train loss is: 0.0008390815520212786\n",
      "test loss is 0.0012465577653778476\n",
      "Batch: 26300,train loss is: 0.0004581099872150138\n",
      "test loss is 0.0011427414292360999\n",
      "Batch: 26400,train loss is: 0.0004812589096251915\n",
      "test loss is 0.0007163522678725537\n",
      "Batch: 26500,train loss is: 0.0004234952116191643\n",
      "test loss is 0.0007700630723746794\n",
      "Batch: 26600,train loss is: 0.0008848467925020032\n",
      "test loss is 0.0005691619729206039\n",
      "Batch: 26700,train loss is: 0.0004973982452087888\n",
      "test loss is 0.000561193774798194\n",
      "Batch: 26800,train loss is: 0.0006782647705871597\n",
      "test loss is 0.0006047293514188603\n",
      "Batch: 26900,train loss is: 0.0007681101810343475\n",
      "test loss is 0.0006979499343083938\n",
      "Batch: 27000,train loss is: 0.0011955202681183295\n",
      "test loss is 0.000701718729917024\n",
      "Batch: 27100,train loss is: 0.0010143922431447695\n",
      "test loss is 0.0012166573436241184\n",
      "Batch: 27200,train loss is: 0.0003279059081409219\n",
      "test loss is 0.0005161437269428846\n",
      "Batch: 27300,train loss is: 0.0020622881453851317\n",
      "test loss is 0.002756538948331764\n",
      "Batch: 27400,train loss is: 0.0005688545853792474\n",
      "test loss is 0.0014839950645542832\n",
      "Batch: 27500,train loss is: 0.00046436498565533625\n",
      "test loss is 0.0011381310898831007\n",
      "Batch: 27600,train loss is: 0.0009009587346793603\n",
      "test loss is 0.0011514903927055348\n",
      "Batch: 27700,train loss is: 0.000477577517609916\n",
      "test loss is 0.0007736701992044308\n",
      "Batch: 27800,train loss is: 0.0009226432738803785\n",
      "test loss is 0.0015903650536718639\n",
      "Batch: 27900,train loss is: 0.0007272822895969539\n",
      "test loss is 0.0007863945394831456\n",
      "Batch: 28000,train loss is: 0.0005321968396228351\n",
      "test loss is 0.0007054091337686816\n",
      "Batch: 28100,train loss is: 0.0013645081529455177\n",
      "test loss is 0.0007499510588144421\n",
      "Batch: 28200,train loss is: 0.0003050780221410913\n",
      "test loss is 0.0009721771313527891\n",
      "Batch: 28300,train loss is: 0.0007948559409110961\n",
      "test loss is 0.0005715723417769131\n",
      "Batch: 28400,train loss is: 0.00048247317825839684\n",
      "test loss is 0.0008719024135669654\n",
      "Batch: 28500,train loss is: 0.001507584989751263\n",
      "test loss is 0.0008818451922916902\n",
      "Batch: 28600,train loss is: 0.0018962397234009488\n",
      "test loss is 0.0016550602052275171\n",
      "Batch: 28700,train loss is: 0.0008160411868687392\n",
      "test loss is 0.0017962071281453975\n",
      "Batch: 28800,train loss is: 0.0005805275202612473\n",
      "test loss is 0.0007421311884806997\n",
      "Batch: 28900,train loss is: 0.0009903204088102708\n",
      "test loss is 0.0012036371511360525\n",
      "Batch: 29000,train loss is: 0.0011259143854824944\n",
      "test loss is 0.0005594843602534252\n",
      "Batch: 29100,train loss is: 0.0002902436582604674\n",
      "test loss is 0.000860161794460955\n",
      "Batch: 29200,train loss is: 0.0009891835415585838\n",
      "test loss is 0.002000594869513314\n",
      "Batch: 29300,train loss is: 0.000890069502487696\n",
      "test loss is 0.00099502322977436\n",
      "Batch: 29400,train loss is: 0.0007520665974276249\n",
      "test loss is 0.0011915733546302655\n",
      "Batch: 29500,train loss is: 0.00025625405394277687\n",
      "test loss is 0.0007505976000421124\n",
      "Batch: 29600,train loss is: 0.0011226565453530382\n",
      "test loss is 0.000652083805539466\n",
      "Batch: 29700,train loss is: 0.0010332309059901596\n",
      "test loss is 0.0012514145810707505\n",
      "Batch: 29800,train loss is: 0.0014441053630963295\n",
      "test loss is 0.0005110131555326442\n",
      "Batch: 29900,train loss is: 0.001614797188437496\n",
      "test loss is 0.0005682436038063035\n",
      "Batch: 30000,train loss is: 0.0004806805246956393\n",
      "test loss is 0.0005769520666133324\n",
      "Batch: 30100,train loss is: 0.000773413495164506\n",
      "test loss is 0.002910814791658991\n",
      "Batch: 30200,train loss is: 0.0005827420500069611\n",
      "test loss is 0.001069199359080899\n",
      "Batch: 30300,train loss is: 0.0013963708940614476\n",
      "test loss is 0.0024201421791523086\n",
      "Batch: 30400,train loss is: 0.006192431147951498\n",
      "test loss is 0.008424641368261218\n",
      "Batch: 30500,train loss is: 0.0007592644942438775\n",
      "test loss is 0.0013363107231621567\n",
      "Batch: 30600,train loss is: 0.0004723154067398248\n",
      "test loss is 0.000564212755890015\n",
      "Batch: 30700,train loss is: 0.00026351091069063223\n",
      "test loss is 0.0016046844500839598\n",
      "Batch: 30800,train loss is: 0.0009379833208391529\n",
      "test loss is 0.0009382547675192002\n",
      "Batch: 30900,train loss is: 0.00023562984251531153\n",
      "test loss is 0.0005947901528694653\n",
      "Batch: 31000,train loss is: 0.000267445309378111\n",
      "test loss is 0.0004748804865215732\n",
      "Batch: 31100,train loss is: 0.0006138672599605666\n",
      "test loss is 0.0010637923587197753\n",
      "Batch: 31200,train loss is: 0.0003868832025769153\n",
      "test loss is 0.0004675872424270331\n",
      "Batch: 31300,train loss is: 0.0007510470708980359\n",
      "test loss is 0.0006202824729670998\n",
      "Batch: 31400,train loss is: 0.0005817295643523945\n",
      "test loss is 0.0018424441718888206\n",
      "Batch: 31500,train loss is: 0.0011008732597495771\n",
      "test loss is 0.0014338362905603728\n",
      "Batch: 31600,train loss is: 0.0009871729101252195\n",
      "test loss is 0.0007072981033425892\n",
      "Batch: 31700,train loss is: 0.00038021796790517774\n",
      "test loss is 0.000551866720095908\n",
      "Batch: 31800,train loss is: 0.0007972368359495679\n",
      "test loss is 0.0004922815546768299\n",
      "Batch: 31900,train loss is: 0.0008213472706293817\n",
      "test loss is 0.0004987110411898979\n",
      "Batch: 32000,train loss is: 0.0025723189121062027\n",
      "test loss is 0.00046780408002036734\n",
      "Batch: 32100,train loss is: 0.0001778412838542489\n",
      "test loss is 0.0007193121555005223\n",
      "Batch: 32200,train loss is: 0.00037178484418668584\n",
      "test loss is 0.00046331375468549907\n",
      "Batch: 32300,train loss is: 0.0012955924153451937\n",
      "test loss is 0.001049804857719927\n",
      "Batch: 32400,train loss is: 0.0011347505933740636\n",
      "test loss is 0.001600196883322148\n",
      "Batch: 32500,train loss is: 0.0007723629256881053\n",
      "test loss is 0.0006461265679389572\n",
      "Batch: 32600,train loss is: 0.00027412360824463183\n",
      "test loss is 0.0005374161118551011\n",
      "Batch: 32700,train loss is: 0.0004873855664057435\n",
      "test loss is 0.000955097636386685\n",
      "Batch: 32800,train loss is: 0.0004871891824245192\n",
      "test loss is 0.0008851436213951202\n",
      "Batch: 32900,train loss is: 0.0014901639547176867\n",
      "test loss is 0.000775293938171925\n",
      "Batch: 33000,train loss is: 0.0003239299140675554\n",
      "test loss is 0.0008009166038022555\n",
      "Batch: 33100,train loss is: 0.0013316982320271404\n",
      "test loss is 0.0012482572290414247\n",
      "Batch: 33200,train loss is: 0.0003287737094344514\n",
      "test loss is 0.0006901637949172156\n",
      "Batch: 33300,train loss is: 0.00047195644479335763\n",
      "test loss is 0.0005897972163348169\n",
      "Batch: 33400,train loss is: 0.0012399594723358474\n",
      "test loss is 0.0012041111386692175\n",
      "Batch: 33500,train loss is: 0.0016491065782295753\n",
      "test loss is 0.0017191999614061895\n",
      "Batch: 33600,train loss is: 0.00040877673491199207\n",
      "test loss is 0.0006716993672470817\n",
      "Batch: 33700,train loss is: 0.0006952216591057276\n",
      "test loss is 0.0006487230064006348\n",
      "Batch: 33800,train loss is: 0.0006225639990950159\n",
      "test loss is 0.0005752245234771627\n",
      "Batch: 33900,train loss is: 0.0010177178431886311\n",
      "test loss is 0.0004723122973290588\n",
      "-----------------------Epoch: 1----------------------------------\n",
      "Batch: 0,train loss is: 0.0005493696391708013\n",
      "test loss is 0.000645174445685861\n",
      "Batch: 100,train loss is: 0.000818814789924567\n",
      "test loss is 0.0008052071788407072\n",
      "Batch: 200,train loss is: 0.00040760206747100997\n",
      "test loss is 0.00060975199951346\n",
      "Batch: 300,train loss is: 0.0006657442887529569\n",
      "test loss is 0.0006375939312620684\n",
      "Batch: 400,train loss is: 0.000812245823505545\n",
      "test loss is 0.0013846935310416022\n",
      "Batch: 500,train loss is: 0.0004279033771893798\n",
      "test loss is 0.0016356326588219938\n",
      "Batch: 600,train loss is: 0.00016793956762542472\n",
      "test loss is 0.0017538981659304571\n",
      "Batch: 700,train loss is: 0.00033294930673852843\n",
      "test loss is 0.0005111516607506219\n",
      "Batch: 800,train loss is: 0.00022488600328127753\n",
      "test loss is 0.000720145053830696\n",
      "Batch: 900,train loss is: 0.0005329159922743045\n",
      "test loss is 0.0007350070739222161\n",
      "Batch: 1000,train loss is: 0.004336542117231607\n",
      "test loss is 0.0010709342936650454\n",
      "Batch: 1100,train loss is: 0.0011692692892343948\n",
      "test loss is 0.0005544423131052187\n",
      "Batch: 1200,train loss is: 0.0007985352840617226\n",
      "test loss is 0.0010600806378086914\n",
      "Batch: 1300,train loss is: 0.0003779204009601333\n",
      "test loss is 0.001931964396845273\n",
      "Batch: 1400,train loss is: 0.00020623204928472248\n",
      "test loss is 0.0006433866441520579\n",
      "Batch: 1500,train loss is: 0.0011313401980680329\n",
      "test loss is 0.0026227789111086704\n",
      "Batch: 1600,train loss is: 0.0005630586355564765\n",
      "test loss is 0.0011446710513511192\n",
      "Batch: 1700,train loss is: 0.0011750508177626239\n",
      "test loss is 0.0007279914400833438\n",
      "Batch: 1800,train loss is: 0.0008232160094129051\n",
      "test loss is 0.0007215412535991224\n",
      "Batch: 1900,train loss is: 0.002062723958190595\n",
      "test loss is 0.0031398345628053143\n",
      "Batch: 2000,train loss is: 0.002272782903973502\n",
      "test loss is 0.0041232849961241585\n",
      "Batch: 2100,train loss is: 0.0003736603694589656\n",
      "test loss is 0.0005292369864490042\n",
      "Batch: 2200,train loss is: 0.0009645905048292792\n",
      "test loss is 0.0005076855235202483\n",
      "Batch: 2300,train loss is: 0.00026895154841008993\n",
      "test loss is 0.000495679816862739\n",
      "Batch: 2400,train loss is: 0.00025518307592028126\n",
      "test loss is 0.0005369265766916163\n",
      "Batch: 2500,train loss is: 0.0005282220916597611\n",
      "test loss is 0.001224644280121616\n",
      "Batch: 2600,train loss is: 0.0017810567133741428\n",
      "test loss is 0.0008838475719546991\n",
      "Batch: 2700,train loss is: 0.0004189619900707592\n",
      "test loss is 0.0005067058129058604\n",
      "Batch: 2800,train loss is: 0.0004019904119520119\n",
      "test loss is 0.000977927298017151\n",
      "Batch: 2900,train loss is: 0.0007426007854870351\n",
      "test loss is 0.0008437956134549205\n",
      "Batch: 3000,train loss is: 0.0006050159820300731\n",
      "test loss is 0.0011674737667836995\n",
      "Batch: 3100,train loss is: 0.0001996645039257626\n",
      "test loss is 0.0007106559341758581\n",
      "Batch: 3200,train loss is: 0.000607706656935669\n",
      "test loss is 0.0006799860594394616\n",
      "Batch: 3300,train loss is: 0.0002916467045482732\n",
      "test loss is 0.0005684947328202365\n",
      "Batch: 3400,train loss is: 0.0003435012559105867\n",
      "test loss is 0.0007040661899284806\n",
      "Batch: 3500,train loss is: 0.0008621116201056947\n",
      "test loss is 0.0011772344078981073\n",
      "Batch: 3600,train loss is: 0.00039122560977692934\n",
      "test loss is 0.0015456837758873391\n",
      "Batch: 3700,train loss is: 0.0014115826809334821\n",
      "test loss is 0.0014027304458407181\n",
      "Batch: 3800,train loss is: 0.000542223921543685\n",
      "test loss is 0.0005450325199010375\n",
      "Batch: 3900,train loss is: 0.0008201019492608715\n",
      "test loss is 0.0007975259238538333\n",
      "Batch: 4000,train loss is: 0.0006586495409563079\n",
      "test loss is 0.0009794240603866829\n",
      "Batch: 4100,train loss is: 0.0008511420852094525\n",
      "test loss is 0.0006822559120452953\n",
      "Batch: 4200,train loss is: 0.0011527967508012415\n",
      "test loss is 0.001851786907859594\n",
      "Batch: 4300,train loss is: 0.0006290986114863485\n",
      "test loss is 0.0010862576371005908\n",
      "Batch: 4400,train loss is: 0.000874564879889578\n",
      "test loss is 0.0012838368952181104\n",
      "Batch: 4500,train loss is: 0.0010897578006171822\n",
      "test loss is 0.0006187258596443768\n",
      "Batch: 4600,train loss is: 0.0020016720192877076\n",
      "test loss is 0.001367019159871209\n",
      "Batch: 4700,train loss is: 0.0007506357007429576\n",
      "test loss is 0.0007351892158073385\n",
      "Batch: 4800,train loss is: 0.0012581168675633383\n",
      "test loss is 0.001281810911217868\n",
      "Batch: 4900,train loss is: 0.00040742191429846695\n",
      "test loss is 0.0006518310857517728\n",
      "Batch: 5000,train loss is: 0.000306171242861238\n",
      "test loss is 0.0004988321824268382\n",
      "Batch: 5100,train loss is: 0.00045681772571285975\n",
      "test loss is 0.0006245040602819374\n",
      "Batch: 5200,train loss is: 0.0008318652409058848\n",
      "test loss is 0.000621433053478148\n",
      "Batch: 5300,train loss is: 0.0009896152198883817\n",
      "test loss is 0.0009176102048260381\n",
      "Batch: 5400,train loss is: 0.000809286772513136\n",
      "test loss is 0.0006821431292567334\n",
      "Batch: 5500,train loss is: 0.0014707197723006324\n",
      "test loss is 0.0007985603477247755\n",
      "Batch: 5600,train loss is: 0.0007464957161347599\n",
      "test loss is 0.0005971653125272044\n",
      "Batch: 5700,train loss is: 0.0006256483486755366\n",
      "test loss is 0.000627148554056798\n",
      "Batch: 5800,train loss is: 0.00045542183148734053\n",
      "test loss is 0.0006136228064757446\n",
      "Batch: 5900,train loss is: 0.0014903339114130788\n",
      "test loss is 0.0006770322712206267\n",
      "Batch: 6000,train loss is: 0.001160042323383763\n",
      "test loss is 0.0006312432300039614\n",
      "Batch: 6100,train loss is: 0.0003823833309258336\n",
      "test loss is 0.0008532064123748079\n",
      "Batch: 6200,train loss is: 0.0006869466566587719\n",
      "test loss is 0.0006566108114286584\n",
      "Batch: 6300,train loss is: 0.0008316497765168912\n",
      "test loss is 0.001037685017904758\n",
      "Batch: 6400,train loss is: 0.0010238613750784475\n",
      "test loss is 0.0016595866539374062\n",
      "Batch: 6500,train loss is: 0.00046084975494448436\n",
      "test loss is 0.000516837727935761\n",
      "Batch: 6600,train loss is: 0.0014457872755634738\n",
      "test loss is 0.0010463407981169052\n",
      "Batch: 6700,train loss is: 0.0006658484223722288\n",
      "test loss is 0.0010577246004681178\n",
      "Batch: 6800,train loss is: 0.00043233265230019846\n",
      "test loss is 0.0007036219446451279\n",
      "Batch: 6900,train loss is: 0.005282879177552906\n",
      "test loss is 0.0013599644345819969\n",
      "Batch: 7000,train loss is: 0.0005315094723779448\n",
      "test loss is 0.0004969398969840204\n",
      "Batch: 7100,train loss is: 0.0005729038781202917\n",
      "test loss is 0.001011090027992505\n",
      "Batch: 7200,train loss is: 0.0003537749357059963\n",
      "test loss is 0.0007854818792396993\n",
      "Batch: 7300,train loss is: 0.0007200686036592998\n",
      "test loss is 0.0011018523011698759\n",
      "Batch: 7400,train loss is: 0.00015303172417893145\n",
      "test loss is 0.0010702715245371462\n",
      "Batch: 7500,train loss is: 0.0012425889271233347\n",
      "test loss is 0.00048128660836225623\n",
      "Batch: 7600,train loss is: 0.0008802459404973035\n",
      "test loss is 0.0006955829976947414\n",
      "Batch: 7700,train loss is: 0.0006811003339588003\n",
      "test loss is 0.0008297480888206909\n",
      "Batch: 7800,train loss is: 0.0006681842630697155\n",
      "test loss is 0.0007258174154209633\n",
      "Batch: 7900,train loss is: 0.0007232292185059243\n",
      "test loss is 0.000656597860319209\n",
      "Batch: 8000,train loss is: 0.000667272547897325\n",
      "test loss is 0.0010960872802879464\n",
      "Batch: 8100,train loss is: 0.0026424256089460905\n",
      "test loss is 0.0007237220200033454\n",
      "Batch: 8200,train loss is: 0.0012919223202683286\n",
      "test loss is 0.0008006324867162704\n",
      "Batch: 8300,train loss is: 0.0008599881651357148\n",
      "test loss is 0.0011066471340899853\n",
      "Batch: 8400,train loss is: 0.0010494211072443233\n",
      "test loss is 0.000961167486075228\n",
      "Batch: 8500,train loss is: 0.0010591900987120739\n",
      "test loss is 0.0006996463024384309\n",
      "Batch: 8600,train loss is: 0.0006684228977342784\n",
      "test loss is 0.0009438683331555454\n",
      "Batch: 8700,train loss is: 0.0001689926268461913\n",
      "test loss is 0.0012230090872320466\n",
      "Batch: 8800,train loss is: 0.00043965670227103144\n",
      "test loss is 0.0006519212802764618\n",
      "Batch: 8900,train loss is: 0.0006810556764377397\n",
      "test loss is 0.0010429409850040464\n",
      "Batch: 9000,train loss is: 0.001936916650705195\n",
      "test loss is 0.0016273096637149501\n",
      "Batch: 9100,train loss is: 0.0009288704865055153\n",
      "test loss is 0.0006790609546195844\n",
      "Batch: 9200,train loss is: 0.0009520668453387945\n",
      "test loss is 0.0009163623679664503\n",
      "Batch: 9300,train loss is: 0.0013618718061408418\n",
      "test loss is 0.000756844734760002\n",
      "Batch: 9400,train loss is: 0.0006470951749873922\n",
      "test loss is 0.00044751713803490895\n",
      "Batch: 9500,train loss is: 0.0003253160097866005\n",
      "test loss is 0.0010985587551676386\n",
      "Batch: 9600,train loss is: 0.000881965919511446\n",
      "test loss is 0.0007134969714839282\n",
      "Batch: 9700,train loss is: 0.0010772988923293118\n",
      "test loss is 0.0007406152172626441\n",
      "Batch: 9800,train loss is: 0.0001709844553790381\n",
      "test loss is 0.0006230804667736324\n",
      "Batch: 9900,train loss is: 0.0012965345956135637\n",
      "test loss is 0.0038654018681180107\n",
      "Batch: 10000,train loss is: 0.0014049802714652783\n",
      "test loss is 0.001946273876559428\n",
      "Batch: 10100,train loss is: 0.0006374884900578927\n",
      "test loss is 0.0008014963824583209\n",
      "Batch: 10200,train loss is: 0.001991323010623469\n",
      "test loss is 0.0005310375912769543\n",
      "Batch: 10300,train loss is: 0.0005456213998548896\n",
      "test loss is 0.0006498765698883299\n",
      "Batch: 10400,train loss is: 0.0005590663968275217\n",
      "test loss is 0.0007200174449956501\n",
      "Batch: 10500,train loss is: 0.0010746131427142075\n",
      "test loss is 0.0010260297689387164\n",
      "Batch: 10600,train loss is: 0.0003275366830424179\n",
      "test loss is 0.000805774347669968\n",
      "Batch: 10700,train loss is: 0.000512970935997826\n",
      "test loss is 0.0005436516763325678\n",
      "Batch: 10800,train loss is: 0.001573630436539801\n",
      "test loss is 0.006365713820242603\n",
      "Batch: 10900,train loss is: 0.0006644099648930537\n",
      "test loss is 0.0007065068974422739\n",
      "Batch: 11000,train loss is: 0.0005538566580594976\n",
      "test loss is 0.0007232448988924445\n",
      "Batch: 11100,train loss is: 0.00036571324044087916\n",
      "test loss is 0.0008645038633218878\n",
      "Batch: 11200,train loss is: 0.0010477323752578881\n",
      "test loss is 0.0005169507860282428\n",
      "Batch: 11300,train loss is: 0.0009907394187651337\n",
      "test loss is 0.001019750142490616\n",
      "Batch: 11400,train loss is: 0.00134015389117874\n",
      "test loss is 0.0017372809032350494\n",
      "Batch: 11500,train loss is: 0.00018604982785456914\n",
      "test loss is 0.0007020773957216211\n",
      "Batch: 11600,train loss is: 0.001545671179371102\n",
      "test loss is 0.0017105246866983767\n",
      "Batch: 11700,train loss is: 0.00023000009836872024\n",
      "test loss is 0.0005403381958313118\n",
      "Batch: 11800,train loss is: 0.00037328314854626873\n",
      "test loss is 0.0005011841725592281\n",
      "Batch: 11900,train loss is: 0.000754911289875964\n",
      "test loss is 0.0009683855945598557\n",
      "Batch: 12000,train loss is: 0.0005425283575537068\n",
      "test loss is 0.0013791818902199272\n",
      "Batch: 12100,train loss is: 0.004845900819032877\n",
      "test loss is 0.003004139372179334\n",
      "Batch: 12200,train loss is: 0.0007893912142577256\n",
      "test loss is 0.0007623283356567964\n",
      "Batch: 12300,train loss is: 0.000273784656892872\n",
      "test loss is 0.0012621415367735428\n",
      "Batch: 12400,train loss is: 0.0016604894659413131\n",
      "test loss is 0.0013468378911196575\n",
      "Batch: 12500,train loss is: 0.0010165149058308722\n",
      "test loss is 0.002745393847315624\n",
      "Batch: 12600,train loss is: 0.0002141821935659749\n",
      "test loss is 0.0004831768490985259\n",
      "Batch: 12700,train loss is: 0.00047753139987656074\n",
      "test loss is 0.0004433310344157737\n",
      "Batch: 12800,train loss is: 0.00017233445575449088\n",
      "test loss is 0.0005116868586162486\n",
      "Batch: 12900,train loss is: 0.0006124373410648333\n",
      "test loss is 0.0006131027102122647\n",
      "Batch: 13000,train loss is: 0.00032383691148472096\n",
      "test loss is 0.000491129755378063\n",
      "Batch: 13100,train loss is: 0.0006783354045759877\n",
      "test loss is 0.000545535609482207\n",
      "Batch: 13200,train loss is: 0.0004053046643230638\n",
      "test loss is 0.0006106504444077049\n",
      "Batch: 13300,train loss is: 0.0006532034447119587\n",
      "test loss is 0.0016999718216596813\n",
      "Batch: 13400,train loss is: 0.0010207906642749828\n",
      "test loss is 0.0015227393496349616\n",
      "Batch: 13500,train loss is: 0.0004804330543478126\n",
      "test loss is 0.0006410004512923986\n",
      "Batch: 13600,train loss is: 0.0031554238464523696\n",
      "test loss is 0.003974075452903342\n",
      "Batch: 13700,train loss is: 0.0004149548008429126\n",
      "test loss is 0.0008713537434020354\n",
      "Batch: 13800,train loss is: 0.00041091978119627714\n",
      "test loss is 0.0007512934726662736\n",
      "Batch: 13900,train loss is: 0.0005097697238196414\n",
      "test loss is 0.0010522891040938478\n",
      "Batch: 14000,train loss is: 0.0009473818738638613\n",
      "test loss is 0.0015838465134915512\n",
      "Batch: 14100,train loss is: 0.0005781530189243064\n",
      "test loss is 0.0009664145662844612\n",
      "Batch: 14200,train loss is: 0.00020629166758860622\n",
      "test loss is 0.0005235560299396196\n",
      "Batch: 14300,train loss is: 0.00029397561685454187\n",
      "test loss is 0.0006370999388623571\n",
      "Batch: 14400,train loss is: 0.0003692840229660411\n",
      "test loss is 0.0005592787249848136\n",
      "Batch: 14500,train loss is: 0.0007657108361923275\n",
      "test loss is 0.00046045533888893717\n",
      "Batch: 14600,train loss is: 0.0005872424034138585\n",
      "test loss is 0.0010004596431229817\n",
      "Batch: 14700,train loss is: 0.002122118946643856\n",
      "test loss is 0.0015941856953337202\n",
      "Batch: 14800,train loss is: 0.0008013854675129631\n",
      "test loss is 0.001155119595614599\n",
      "Batch: 14900,train loss is: 0.0004925639304569878\n",
      "test loss is 0.0004848010401676348\n",
      "Batch: 15000,train loss is: 0.0003917313411424834\n",
      "test loss is 0.0006335538265000031\n",
      "Batch: 15100,train loss is: 0.00035544551572594667\n",
      "test loss is 0.0006010698702592688\n",
      "Batch: 15200,train loss is: 0.0008551466222502606\n",
      "test loss is 0.0005617595193360112\n",
      "Batch: 15300,train loss is: 0.00031114556819008074\n",
      "test loss is 0.00048074985483669724\n",
      "Batch: 15400,train loss is: 0.0007147365037054007\n",
      "test loss is 0.0006950506949468854\n",
      "Batch: 15500,train loss is: 0.0004416510941829684\n",
      "test loss is 0.0005449084719150058\n",
      "Batch: 15600,train loss is: 0.004535605407452509\n",
      "test loss is 0.0012377574564453027\n",
      "Batch: 15700,train loss is: 0.0003295355334391621\n",
      "test loss is 0.002190018340070581\n",
      "Batch: 15800,train loss is: 0.0004105838737025668\n",
      "test loss is 0.0008349865144690945\n",
      "Batch: 15900,train loss is: 0.0021185401545402317\n",
      "test loss is 0.0008767532931669606\n",
      "Batch: 16000,train loss is: 0.0005002298600382599\n",
      "test loss is 0.0004823137013896974\n",
      "Batch: 16100,train loss is: 0.0018283967467779133\n",
      "test loss is 0.0005901250715855426\n",
      "Batch: 16200,train loss is: 0.00035205434515866443\n",
      "test loss is 0.0006271008256258279\n",
      "Batch: 16300,train loss is: 0.00026283215390825176\n",
      "test loss is 0.0004532843008207668\n",
      "Batch: 16400,train loss is: 0.0005458274557128887\n",
      "test loss is 0.0006240753669557923\n",
      "Batch: 16500,train loss is: 0.0010026368282864697\n",
      "test loss is 0.0006003161293031998\n",
      "Batch: 16600,train loss is: 0.00022483260178534443\n",
      "test loss is 0.0005246943097273726\n",
      "Batch: 16700,train loss is: 0.0008470500844151411\n",
      "test loss is 0.0005923902723751247\n",
      "Batch: 16800,train loss is: 0.00032769371971213185\n",
      "test loss is 0.0006107368020488124\n",
      "Batch: 16900,train loss is: 0.002829160780273462\n",
      "test loss is 0.0014448701580784744\n",
      "Batch: 17000,train loss is: 0.0004998377533459623\n",
      "test loss is 0.0004262231231589004\n",
      "Batch: 17100,train loss is: 0.0006110407338472308\n",
      "test loss is 0.0008315208183815329\n",
      "Batch: 17200,train loss is: 0.0002810958674502807\n",
      "test loss is 0.0006666329580221466\n",
      "Batch: 17300,train loss is: 0.001236443620320628\n",
      "test loss is 0.001264680054458783\n",
      "Batch: 17400,train loss is: 0.0006137100994303328\n",
      "test loss is 0.0006740658743965156\n",
      "Batch: 17500,train loss is: 0.0007489768724338212\n",
      "test loss is 0.000626631140552989\n",
      "Batch: 17600,train loss is: 0.002229692892090232\n",
      "test loss is 0.0019260348278781628\n",
      "Batch: 17700,train loss is: 0.0003668071007776862\n",
      "test loss is 0.0009719550741886039\n",
      "Batch: 17800,train loss is: 0.0014385160201367062\n",
      "test loss is 0.0014741486888287746\n",
      "Batch: 17900,train loss is: 0.0008338769189345671\n",
      "test loss is 0.00061270419678314\n",
      "Batch: 18000,train loss is: 0.0006170649775923831\n",
      "test loss is 0.0004721727572832239\n",
      "Batch: 18100,train loss is: 0.0008868455015119364\n",
      "test loss is 0.0009021723285555304\n",
      "Batch: 18200,train loss is: 0.000388658466582551\n",
      "test loss is 0.0009458337308993572\n",
      "Batch: 18300,train loss is: 0.0017901495653946241\n",
      "test loss is 0.0006014878831000931\n",
      "Batch: 18400,train loss is: 0.0004839305398106628\n",
      "test loss is 0.000658344154857735\n",
      "Batch: 18500,train loss is: 0.0010658403417840225\n",
      "test loss is 0.0005582151370973087\n",
      "Batch: 18600,train loss is: 0.0002884343828480551\n",
      "test loss is 0.0007600407644617958\n",
      "Batch: 18700,train loss is: 0.000313659934441637\n",
      "test loss is 0.0007175696193237584\n",
      "Batch: 18800,train loss is: 0.0002063770287112526\n",
      "test loss is 0.0007234021265364556\n",
      "Batch: 18900,train loss is: 0.0003577814575825011\n",
      "test loss is 0.00043951844337154896\n",
      "Batch: 19000,train loss is: 0.00019144305288609133\n",
      "test loss is 0.000737201365431356\n",
      "Batch: 19100,train loss is: 0.0002933232318478156\n",
      "test loss is 0.0006294028382828425\n",
      "Batch: 19200,train loss is: 0.00042158207550425263\n",
      "test loss is 0.0005142988423742531\n",
      "Batch: 19300,train loss is: 0.0007132187231905003\n",
      "test loss is 0.0006514553010054934\n",
      "Batch: 19400,train loss is: 0.00020684578231166067\n",
      "test loss is 0.0009599914040448214\n",
      "Batch: 19500,train loss is: 0.00033283523403389575\n",
      "test loss is 0.0008246728564513607\n",
      "Batch: 19600,train loss is: 0.00039595196516570237\n",
      "test loss is 0.0007817851943225406\n",
      "Batch: 19700,train loss is: 0.0010242903535925406\n",
      "test loss is 0.0005563599430814954\n",
      "Batch: 19800,train loss is: 0.00029941734038316437\n",
      "test loss is 0.0007776099082428177\n",
      "Batch: 19900,train loss is: 0.0005262294026683809\n",
      "test loss is 0.0007271907525160271\n",
      "Batch: 20000,train loss is: 0.0005314388617257296\n",
      "test loss is 0.000587401870842866\n",
      "Batch: 20100,train loss is: 0.0009662271628427971\n",
      "test loss is 0.0005585332878207037\n",
      "Batch: 20200,train loss is: 0.00021407194670608706\n",
      "test loss is 0.0004561323219605787\n",
      "Batch: 20300,train loss is: 0.00014082345307187555\n",
      "test loss is 0.0006712667850176094\n",
      "Batch: 20400,train loss is: 0.00676284951356334\n",
      "test loss is 0.0038989944473529115\n",
      "Batch: 20500,train loss is: 0.00045301586074218654\n",
      "test loss is 0.0008474250691951316\n",
      "Batch: 20600,train loss is: 0.004042959583821264\n",
      "test loss is 0.0010576499516894067\n",
      "Batch: 20700,train loss is: 0.00038641143392359565\n",
      "test loss is 0.0005712554940647709\n",
      "Batch: 20800,train loss is: 0.0011945439230315288\n",
      "test loss is 0.0005433161273833475\n",
      "Batch: 20900,train loss is: 0.00019770913157213468\n",
      "test loss is 0.0004360874331541269\n",
      "Batch: 21000,train loss is: 0.0009566581151781861\n",
      "test loss is 0.0005568268288879773\n",
      "Batch: 21100,train loss is: 0.0005384322704139201\n",
      "test loss is 0.0005876023279087589\n",
      "Batch: 21200,train loss is: 0.0008949031659169273\n",
      "test loss is 0.0010452645135301667\n",
      "Batch: 21300,train loss is: 0.0006861503330588596\n",
      "test loss is 0.0006858527814702827\n",
      "Batch: 21400,train loss is: 0.0003239995976012418\n",
      "test loss is 0.0005148498945760102\n",
      "Batch: 21500,train loss is: 0.00029365762078184526\n",
      "test loss is 0.0009045737916035004\n",
      "Batch: 21600,train loss is: 0.0006573254951297976\n",
      "test loss is 0.0006818547695958612\n",
      "Batch: 21700,train loss is: 0.0004249465107861513\n",
      "test loss is 0.0005639812619394134\n",
      "Batch: 21800,train loss is: 0.00042631178348176787\n",
      "test loss is 0.00040522156232167375\n",
      "Batch: 21900,train loss is: 0.00020388838330376484\n",
      "test loss is 0.0006887139663252934\n",
      "Batch: 22000,train loss is: 0.0022960738673160615\n",
      "test loss is 0.002555861524485051\n",
      "Batch: 22100,train loss is: 0.0003810910976329802\n",
      "test loss is 0.0009540655637268732\n",
      "Batch: 22200,train loss is: 0.004656277871416284\n",
      "test loss is 0.0016142818797888895\n",
      "Batch: 22300,train loss is: 0.0026624814747045695\n",
      "test loss is 0.004560230673311198\n",
      "Batch: 22400,train loss is: 0.000991576494602825\n",
      "test loss is 0.0008117697237283952\n",
      "Batch: 22500,train loss is: 0.00041252219406269497\n",
      "test loss is 0.000626318401256987\n",
      "Batch: 22600,train loss is: 0.0012466042065695118\n",
      "test loss is 0.0011667430999598805\n",
      "Batch: 22700,train loss is: 0.0005182358374019432\n",
      "test loss is 0.001024113816792086\n",
      "Batch: 22800,train loss is: 0.0005089978860789851\n",
      "test loss is 0.0004963286676052154\n",
      "Batch: 22900,train loss is: 0.00036111079476686695\n",
      "test loss is 0.0008484559245452208\n",
      "Batch: 23000,train loss is: 0.0006472037329554172\n",
      "test loss is 0.0004226521253615695\n",
      "Batch: 23100,train loss is: 0.00019115446034303945\n",
      "test loss is 0.00045443202969407397\n",
      "Batch: 23200,train loss is: 0.0003132285861419589\n",
      "test loss is 0.0005482250615050136\n",
      "Batch: 23300,train loss is: 0.001022362979572699\n",
      "test loss is 0.0010258235559813735\n",
      "Batch: 23400,train loss is: 0.0010935390216589604\n",
      "test loss is 0.00084625772767203\n",
      "Batch: 23500,train loss is: 0.0012498099732612198\n",
      "test loss is 0.0005385353354835441\n",
      "Batch: 23600,train loss is: 0.0004075549576213796\n",
      "test loss is 0.0005093652708820849\n",
      "Batch: 23700,train loss is: 0.0005676528930304538\n",
      "test loss is 0.0009093521096482875\n",
      "Batch: 23800,train loss is: 0.0007237487786337084\n",
      "test loss is 0.0007744060432595853\n",
      "Batch: 23900,train loss is: 0.0005557924040985343\n",
      "test loss is 0.00236635426436485\n",
      "Batch: 24000,train loss is: 0.0017430717442772485\n",
      "test loss is 0.001908257300546758\n",
      "Batch: 24100,train loss is: 0.0005819800820160225\n",
      "test loss is 0.0009126055335494626\n",
      "Batch: 24200,train loss is: 0.0022461763867191453\n",
      "test loss is 0.0004919254654023173\n",
      "Batch: 24300,train loss is: 0.000491370335793028\n",
      "test loss is 0.0007013183578925587\n",
      "Batch: 24400,train loss is: 0.0012281558856300227\n",
      "test loss is 0.0006967481099444215\n",
      "Batch: 24500,train loss is: 0.0019450796843779497\n",
      "test loss is 0.0008175462191932348\n",
      "Batch: 24600,train loss is: 0.00110787533483331\n",
      "test loss is 0.0006280171731124527\n",
      "Batch: 24700,train loss is: 0.0008339588706359052\n",
      "test loss is 0.0010914460567391406\n",
      "Batch: 24800,train loss is: 0.0006046015070503153\n",
      "test loss is 0.0005228546318560828\n",
      "Batch: 24900,train loss is: 0.0007717806316458085\n",
      "test loss is 0.0006284062140315778\n",
      "Batch: 25000,train loss is: 0.0003336827142375662\n",
      "test loss is 0.00045944465305551514\n",
      "Batch: 25100,train loss is: 0.0004327500592636131\n",
      "test loss is 0.0009982638731947406\n",
      "Batch: 25200,train loss is: 0.00044475078845135856\n",
      "test loss is 0.001027049098267302\n",
      "Batch: 25300,train loss is: 0.0008305470175911252\n",
      "test loss is 0.0007120888687773251\n",
      "Batch: 25400,train loss is: 0.0008241379602804677\n",
      "test loss is 0.0007481449805757544\n",
      "Batch: 25500,train loss is: 0.00028336788305633484\n",
      "test loss is 0.0004642430648346167\n",
      "Batch: 25600,train loss is: 0.0007285615374934967\n",
      "test loss is 0.0004855995303027152\n",
      "Batch: 25700,train loss is: 0.001466750409086912\n",
      "test loss is 0.0005332476078655973\n",
      "Batch: 25800,train loss is: 0.0016148174993860806\n",
      "test loss is 0.0011462045384530483\n",
      "Batch: 25900,train loss is: 0.00019465818687765715\n",
      "test loss is 0.0006286886441473696\n",
      "Batch: 26000,train loss is: 0.0021167885890855716\n",
      "test loss is 0.0012037880966919703\n",
      "Batch: 26100,train loss is: 0.00037238885469526045\n",
      "test loss is 0.00101789564875195\n",
      "Batch: 26200,train loss is: 0.00058710077979925\n",
      "test loss is 0.0009957866427975969\n",
      "Batch: 26300,train loss is: 0.00041432739540087966\n",
      "test loss is 0.0009921591816329477\n",
      "Batch: 26400,train loss is: 0.0003527321347367771\n",
      "test loss is 0.0005320396298767027\n",
      "Batch: 26500,train loss is: 0.000270182791671178\n",
      "test loss is 0.000553018145424413\n",
      "Batch: 26600,train loss is: 0.0005015628066266577\n",
      "test loss is 0.0006604228338372093\n",
      "Batch: 26700,train loss is: 0.00039287968583767756\n",
      "test loss is 0.0006162913115090808\n",
      "Batch: 26800,train loss is: 0.00035918887104876884\n",
      "test loss is 0.0006873032098680517\n",
      "Batch: 26900,train loss is: 0.0006228940605984366\n",
      "test loss is 0.0005333451009730486\n",
      "Batch: 27000,train loss is: 0.001347724619614857\n",
      "test loss is 0.00048385285019845445\n",
      "Batch: 27100,train loss is: 0.0006271618000813566\n",
      "test loss is 0.0008395737623039063\n",
      "Batch: 27200,train loss is: 0.0005708667628767139\n",
      "test loss is 0.0005484356433284758\n",
      "Batch: 27300,train loss is: 0.00229313871370657\n",
      "test loss is 0.002970310926303309\n",
      "Batch: 27400,train loss is: 0.0005045207630035935\n",
      "test loss is 0.0014461493550407825\n",
      "Batch: 27500,train loss is: 0.0005995164233214864\n",
      "test loss is 0.0010218536460452681\n",
      "Batch: 27600,train loss is: 0.0005381825237357545\n",
      "test loss is 0.0007480221791939667\n",
      "Batch: 27700,train loss is: 0.0005754729857048448\n",
      "test loss is 0.0006176635086947606\n",
      "Batch: 27800,train loss is: 0.0005419206690107136\n",
      "test loss is 0.0010176239684815621\n",
      "Batch: 27900,train loss is: 0.0006813355542895623\n",
      "test loss is 0.0007119041872439985\n",
      "Batch: 28000,train loss is: 0.0005072927696140229\n",
      "test loss is 0.0004987718373456457\n",
      "Batch: 28100,train loss is: 0.0006975563584171322\n",
      "test loss is 0.0006334289874872258\n",
      "Batch: 28200,train loss is: 0.0003319695042439008\n",
      "test loss is 0.0004610101674752959\n",
      "Batch: 28300,train loss is: 0.0008515218979127297\n",
      "test loss is 0.000503460990634818\n",
      "Batch: 28400,train loss is: 0.0005700312569097136\n",
      "test loss is 0.0006912031594508422\n",
      "Batch: 28500,train loss is: 0.0010687128837982215\n",
      "test loss is 0.000744994662999351\n",
      "Batch: 28600,train loss is: 0.001244034547049999\n",
      "test loss is 0.001077504675098667\n",
      "Batch: 28700,train loss is: 0.0005032517247100366\n",
      "test loss is 0.0014022087793656595\n",
      "Batch: 28800,train loss is: 0.0004588723159285754\n",
      "test loss is 0.0006514029217231834\n",
      "Batch: 28900,train loss is: 0.0007166557428774512\n",
      "test loss is 0.0008943122388998796\n",
      "Batch: 29000,train loss is: 0.0006714155793922694\n",
      "test loss is 0.0004380622153003974\n",
      "Batch: 29100,train loss is: 0.000598514414501747\n",
      "test loss is 0.0007565372754764403\n",
      "Batch: 29200,train loss is: 0.0007033996799952144\n",
      "test loss is 0.00158196676276601\n",
      "Batch: 29300,train loss is: 0.0006582288607493752\n",
      "test loss is 0.0007560338033271198\n",
      "Batch: 29400,train loss is: 0.0007253630530990121\n",
      "test loss is 0.0013453617338211358\n",
      "Batch: 29500,train loss is: 0.00023675506756452353\n",
      "test loss is 0.0010240327094530202\n",
      "Batch: 29600,train loss is: 0.0015105172843005962\n",
      "test loss is 0.0006685394954594813\n",
      "Batch: 29700,train loss is: 0.0010169296541478257\n",
      "test loss is 0.001455193732376815\n",
      "Batch: 29800,train loss is: 0.0011188024381475624\n",
      "test loss is 0.0006159235829590226\n",
      "Batch: 29900,train loss is: 0.0011543030406680251\n",
      "test loss is 0.0005026546403079222\n",
      "Batch: 30000,train loss is: 0.0003963741758191976\n",
      "test loss is 0.0005407629961482921\n",
      "Batch: 30100,train loss is: 0.0007962373825784876\n",
      "test loss is 0.002447633736051058\n",
      "Batch: 30200,train loss is: 0.0005363135406887267\n",
      "test loss is 0.0005129277609417283\n",
      "Batch: 30300,train loss is: 0.00044865023277117287\n",
      "test loss is 0.0009288512093782999\n",
      "Batch: 30400,train loss is: 0.004023079315465599\n",
      "test loss is 0.005849636808481731\n",
      "Batch: 30500,train loss is: 0.0005544940804140299\n",
      "test loss is 0.0009155514360191104\n",
      "Batch: 30600,train loss is: 0.00024148558682656984\n",
      "test loss is 0.0004297111245169694\n",
      "Batch: 30700,train loss is: 0.00019068789983164053\n",
      "test loss is 0.001170721985042894\n",
      "Batch: 30800,train loss is: 0.0011752346577732814\n",
      "test loss is 0.000994152815765241\n",
      "Batch: 30900,train loss is: 0.0001860391899393237\n",
      "test loss is 0.0005563768280223821\n",
      "Batch: 31000,train loss is: 0.00024529913763162033\n",
      "test loss is 0.0004352353712053096\n",
      "Batch: 31100,train loss is: 0.0004800298278471745\n",
      "test loss is 0.0009359155044088112\n",
      "Batch: 31200,train loss is: 0.00031141127139923706\n",
      "test loss is 0.00040278725169656267\n",
      "Batch: 31300,train loss is: 0.0005709066803680007\n",
      "test loss is 0.0004792307617792499\n",
      "Batch: 31400,train loss is: 0.0006199234752362955\n",
      "test loss is 0.0012670109297055958\n",
      "Batch: 31500,train loss is: 0.0006993451890393603\n",
      "test loss is 0.0012773483872969005\n",
      "Batch: 31600,train loss is: 0.0010407508982177458\n",
      "test loss is 0.0006366427499474258\n",
      "Batch: 31700,train loss is: 0.0003105306514578565\n",
      "test loss is 0.00048916171417678\n",
      "Batch: 31800,train loss is: 0.0007228648008306655\n",
      "test loss is 0.00041650873347321506\n",
      "Batch: 31900,train loss is: 0.0013644754164721363\n",
      "test loss is 0.0006532589915522064\n",
      "Batch: 32000,train loss is: 0.0024096646162906875\n",
      "test loss is 0.00037236777934444046\n",
      "Batch: 32100,train loss is: 0.00012642558310282733\n",
      "test loss is 0.0005877677575880778\n",
      "Batch: 32200,train loss is: 0.00034722921430376994\n",
      "test loss is 0.0004332728418020713\n",
      "Batch: 32300,train loss is: 0.0012921759949182502\n",
      "test loss is 0.0012455639755193332\n",
      "Batch: 32400,train loss is: 0.0012761583561385094\n",
      "test loss is 0.0016142155383386705\n",
      "Batch: 32500,train loss is: 0.0005180087088666951\n",
      "test loss is 0.00043232187283857116\n",
      "Batch: 32600,train loss is: 0.00022884141295855272\n",
      "test loss is 0.00046421736169939814\n",
      "Batch: 32700,train loss is: 0.00040117107628072357\n",
      "test loss is 0.0007288598099553456\n",
      "Batch: 32800,train loss is: 0.0004672643483374528\n",
      "test loss is 0.0007752420924007728\n",
      "Batch: 32900,train loss is: 0.0004368059416102486\n",
      "test loss is 0.0005995996434030834\n",
      "Batch: 33000,train loss is: 0.0003142354592355647\n",
      "test loss is 0.0004838075441512565\n",
      "Batch: 33100,train loss is: 0.0011458948315253672\n",
      "test loss is 0.0010123716709585773\n",
      "Batch: 33200,train loss is: 0.0002135534875661364\n",
      "test loss is 0.0005370830657890735\n",
      "Batch: 33300,train loss is: 0.0003323595484587357\n",
      "test loss is 0.0004906323901691942\n",
      "Batch: 33400,train loss is: 0.0015515090568789113\n",
      "test loss is 0.0010115471235233341\n",
      "Batch: 33500,train loss is: 0.0013892081114521826\n",
      "test loss is 0.0016751704423089392\n",
      "Batch: 33600,train loss is: 0.0003317637513327604\n",
      "test loss is 0.0005662208411934243\n",
      "Batch: 33700,train loss is: 0.00036020738870304255\n",
      "test loss is 0.0005610319350247475\n",
      "Batch: 33800,train loss is: 0.0005521227702291303\n",
      "test loss is 0.000395663605774906\n",
      "Batch: 33900,train loss is: 0.0009010071536885002\n",
      "test loss is 0.00040558292562567624\n",
      "-----------------------Epoch: 2----------------------------------\n",
      "Batch: 0,train loss is: 0.0004579221561421669\n",
      "test loss is 0.0005699819427188328\n",
      "Batch: 100,train loss is: 0.0005873474500914885\n",
      "test loss is 0.0006143751505506475\n",
      "Batch: 200,train loss is: 0.00031951734871340777\n",
      "test loss is 0.0004019228779255124\n",
      "Batch: 300,train loss is: 0.0005111867395858252\n",
      "test loss is 0.0005050532267849248\n",
      "Batch: 400,train loss is: 0.0005067841220711838\n",
      "test loss is 0.0010188002983062803\n",
      "Batch: 500,train loss is: 0.0003273015184335815\n",
      "test loss is 0.001142362731402181\n",
      "Batch: 600,train loss is: 0.0001222575312140708\n",
      "test loss is 0.0011157162400381206\n",
      "Batch: 700,train loss is: 0.00029767740170106384\n",
      "test loss is 0.0004055987858609435\n",
      "Batch: 800,train loss is: 0.00017699335982422282\n",
      "test loss is 0.0005893257300834875\n",
      "Batch: 900,train loss is: 0.0005483546482745657\n",
      "test loss is 0.0005935176203884225\n",
      "Batch: 1000,train loss is: 0.00424643710788979\n",
      "test loss is 0.0011375965939175157\n",
      "Batch: 1100,train loss is: 0.0006268658409417784\n",
      "test loss is 0.00047409015118036037\n",
      "Batch: 1200,train loss is: 0.0003179120339064539\n",
      "test loss is 0.0006152021385549548\n",
      "Batch: 1300,train loss is: 0.0002793193751470814\n",
      "test loss is 0.0007727553787262169\n",
      "Batch: 1400,train loss is: 0.00019703520208231765\n",
      "test loss is 0.00042978945341347373\n",
      "Batch: 1500,train loss is: 0.0009458547590322037\n",
      "test loss is 0.001320704398918994\n",
      "Batch: 1600,train loss is: 0.0005833953724474597\n",
      "test loss is 0.0013346813729434965\n",
      "Batch: 1700,train loss is: 0.0023864838355844974\n",
      "test loss is 0.0007682022529362468\n",
      "Batch: 1800,train loss is: 0.0006319415696613599\n",
      "test loss is 0.0007326178387975656\n",
      "Batch: 1900,train loss is: 0.002786575044261441\n",
      "test loss is 0.004472716582792295\n",
      "Batch: 2000,train loss is: 0.003935075353531162\n",
      "test loss is 0.008732504256216984\n",
      "Batch: 2100,train loss is: 0.00034622156008025807\n",
      "test loss is 0.0005269034977524914\n",
      "Batch: 2200,train loss is: 0.0007386469891204464\n",
      "test loss is 0.0005230778553916747\n",
      "Batch: 2300,train loss is: 0.00024749096528338984\n",
      "test loss is 0.00039448385200819403\n",
      "Batch: 2400,train loss is: 0.00021789572805725407\n",
      "test loss is 0.0003694715539238466\n",
      "Batch: 2500,train loss is: 0.0007018993769869596\n",
      "test loss is 0.000544643636670536\n",
      "Batch: 2600,train loss is: 0.0005542307520293863\n",
      "test loss is 0.0005172119784368329\n",
      "Batch: 2700,train loss is: 0.0003916679815531267\n",
      "test loss is 0.00038666503877919504\n",
      "Batch: 2800,train loss is: 0.0002804559635436528\n",
      "test loss is 0.0005882644811715347\n",
      "Batch: 2900,train loss is: 0.0005573599269531131\n",
      "test loss is 0.0004391630388132646\n",
      "Batch: 3000,train loss is: 0.000541436582032664\n",
      "test loss is 0.0006982583358157435\n",
      "Batch: 3100,train loss is: 0.0002494877006491491\n",
      "test loss is 0.00041123074423255227\n",
      "Batch: 3200,train loss is: 0.0005206712008117773\n",
      "test loss is 0.0005476488493614387\n",
      "Batch: 3300,train loss is: 0.0002439760798268075\n",
      "test loss is 0.00048029112429953034\n",
      "Batch: 3400,train loss is: 0.00024137888833802068\n",
      "test loss is 0.0006258025445059518\n",
      "Batch: 3500,train loss is: 0.0010748462571139679\n",
      "test loss is 0.0008799370492216529\n",
      "Batch: 3600,train loss is: 0.00030069409669161907\n",
      "test loss is 0.000937042839111497\n",
      "Batch: 3700,train loss is: 0.0014204610116905253\n",
      "test loss is 0.0013020184856587283\n",
      "Batch: 3800,train loss is: 0.0008595382391165384\n",
      "test loss is 0.0008719571960593887\n",
      "Batch: 3900,train loss is: 0.002448425639438693\n",
      "test loss is 0.0009064595913290941\n",
      "Batch: 4000,train loss is: 0.00024527717660653085\n",
      "test loss is 0.0010628700700239684\n",
      "Batch: 4100,train loss is: 0.00036003479453486397\n",
      "test loss is 0.0006240525156028506\n",
      "Batch: 4200,train loss is: 0.0007976639679659564\n",
      "test loss is 0.0011006487769177162\n",
      "Batch: 4300,train loss is: 0.0003848154595876955\n",
      "test loss is 0.000766881161368487\n",
      "Batch: 4400,train loss is: 0.0003449670890429588\n",
      "test loss is 0.0007610902172271142\n",
      "Batch: 4500,train loss is: 0.0005545728640711597\n",
      "test loss is 0.0004337002186634879\n",
      "Batch: 4600,train loss is: 0.006909100158942674\n",
      "test loss is 0.004460233609868375\n",
      "Batch: 4700,train loss is: 0.0009633075781193531\n",
      "test loss is 0.0007253159486760415\n",
      "Batch: 4800,train loss is: 0.0014374947122683208\n",
      "test loss is 0.0017992174907993424\n",
      "Batch: 4900,train loss is: 0.0003353093601199494\n",
      "test loss is 0.000575000140912012\n",
      "Batch: 5000,train loss is: 0.00020317256490323787\n",
      "test loss is 0.00037999449231672643\n",
      "Batch: 5100,train loss is: 0.0003614305731818524\n",
      "test loss is 0.00046128635383288995\n",
      "Batch: 5200,train loss is: 0.0003670355235890805\n",
      "test loss is 0.00041327772658940467\n",
      "Batch: 5300,train loss is: 0.0004341810330817637\n",
      "test loss is 0.0012165750847925067\n",
      "Batch: 5400,train loss is: 0.0007809225964666683\n",
      "test loss is 0.0004336578061665609\n",
      "Batch: 5500,train loss is: 0.0021149553277256873\n",
      "test loss is 0.0013212456101682317\n",
      "Batch: 5600,train loss is: 0.0006147550203767077\n",
      "test loss is 0.0010921767079419347\n",
      "Batch: 5700,train loss is: 0.0007304524682523692\n",
      "test loss is 0.000655943121039448\n",
      "Batch: 5800,train loss is: 0.000269527138286941\n",
      "test loss is 0.0005652107464807913\n",
      "Batch: 5900,train loss is: 0.0008901290229909639\n",
      "test loss is 0.0007431988810543313\n",
      "Batch: 6000,train loss is: 0.0019856875008897944\n",
      "test loss is 0.000520899779702174\n",
      "Batch: 6100,train loss is: 0.0001949962575173351\n",
      "test loss is 0.0005539010827813247\n",
      "Batch: 6200,train loss is: 0.00032149246673091004\n",
      "test loss is 0.0004212145935362677\n",
      "Batch: 6300,train loss is: 0.0007967487109902631\n",
      "test loss is 0.0015969858586190847\n",
      "Batch: 6400,train loss is: 0.002265007133912525\n",
      "test loss is 0.003591833495641306\n",
      "Batch: 6500,train loss is: 0.0005337533504768412\n",
      "test loss is 0.0007603755305037333\n",
      "Batch: 6600,train loss is: 0.0019258791142870706\n",
      "test loss is 0.00048314147584262043\n",
      "Batch: 6700,train loss is: 0.00027083468077843364\n",
      "test loss is 0.0007246550194915698\n",
      "Batch: 6800,train loss is: 0.0009330398257088842\n",
      "test loss is 0.0006564550307173583\n",
      "Batch: 6900,train loss is: 0.002205288667933206\n",
      "test loss is 0.0010846880972114246\n",
      "Batch: 7000,train loss is: 0.00039249240924545424\n",
      "test loss is 0.0005095392258165347\n",
      "Batch: 7100,train loss is: 0.0008390766237618733\n",
      "test loss is 0.0008769035400161802\n",
      "Batch: 7200,train loss is: 0.00030539143519547267\n",
      "test loss is 0.0006184146163778849\n",
      "Batch: 7300,train loss is: 0.000650954677244482\n",
      "test loss is 0.0010298419154221496\n",
      "Batch: 7400,train loss is: 0.00014775525370837474\n",
      "test loss is 0.0009338766950531424\n",
      "Batch: 7500,train loss is: 0.0002731524700222782\n",
      "test loss is 0.0011267614083302197\n",
      "Batch: 7600,train loss is: 0.0007609708795805244\n",
      "test loss is 0.0006816044545298651\n",
      "Batch: 7700,train loss is: 0.00065945807675801\n",
      "test loss is 0.0005544375460227059\n",
      "Batch: 7800,train loss is: 0.0013105841574565767\n",
      "test loss is 0.0007554238258322998\n",
      "Batch: 7900,train loss is: 0.0009795119748806226\n",
      "test loss is 0.0009621377916098067\n",
      "Batch: 8000,train loss is: 0.0002755366423849353\n",
      "test loss is 0.0005920295605246879\n",
      "Batch: 8100,train loss is: 0.0009549949843167427\n",
      "test loss is 0.0006034247135710113\n",
      "Batch: 8200,train loss is: 0.0006894906924587885\n",
      "test loss is 0.0005220505313489525\n",
      "Batch: 8300,train loss is: 0.0003548087946137347\n",
      "test loss is 0.0007024135560502723\n",
      "Batch: 8400,train loss is: 0.0004544837950708756\n",
      "test loss is 0.0007970686143801976\n",
      "Batch: 8500,train loss is: 0.0006053283007724999\n",
      "test loss is 0.0005902927218661448\n",
      "Batch: 8600,train loss is: 0.0007073214889132548\n",
      "test loss is 0.0008544529036664298\n",
      "Batch: 8700,train loss is: 0.00015056751932512373\n",
      "test loss is 0.0012381755706559333\n",
      "Batch: 8800,train loss is: 0.0003262887016068138\n",
      "test loss is 0.0005652693043494344\n",
      "Batch: 8900,train loss is: 0.000739512031419973\n",
      "test loss is 0.0009830708889688325\n",
      "Batch: 9000,train loss is: 0.001086461673778681\n",
      "test loss is 0.0010437455455461575\n",
      "Batch: 9100,train loss is: 0.0014544224661756045\n",
      "test loss is 0.0005847767945575406\n",
      "Batch: 9200,train loss is: 0.0009467587594994765\n",
      "test loss is 0.0009034166152366438\n",
      "Batch: 9300,train loss is: 0.0011981230761684701\n",
      "test loss is 0.0006384290545396209\n",
      "Batch: 9400,train loss is: 0.0006067969662896486\n",
      "test loss is 0.0004183240446302522\n",
      "Batch: 9500,train loss is: 0.00035183253746278386\n",
      "test loss is 0.0009236799344077122\n",
      "Batch: 9600,train loss is: 0.00043686328217684077\n",
      "test loss is 0.0005241648649847266\n",
      "Batch: 9700,train loss is: 0.0010975298118695235\n",
      "test loss is 0.000465421212422682\n",
      "Batch: 9800,train loss is: 0.00012362524949626662\n",
      "test loss is 0.0006611298160602006\n",
      "Batch: 9900,train loss is: 0.000791600997796104\n",
      "test loss is 0.002618870978373219\n",
      "Batch: 10000,train loss is: 0.0004993872057836498\n",
      "test loss is 0.0011843210759540069\n",
      "Batch: 10100,train loss is: 0.0005912429145231496\n",
      "test loss is 0.0007201522554876389\n",
      "Batch: 10200,train loss is: 0.0023522682119780083\n",
      "test loss is 0.0005520571270335956\n",
      "Batch: 10300,train loss is: 0.00040199086155546534\n",
      "test loss is 0.0006182400824222687\n",
      "Batch: 10400,train loss is: 0.00037540397576731803\n",
      "test loss is 0.0005227808295237321\n",
      "Batch: 10500,train loss is: 0.0007198016404672899\n",
      "test loss is 0.0008394906815829875\n",
      "Batch: 10600,train loss is: 0.0002640313285284805\n",
      "test loss is 0.0006382224405384844\n",
      "Batch: 10700,train loss is: 0.00030524054401252825\n",
      "test loss is 0.00043326002820840066\n",
      "Batch: 10800,train loss is: 0.0007004286393734587\n",
      "test loss is 0.001966512391751031\n",
      "Batch: 10900,train loss is: 0.0005790281862867512\n",
      "test loss is 0.0006479408763633871\n",
      "Batch: 11000,train loss is: 0.0004955250116504761\n",
      "test loss is 0.0005586936521327897\n",
      "Batch: 11100,train loss is: 0.0003137043971820033\n",
      "test loss is 0.0008434970683444554\n",
      "Batch: 11200,train loss is: 0.0008452876404574447\n",
      "test loss is 0.0004670731549487118\n",
      "Batch: 11300,train loss is: 0.0010293456082493555\n",
      "test loss is 0.001126442741337042\n",
      "Batch: 11400,train loss is: 0.0008013474367586471\n",
      "test loss is 0.0011482678187556207\n",
      "Batch: 11500,train loss is: 0.00024302562190047097\n",
      "test loss is 0.0006304913180719964\n",
      "Batch: 11600,train loss is: 0.0011970072121465043\n",
      "test loss is 0.001548640319294349\n",
      "Batch: 11700,train loss is: 0.00020991821664106662\n",
      "test loss is 0.0005450744512016202\n",
      "Batch: 11800,train loss is: 0.0002248887571324398\n",
      "test loss is 0.00038752084704592063\n",
      "Batch: 11900,train loss is: 0.0009879696798082978\n",
      "test loss is 0.0006594720498186784\n",
      "Batch: 12000,train loss is: 0.00044520534644583074\n",
      "test loss is 0.0009595712041884036\n",
      "Batch: 12100,train loss is: 0.003375903926445049\n",
      "test loss is 0.002393636938924776\n",
      "Batch: 12200,train loss is: 0.0004996075822336148\n",
      "test loss is 0.000644459721584579\n",
      "Batch: 12300,train loss is: 0.00015899357649943008\n",
      "test loss is 0.0008950554498971542\n",
      "Batch: 12400,train loss is: 0.0014100193930458198\n",
      "test loss is 0.00093950823959754\n",
      "Batch: 12500,train loss is: 0.000797455518997011\n",
      "test loss is 0.0015610522277802851\n",
      "Batch: 12600,train loss is: 0.0002043305357628436\n",
      "test loss is 0.0004332330737736433\n",
      "Batch: 12700,train loss is: 0.00032127708424788457\n",
      "test loss is 0.00035468786079725304\n",
      "Batch: 12800,train loss is: 0.00014461846124932414\n",
      "test loss is 0.0005077510090766645\n",
      "Batch: 12900,train loss is: 0.0003887684735160369\n",
      "test loss is 0.00045256399166828976\n",
      "Batch: 13000,train loss is: 0.00034665152183690416\n",
      "test loss is 0.00042316787540624615\n",
      "Batch: 13100,train loss is: 0.0006837193598566847\n",
      "test loss is 0.00043902101844608184\n",
      "Batch: 13200,train loss is: 0.00040783185558391756\n",
      "test loss is 0.000659263833730698\n",
      "Batch: 13300,train loss is: 0.000760930270947627\n",
      "test loss is 0.0021916022019274695\n",
      "Batch: 13400,train loss is: 0.0011662052386267505\n",
      "test loss is 0.0013314389111332388\n",
      "Batch: 13500,train loss is: 0.0004739502111287681\n",
      "test loss is 0.000677448021537955\n",
      "Batch: 13600,train loss is: 0.0025886344895597723\n",
      "test loss is 0.00361079944255598\n",
      "Batch: 13700,train loss is: 0.0005504993145584397\n",
      "test loss is 0.000655643665931982\n",
      "Batch: 13800,train loss is: 0.00031128893208063355\n",
      "test loss is 0.0005688763861007297\n",
      "Batch: 13900,train loss is: 0.00018987531514770792\n",
      "test loss is 0.0005579540535169559\n",
      "Batch: 14000,train loss is: 0.0008601505155737884\n",
      "test loss is 0.0011179738699232417\n",
      "Batch: 14100,train loss is: 0.0005721099483815232\n",
      "test loss is 0.0007736504356557263\n",
      "Batch: 14200,train loss is: 0.0002487907701164792\n",
      "test loss is 0.00041794950381011443\n",
      "Batch: 14300,train loss is: 0.0002653687288362275\n",
      "test loss is 0.0005413824229199151\n",
      "Batch: 14400,train loss is: 0.00032968231093589143\n",
      "test loss is 0.0004392221713069864\n",
      "Batch: 14500,train loss is: 0.00047817572986155364\n",
      "test loss is 0.00036582027376904914\n",
      "Batch: 14600,train loss is: 0.0004048221946022279\n",
      "test loss is 0.0008305644149703763\n",
      "Batch: 14700,train loss is: 0.0019952300786601003\n",
      "test loss is 0.002290316249178554\n",
      "Batch: 14800,train loss is: 0.0006419437636560929\n",
      "test loss is 0.0008100695774568705\n",
      "Batch: 14900,train loss is: 0.00038701736878036635\n",
      "test loss is 0.00034355821565841636\n",
      "Batch: 15000,train loss is: 0.0002971918780968198\n",
      "test loss is 0.00045678930372970673\n",
      "Batch: 15100,train loss is: 0.00028982256941332136\n",
      "test loss is 0.0006142914226678454\n",
      "Batch: 15200,train loss is: 0.0011210650275421327\n",
      "test loss is 0.0005018754159753944\n",
      "Batch: 15300,train loss is: 0.00027698425591235053\n",
      "test loss is 0.0004262070763162202\n",
      "Batch: 15400,train loss is: 0.0007317965697279321\n",
      "test loss is 0.0008044000893309437\n",
      "Batch: 15500,train loss is: 0.00044576370079813915\n",
      "test loss is 0.0006527478118185063\n",
      "Batch: 15600,train loss is: 0.006181584859702152\n",
      "test loss is 0.0012075982422607924\n",
      "Batch: 15700,train loss is: 0.00023907663851371162\n",
      "test loss is 0.0010879278839882258\n",
      "Batch: 15800,train loss is: 0.0002801554104683842\n",
      "test loss is 0.0007401286492731312\n",
      "Batch: 15900,train loss is: 0.0017150069901331623\n",
      "test loss is 0.00066368283687299\n",
      "Batch: 16000,train loss is: 0.0003493967526945308\n",
      "test loss is 0.0004074708884371886\n",
      "Batch: 16100,train loss is: 0.0015810925694527305\n",
      "test loss is 0.00048118578982273854\n",
      "Batch: 16200,train loss is: 0.0004166080477691831\n",
      "test loss is 0.0006128134913152742\n",
      "Batch: 16300,train loss is: 0.0001727656850097314\n",
      "test loss is 0.00046525139573414177\n",
      "Batch: 16400,train loss is: 0.00045690855443391833\n",
      "test loss is 0.0005107151088840071\n",
      "Batch: 16500,train loss is: 0.0008889176143756028\n",
      "test loss is 0.0006138008868420321\n",
      "Batch: 16600,train loss is: 0.00021820512753271558\n",
      "test loss is 0.00043994111579151874\n",
      "Batch: 16700,train loss is: 0.0003728191858532083\n",
      "test loss is 0.0004613101549912628\n",
      "Batch: 16800,train loss is: 0.0003191151959298552\n",
      "test loss is 0.0005747131676310397\n",
      "Batch: 16900,train loss is: 0.0022641911705379766\n",
      "test loss is 0.0013563835752275657\n",
      "Batch: 17000,train loss is: 0.00032753687465434105\n",
      "test loss is 0.0004361799349703863\n",
      "Batch: 17100,train loss is: 0.0008586035156784874\n",
      "test loss is 0.0011849979339693725\n",
      "Batch: 17200,train loss is: 0.00030673996517025464\n",
      "test loss is 0.0005728939383923698\n",
      "Batch: 17300,train loss is: 0.0004440016941479324\n",
      "test loss is 0.0009694461822121092\n",
      "Batch: 17400,train loss is: 0.00036939446609548664\n",
      "test loss is 0.0004737040746530967\n",
      "Batch: 17500,train loss is: 0.0009459095668243513\n",
      "test loss is 0.0006814807036347903\n",
      "Batch: 17600,train loss is: 0.0016136569376346208\n",
      "test loss is 0.002130680810541841\n",
      "Batch: 17700,train loss is: 0.00035850517020616905\n",
      "test loss is 0.0007377659616114936\n",
      "Batch: 17800,train loss is: 0.0012662467689484144\n",
      "test loss is 0.001318486438890068\n",
      "Batch: 17900,train loss is: 0.00041786026728188905\n",
      "test loss is 0.0003988587698819057\n",
      "Batch: 18000,train loss is: 0.00038148138074043384\n",
      "test loss is 0.00035011648030382555\n",
      "Batch: 18100,train loss is: 0.0004575539539269133\n",
      "test loss is 0.0006000520883094063\n",
      "Batch: 18200,train loss is: 0.00027408671844615315\n",
      "test loss is 0.0005612090347452945\n",
      "Batch: 18300,train loss is: 0.0011489479441653696\n",
      "test loss is 0.0003859568733301052\n",
      "Batch: 18400,train loss is: 0.00044627643734664757\n",
      "test loss is 0.0006627522032831388\n",
      "Batch: 18500,train loss is: 0.0004767901858614582\n",
      "test loss is 0.00033975192814439555\n",
      "Batch: 18600,train loss is: 0.00028444222776741895\n",
      "test loss is 0.000659000121674721\n",
      "Batch: 18700,train loss is: 0.00032118748178938894\n",
      "test loss is 0.0005935499895276491\n",
      "Batch: 18800,train loss is: 0.0006132686994803516\n",
      "test loss is 0.0006118507899496128\n",
      "Batch: 18900,train loss is: 0.0003984870880485522\n",
      "test loss is 0.0004689698378464356\n",
      "Batch: 19000,train loss is: 0.0004328009830766877\n",
      "test loss is 0.0007101243371135119\n",
      "Batch: 19100,train loss is: 0.0005681809850412468\n",
      "test loss is 0.000427081673598374\n",
      "Batch: 19200,train loss is: 0.0003357818326178799\n",
      "test loss is 0.0005113383273654932\n",
      "Batch: 19300,train loss is: 0.0007795741019817808\n",
      "test loss is 0.0005539982000308575\n",
      "Batch: 19400,train loss is: 0.0002543508127854982\n",
      "test loss is 0.0012407168055209557\n",
      "Batch: 19500,train loss is: 0.00025553711575353324\n",
      "test loss is 0.0007217162069631814\n",
      "Batch: 19600,train loss is: 0.00048823902626861127\n",
      "test loss is 0.0006131614744045505\n",
      "Batch: 19700,train loss is: 0.001029377680726091\n",
      "test loss is 0.0007346663213369406\n",
      "Batch: 19800,train loss is: 0.0015193532904935747\n",
      "test loss is 0.0007934534924128429\n",
      "Batch: 19900,train loss is: 0.0006570981702533171\n",
      "test loss is 0.0009295797408663975\n",
      "Batch: 20000,train loss is: 0.0035725344870660503\n",
      "test loss is 0.0021622112216367383\n",
      "Batch: 20100,train loss is: 0.0003453287890468775\n",
      "test loss is 0.0005676516918375773\n",
      "Batch: 20200,train loss is: 0.00033478028840139236\n",
      "test loss is 0.00037618323565164825\n",
      "Batch: 20300,train loss is: 0.0004841581675265752\n",
      "test loss is 0.00039720328172394566\n",
      "Batch: 20400,train loss is: 0.0011476090736726502\n",
      "test loss is 0.001529266789079192\n",
      "Batch: 20500,train loss is: 0.00032428898405717895\n",
      "test loss is 0.0005787085934748791\n",
      "Batch: 20600,train loss is: 0.002088027482729737\n",
      "test loss is 0.0012460311260673424\n",
      "Batch: 20700,train loss is: 0.0005823045580744335\n",
      "test loss is 0.000516720392462638\n",
      "Batch: 20800,train loss is: 0.0010209960453443614\n",
      "test loss is 0.000514429039734257\n",
      "Batch: 20900,train loss is: 0.00022559226581511234\n",
      "test loss is 0.0004108965921209815\n",
      "Batch: 21000,train loss is: 0.000413458453421059\n",
      "test loss is 0.0008752499435528363\n",
      "Batch: 21100,train loss is: 0.0006698682382839729\n",
      "test loss is 0.0005111279257270816\n",
      "Batch: 21200,train loss is: 0.000848581535273693\n",
      "test loss is 0.0010116257901373422\n",
      "Batch: 21300,train loss is: 0.0004103256317527698\n",
      "test loss is 0.0005972560201354919\n",
      "Batch: 21400,train loss is: 0.00035215924425610764\n",
      "test loss is 0.00043589333359051943\n",
      "Batch: 21500,train loss is: 0.00022687656010140592\n",
      "test loss is 0.0008813300454481943\n",
      "Batch: 21600,train loss is: 0.0006258552787100087\n",
      "test loss is 0.0005559583433175139\n",
      "Batch: 21700,train loss is: 0.0003210507028527342\n",
      "test loss is 0.000582721427824718\n",
      "Batch: 21800,train loss is: 0.0003238972393087095\n",
      "test loss is 0.00034754141181755893\n",
      "Batch: 21900,train loss is: 0.00019846153406387988\n",
      "test loss is 0.0006541581547000535\n",
      "Batch: 22000,train loss is: 0.001580549134124859\n",
      "test loss is 0.0025673486136217625\n",
      "Batch: 22100,train loss is: 0.0003832947539295502\n",
      "test loss is 0.0008483023436179658\n",
      "Batch: 22200,train loss is: 0.0030735192560283064\n",
      "test loss is 0.0019287421169310137\n",
      "Batch: 22300,train loss is: 0.0016118588764432783\n",
      "test loss is 0.0028575222783124516\n",
      "Batch: 22400,train loss is: 0.0007292108269358765\n",
      "test loss is 0.0005965128005670048\n",
      "Batch: 22500,train loss is: 0.000357519824559757\n",
      "test loss is 0.0005044941035317782\n",
      "Batch: 22600,train loss is: 0.000741246103925204\n",
      "test loss is 0.0006921613834956269\n",
      "Batch: 22700,train loss is: 0.0002662530587866381\n",
      "test loss is 0.0005546793350912582\n",
      "Batch: 22800,train loss is: 0.0008152833923385174\n",
      "test loss is 0.0004992292247119167\n",
      "Batch: 22900,train loss is: 0.00025601867636549345\n",
      "test loss is 0.0007433664061527684\n",
      "Batch: 23000,train loss is: 0.0006615178841704385\n",
      "test loss is 0.00042773143728618197\n",
      "Batch: 23100,train loss is: 0.0001883225436747571\n",
      "test loss is 0.00040732541238130894\n",
      "Batch: 23200,train loss is: 0.00025076826964398277\n",
      "test loss is 0.0004376287810684516\n",
      "Batch: 23300,train loss is: 0.000994516340910817\n",
      "test loss is 0.0009268559137021787\n",
      "Batch: 23400,train loss is: 0.0012941491992329067\n",
      "test loss is 0.0008197513439751698\n",
      "Batch: 23500,train loss is: 0.0010955788318659488\n",
      "test loss is 0.0004923377005332675\n",
      "Batch: 23600,train loss is: 0.0002746197113539125\n",
      "test loss is 0.0004960726537767506\n",
      "Batch: 23700,train loss is: 0.000576180802152528\n",
      "test loss is 0.0007869242835403587\n",
      "Batch: 23800,train loss is: 0.000619723708498265\n",
      "test loss is 0.0007322933534046182\n",
      "Batch: 23900,train loss is: 0.00044985997520904334\n",
      "test loss is 0.0011286144260577366\n",
      "Batch: 24000,train loss is: 0.0012610252864230468\n",
      "test loss is 0.00124025351653178\n",
      "Batch: 24100,train loss is: 0.0004499676110710188\n",
      "test loss is 0.0006928423912152639\n",
      "Batch: 24200,train loss is: 0.0024333320661054504\n",
      "test loss is 0.0004480865802976786\n",
      "Batch: 24300,train loss is: 0.00039693675719156614\n",
      "test loss is 0.0006529224242519402\n",
      "Batch: 24400,train loss is: 0.002322227542497915\n",
      "test loss is 0.0007038956005461201\n",
      "Batch: 24500,train loss is: 0.001675844438021174\n",
      "test loss is 0.0006813464282220839\n",
      "Batch: 24600,train loss is: 0.0009458460450387561\n",
      "test loss is 0.0006765576371187947\n",
      "Batch: 24700,train loss is: 0.0007896203159941634\n",
      "test loss is 0.0010890655010274833\n",
      "Batch: 24800,train loss is: 0.0005496338783575171\n",
      "test loss is 0.000573253930621201\n",
      "Batch: 24900,train loss is: 0.000569106849169408\n",
      "test loss is 0.0006109080954950618\n",
      "Batch: 25000,train loss is: 0.0004057075384690042\n",
      "test loss is 0.0004761336591017972\n",
      "Batch: 25100,train loss is: 0.00027431415449869715\n",
      "test loss is 0.0007888955768348031\n",
      "Batch: 25200,train loss is: 0.0005325449327242006\n",
      "test loss is 0.0009078439751618974\n",
      "Batch: 25300,train loss is: 0.0007506943769941941\n",
      "test loss is 0.0005678424169720687\n",
      "Batch: 25400,train loss is: 0.0006609753622231877\n",
      "test loss is 0.0005899239761471299\n",
      "Batch: 25500,train loss is: 0.00019321939597253979\n",
      "test loss is 0.0004379695704746307\n",
      "Batch: 25600,train loss is: 0.0005969797364828317\n",
      "test loss is 0.00038390358217957007\n",
      "Batch: 25700,train loss is: 0.0013980622381012562\n",
      "test loss is 0.0004611632007564431\n",
      "Batch: 25800,train loss is: 0.0013295993714531379\n",
      "test loss is 0.0009509987659672846\n",
      "Batch: 25900,train loss is: 0.00018039818368406174\n",
      "test loss is 0.0005699281445614384\n",
      "Batch: 26000,train loss is: 0.0015120608556624185\n",
      "test loss is 0.0011312635355909377\n",
      "Batch: 26100,train loss is: 0.0002812111509228488\n",
      "test loss is 0.0007281017236792331\n",
      "Batch: 26200,train loss is: 0.0004284700842619389\n",
      "test loss is 0.0008519825780265741\n",
      "Batch: 26300,train loss is: 0.0002732342620372786\n",
      "test loss is 0.0008394959675308429\n",
      "Batch: 26400,train loss is: 0.0003173839376357543\n",
      "test loss is 0.00041405973235407884\n",
      "Batch: 26500,train loss is: 0.00028219827732724927\n",
      "test loss is 0.00042164630374526204\n",
      "Batch: 26600,train loss is: 0.000375821733986805\n",
      "test loss is 0.0006534306473343735\n",
      "Batch: 26700,train loss is: 0.00030563493248109423\n",
      "test loss is 0.0006075919558565234\n",
      "Batch: 26800,train loss is: 0.00026956313422746823\n",
      "test loss is 0.0006771191633947035\n",
      "Batch: 26900,train loss is: 0.0003366841089849621\n",
      "test loss is 0.00040627337502722045\n",
      "Batch: 27000,train loss is: 0.0011289151322956433\n",
      "test loss is 0.0004306177929424804\n",
      "Batch: 27100,train loss is: 0.00048016236823617724\n",
      "test loss is 0.0007843525462756626\n",
      "Batch: 27200,train loss is: 0.00036136470101307737\n",
      "test loss is 0.0005007493047602891\n",
      "Batch: 27300,train loss is: 0.0017419915062971984\n",
      "test loss is 0.002307004564656052\n",
      "Batch: 27400,train loss is: 0.00037778420888010466\n",
      "test loss is 0.0011796828789676459\n",
      "Batch: 27500,train loss is: 0.0005915738751502274\n",
      "test loss is 0.0007794021360458545\n",
      "Batch: 27600,train loss is: 0.0003069541059732347\n",
      "test loss is 0.000582581708970954\n",
      "Batch: 27700,train loss is: 0.000652347598513565\n",
      "test loss is 0.0004610310044565633\n",
      "Batch: 27800,train loss is: 0.00053158972349828\n",
      "test loss is 0.0008705245694566701\n",
      "Batch: 27900,train loss is: 0.0006439786827058421\n",
      "test loss is 0.0005832772261482809\n",
      "Batch: 28000,train loss is: 0.00045583218958110676\n",
      "test loss is 0.00044151539545750763\n",
      "Batch: 28100,train loss is: 0.00043063434356707165\n",
      "test loss is 0.0005962833065876634\n",
      "Batch: 28200,train loss is: 0.00025158720055092743\n",
      "test loss is 0.00042607414791309017\n",
      "Batch: 28300,train loss is: 0.0005529747510901527\n",
      "test loss is 0.0003423350105058527\n",
      "Batch: 28400,train loss is: 0.0005362050336510067\n",
      "test loss is 0.0006728142552900648\n",
      "Batch: 28500,train loss is: 0.0011389935521889665\n",
      "test loss is 0.0007411926513708966\n",
      "Batch: 28600,train loss is: 0.000885194691622973\n",
      "test loss is 0.0007195707997161152\n",
      "Batch: 28700,train loss is: 0.0004329052414965889\n",
      "test loss is 0.0008454377450360548\n",
      "Batch: 28800,train loss is: 0.000563567738903089\n",
      "test loss is 0.0006019773442639072\n",
      "Batch: 28900,train loss is: 0.0006870911794649118\n",
      "test loss is 0.0007296379096331244\n",
      "Batch: 29000,train loss is: 0.0004982278844102892\n",
      "test loss is 0.0004343923110558421\n",
      "Batch: 29100,train loss is: 0.0010918400961004038\n",
      "test loss is 0.0013609697212780122\n",
      "Batch: 29200,train loss is: 0.0004997117311680562\n",
      "test loss is 0.0011103164107724055\n",
      "Batch: 29300,train loss is: 0.0005754785709441188\n",
      "test loss is 0.000969006711346886\n",
      "Batch: 29400,train loss is: 0.0004545822341839744\n",
      "test loss is 0.0007779420838354483\n",
      "Batch: 29500,train loss is: 0.0002277891700483252\n",
      "test loss is 0.0007532539194389862\n",
      "Batch: 29600,train loss is: 0.0007712989400962693\n",
      "test loss is 0.00047326031840037527\n",
      "Batch: 29700,train loss is: 0.0007517208663844054\n",
      "test loss is 0.0009704615499680685\n",
      "Batch: 29800,train loss is: 0.0006601116889845435\n",
      "test loss is 0.00043338103980256965\n",
      "Batch: 29900,train loss is: 0.0008818567399633215\n",
      "test loss is 0.0004731676787544157\n",
      "Batch: 30000,train loss is: 0.0004258759625683525\n",
      "test loss is 0.0004332368001575381\n",
      "Batch: 30100,train loss is: 0.0008607586381929838\n",
      "test loss is 0.002491197299266662\n",
      "Batch: 30200,train loss is: 0.0004945170047387173\n",
      "test loss is 0.0004970859557836058\n",
      "Batch: 30300,train loss is: 0.0006540649408942372\n",
      "test loss is 0.0014839948008024892\n",
      "Batch: 30400,train loss is: 0.0038395823555302977\n",
      "test loss is 0.006601392580412967\n",
      "Batch: 30500,train loss is: 0.0005857971888154585\n",
      "test loss is 0.000797123999692626\n",
      "Batch: 30600,train loss is: 0.0002303952907399339\n",
      "test loss is 0.00043867840382584973\n",
      "Batch: 30700,train loss is: 0.00019729814101202255\n",
      "test loss is 0.0012817901174465118\n",
      "Batch: 30800,train loss is: 0.00038072315736430883\n",
      "test loss is 0.0005532306550964797\n",
      "Batch: 30900,train loss is: 0.00016516603255609238\n",
      "test loss is 0.00041159317776318707\n",
      "Batch: 31000,train loss is: 0.0002074328583209088\n",
      "test loss is 0.0003277195257429946\n",
      "Batch: 31100,train loss is: 0.0002623208236796627\n",
      "test loss is 0.0005340626273193377\n",
      "Batch: 31200,train loss is: 0.00027257386783294935\n",
      "test loss is 0.000369648339074773\n",
      "Batch: 31300,train loss is: 0.0003902064687785751\n",
      "test loss is 0.00039597507775990796\n",
      "Batch: 31400,train loss is: 0.000471597585412483\n",
      "test loss is 0.0008524883881889355\n",
      "Batch: 31500,train loss is: 0.0005582684352505516\n",
      "test loss is 0.0016444127186855254\n",
      "Batch: 31600,train loss is: 0.0010471172118459743\n",
      "test loss is 0.000620785154729332\n",
      "Batch: 31700,train loss is: 0.00020136183680781605\n",
      "test loss is 0.000478647109875855\n",
      "Batch: 31800,train loss is: 0.0007405923219988089\n",
      "test loss is 0.0003992840577842724\n",
      "Batch: 31900,train loss is: 0.0014631486055008461\n",
      "test loss is 0.0008378816362865305\n",
      "Batch: 32000,train loss is: 0.0019621902410799103\n",
      "test loss is 0.00034271177369964097\n",
      "Batch: 32100,train loss is: 0.00011708024190081006\n",
      "test loss is 0.000474776390437771\n",
      "Batch: 32200,train loss is: 0.00032399406037653984\n",
      "test loss is 0.00037111695794767003\n",
      "Batch: 32300,train loss is: 0.0011369896322635268\n",
      "test loss is 0.0011662236777603271\n",
      "Batch: 32400,train loss is: 0.0009845886472481464\n",
      "test loss is 0.0011938704091342496\n",
      "Batch: 32500,train loss is: 0.0005437289142722212\n",
      "test loss is 0.0003385432685189886\n",
      "Batch: 32600,train loss is: 0.0001806343405905565\n",
      "test loss is 0.00042371312972531614\n",
      "Batch: 32700,train loss is: 0.00039937215884032517\n",
      "test loss is 0.0005512600009526574\n",
      "Batch: 32800,train loss is: 0.00038964001294850475\n",
      "test loss is 0.000474928974754945\n",
      "Batch: 32900,train loss is: 0.0003128666995267568\n",
      "test loss is 0.0004483618763336491\n",
      "Batch: 33000,train loss is: 0.0002474461450964095\n",
      "test loss is 0.0005109190231579208\n",
      "Batch: 33100,train loss is: 0.002495490196509965\n",
      "test loss is 0.0015443433665456598\n",
      "Batch: 33200,train loss is: 0.00017625574568074466\n",
      "test loss is 0.0006763082541092383\n",
      "Batch: 33300,train loss is: 0.0003843990813845177\n",
      "test loss is 0.00037933342904039044\n",
      "Batch: 33400,train loss is: 0.001558322279185814\n",
      "test loss is 0.0015609696829207275\n",
      "Batch: 33500,train loss is: 0.0007386502589467782\n",
      "test loss is 0.0012804011976562779\n",
      "Batch: 33600,train loss is: 0.00032037381574358254\n",
      "test loss is 0.000497136990212966\n",
      "Batch: 33700,train loss is: 0.0003977378076309716\n",
      "test loss is 0.0004962185794045513\n",
      "Batch: 33800,train loss is: 0.0007230383180671266\n",
      "test loss is 0.00038280003251416945\n",
      "Batch: 33900,train loss is: 0.0003520380915689934\n",
      "test loss is 0.0003199662922150991\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model(loss_MSE,optim_Adam,model,data_loader,train_data,test_data,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Epoch: 0----------------------------------\n",
      "Batch: 0,train loss is: 0.0004210850603232282\n",
      "test loss is 0.0004289753421299835\n",
      "Batch: 100,train loss is: 0.0004999100348358975\n",
      "test loss is 0.00038617009205925006\n",
      "Batch: 200,train loss is: 0.00025682252201726863\n",
      "test loss is 0.0003168393814536375\n",
      "Batch: 300,train loss is: 0.000255132847479449\n",
      "test loss is 0.0003712478280225658\n",
      "Batch: 400,train loss is: 0.0007039415541448007\n",
      "test loss is 0.0012375722414230453\n",
      "Batch: 500,train loss is: 0.00024853602936462775\n",
      "test loss is 0.0007760825522319763\n",
      "Batch: 600,train loss is: 0.00010608532828485901\n",
      "test loss is 0.000549523461381982\n",
      "Batch: 700,train loss is: 0.00026377819518345614\n",
      "test loss is 0.00036689193464472\n",
      "Batch: 800,train loss is: 0.00014978465587457568\n",
      "test loss is 0.00044074464585414574\n",
      "Batch: 900,train loss is: 0.0003956678519817975\n",
      "test loss is 0.0004995601799630069\n",
      "Batch: 1000,train loss is: 0.002747061352164178\n",
      "test loss is 0.0008959545746163091\n",
      "Batch: 1100,train loss is: 0.00040213139142443016\n",
      "test loss is 0.0003849642141693221\n",
      "Batch: 1200,train loss is: 0.00018007231833739\n",
      "test loss is 0.0004434719072988117\n",
      "Batch: 1300,train loss is: 0.00030336962013918285\n",
      "test loss is 0.0004727634886414618\n",
      "Batch: 1400,train loss is: 0.00021167465459386007\n",
      "test loss is 0.0003965421726961163\n",
      "Batch: 1500,train loss is: 0.00047905506118414937\n",
      "test loss is 0.0007063679544117704\n",
      "Batch: 1600,train loss is: 0.0010713333353192446\n",
      "test loss is 0.0015345228307015316\n",
      "Batch: 1700,train loss is: 0.002058573627933924\n",
      "test loss is 0.0007862676048795459\n",
      "Batch: 1800,train loss is: 0.0005304524415968998\n",
      "test loss is 0.0009926437383399463\n",
      "Batch: 1900,train loss is: 0.0015540286154882897\n",
      "test loss is 0.0039040443102554096\n",
      "Batch: 2000,train loss is: 0.0027779089730434275\n",
      "test loss is 0.00630105422109262\n",
      "Batch: 2100,train loss is: 0.00036891706388333316\n",
      "test loss is 0.0005061235218522089\n",
      "Batch: 2200,train loss is: 0.0006112148707789896\n",
      "test loss is 0.00045340067739978857\n",
      "Batch: 2300,train loss is: 0.00033718496888006106\n",
      "test loss is 0.00034133538713429067\n",
      "Batch: 2400,train loss is: 0.00022263042497437505\n",
      "test loss is 0.00032905073636459365\n",
      "Batch: 2500,train loss is: 0.0004830428666424191\n",
      "test loss is 0.000454529296299993\n",
      "Batch: 2600,train loss is: 0.0004790389515452646\n",
      "test loss is 0.0004758102753930535\n",
      "Batch: 2700,train loss is: 0.0002359169220695239\n",
      "test loss is 0.000325400762240409\n",
      "Batch: 2800,train loss is: 0.00023028119569284958\n",
      "test loss is 0.000421138555888373\n",
      "Batch: 2900,train loss is: 0.00045722038276795323\n",
      "test loss is 0.0003543274645528455\n",
      "Batch: 3000,train loss is: 0.0004390548243817786\n",
      "test loss is 0.00044129116113645\n",
      "Batch: 3100,train loss is: 0.00028297313629155557\n",
      "test loss is 0.0004018972829601108\n",
      "Batch: 3200,train loss is: 0.000563520245231544\n",
      "test loss is 0.0004966837789217579\n",
      "Batch: 3300,train loss is: 0.00022593373052130682\n",
      "test loss is 0.0003566523847852539\n",
      "Batch: 3400,train loss is: 0.00019567988934118894\n",
      "test loss is 0.0005130749379059007\n",
      "Batch: 3500,train loss is: 0.0009609542045204835\n",
      "test loss is 0.0007504745633669889\n",
      "Batch: 3600,train loss is: 0.00017408369471192506\n",
      "test loss is 0.0006470356652585568\n",
      "Batch: 3700,train loss is: 0.001008850134671386\n",
      "test loss is 0.0008721764439101842\n",
      "Batch: 3800,train loss is: 0.0007967167199712143\n",
      "test loss is 0.0008344824340063765\n",
      "Batch: 3900,train loss is: 0.002890914727464031\n",
      "test loss is 0.0012292667831468095\n",
      "Batch: 4000,train loss is: 0.00028571245728496573\n",
      "test loss is 0.0010090613547538904\n",
      "Batch: 4100,train loss is: 0.0002438890901626491\n",
      "test loss is 0.0007581521704798527\n",
      "Batch: 4200,train loss is: 0.00045540202657057445\n",
      "test loss is 0.00045146673907669674\n",
      "Batch: 4300,train loss is: 0.00033787293199301983\n",
      "test loss is 0.000702305162295262\n",
      "Batch: 4400,train loss is: 0.0002706519395704349\n",
      "test loss is 0.0005261232558903453\n",
      "Batch: 4500,train loss is: 0.0008593961488297333\n",
      "test loss is 0.0003477277468186358\n",
      "Batch: 4600,train loss is: 0.0005204795616592424\n",
      "test loss is 0.00048478428163705644\n",
      "Batch: 4700,train loss is: 0.0006556008860088093\n",
      "test loss is 0.0003744709949309905\n",
      "Batch: 4800,train loss is: 0.001117757257478241\n",
      "test loss is 0.0012293953236183113\n",
      "Batch: 4900,train loss is: 0.0002791859141055393\n",
      "test loss is 0.0004929789596213988\n",
      "Batch: 5000,train loss is: 0.00030357112499732167\n",
      "test loss is 0.00045117858580049356\n",
      "Batch: 5100,train loss is: 0.0002566135138378819\n",
      "test loss is 0.00041877560219020355\n",
      "Batch: 5200,train loss is: 0.0005827353123251236\n",
      "test loss is 0.0004662223066035986\n",
      "Batch: 5300,train loss is: 0.0005433329761511621\n",
      "test loss is 0.0007562812796717253\n",
      "Batch: 5400,train loss is: 0.00031711260797360476\n",
      "test loss is 0.0004043118053434219\n",
      "Batch: 5500,train loss is: 0.0017012216623933643\n",
      "test loss is 0.001330785602051366\n",
      "Batch: 5600,train loss is: 0.0017787847475584037\n",
      "test loss is 0.002033958820156086\n",
      "Batch: 5700,train loss is: 0.0008983579582415814\n",
      "test loss is 0.0004982758841299056\n",
      "Batch: 5800,train loss is: 0.00039247140341685887\n",
      "test loss is 0.000610250503533298\n",
      "Batch: 5900,train loss is: 0.0009579237275508107\n",
      "test loss is 0.0007388896244166783\n",
      "Batch: 6000,train loss is: 0.0024820870112305187\n",
      "test loss is 0.0005801024284439067\n",
      "Batch: 6100,train loss is: 0.0002753168658020325\n",
      "test loss is 0.0005868422292507391\n",
      "Batch: 6200,train loss is: 0.0003823911241018195\n",
      "test loss is 0.0005538839095707193\n",
      "Batch: 6300,train loss is: 0.0007011270733622558\n",
      "test loss is 0.001183901504335326\n",
      "Batch: 6400,train loss is: 0.0008867649562851881\n",
      "test loss is 0.0011377761751084929\n",
      "Batch: 6500,train loss is: 0.0005431663561952847\n",
      "test loss is 0.0006992594636918108\n",
      "Batch: 6600,train loss is: 0.001863738249318046\n",
      "test loss is 0.0008873365597422576\n",
      "Batch: 6700,train loss is: 0.0001987253921952343\n",
      "test loss is 0.0007172158095453847\n",
      "Batch: 6800,train loss is: 0.001587181466077009\n",
      "test loss is 0.0006531301215360034\n",
      "Batch: 6900,train loss is: 0.00039959563883334314\n",
      "test loss is 0.0007456775372734214\n",
      "Batch: 7000,train loss is: 0.00044621749314116744\n",
      "test loss is 0.0004960484772532047\n",
      "Batch: 7100,train loss is: 0.0009721277451659008\n",
      "test loss is 0.0019997556907988905\n",
      "Batch: 7200,train loss is: 0.00015025164137923722\n",
      "test loss is 0.0004376682327462537\n",
      "Batch: 7300,train loss is: 0.0009814061040781914\n",
      "test loss is 0.0011186417189129449\n",
      "Batch: 7400,train loss is: 0.0001664146321581616\n",
      "test loss is 0.0008394616606154509\n",
      "Batch: 7500,train loss is: 0.00026441475596974224\n",
      "test loss is 0.000632800378679883\n",
      "Batch: 7600,train loss is: 0.0005074886139033904\n",
      "test loss is 0.0004377029865405773\n",
      "Batch: 7700,train loss is: 0.0012682104048465797\n",
      "test loss is 0.0005916895254901723\n",
      "Batch: 7800,train loss is: 0.0009644747369205408\n",
      "test loss is 0.0006630784157895849\n",
      "Batch: 7900,train loss is: 0.0004856236459726435\n",
      "test loss is 0.0005329756758445392\n",
      "Batch: 8000,train loss is: 0.0001352558694155788\n",
      "test loss is 0.0003741604855602163\n",
      "Batch: 8100,train loss is: 0.000652346095146601\n",
      "test loss is 0.0008136591498806879\n",
      "Batch: 8200,train loss is: 0.0011617015557817013\n",
      "test loss is 0.00047786596214282977\n",
      "Batch: 8300,train loss is: 0.00016491319741011317\n",
      "test loss is 0.0004013773394836078\n",
      "Batch: 8400,train loss is: 0.0008427779244749736\n",
      "test loss is 0.0009416252737216442\n",
      "Batch: 8500,train loss is: 0.0004528782948464944\n",
      "test loss is 0.0005317073577671908\n",
      "Batch: 8600,train loss is: 0.0008379252910826941\n",
      "test loss is 0.0009348599875664772\n",
      "Batch: 8700,train loss is: 0.00028426293831476207\n",
      "test loss is 0.001036763973657653\n",
      "Batch: 8800,train loss is: 0.0004790547122973717\n",
      "test loss is 0.0007531437762491643\n",
      "Batch: 8900,train loss is: 0.0007690357305635116\n",
      "test loss is 0.00051594328087096\n",
      "Batch: 9000,train loss is: 0.0004811556713717945\n",
      "test loss is 0.0005938498671360402\n",
      "Batch: 9100,train loss is: 0.0005141544254183677\n",
      "test loss is 0.0004406356592523272\n",
      "Batch: 9200,train loss is: 0.0006525635422052772\n",
      "test loss is 0.0006350701267273267\n",
      "Batch: 9300,train loss is: 0.0019693166179304035\n",
      "test loss is 0.0007502317290311989\n",
      "Batch: 9400,train loss is: 0.0007234653281950297\n",
      "test loss is 0.0004617412747482063\n",
      "Batch: 9500,train loss is: 0.0004012031756245662\n",
      "test loss is 0.0008257401239344832\n",
      "Batch: 9600,train loss is: 0.00028136661256007966\n",
      "test loss is 0.000459427498944485\n",
      "Batch: 9700,train loss is: 0.000923440661551191\n",
      "test loss is 0.0004472329949520111\n",
      "Batch: 9800,train loss is: 0.00016389032249297666\n",
      "test loss is 0.0007074927981085798\n",
      "Batch: 9900,train loss is: 0.00030129365165478444\n",
      "test loss is 0.0009332642652885196\n",
      "Batch: 10000,train loss is: 0.00022523906565353053\n",
      "test loss is 0.0006899295642101263\n",
      "Batch: 10100,train loss is: 0.0004002097289539208\n",
      "test loss is 0.00047834817940734577\n",
      "Batch: 10200,train loss is: 0.0012447313358535058\n",
      "test loss is 0.0004322846465002724\n",
      "Batch: 10300,train loss is: 0.0005807786374497234\n",
      "test loss is 0.0012756635120975788\n",
      "Batch: 10400,train loss is: 0.0001398180424871482\n",
      "test loss is 0.0007533155795996448\n",
      "Batch: 10500,train loss is: 0.0009321321238105403\n",
      "test loss is 0.0011149307117162793\n",
      "Batch: 10600,train loss is: 0.00023133486401731478\n",
      "test loss is 0.00043856053950422537\n",
      "Batch: 10700,train loss is: 0.0003372727027341288\n",
      "test loss is 0.000540786878959288\n",
      "Batch: 10800,train loss is: 0.0012659596621613077\n",
      "test loss is 0.0040661644392242435\n",
      "Batch: 10900,train loss is: 0.0005344685438878993\n",
      "test loss is 0.0005796317468050958\n",
      "Batch: 11000,train loss is: 0.0005717949919353932\n",
      "test loss is 0.000456269122677468\n",
      "Batch: 11100,train loss is: 0.00023410713593488008\n",
      "test loss is 0.000455885456753922\n",
      "Batch: 11200,train loss is: 0.0004523519106963917\n",
      "test loss is 0.0007091316905366445\n",
      "Batch: 11300,train loss is: 0.0008309207036830329\n",
      "test loss is 0.0016544124253468297\n",
      "Batch: 11400,train loss is: 0.0007368986350787715\n",
      "test loss is 0.000742854490133151\n",
      "Batch: 11500,train loss is: 0.0002644450582632757\n",
      "test loss is 0.0004370082089887388\n",
      "Batch: 11600,train loss is: 0.0006475482530712951\n",
      "test loss is 0.0006426567924781432\n",
      "Batch: 11700,train loss is: 0.00034666479565663975\n",
      "test loss is 0.00048548385184603017\n",
      "Batch: 11800,train loss is: 0.000208837580868588\n",
      "test loss is 0.0004933833277861995\n",
      "Batch: 11900,train loss is: 0.0005107691950837067\n",
      "test loss is 0.0004595359109577002\n",
      "Batch: 12000,train loss is: 0.0010139861558583698\n",
      "test loss is 0.0007109070520387416\n",
      "Batch: 12100,train loss is: 0.0014196826835878881\n",
      "test loss is 0.0033055904472089294\n",
      "Batch: 12200,train loss is: 0.000254312987060239\n",
      "test loss is 0.0009542904765436154\n",
      "Batch: 12300,train loss is: 0.0007169544277783463\n",
      "test loss is 0.0006333097416826753\n",
      "Batch: 12400,train loss is: 0.0003330678193604322\n",
      "test loss is 0.00037536381047681395\n",
      "Batch: 12500,train loss is: 0.000780869415650189\n",
      "test loss is 0.0010811701228078186\n",
      "Batch: 12600,train loss is: 0.00034679162852835035\n",
      "test loss is 0.0004290622346295617\n",
      "Batch: 12700,train loss is: 0.0002278330230686292\n",
      "test loss is 0.0004732943277906068\n",
      "Batch: 12800,train loss is: 0.00012896194107998106\n",
      "test loss is 0.0006478004575283057\n",
      "Batch: 12900,train loss is: 0.000523543295134161\n",
      "test loss is 0.000990537563918921\n",
      "Batch: 13000,train loss is: 0.00027668760200814815\n",
      "test loss is 0.00038799895470898447\n",
      "Batch: 13100,train loss is: 0.0006779825115885981\n",
      "test loss is 0.0003657936502783133\n",
      "Batch: 13200,train loss is: 0.0004375748260888527\n",
      "test loss is 0.0005493037345865547\n",
      "Batch: 13300,train loss is: 0.0009582575236482387\n",
      "test loss is 0.0025255280701614027\n",
      "Batch: 13400,train loss is: 0.0007595218765360313\n",
      "test loss is 0.0008263083782756819\n",
      "Batch: 13500,train loss is: 0.0007935165424298466\n",
      "test loss is 0.0006171771005161187\n",
      "Batch: 13600,train loss is: 0.001615700462466924\n",
      "test loss is 0.0020455983806821607\n",
      "Batch: 13700,train loss is: 0.0007190252475436662\n",
      "test loss is 0.0006129591468783069\n",
      "Batch: 13800,train loss is: 0.00022790670185011977\n",
      "test loss is 0.0004920051802666589\n",
      "Batch: 13900,train loss is: 0.00030454121571265295\n",
      "test loss is 0.0005984606623201349\n",
      "Batch: 14000,train loss is: 0.0008760075263189215\n",
      "test loss is 0.0010410426253080192\n",
      "Batch: 14100,train loss is: 0.0004822165418441757\n",
      "test loss is 0.0007186734493949032\n",
      "Batch: 14200,train loss is: 0.00022157521991334234\n",
      "test loss is 0.00035391476004466413\n",
      "Batch: 14300,train loss is: 0.00023947758586488266\n",
      "test loss is 0.0005609553705161636\n",
      "Batch: 14400,train loss is: 0.0003877387035201185\n",
      "test loss is 0.00032203039598463365\n",
      "Batch: 14500,train loss is: 0.0003014772268263338\n",
      "test loss is 0.00029156763157061526\n",
      "Batch: 14600,train loss is: 0.0004151290629972963\n",
      "test loss is 0.0005826818567278259\n",
      "Batch: 14700,train loss is: 0.002075415351475281\n",
      "test loss is 0.001547941126760589\n",
      "Batch: 14800,train loss is: 0.0005742785227833652\n",
      "test loss is 0.0008158886815760554\n",
      "Batch: 14900,train loss is: 0.0002560039572302463\n",
      "test loss is 0.0003245003603250591\n",
      "Batch: 15000,train loss is: 0.0005285660896285574\n",
      "test loss is 0.0003860050848698852\n",
      "Batch: 15100,train loss is: 0.00029935126600779023\n",
      "test loss is 0.0005511887261341238\n",
      "Batch: 15200,train loss is: 0.0014875693691245298\n",
      "test loss is 0.0004532208229206152\n",
      "Batch: 15300,train loss is: 0.0003521902729100822\n",
      "test loss is 0.00041585060010866946\n",
      "Batch: 15400,train loss is: 0.0005567110966728074\n",
      "test loss is 0.000621939917231538\n",
      "Batch: 15500,train loss is: 0.0004364711767873617\n",
      "test loss is 0.0005434426378157091\n",
      "Batch: 15600,train loss is: 0.00637693591466604\n",
      "test loss is 0.0014711115869317844\n",
      "Batch: 15700,train loss is: 0.00020230574175731346\n",
      "test loss is 0.0009615617761391874\n",
      "Batch: 15800,train loss is: 0.0002727582524329745\n",
      "test loss is 0.0005408097822473666\n",
      "Batch: 15900,train loss is: 0.0014236391053965192\n",
      "test loss is 0.0005951164819694376\n",
      "Batch: 16000,train loss is: 0.0002674545651985837\n",
      "test loss is 0.0003726982965073098\n",
      "Batch: 16100,train loss is: 0.0010742832544207643\n",
      "test loss is 0.0004839463274006866\n",
      "Batch: 16200,train loss is: 0.0004620053038211833\n",
      "test loss is 0.00057063757926767\n",
      "Batch: 16300,train loss is: 0.00013142714471315076\n",
      "test loss is 0.0005201753453340992\n",
      "Batch: 16400,train loss is: 0.0004543047428314411\n",
      "test loss is 0.00044190788829440413\n",
      "Batch: 16500,train loss is: 0.0007355341003539153\n",
      "test loss is 0.0005428618641233849\n",
      "Batch: 16600,train loss is: 0.00019296440773809083\n",
      "test loss is 0.0003497010863256204\n",
      "Batch: 16700,train loss is: 0.0005526788809516611\n",
      "test loss is 0.00046041269427217295\n",
      "Batch: 16800,train loss is: 0.00034453879548909395\n",
      "test loss is 0.0005480965341667929\n",
      "Batch: 16900,train loss is: 0.0019570951344442707\n",
      "test loss is 0.0013636937900134477\n",
      "Batch: 17000,train loss is: 0.0003065127146856114\n",
      "test loss is 0.00047069176813542026\n",
      "Batch: 17100,train loss is: 0.0006188930899137833\n",
      "test loss is 0.0009954027068235492\n",
      "Batch: 17200,train loss is: 0.00030770092175726036\n",
      "test loss is 0.0005115047509309878\n",
      "Batch: 17300,train loss is: 0.00037222695942536893\n",
      "test loss is 0.000832609152120129\n",
      "Batch: 17400,train loss is: 0.0002960392836484996\n",
      "test loss is 0.0004340380513546567\n",
      "Batch: 17500,train loss is: 0.0007473293209458892\n",
      "test loss is 0.0006014887300003707\n",
      "Batch: 17600,train loss is: 0.0014103114755263464\n",
      "test loss is 0.0018603247333741414\n",
      "Batch: 17700,train loss is: 0.00032302712294475245\n",
      "test loss is 0.0006812536439034703\n",
      "Batch: 17800,train loss is: 0.0009743634890180816\n",
      "test loss is 0.0011347509880571308\n",
      "Batch: 17900,train loss is: 0.00046941616385323\n",
      "test loss is 0.0003434688596255666\n",
      "Batch: 18000,train loss is: 0.00034046437955350566\n",
      "test loss is 0.0003356985755343333\n",
      "Batch: 18100,train loss is: 0.00039623679494595434\n",
      "test loss is 0.0004671841781079157\n",
      "Batch: 18200,train loss is: 0.0002444301716401224\n",
      "test loss is 0.0004653726322502055\n",
      "Batch: 18300,train loss is: 0.0009161934789498026\n",
      "test loss is 0.0003378914606538655\n",
      "Batch: 18400,train loss is: 0.0004030967154146668\n",
      "test loss is 0.0005590530152732797\n",
      "Batch: 18500,train loss is: 0.00022056187505913022\n",
      "test loss is 0.00031406058788809146\n",
      "Batch: 18600,train loss is: 0.0003111197134051571\n",
      "test loss is 0.0006206088702117936\n",
      "Batch: 18700,train loss is: 0.00039127503456700183\n",
      "test loss is 0.0005072024461717343\n",
      "Batch: 18800,train loss is: 0.00046254339940588856\n",
      "test loss is 0.00044820059212449884\n",
      "Batch: 18900,train loss is: 0.0004435599130095729\n",
      "test loss is 0.0003541410246677243\n",
      "Batch: 19000,train loss is: 0.0005397109578707641\n",
      "test loss is 0.0005663691548774589\n",
      "Batch: 19100,train loss is: 0.000409734601932149\n",
      "test loss is 0.0005207775109345677\n",
      "Batch: 19200,train loss is: 0.00033640881172995187\n",
      "test loss is 0.00047208469078739707\n",
      "Batch: 19300,train loss is: 0.0004475445166920411\n",
      "test loss is 0.0004593339682799658\n",
      "Batch: 19400,train loss is: 0.00020000649455909704\n",
      "test loss is 0.00090680344971735\n",
      "Batch: 19500,train loss is: 0.00030107032160098344\n",
      "test loss is 0.0006419846397393903\n",
      "Batch: 19600,train loss is: 0.0004241747283080002\n",
      "test loss is 0.0005277025438858849\n",
      "Batch: 19700,train loss is: 0.0008302404452472038\n",
      "test loss is 0.0005487248826530268\n",
      "Batch: 19800,train loss is: 0.0011659208102195947\n",
      "test loss is 0.0006456997592880905\n",
      "Batch: 19900,train loss is: 0.0003705473620673775\n",
      "test loss is 0.0008864064812472561\n",
      "Batch: 20000,train loss is: 0.0028174684601228917\n",
      "test loss is 0.0016877561161478048\n",
      "Batch: 20100,train loss is: 0.00038963893198656316\n",
      "test loss is 0.0006293580919957742\n",
      "Batch: 20200,train loss is: 0.0003039615851277703\n",
      "test loss is 0.00032273337093528337\n",
      "Batch: 20300,train loss is: 0.00017647169175485212\n",
      "test loss is 0.00030715606325787235\n",
      "Batch: 20400,train loss is: 0.0007371318829131786\n",
      "test loss is 0.0011103446749342728\n",
      "Batch: 20500,train loss is: 0.0004577512180618115\n",
      "test loss is 0.0006562011788535002\n",
      "Batch: 20600,train loss is: 0.0015567612051710707\n",
      "test loss is 0.0011215615308663596\n",
      "Batch: 20700,train loss is: 0.0006118785182499919\n",
      "test loss is 0.0005459608390251721\n",
      "Batch: 20800,train loss is: 0.000809422343771432\n",
      "test loss is 0.00042208570951608\n",
      "Batch: 20900,train loss is: 0.00021219738660631588\n",
      "test loss is 0.00036440100698299267\n",
      "Batch: 21000,train loss is: 0.0003061794620862103\n",
      "test loss is 0.0005543129974842169\n",
      "Batch: 21100,train loss is: 0.0004567392157555071\n",
      "test loss is 0.00042272922336747984\n",
      "Batch: 21200,train loss is: 0.000781112190057456\n",
      "test loss is 0.0009384749171917578\n",
      "Batch: 21300,train loss is: 0.00034799100908619124\n",
      "test loss is 0.0005966732655916442\n",
      "Batch: 21400,train loss is: 0.00033730924804255036\n",
      "test loss is 0.0003996557372906577\n",
      "Batch: 21500,train loss is: 0.00024968091714580815\n",
      "test loss is 0.0007226924287667013\n",
      "Batch: 21600,train loss is: 0.0006087334811891994\n",
      "test loss is 0.0004885784153032488\n",
      "Batch: 21700,train loss is: 0.0002005719496611477\n",
      "test loss is 0.0005227173658754946\n",
      "Batch: 21800,train loss is: 0.0003353356685156214\n",
      "test loss is 0.0003157174657902846\n",
      "Batch: 21900,train loss is: 0.00013661569202217503\n",
      "test loss is 0.0006314385408179014\n",
      "Batch: 22000,train loss is: 0.0003240237471248907\n",
      "test loss is 0.0015873215847972476\n",
      "Batch: 22100,train loss is: 0.00025206702357190536\n",
      "test loss is 0.000769004912975028\n",
      "Batch: 22200,train loss is: 0.0013238701431065147\n",
      "test loss is 0.0012953446531240988\n",
      "Batch: 22300,train loss is: 0.0008361091139864946\n",
      "test loss is 0.0014207250652079762\n",
      "Batch: 22400,train loss is: 0.00048723743659777693\n",
      "test loss is 0.0004883753172903889\n",
      "Batch: 22500,train loss is: 0.000374245946879667\n",
      "test loss is 0.00046348785107160013\n",
      "Batch: 22600,train loss is: 0.0003296977808578402\n",
      "test loss is 0.00037515058806921104\n",
      "Batch: 22700,train loss is: 0.0006139091229821797\n",
      "test loss is 0.0010180827589748769\n",
      "Batch: 22800,train loss is: 0.0008474046116647212\n",
      "test loss is 0.00046481897073824145\n",
      "Batch: 22900,train loss is: 0.00015444924478857236\n",
      "test loss is 0.0005240001101176334\n",
      "Batch: 23000,train loss is: 0.000625423966288042\n",
      "test loss is 0.0004116985864701693\n",
      "Batch: 23100,train loss is: 0.00022926445715278638\n",
      "test loss is 0.00038338779755706977\n",
      "Batch: 23200,train loss is: 0.0001251224444768185\n",
      "test loss is 0.0004424643548190468\n",
      "Batch: 23300,train loss is: 0.0008602234957627581\n",
      "test loss is 0.0008305520087782081\n",
      "Batch: 23400,train loss is: 0.00039805549154400805\n",
      "test loss is 0.0004320181640525482\n",
      "Batch: 23500,train loss is: 0.0006729253414387821\n",
      "test loss is 0.0003788858867751412\n",
      "Batch: 23600,train loss is: 0.00018401246230868317\n",
      "test loss is 0.0003389390834129323\n",
      "Batch: 23700,train loss is: 0.00043011548345617425\n",
      "test loss is 0.001023655086234429\n",
      "Batch: 23800,train loss is: 0.001185427231613917\n",
      "test loss is 0.0008807914912526336\n",
      "Batch: 23900,train loss is: 0.00043287280513157507\n",
      "test loss is 0.0005914742847660037\n",
      "Batch: 24000,train loss is: 0.0007170184766636538\n",
      "test loss is 0.000530278352305845\n",
      "Batch: 24100,train loss is: 0.000261348551025522\n",
      "test loss is 0.0003360969885857817\n",
      "Batch: 24200,train loss is: 0.0008065398813655789\n",
      "test loss is 0.0004916375624941296\n",
      "Batch: 24300,train loss is: 0.00031808557982807424\n",
      "test loss is 0.0003857841549659953\n",
      "Batch: 24400,train loss is: 0.0009371849473378782\n",
      "test loss is 0.0007764964476126895\n",
      "Batch: 24500,train loss is: 0.0015760321430805951\n",
      "test loss is 0.0010434002633150192\n",
      "Batch: 24600,train loss is: 0.0007790984287017446\n",
      "test loss is 0.002091849410332216\n",
      "Batch: 24700,train loss is: 0.0010922227620018571\n",
      "test loss is 0.0011095006785730116\n",
      "Batch: 24800,train loss is: 0.0005002469922346192\n",
      "test loss is 0.0005097786322926953\n",
      "Batch: 24900,train loss is: 0.0002505845598110432\n",
      "test loss is 0.00031261917439707207\n",
      "Batch: 25000,train loss is: 0.00033036328898715286\n",
      "test loss is 0.0003706883574987158\n",
      "Batch: 25100,train loss is: 0.0005555531770187465\n",
      "test loss is 0.0006918100604592844\n",
      "Batch: 25200,train loss is: 0.0018842250994647839\n",
      "test loss is 0.001356395193143493\n",
      "Batch: 25300,train loss is: 0.0011824471309555967\n",
      "test loss is 0.00047436429925320817\n",
      "Batch: 25400,train loss is: 0.0006909115223417509\n",
      "test loss is 0.0007265664717561347\n",
      "Batch: 25500,train loss is: 0.00017155581326728622\n",
      "test loss is 0.0003565940753146284\n",
      "Batch: 25600,train loss is: 0.0003903000284691333\n",
      "test loss is 0.00039814172945385054\n",
      "Batch: 25700,train loss is: 0.001244897583197296\n",
      "test loss is 0.0004194281833852328\n",
      "Batch: 25800,train loss is: 0.0010694578409026704\n",
      "test loss is 0.000890965929401522\n",
      "Batch: 25900,train loss is: 0.00018356368176105018\n",
      "test loss is 0.0004498535817280535\n",
      "Batch: 26000,train loss is: 0.0010128890966059857\n",
      "test loss is 0.0010477366118824278\n",
      "Batch: 26100,train loss is: 0.00018439884906199886\n",
      "test loss is 0.00043814794863509355\n",
      "Batch: 26200,train loss is: 0.00029126594866200645\n",
      "test loss is 0.0006070881256172417\n",
      "Batch: 26300,train loss is: 0.0002343107277618957\n",
      "test loss is 0.0007631311767129933\n",
      "Batch: 26400,train loss is: 0.00032528793480995433\n",
      "test loss is 0.00038736756274659206\n",
      "Batch: 26500,train loss is: 0.00028120406402648535\n",
      "test loss is 0.00040337720726470823\n",
      "Batch: 26600,train loss is: 0.0005012976420438751\n",
      "test loss is 0.000627281580502981\n",
      "Batch: 26700,train loss is: 0.0003332333622250421\n",
      "test loss is 0.0005588952595850205\n",
      "Batch: 26800,train loss is: 0.00024104727269903093\n",
      "test loss is 0.0006745800523829953\n",
      "Batch: 26900,train loss is: 0.0002895481782126712\n",
      "test loss is 0.00034766135175892703\n",
      "Batch: 27000,train loss is: 0.0011029801286638987\n",
      "test loss is 0.0003854679354328092\n",
      "Batch: 27100,train loss is: 0.0003335211562827421\n",
      "test loss is 0.0005466450188305885\n",
      "Batch: 27200,train loss is: 0.0004983591572928194\n",
      "test loss is 0.0004653190811725628\n",
      "Batch: 27300,train loss is: 0.001135515367510926\n",
      "test loss is 0.0014895677855150955\n",
      "Batch: 27400,train loss is: 0.0003576328116644567\n",
      "test loss is 0.0010743170985782738\n",
      "Batch: 27500,train loss is: 0.0005006035272994095\n",
      "test loss is 0.0005970930031891127\n",
      "Batch: 27600,train loss is: 0.0004328864127196248\n",
      "test loss is 0.0007772779333593524\n",
      "Batch: 27700,train loss is: 0.000738578252151131\n",
      "test loss is 0.00047474453014716746\n",
      "Batch: 27800,train loss is: 0.00045244314356070436\n",
      "test loss is 0.0007970919894891884\n",
      "Batch: 27900,train loss is: 0.0006349852505891802\n",
      "test loss is 0.00048259059824710824\n",
      "Batch: 28000,train loss is: 0.00030226298220484277\n",
      "test loss is 0.00038525964549865804\n",
      "Batch: 28100,train loss is: 0.00040895725995877206\n",
      "test loss is 0.0005577563586936335\n",
      "Batch: 28200,train loss is: 0.00025327177143422407\n",
      "test loss is 0.0003953646962780776\n",
      "Batch: 28300,train loss is: 0.00062946565655277\n",
      "test loss is 0.00030763957952158086\n",
      "Batch: 28400,train loss is: 0.0006111749036743482\n",
      "test loss is 0.0006235215369680243\n",
      "Batch: 28500,train loss is: 0.0009128655594275874\n",
      "test loss is 0.0004731969590846439\n",
      "Batch: 28600,train loss is: 0.0003906098936249207\n",
      "test loss is 0.0004964111454012687\n",
      "Batch: 28700,train loss is: 0.000336110211145884\n",
      "test loss is 0.0006142010502639247\n",
      "Batch: 28800,train loss is: 0.0005713292738843029\n",
      "test loss is 0.00046142357120034866\n",
      "Batch: 28900,train loss is: 0.0007931750717475025\n",
      "test loss is 0.0006934694546394022\n",
      "Batch: 29000,train loss is: 0.0003009433356979458\n",
      "test loss is 0.0003586423627513638\n",
      "Batch: 29100,train loss is: 0.0008009695030761235\n",
      "test loss is 0.0007036488153036864\n",
      "Batch: 29200,train loss is: 0.0008133746886573662\n",
      "test loss is 0.0014595221714773668\n",
      "Batch: 29300,train loss is: 0.0006524532530793958\n",
      "test loss is 0.0008371416855065652\n",
      "Batch: 29400,train loss is: 0.0005983464093034994\n",
      "test loss is 0.0006247849388666327\n",
      "Batch: 29500,train loss is: 0.0004114112966856575\n",
      "test loss is 0.0003512812470115019\n",
      "Batch: 29600,train loss is: 0.0008785275541267665\n",
      "test loss is 0.00042743870013087546\n",
      "Batch: 29700,train loss is: 0.00047152267520905847\n",
      "test loss is 0.0007179947753512417\n",
      "Batch: 29800,train loss is: 0.0005738326741002361\n",
      "test loss is 0.00043501364022394487\n",
      "Batch: 29900,train loss is: 0.0009594603961179129\n",
      "test loss is 0.00039919694688865965\n",
      "Batch: 30000,train loss is: 0.0004321809001743532\n",
      "test loss is 0.00033074227731874883\n",
      "Batch: 30100,train loss is: 0.0007596209257490465\n",
      "test loss is 0.0017816823783561395\n",
      "Batch: 30200,train loss is: 0.0003804819273133649\n",
      "test loss is 0.0003207683412226325\n",
      "Batch: 30300,train loss is: 0.0005498927202953443\n",
      "test loss is 0.0012448411885748751\n",
      "Batch: 30400,train loss is: 0.003216885525926569\n",
      "test loss is 0.006676077585578817\n",
      "Batch: 30500,train loss is: 0.0006334200785468873\n",
      "test loss is 0.0008202571171300184\n",
      "Batch: 30600,train loss is: 0.0001881282169897933\n",
      "test loss is 0.0003993379534024628\n",
      "Batch: 30700,train loss is: 0.0002596547611807465\n",
      "test loss is 0.001017110532339861\n",
      "Batch: 30800,train loss is: 0.0001903288511765286\n",
      "test loss is 0.0003614751683538228\n",
      "Batch: 30900,train loss is: 0.00013374202898697627\n",
      "test loss is 0.0004012014762362254\n",
      "Batch: 31000,train loss is: 0.00028955308709679664\n",
      "test loss is 0.00033077716528971815\n",
      "Batch: 31100,train loss is: 0.00018579876940905238\n",
      "test loss is 0.00041316281412278737\n",
      "Batch: 31200,train loss is: 0.00023592573417973375\n",
      "test loss is 0.0003405196243958078\n",
      "Batch: 31300,train loss is: 0.00027959136521883086\n",
      "test loss is 0.00030633364311394957\n",
      "Batch: 31400,train loss is: 0.0004325821867203133\n",
      "test loss is 0.0006645260085026401\n",
      "Batch: 31500,train loss is: 0.0004233970357764772\n",
      "test loss is 0.0016446719584295734\n",
      "Batch: 31600,train loss is: 0.0007393387554007673\n",
      "test loss is 0.0004957641864401009\n",
      "Batch: 31700,train loss is: 0.00013312940273954045\n",
      "test loss is 0.0004964940165910873\n",
      "Batch: 31800,train loss is: 0.0006539831875820523\n",
      "test loss is 0.000349716301572788\n",
      "Batch: 31900,train loss is: 0.0012907431955955914\n",
      "test loss is 0.0009966617966911183\n",
      "Batch: 32000,train loss is: 0.0021289299471291693\n",
      "test loss is 0.000329039254730713\n",
      "Batch: 32100,train loss is: 0.0001471441179262591\n",
      "test loss is 0.0003834718749994899\n",
      "Batch: 32200,train loss is: 0.0002890887026084313\n",
      "test loss is 0.00037874018853624876\n",
      "Batch: 32300,train loss is: 0.000732382873278313\n",
      "test loss is 0.0009317880843194891\n",
      "Batch: 32400,train loss is: 0.0005750826555772564\n",
      "test loss is 0.0007272302670650243\n",
      "Batch: 32500,train loss is: 0.0005201732466320586\n",
      "test loss is 0.0003335265215901235\n",
      "Batch: 32600,train loss is: 0.00016479485006026316\n",
      "test loss is 0.0004076380953166213\n",
      "Batch: 32700,train loss is: 0.00045609363089993134\n",
      "test loss is 0.0004318072900237582\n",
      "Batch: 32800,train loss is: 0.0003453997288717807\n",
      "test loss is 0.0004140620726117492\n",
      "Batch: 32900,train loss is: 0.0002592859784964017\n",
      "test loss is 0.0004743002056064585\n",
      "Batch: 33000,train loss is: 0.0002125517540281229\n",
      "test loss is 0.00038185701990388696\n",
      "Batch: 33100,train loss is: 0.0016743479137108835\n",
      "test loss is 0.0010853798580379062\n",
      "Batch: 33200,train loss is: 0.00013667572920670647\n",
      "test loss is 0.0006085423054718803\n",
      "Batch: 33300,train loss is: 0.00037121089289008995\n",
      "test loss is 0.00034307114702521424\n",
      "Batch: 33400,train loss is: 0.0018937556664883112\n",
      "test loss is 0.0015989428357488624\n",
      "Batch: 33500,train loss is: 0.0006453338065072086\n",
      "test loss is 0.0014962013293829232\n",
      "Batch: 33600,train loss is: 0.00036570603763940097\n",
      "test loss is 0.00046837379199383267\n",
      "Batch: 33700,train loss is: 0.0004058946697138478\n",
      "test loss is 0.0004195888240060408\n",
      "Batch: 33800,train loss is: 0.00046015299796096747\n",
      "test loss is 0.00031129766189800964\n",
      "Batch: 33900,train loss is: 0.0004243638393117966\n",
      "test loss is 0.000308619032377004\n",
      "-----------------------Epoch: 1----------------------------------\n",
      "Batch: 0,train loss is: 0.0003792031887262708\n",
      "test loss is 0.00040916578334274415\n",
      "Batch: 100,train loss is: 0.0004636231484160247\n",
      "test loss is 0.0002885976981798794\n",
      "Batch: 200,train loss is: 0.00017989237895684498\n",
      "test loss is 0.0002761808689550663\n",
      "Batch: 300,train loss is: 0.00017347718512893606\n",
      "test loss is 0.00029698282972556364\n",
      "Batch: 400,train loss is: 0.0006211984335910963\n",
      "test loss is 0.0011156651892139345\n",
      "Batch: 500,train loss is: 0.0002395538691404102\n",
      "test loss is 0.0006341766381257423\n",
      "Batch: 600,train loss is: 0.00012720402158219042\n",
      "test loss is 0.0003811528633317528\n",
      "Batch: 700,train loss is: 0.00020631428161051022\n",
      "test loss is 0.0003526804489259781\n",
      "Batch: 800,train loss is: 0.00016038627725882765\n",
      "test loss is 0.0002976582107152492\n",
      "Batch: 900,train loss is: 0.00029776755332218234\n",
      "test loss is 0.0003972583456615805\n",
      "Batch: 1000,train loss is: 0.0019264588158184191\n",
      "test loss is 0.0007547719027726534\n",
      "Batch: 1100,train loss is: 0.00031011017737506933\n",
      "test loss is 0.0003218278419951094\n",
      "Batch: 1200,train loss is: 0.00012430754479549888\n",
      "test loss is 0.00038795814333492313\n",
      "Batch: 1300,train loss is: 0.00022248176086189938\n",
      "test loss is 0.000309220997691371\n",
      "Batch: 1400,train loss is: 0.00029689342247760846\n",
      "test loss is 0.0004361395969135765\n",
      "Batch: 1500,train loss is: 0.00027241456299457664\n",
      "test loss is 0.000470762462266317\n",
      "Batch: 1600,train loss is: 0.0010640182747137764\n",
      "test loss is 0.0011080153015987652\n",
      "Batch: 1700,train loss is: 0.0007165241647594136\n",
      "test loss is 0.000651844293745175\n",
      "Batch: 1800,train loss is: 0.0004194837388149384\n",
      "test loss is 0.0012961342688185245\n",
      "Batch: 1900,train loss is: 0.0005638270406839518\n",
      "test loss is 0.002230301989115286\n",
      "Batch: 2000,train loss is: 0.0013643371895755928\n",
      "test loss is 0.0027593542487201568\n",
      "Batch: 2100,train loss is: 0.0002927380280780312\n",
      "test loss is 0.0003948751951188167\n",
      "Batch: 2200,train loss is: 0.0005198427837950911\n",
      "test loss is 0.00037687539515993156\n",
      "Batch: 2300,train loss is: 0.00034650633947837836\n",
      "test loss is 0.00029930814896108693\n",
      "Batch: 2400,train loss is: 0.00019762471744028996\n",
      "test loss is 0.0002998324313684463\n",
      "Batch: 2500,train loss is: 0.0005446129213578028\n",
      "test loss is 0.00034081170130202215\n",
      "Batch: 2600,train loss is: 0.0004146847271226346\n",
      "test loss is 0.0003888177304694533\n",
      "Batch: 2700,train loss is: 0.0001886121514210836\n",
      "test loss is 0.0002798827718761284\n",
      "Batch: 2800,train loss is: 0.00019438934452839664\n",
      "test loss is 0.0003405195999914784\n",
      "Batch: 2900,train loss is: 0.00031452281055419844\n",
      "test loss is 0.00032653733838300164\n",
      "Batch: 3000,train loss is: 0.0004085779997174847\n",
      "test loss is 0.0004154781074561405\n",
      "Batch: 3100,train loss is: 0.00022066738273793636\n",
      "test loss is 0.00031120617004753466\n",
      "Batch: 3200,train loss is: 0.0005126036430927538\n",
      "test loss is 0.00043636375540063153\n",
      "Batch: 3300,train loss is: 0.00019777061667894112\n",
      "test loss is 0.00030595394230236724\n",
      "Batch: 3400,train loss is: 0.0001611662762740278\n",
      "test loss is 0.00044481202021805704\n",
      "Batch: 3500,train loss is: 0.0007942856711110127\n",
      "test loss is 0.0006392348206403597\n",
      "Batch: 3600,train loss is: 0.0001694290886735324\n",
      "test loss is 0.0006430856736017166\n",
      "Batch: 3700,train loss is: 0.000986386811320135\n",
      "test loss is 0.0007490170460996138\n",
      "Batch: 3800,train loss is: 0.0006306235657760462\n",
      "test loss is 0.0006857846349255436\n",
      "Batch: 3900,train loss is: 0.0024795211283447755\n",
      "test loss is 0.0013157046322540404\n",
      "Batch: 4000,train loss is: 0.0003323064955732204\n",
      "test loss is 0.000744911533604411\n",
      "Batch: 4100,train loss is: 0.0002531843188932186\n",
      "test loss is 0.0006742516073913978\n",
      "Batch: 4200,train loss is: 0.0005058965894148559\n",
      "test loss is 0.0003981337564322862\n",
      "Batch: 4300,train loss is: 0.00025848468209561557\n",
      "test loss is 0.0005759452886674179\n",
      "Batch: 4400,train loss is: 0.0003023270704674504\n",
      "test loss is 0.00042091868705201234\n",
      "Batch: 4500,train loss is: 0.0008931082567785765\n",
      "test loss is 0.00031390732195331876\n",
      "Batch: 4600,train loss is: 0.0006480877990639469\n",
      "test loss is 0.0006042785273938549\n",
      "Batch: 4700,train loss is: 0.0006530595121777672\n",
      "test loss is 0.0003791442200987679\n",
      "Batch: 4800,train loss is: 0.0009679595237969942\n",
      "test loss is 0.0005683555286440371\n",
      "Batch: 4900,train loss is: 0.00030666194037661735\n",
      "test loss is 0.0004629432421164128\n",
      "Batch: 5000,train loss is: 0.00016219307128246237\n",
      "test loss is 0.0003689493228852227\n",
      "Batch: 5100,train loss is: 0.00022023316315635905\n",
      "test loss is 0.00032038274596546444\n",
      "Batch: 5200,train loss is: 0.000491905710586901\n",
      "test loss is 0.0005385683109827479\n",
      "Batch: 5300,train loss is: 0.0002822297308995219\n",
      "test loss is 0.000785227131446089\n",
      "Batch: 5400,train loss is: 0.0005781391350513964\n",
      "test loss is 0.00030960203276367916\n",
      "Batch: 5500,train loss is: 0.004028093978916429\n",
      "test loss is 0.00295473320061494\n",
      "Batch: 5600,train loss is: 0.0002620471337450502\n",
      "test loss is 0.0021020565898165816\n",
      "Batch: 5700,train loss is: 0.00048421464541078293\n",
      "test loss is 0.0003743044455748494\n",
      "Batch: 5800,train loss is: 0.0002834791468617592\n",
      "test loss is 0.00033802893741797445\n",
      "Batch: 5900,train loss is: 0.0011010801153947976\n",
      "test loss is 0.0004501681813739431\n",
      "Batch: 6000,train loss is: 0.0007047390440027931\n",
      "test loss is 0.00038849221597791046\n",
      "Batch: 6100,train loss is: 0.0004143350359805262\n",
      "test loss is 0.0006766281191097776\n",
      "Batch: 6200,train loss is: 0.0003192491363427363\n",
      "test loss is 0.0004172244311000878\n",
      "Batch: 6300,train loss is: 0.00028658881020689355\n",
      "test loss is 0.0010236852062472175\n",
      "Batch: 6400,train loss is: 0.0008089042973322829\n",
      "test loss is 0.0010837236935751333\n",
      "Batch: 6500,train loss is: 0.00017851268737857206\n",
      "test loss is 0.00033254850761742905\n",
      "Batch: 6600,train loss is: 0.0012288281004385244\n",
      "test loss is 0.0007260390676590722\n",
      "Batch: 6700,train loss is: 0.00011279987196661105\n",
      "test loss is 0.00039497534318993615\n",
      "Batch: 6800,train loss is: 0.0010495177592490428\n",
      "test loss is 0.0005578603590971078\n",
      "Batch: 6900,train loss is: 0.0005143646584425082\n",
      "test loss is 0.0004162163864177207\n",
      "Batch: 7000,train loss is: 0.0004703353689813946\n",
      "test loss is 0.0004271060598999448\n",
      "Batch: 7100,train loss is: 0.00029244225613817395\n",
      "test loss is 0.0009259384565342299\n",
      "Batch: 7200,train loss is: 0.00012892717817992953\n",
      "test loss is 0.00033344810392063183\n",
      "Batch: 7300,train loss is: 0.0008225671818830122\n",
      "test loss is 0.000762976598521169\n",
      "Batch: 7400,train loss is: 0.00014316309872543253\n",
      "test loss is 0.0005508200810361441\n",
      "Batch: 7500,train loss is: 0.0003591689788075909\n",
      "test loss is 0.00046092906788079534\n",
      "Batch: 7600,train loss is: 0.0004712410660179945\n",
      "test loss is 0.00043173302405050243\n",
      "Batch: 7700,train loss is: 0.00029684397785954863\n",
      "test loss is 0.0004167411069760951\n",
      "Batch: 7800,train loss is: 0.000860289459322231\n",
      "test loss is 0.0005374295397905697\n",
      "Batch: 7900,train loss is: 0.00027078944755436153\n",
      "test loss is 0.0004991206048717945\n",
      "Batch: 8000,train loss is: 0.00021270686779207575\n",
      "test loss is 0.00046415978053454\n",
      "Batch: 8100,train loss is: 0.0005052723912339243\n",
      "test loss is 0.0006778874807864424\n",
      "Batch: 8200,train loss is: 0.0006456145681192897\n",
      "test loss is 0.00042861413684039455\n",
      "Batch: 8300,train loss is: 0.00033056354619907173\n",
      "test loss is 0.0005715219485774587\n",
      "Batch: 8400,train loss is: 0.00036014245505817054\n",
      "test loss is 0.0008431416201781522\n",
      "Batch: 8500,train loss is: 0.0005550129382229692\n",
      "test loss is 0.0004192401574490392\n",
      "Batch: 8600,train loss is: 0.0005643169273271491\n",
      "test loss is 0.0010305352249680882\n",
      "Batch: 8700,train loss is: 0.0009721731000029175\n",
      "test loss is 0.0008152713099308481\n",
      "Batch: 8800,train loss is: 0.00034050874043487156\n",
      "test loss is 0.0005418331179160363\n",
      "Batch: 8900,train loss is: 0.0006515459221069815\n",
      "test loss is 0.00033598627929305535\n",
      "Batch: 9000,train loss is: 0.0002912895696997934\n",
      "test loss is 0.0004374988845424389\n",
      "Batch: 9100,train loss is: 0.00019850023283510723\n",
      "test loss is 0.000338021817776534\n",
      "Batch: 9200,train loss is: 0.0004450861317381774\n",
      "test loss is 0.0004943391399754977\n",
      "Batch: 9300,train loss is: 0.0022534069422933093\n",
      "test loss is 0.0007929389115483663\n",
      "Batch: 9400,train loss is: 0.0005360060351112686\n",
      "test loss is 0.00040334274127716644\n",
      "Batch: 9500,train loss is: 0.00041874229985312036\n",
      "test loss is 0.0006270437524099608\n",
      "Batch: 9600,train loss is: 0.00028015510163352015\n",
      "test loss is 0.00039183590576049313\n",
      "Batch: 9700,train loss is: 0.0009789304742561862\n",
      "test loss is 0.00042883436422233795\n",
      "Batch: 9800,train loss is: 0.00015325945884125903\n",
      "test loss is 0.0007128648365461535\n",
      "Batch: 9900,train loss is: 0.00036094966277792104\n",
      "test loss is 0.0008694459968113788\n",
      "Batch: 10000,train loss is: 0.00027808714331585126\n",
      "test loss is 0.000601061140345493\n",
      "Batch: 10100,train loss is: 0.00033774017863244483\n",
      "test loss is 0.0005314292653600137\n",
      "Batch: 10200,train loss is: 0.0017689731343532925\n",
      "test loss is 0.0004450431805066284\n",
      "Batch: 10300,train loss is: 0.00027564818630360294\n",
      "test loss is 0.0005398473601563136\n",
      "Batch: 10400,train loss is: 0.0001373329917716592\n",
      "test loss is 0.00044862190362238006\n",
      "Batch: 10500,train loss is: 0.0006956056434037862\n",
      "test loss is 0.0008168091563509931\n",
      "Batch: 10600,train loss is: 0.0002799476243067368\n",
      "test loss is 0.0003910735863866732\n",
      "Batch: 10700,train loss is: 0.0004831659639051189\n",
      "test loss is 0.000488968494838714\n",
      "Batch: 10800,train loss is: 0.0010198800884198167\n",
      "test loss is 0.003012018302561124\n",
      "Batch: 10900,train loss is: 0.00043742265400193765\n",
      "test loss is 0.00047144180058246893\n",
      "Batch: 11000,train loss is: 0.0005310478853958207\n",
      "test loss is 0.00042646197070550523\n",
      "Batch: 11100,train loss is: 0.00017934582778586592\n",
      "test loss is 0.000356199815425576\n",
      "Batch: 11200,train loss is: 0.00030559790278002365\n",
      "test loss is 0.0006661125409527318\n",
      "Batch: 11300,train loss is: 0.0009489165384661619\n",
      "test loss is 0.001766561400197901\n",
      "Batch: 11400,train loss is: 0.0005170429839611618\n",
      "test loss is 0.0007662946317969819\n",
      "Batch: 11500,train loss is: 0.00020239409543079822\n",
      "test loss is 0.00037713863910303884\n",
      "Batch: 11600,train loss is: 0.0007688277767348731\n",
      "test loss is 0.0005912599970162536\n",
      "Batch: 11700,train loss is: 0.00031771929221861633\n",
      "test loss is 0.000411412080144888\n",
      "Batch: 11800,train loss is: 0.0001565060400980705\n",
      "test loss is 0.00041354581159691654\n",
      "Batch: 11900,train loss is: 0.0007627046361077649\n",
      "test loss is 0.00041429582596772685\n",
      "Batch: 12000,train loss is: 0.001046397091382967\n",
      "test loss is 0.0007052454940234977\n",
      "Batch: 12100,train loss is: 0.001273242766853402\n",
      "test loss is 0.003112563307839503\n",
      "Batch: 12200,train loss is: 0.00022127475754036145\n",
      "test loss is 0.0010895052605055877\n",
      "Batch: 12300,train loss is: 0.0004972869608225092\n",
      "test loss is 0.0006259363140071477\n",
      "Batch: 12400,train loss is: 0.00023268165684739366\n",
      "test loss is 0.0003868450504906985\n",
      "Batch: 12500,train loss is: 0.0008366902120547506\n",
      "test loss is 0.0009900464426169174\n",
      "Batch: 12600,train loss is: 0.00024015781614409999\n",
      "test loss is 0.0003670824492739622\n",
      "Batch: 12700,train loss is: 0.00020561934336636235\n",
      "test loss is 0.000323941565198517\n",
      "Batch: 12800,train loss is: 0.00010092684723013032\n",
      "test loss is 0.0004854828963084167\n",
      "Batch: 12900,train loss is: 0.000228357734796291\n",
      "test loss is 0.0004864916130964127\n",
      "Batch: 13000,train loss is: 0.000394611159058606\n",
      "test loss is 0.0003308162963911883\n",
      "Batch: 13100,train loss is: 0.0005564526224478155\n",
      "test loss is 0.00025959275024737703\n",
      "Batch: 13200,train loss is: 0.00023172567890196514\n",
      "test loss is 0.0003656389746947314\n",
      "Batch: 13300,train loss is: 0.0009701524528279822\n",
      "test loss is 0.002827735748207715\n",
      "Batch: 13400,train loss is: 0.0006667596137563034\n",
      "test loss is 0.0011462192047548513\n",
      "Batch: 13500,train loss is: 0.0006556685860793314\n",
      "test loss is 0.0006450030010036886\n",
      "Batch: 13600,train loss is: 0.0017316531584332683\n",
      "test loss is 0.0019116334445909485\n",
      "Batch: 13700,train loss is: 0.0009633795656503414\n",
      "test loss is 0.0009181074923026042\n",
      "Batch: 13800,train loss is: 0.00019602489062647726\n",
      "test loss is 0.000485269206491431\n",
      "Batch: 13900,train loss is: 0.00017254754078681382\n",
      "test loss is 0.0003863090300867488\n",
      "Batch: 14000,train loss is: 0.0006247887843703129\n",
      "test loss is 0.0007551525927976062\n",
      "Batch: 14100,train loss is: 0.0005757287946575386\n",
      "test loss is 0.0008598421104338191\n",
      "Batch: 14200,train loss is: 0.00017434897648624755\n",
      "test loss is 0.0003194258490102632\n",
      "Batch: 14300,train loss is: 0.00014042936468071726\n",
      "test loss is 0.00036981868692462417\n",
      "Batch: 14400,train loss is: 0.0004574427492197736\n",
      "test loss is 0.0002665992164646828\n",
      "Batch: 14500,train loss is: 0.00021951458587827845\n",
      "test loss is 0.0002536184314947163\n",
      "Batch: 14600,train loss is: 0.0007012443800699794\n",
      "test loss is 0.0006992566518664498\n",
      "Batch: 14700,train loss is: 0.0016632511947744005\n",
      "test loss is 0.0012378825290474192\n",
      "Batch: 14800,train loss is: 0.0004656022369663392\n",
      "test loss is 0.0005551157985107041\n",
      "Batch: 14900,train loss is: 0.0002409788095402276\n",
      "test loss is 0.0003442367204980668\n",
      "Batch: 15000,train loss is: 0.0006451035247813766\n",
      "test loss is 0.00040558649147754555\n",
      "Batch: 15100,train loss is: 0.0002455449374982007\n",
      "test loss is 0.0005133220207873697\n",
      "Batch: 15200,train loss is: 0.0013008720605839639\n",
      "test loss is 0.00043902850814600547\n",
      "Batch: 15300,train loss is: 0.0005510728673175594\n",
      "test loss is 0.0004681647186286523\n",
      "Batch: 15400,train loss is: 0.0005440413977514086\n",
      "test loss is 0.0005710844474984244\n",
      "Batch: 15500,train loss is: 0.00046972413061356524\n",
      "test loss is 0.0005725789561058132\n",
      "Batch: 15600,train loss is: 0.004366302941181301\n",
      "test loss is 0.0013362933794341446\n",
      "Batch: 15700,train loss is: 0.000162935750789196\n",
      "test loss is 0.0007814578350895898\n",
      "Batch: 15800,train loss is: 0.00020933633263559205\n",
      "test loss is 0.0005279098138588937\n",
      "Batch: 15900,train loss is: 0.0012214160901886585\n",
      "test loss is 0.0004874467074754995\n",
      "Batch: 16000,train loss is: 0.000232691648132957\n",
      "test loss is 0.00033423510460269576\n",
      "Batch: 16100,train loss is: 0.0009604211509617537\n",
      "test loss is 0.00046750558193965033\n",
      "Batch: 16200,train loss is: 0.0005140701722390186\n",
      "test loss is 0.000599044388680921\n",
      "Batch: 16300,train loss is: 0.00011122678104108265\n",
      "test loss is 0.0004517783366805298\n",
      "Batch: 16400,train loss is: 0.00044493263536774896\n",
      "test loss is 0.00037760866455872135\n",
      "Batch: 16500,train loss is: 0.000493156380207117\n",
      "test loss is 0.0005133051930469513\n",
      "Batch: 16600,train loss is: 0.00016035142637265676\n",
      "test loss is 0.0002963300852795683\n",
      "Batch: 16700,train loss is: 0.000490660808165484\n",
      "test loss is 0.00040206426615871\n",
      "Batch: 16800,train loss is: 0.00024788015457342954\n",
      "test loss is 0.000420762800590401\n",
      "Batch: 16900,train loss is: 0.0014350453958991863\n",
      "test loss is 0.0010884304592946852\n",
      "Batch: 17000,train loss is: 0.000270934230557493\n",
      "test loss is 0.0004365178745042157\n",
      "Batch: 17100,train loss is: 0.0007456384816290948\n",
      "test loss is 0.0011728639429601658\n",
      "Batch: 17200,train loss is: 0.00036830873397519586\n",
      "test loss is 0.000490416519376264\n",
      "Batch: 17300,train loss is: 0.00020807444088455983\n",
      "test loss is 0.0006497238598311216\n",
      "Batch: 17400,train loss is: 0.00022708931421837214\n",
      "test loss is 0.0003980568170045681\n",
      "Batch: 17500,train loss is: 0.0005969003954395201\n",
      "test loss is 0.000522809229319609\n",
      "Batch: 17600,train loss is: 0.001031090529567144\n",
      "test loss is 0.0015019490499162927\n",
      "Batch: 17700,train loss is: 0.0002947603491646721\n",
      "test loss is 0.0005969933525450973\n",
      "Batch: 17800,train loss is: 0.0006890278881952442\n",
      "test loss is 0.0008505740277500528\n",
      "Batch: 17900,train loss is: 0.0005336961253605106\n",
      "test loss is 0.00031728625065525514\n",
      "Batch: 18000,train loss is: 0.0002841872437969\n",
      "test loss is 0.0003106305649708677\n",
      "Batch: 18100,train loss is: 0.00042755734192435814\n",
      "test loss is 0.0004955020550744929\n",
      "Batch: 18200,train loss is: 0.0003201926083226451\n",
      "test loss is 0.0005391771403977804\n",
      "Batch: 18300,train loss is: 0.0008740173510173722\n",
      "test loss is 0.00035594591812402015\n",
      "Batch: 18400,train loss is: 0.0003761487321853364\n",
      "test loss is 0.0006764580492707172\n",
      "Batch: 18500,train loss is: 0.0002623520318401599\n",
      "test loss is 0.00033226064995793146\n",
      "Batch: 18600,train loss is: 0.0004699497754704424\n",
      "test loss is 0.0008623356478995399\n",
      "Batch: 18700,train loss is: 0.0005284082769842256\n",
      "test loss is 0.0006273690760106833\n",
      "Batch: 18800,train loss is: 0.0005977710522814896\n",
      "test loss is 0.0004604114183659753\n",
      "Batch: 18900,train loss is: 0.0004929404119258228\n",
      "test loss is 0.00035478972120697857\n",
      "Batch: 19000,train loss is: 0.0009996045185123028\n",
      "test loss is 0.0006157491271486584\n",
      "Batch: 19100,train loss is: 0.0006038447571389724\n",
      "test loss is 0.00033786207164875895\n",
      "Batch: 19200,train loss is: 0.00020922674823639558\n",
      "test loss is 0.0003395129705750895\n",
      "Batch: 19300,train loss is: 0.0005658916798930554\n",
      "test loss is 0.0003764268371526166\n",
      "Batch: 19400,train loss is: 0.00023793309869964236\n",
      "test loss is 0.0009166234358517153\n",
      "Batch: 19500,train loss is: 0.00024625963647346717\n",
      "test loss is 0.00048727485890413537\n",
      "Batch: 19600,train loss is: 0.00046292662678099425\n",
      "test loss is 0.0006064735089956099\n",
      "Batch: 19700,train loss is: 0.0003817407014357369\n",
      "test loss is 0.0003422673246017209\n",
      "Batch: 19800,train loss is: 0.000462285284079938\n",
      "test loss is 0.000371619551984247\n",
      "Batch: 19900,train loss is: 0.0001633154014748176\n",
      "test loss is 0.0005595679488477832\n",
      "Batch: 20000,train loss is: 0.0007696247706499342\n",
      "test loss is 0.0006129551569759061\n",
      "Batch: 20100,train loss is: 0.00024287520911912223\n",
      "test loss is 0.0005412647686467871\n",
      "Batch: 20200,train loss is: 0.00013709084795392458\n",
      "test loss is 0.00029716338163299807\n",
      "Batch: 20300,train loss is: 0.0004244979238683875\n",
      "test loss is 0.0003348927590940275\n",
      "Batch: 20400,train loss is: 0.0010066744726555334\n",
      "test loss is 0.0013304536959614442\n",
      "Batch: 20500,train loss is: 0.00026980263627071014\n",
      "test loss is 0.0005453061859155671\n",
      "Batch: 20600,train loss is: 0.0016001179992260095\n",
      "test loss is 0.001029524867770499\n",
      "Batch: 20700,train loss is: 0.0005318083219429566\n",
      "test loss is 0.0004221952770296769\n",
      "Batch: 20800,train loss is: 0.0004553064111864777\n",
      "test loss is 0.0003607704643655912\n",
      "Batch: 20900,train loss is: 0.0002816195859348672\n",
      "test loss is 0.0003872691272584359\n",
      "Batch: 21000,train loss is: 0.0005403027782599316\n",
      "test loss is 0.0003297939251228317\n",
      "Batch: 21100,train loss is: 0.0004913442701599984\n",
      "test loss is 0.00047183815837760365\n",
      "Batch: 21200,train loss is: 0.0007842877569147814\n",
      "test loss is 0.0008380850316318766\n",
      "Batch: 21300,train loss is: 0.00037077006314450344\n",
      "test loss is 0.00047035720451198425\n",
      "Batch: 21400,train loss is: 0.00029847274869208845\n",
      "test loss is 0.00039759901026508354\n",
      "Batch: 21500,train loss is: 0.0002998736350467311\n",
      "test loss is 0.00044306626209631816\n",
      "Batch: 21600,train loss is: 0.000603335235886933\n",
      "test loss is 0.0005209888949704353\n",
      "Batch: 21700,train loss is: 0.00023700622831717466\n",
      "test loss is 0.00047834169092951506\n",
      "Batch: 21800,train loss is: 0.0003357314173280362\n",
      "test loss is 0.0002783298732379976\n",
      "Batch: 21900,train loss is: 0.00011719923896065079\n",
      "test loss is 0.0005094482761190936\n",
      "Batch: 22000,train loss is: 0.00030496149537686455\n",
      "test loss is 0.001528721081119149\n",
      "Batch: 22100,train loss is: 0.00021654299407734834\n",
      "test loss is 0.0007975598795894684\n",
      "Batch: 22200,train loss is: 0.0009642211742417582\n",
      "test loss is 0.0010282219098580985\n",
      "Batch: 22300,train loss is: 0.0004487834108120917\n",
      "test loss is 0.0007136806670818426\n",
      "Batch: 22400,train loss is: 0.00043033244706770806\n",
      "test loss is 0.00046640078992813403\n",
      "Batch: 22500,train loss is: 0.00043226105947157683\n",
      "test loss is 0.00045443422633868605\n",
      "Batch: 22600,train loss is: 0.00025544819652508144\n",
      "test loss is 0.00031410883884408014\n",
      "Batch: 22700,train loss is: 0.0004217550344894365\n",
      "test loss is 0.0008655832148254768\n",
      "Batch: 22800,train loss is: 0.0007020040729714029\n",
      "test loss is 0.0004047508902542895\n",
      "Batch: 22900,train loss is: 0.00013292045438918038\n",
      "test loss is 0.0003974092585102398\n",
      "Batch: 23000,train loss is: 0.000380403611336832\n",
      "test loss is 0.00030301818503131563\n",
      "Batch: 23100,train loss is: 0.0002511782506067824\n",
      "test loss is 0.0004923301066722356\n",
      "Batch: 23200,train loss is: 0.00010826653041755396\n",
      "test loss is 0.00038266026256645095\n",
      "Batch: 23300,train loss is: 0.0005454608627132926\n",
      "test loss is 0.000662708249197076\n",
      "Batch: 23400,train loss is: 0.00033325097871330983\n",
      "test loss is 0.00037815576231664945\n",
      "Batch: 23500,train loss is: 0.0006263968565525133\n",
      "test loss is 0.00036532378023116714\n",
      "Batch: 23600,train loss is: 0.00014308036518976484\n",
      "test loss is 0.00034314837516621037\n",
      "Batch: 23700,train loss is: 0.0003488935101937949\n",
      "test loss is 0.0007487407401300696\n",
      "Batch: 23800,train loss is: 0.0009118443698491732\n",
      "test loss is 0.0007670742845804402\n",
      "Batch: 23900,train loss is: 0.0004417760124999285\n",
      "test loss is 0.0005770314351725484\n",
      "Batch: 24000,train loss is: 0.0006417851786723972\n",
      "test loss is 0.0005341394254832595\n",
      "Batch: 24100,train loss is: 0.00026480654291561603\n",
      "test loss is 0.00026355682711687577\n",
      "Batch: 24200,train loss is: 0.0003976209618037217\n",
      "test loss is 0.000362320076186835\n",
      "Batch: 24300,train loss is: 0.0005371146264500427\n",
      "test loss is 0.00043788705242676115\n",
      "Batch: 24400,train loss is: 0.0003000455297468033\n",
      "test loss is 0.00048355574841934363\n",
      "Batch: 24500,train loss is: 0.0016013413141026682\n",
      "test loss is 0.0008869402235488775\n",
      "Batch: 24600,train loss is: 0.0003427944782528886\n",
      "test loss is 0.0013053338917533642\n",
      "Batch: 24700,train loss is: 0.0004955117505490074\n",
      "test loss is 0.0005440531218810196\n",
      "Batch: 24800,train loss is: 0.0003063571175819324\n",
      "test loss is 0.00037993701956294204\n",
      "Batch: 24900,train loss is: 0.00035947470610713027\n",
      "test loss is 0.00038065410896496695\n",
      "Batch: 25000,train loss is: 0.0003036641266581305\n",
      "test loss is 0.0003197809186759176\n",
      "Batch: 25100,train loss is: 0.0004198188531012477\n",
      "test loss is 0.00048334012461442145\n",
      "Batch: 25200,train loss is: 0.0012787750668579946\n",
      "test loss is 0.0008873919205079596\n",
      "Batch: 25300,train loss is: 0.0007331132033115963\n",
      "test loss is 0.0003733623030156727\n",
      "Batch: 25400,train loss is: 0.0006179214921153182\n",
      "test loss is 0.0005491147767540988\n",
      "Batch: 25500,train loss is: 0.00012685034570784562\n",
      "test loss is 0.0003267801497173179\n",
      "Batch: 25600,train loss is: 0.00029354714463618596\n",
      "test loss is 0.0003553050899841298\n",
      "Batch: 25700,train loss is: 0.001191358259447961\n",
      "test loss is 0.0003481684217880356\n",
      "Batch: 25800,train loss is: 0.0009634193744559452\n",
      "test loss is 0.0007121123605642396\n",
      "Batch: 25900,train loss is: 0.00014841244554274671\n",
      "test loss is 0.0005610246834344767\n",
      "Batch: 26000,train loss is: 0.0010688960913595006\n",
      "test loss is 0.001187294893552018\n",
      "Batch: 26100,train loss is: 0.00022489158158158753\n",
      "test loss is 0.00047325298898763616\n",
      "Batch: 26200,train loss is: 0.00024389565794759001\n",
      "test loss is 0.0005717861059644133\n",
      "Batch: 26300,train loss is: 0.00018081209228178963\n",
      "test loss is 0.0007469295787245633\n",
      "Batch: 26400,train loss is: 0.00026042018546063337\n",
      "test loss is 0.0003400208090223211\n",
      "Batch: 26500,train loss is: 0.0003616243663971974\n",
      "test loss is 0.00043321893316965995\n",
      "Batch: 26600,train loss is: 0.0004439169635573012\n",
      "test loss is 0.00048501738537307337\n",
      "Batch: 26700,train loss is: 0.00025402518133732163\n",
      "test loss is 0.0005674619412727417\n",
      "Batch: 26800,train loss is: 0.00026360661089868103\n",
      "test loss is 0.0006521242332477858\n",
      "Batch: 26900,train loss is: 0.0001802506755675182\n",
      "test loss is 0.0002869186186494467\n",
      "Batch: 27000,train loss is: 0.0009335738515446193\n",
      "test loss is 0.00036456056619345057\n",
      "Batch: 27100,train loss is: 0.00026592706996549\n",
      "test loss is 0.00044233212522016934\n",
      "Batch: 27200,train loss is: 0.00030172234779948527\n",
      "test loss is 0.0003854959603720083\n",
      "Batch: 27300,train loss is: 0.0006737350076052843\n",
      "test loss is 0.0007837367465663516\n",
      "Batch: 27400,train loss is: 0.00044953186716771234\n",
      "test loss is 0.0009382391377223474\n",
      "Batch: 27500,train loss is: 0.00023970477337099052\n",
      "test loss is 0.0005858801961422484\n",
      "Batch: 27600,train loss is: 0.0004960274717704147\n",
      "test loss is 0.0010323209423283022\n",
      "Batch: 27700,train loss is: 0.0005032112116669026\n",
      "test loss is 0.0006325584359784696\n",
      "Batch: 27800,train loss is: 0.0005574264090615598\n",
      "test loss is 0.0010843349898477731\n",
      "Batch: 27900,train loss is: 0.0006290766473584067\n",
      "test loss is 0.0003351897969523846\n",
      "Batch: 28000,train loss is: 0.0002906288091488671\n",
      "test loss is 0.00035190387176420724\n",
      "Batch: 28100,train loss is: 0.0003083694818362233\n",
      "test loss is 0.0004275463709178281\n",
      "Batch: 28200,train loss is: 0.0002061315748175465\n",
      "test loss is 0.0005001904425392625\n",
      "Batch: 28300,train loss is: 0.0004738695091256571\n",
      "test loss is 0.0002409890175800224\n",
      "Batch: 28400,train loss is: 0.00029494407414535267\n",
      "test loss is 0.00035088848324662164\n",
      "Batch: 28500,train loss is: 0.0007529623471617675\n",
      "test loss is 0.0006027632828588676\n",
      "Batch: 28600,train loss is: 0.0004917490802142665\n",
      "test loss is 0.0005007094926629582\n",
      "Batch: 28700,train loss is: 0.0003269145226440201\n",
      "test loss is 0.00045595414044012546\n",
      "Batch: 28800,train loss is: 0.00046856089893111653\n",
      "test loss is 0.00043278116513463104\n",
      "Batch: 28900,train loss is: 0.0008495596941937466\n",
      "test loss is 0.0005693880570984381\n",
      "Batch: 29000,train loss is: 0.00043122479783101026\n",
      "test loss is 0.00035772471021511135\n",
      "Batch: 29100,train loss is: 0.0007533239053818714\n",
      "test loss is 0.0009175622749367754\n",
      "Batch: 29200,train loss is: 0.0006698501105903194\n",
      "test loss is 0.0011807948799194474\n",
      "Batch: 29300,train loss is: 0.0006165334691848054\n",
      "test loss is 0.0007185018843370505\n",
      "Batch: 29400,train loss is: 0.0004556387131619833\n",
      "test loss is 0.000447841381709665\n",
      "Batch: 29500,train loss is: 0.0004194580301765343\n",
      "test loss is 0.0003625533644007914\n",
      "Batch: 29600,train loss is: 0.00057497813153433\n",
      "test loss is 0.00032240952979616983\n",
      "Batch: 29700,train loss is: 0.00033099757676014564\n",
      "test loss is 0.0005346869956149843\n",
      "Batch: 29800,train loss is: 0.0005255951150226553\n",
      "test loss is 0.00034715430402230935\n",
      "Batch: 29900,train loss is: 0.0010115249985840884\n",
      "test loss is 0.0004471749410231124\n",
      "Batch: 30000,train loss is: 0.0004470069676331415\n",
      "test loss is 0.0002984658407696501\n",
      "Batch: 30100,train loss is: 0.0007144988411498791\n",
      "test loss is 0.0012893743701765609\n",
      "Batch: 30200,train loss is: 0.00025535503258775023\n",
      "test loss is 0.0002690498366437843\n",
      "Batch: 30300,train loss is: 0.0004217095340454548\n",
      "test loss is 0.0008003323150925358\n",
      "Batch: 30400,train loss is: 0.0028560696622660768\n",
      "test loss is 0.006111409010785903\n",
      "Batch: 30500,train loss is: 0.0006686842974097981\n",
      "test loss is 0.0007230893685104692\n",
      "Batch: 30600,train loss is: 0.00020145080512101832\n",
      "test loss is 0.0003927122305526783\n",
      "Batch: 30700,train loss is: 0.0002843227609922185\n",
      "test loss is 0.0008676871772207055\n",
      "Batch: 30800,train loss is: 0.0001175804456767163\n",
      "test loss is 0.000277445709487336\n",
      "Batch: 30900,train loss is: 0.00013512381717793932\n",
      "test loss is 0.00039339416928845005\n",
      "Batch: 31000,train loss is: 0.00032651294751030025\n",
      "test loss is 0.0003814591043688082\n",
      "Batch: 31100,train loss is: 0.00019630895722912587\n",
      "test loss is 0.0004156050831881222\n",
      "Batch: 31200,train loss is: 0.00021705011672162641\n",
      "test loss is 0.00031708940690911046\n",
      "Batch: 31300,train loss is: 0.00023159443243916456\n",
      "test loss is 0.00027774521225778457\n",
      "Batch: 31400,train loss is: 0.0002435814057317495\n",
      "test loss is 0.0004719108845233542\n",
      "Batch: 31500,train loss is: 0.00038222975602582597\n",
      "test loss is 0.0018519565489209715\n",
      "Batch: 31600,train loss is: 0.000889852869789559\n",
      "test loss is 0.0005022898695551864\n",
      "Batch: 31700,train loss is: 0.00016415747282080414\n",
      "test loss is 0.00046478118166266554\n",
      "Batch: 31800,train loss is: 0.00046152585129957186\n",
      "test loss is 0.00032364840758128746\n",
      "Batch: 31900,train loss is: 0.0013319912770582989\n",
      "test loss is 0.0011008133308649682\n",
      "Batch: 32000,train loss is: 0.0024286763297209574\n",
      "test loss is 0.0003655769993790403\n",
      "Batch: 32100,train loss is: 0.00016831298393476374\n",
      "test loss is 0.00032511897932050555\n",
      "Batch: 32200,train loss is: 0.0003157186467413393\n",
      "test loss is 0.00039622115000405615\n",
      "Batch: 32300,train loss is: 0.00041875322333879365\n",
      "test loss is 0.0005894014391465132\n",
      "Batch: 32400,train loss is: 0.00023127387977085517\n",
      "test loss is 0.0004582497111496594\n",
      "Batch: 32500,train loss is: 0.00064129468648272\n",
      "test loss is 0.0003250775072694853\n",
      "Batch: 32600,train loss is: 0.00013750267434290046\n",
      "test loss is 0.00037629965102881893\n",
      "Batch: 32700,train loss is: 0.00038360119101328957\n",
      "test loss is 0.00031805292945649835\n",
      "Batch: 32800,train loss is: 0.00029091121146248265\n",
      "test loss is 0.0002986311213439469\n",
      "Batch: 32900,train loss is: 0.00018828904963468177\n",
      "test loss is 0.00038404092699832347\n",
      "Batch: 33000,train loss is: 0.00021304147406073085\n",
      "test loss is 0.00036463515222799636\n",
      "Batch: 33100,train loss is: 0.0015307607530837633\n",
      "test loss is 0.0011467555027777244\n",
      "Batch: 33200,train loss is: 0.00010918268568459546\n",
      "test loss is 0.00044509975459481937\n",
      "Batch: 33300,train loss is: 0.0002644035792562588\n",
      "test loss is 0.0003115494752355455\n",
      "Batch: 33400,train loss is: 0.0018609883568986578\n",
      "test loss is 0.0017207522292733177\n",
      "Batch: 33500,train loss is: 0.0005298996287728681\n",
      "test loss is 0.0020587239919870907\n",
      "Batch: 33600,train loss is: 0.00046455074329111235\n",
      "test loss is 0.0004941391627421956\n",
      "Batch: 33700,train loss is: 0.00041399147895476105\n",
      "test loss is 0.0003570908736571535\n",
      "Batch: 33800,train loss is: 0.00041806841217581476\n",
      "test loss is 0.000304187538080687\n",
      "Batch: 33900,train loss is: 0.0004790935308541527\n",
      "test loss is 0.0002893488572101159\n",
      "-----------------------Epoch: 2----------------------------------\n",
      "Batch: 0,train loss is: 0.0003211386703929051\n",
      "test loss is 0.0003338099237735663\n",
      "Batch: 100,train loss is: 0.00041872965146749086\n",
      "test loss is 0.0002445403036710776\n",
      "Batch: 200,train loss is: 0.00018550752825303624\n",
      "test loss is 0.0002524016347662949\n",
      "Batch: 300,train loss is: 0.00012833701321573728\n",
      "test loss is 0.00025059477747214247\n",
      "Batch: 400,train loss is: 0.00047569876304533686\n",
      "test loss is 0.0008544321979564993\n",
      "Batch: 500,train loss is: 0.0002491292425565927\n",
      "test loss is 0.0005066234534704111\n",
      "Batch: 600,train loss is: 8.550441806651477e-05\n",
      "test loss is 0.00033765635031820194\n",
      "Batch: 700,train loss is: 0.00017135935877557\n",
      "test loss is 0.00033130859200781136\n",
      "Batch: 800,train loss is: 0.00022553150585436863\n",
      "test loss is 0.00025993501563721113\n",
      "Batch: 900,train loss is: 0.0002611629168540715\n",
      "test loss is 0.00034203122717159476\n",
      "Batch: 1000,train loss is: 0.000991894203898253\n",
      "test loss is 0.0005181016010132883\n",
      "Batch: 1100,train loss is: 0.00024403796773864023\n",
      "test loss is 0.00025860283549798745\n",
      "Batch: 1200,train loss is: 0.00010610014793676552\n",
      "test loss is 0.00041281284680609847\n",
      "Batch: 1300,train loss is: 0.00030017823544759215\n",
      "test loss is 0.00030701862596706984\n",
      "Batch: 1400,train loss is: 0.00026973449407966416\n",
      "test loss is 0.0005780623340316381\n",
      "Batch: 1500,train loss is: 0.0002668897054973589\n",
      "test loss is 0.0006276000469959812\n",
      "Batch: 1600,train loss is: 0.00028231770074236537\n",
      "test loss is 0.0003078244654607257\n",
      "Batch: 1700,train loss is: 0.0003805041209051744\n",
      "test loss is 0.00042556688017648774\n",
      "Batch: 1800,train loss is: 0.0007904918995785388\n",
      "test loss is 0.00037780314994584417\n",
      "Batch: 1900,train loss is: 0.00030305934420039177\n",
      "test loss is 0.0014589911615933233\n",
      "Batch: 2000,train loss is: 0.0013984391046405326\n",
      "test loss is 0.0016189285668669463\n",
      "Batch: 2100,train loss is: 0.00021412750705371183\n",
      "test loss is 0.00036211458836800695\n",
      "Batch: 2200,train loss is: 0.0004227419792070433\n",
      "test loss is 0.00029717229077946486\n",
      "Batch: 2300,train loss is: 0.00013824966086067416\n",
      "test loss is 0.00029478508999511834\n",
      "Batch: 2400,train loss is: 0.000131818497056331\n",
      "test loss is 0.0002797039544721527\n",
      "Batch: 2500,train loss is: 0.0004706451249326455\n",
      "test loss is 0.0002514735100120096\n",
      "Batch: 2600,train loss is: 0.0005175719806998811\n",
      "test loss is 0.0004247527557973867\n",
      "Batch: 2700,train loss is: 0.00017808981970406123\n",
      "test loss is 0.00028120876750564733\n",
      "Batch: 2800,train loss is: 0.0001783492354917305\n",
      "test loss is 0.0003256568160436229\n",
      "Batch: 2900,train loss is: 0.00034648247089742387\n",
      "test loss is 0.0005572211901158518\n",
      "Batch: 3000,train loss is: 0.0004138276451565937\n",
      "test loss is 0.0007432421950117027\n",
      "Batch: 3100,train loss is: 0.00022875687531295874\n",
      "test loss is 0.0003860357454634281\n",
      "Batch: 3200,train loss is: 0.0003769593927874725\n",
      "test loss is 0.0005223337331666743\n",
      "Batch: 3300,train loss is: 0.0001348685018455051\n",
      "test loss is 0.00027605796986988575\n",
      "Batch: 3400,train loss is: 0.00017791308924683193\n",
      "test loss is 0.0005802472230934745\n",
      "Batch: 3500,train loss is: 0.001518036773984178\n",
      "test loss is 0.0014198539054371283\n",
      "Batch: 3600,train loss is: 0.00025601411359657414\n",
      "test loss is 0.0008156020019344759\n",
      "Batch: 3700,train loss is: 0.00032649079372160897\n",
      "test loss is 0.00045590663594691834\n",
      "Batch: 3800,train loss is: 0.00044928775360757655\n",
      "test loss is 0.000369321360933307\n",
      "Batch: 3900,train loss is: 0.0011080351063163017\n",
      "test loss is 0.0007633520925915263\n",
      "Batch: 4000,train loss is: 0.00023740723760570939\n",
      "test loss is 0.000791538460775114\n",
      "Batch: 4100,train loss is: 0.00021006710192963103\n",
      "test loss is 0.0005858745976504176\n",
      "Batch: 4200,train loss is: 0.0003515178357268344\n",
      "test loss is 0.00031605811664898905\n",
      "Batch: 4300,train loss is: 0.00016070554584539888\n",
      "test loss is 0.0004823058779857199\n",
      "Batch: 4400,train loss is: 0.0004798374872063793\n",
      "test loss is 0.000482378008633193\n",
      "Batch: 4500,train loss is: 0.0009000006303991097\n",
      "test loss is 0.00030493357689590053\n",
      "Batch: 4600,train loss is: 0.0010553634787311196\n",
      "test loss is 0.0008242840612198598\n",
      "Batch: 4700,train loss is: 0.0005466913080919518\n",
      "test loss is 0.0003975341930588999\n",
      "Batch: 4800,train loss is: 0.000799689579368211\n",
      "test loss is 0.0003274564495831232\n",
      "Batch: 4900,train loss is: 0.0003997105438681028\n",
      "test loss is 0.0005328740158828303\n",
      "Batch: 5000,train loss is: 0.00012973000278874587\n",
      "test loss is 0.0002922164060054283\n",
      "Batch: 5100,train loss is: 0.0001646242074330331\n",
      "test loss is 0.00037920915724250833\n",
      "Batch: 5200,train loss is: 0.0005077442605275277\n",
      "test loss is 0.0005168259667159838\n",
      "Batch: 5300,train loss is: 0.00035559653485121086\n",
      "test loss is 0.0011649841084479932\n",
      "Batch: 5400,train loss is: 0.00030854276277033314\n",
      "test loss is 0.0003240202599631252\n",
      "Batch: 5500,train loss is: 0.0017218480838680835\n",
      "test loss is 0.0017388332673774296\n",
      "Batch: 5600,train loss is: 0.00024108661365113772\n",
      "test loss is 0.001507713277756228\n",
      "Batch: 5700,train loss is: 0.0003472280335795292\n",
      "test loss is 0.00035077208532081173\n",
      "Batch: 5800,train loss is: 0.00021670624251889106\n",
      "test loss is 0.00027301200957114536\n",
      "Batch: 5900,train loss is: 0.0008903415468638723\n",
      "test loss is 0.0004547673022753891\n",
      "Batch: 6000,train loss is: 0.0006976508267767417\n",
      "test loss is 0.00035983817290684424\n",
      "Batch: 6100,train loss is: 0.0001657567445724733\n",
      "test loss is 0.0004482146641085088\n",
      "Batch: 6200,train loss is: 0.0002220745287096564\n",
      "test loss is 0.00033968054806633876\n",
      "Batch: 6300,train loss is: 0.00032813199647720344\n",
      "test loss is 0.0010166107762313006\n",
      "Batch: 6400,train loss is: 0.0005671265302765186\n",
      "test loss is 0.0007651329735682173\n",
      "Batch: 6500,train loss is: 0.00018347621694424663\n",
      "test loss is 0.00027484381367876454\n",
      "Batch: 6600,train loss is: 0.0010694054033108344\n",
      "test loss is 0.0004444039540386901\n",
      "Batch: 6700,train loss is: 8.463792740122263e-05\n",
      "test loss is 0.0003093822514986488\n",
      "Batch: 6800,train loss is: 0.0006241006086542866\n",
      "test loss is 0.00044140742594503423\n",
      "Batch: 6900,train loss is: 0.0006196889942560239\n",
      "test loss is 0.00035708652260743693\n",
      "Batch: 7000,train loss is: 0.0004976129738920283\n",
      "test loss is 0.00040633737535975266\n",
      "Batch: 7100,train loss is: 0.0002910371614221372\n",
      "test loss is 0.0007738083937458018\n",
      "Batch: 7200,train loss is: 0.00012452043945381316\n",
      "test loss is 0.0002877422514130463\n",
      "Batch: 7300,train loss is: 0.0005930772578531752\n",
      "test loss is 0.0006352207021499389\n",
      "Batch: 7400,train loss is: 0.000137094367800873\n",
      "test loss is 0.0005411570523463345\n",
      "Batch: 7500,train loss is: 0.0002641560219056791\n",
      "test loss is 0.0003878259882607\n",
      "Batch: 7600,train loss is: 0.0004312544413719751\n",
      "test loss is 0.0003665639881913462\n",
      "Batch: 7700,train loss is: 0.0004578209361431381\n",
      "test loss is 0.0004574365739681874\n",
      "Batch: 7800,train loss is: 0.0007603367934731795\n",
      "test loss is 0.0005078396911525153\n",
      "Batch: 7900,train loss is: 0.00017347285657755662\n",
      "test loss is 0.0003536265380911209\n",
      "Batch: 8000,train loss is: 0.00012725494718400987\n",
      "test loss is 0.0003873635842691774\n",
      "Batch: 8100,train loss is: 0.0004527300926784608\n",
      "test loss is 0.0007951109132246913\n",
      "Batch: 8200,train loss is: 0.0008022786503356898\n",
      "test loss is 0.00043454216432640703\n",
      "Batch: 8300,train loss is: 0.00014164709098354826\n",
      "test loss is 0.00036698000025457114\n",
      "Batch: 8400,train loss is: 0.00047308788906323516\n",
      "test loss is 0.0008193934772559388\n",
      "Batch: 8500,train loss is: 0.00043854455939780224\n",
      "test loss is 0.00039165633245925635\n",
      "Batch: 8600,train loss is: 0.00043517431416719945\n",
      "test loss is 0.0008555383147690253\n",
      "Batch: 8700,train loss is: 0.0013747799829548052\n",
      "test loss is 0.0009015928379198923\n",
      "Batch: 8800,train loss is: 0.0003888773939448397\n",
      "test loss is 0.00045101054168413993\n",
      "Batch: 8900,train loss is: 0.0005353198790428302\n",
      "test loss is 0.00032553735870590524\n",
      "Batch: 9000,train loss is: 0.00022049323913050608\n",
      "test loss is 0.0003737936733559956\n",
      "Batch: 9100,train loss is: 0.00014372424445977094\n",
      "test loss is 0.0003055673568421391\n",
      "Batch: 9200,train loss is: 0.0003156458523188261\n",
      "test loss is 0.0004704171179398986\n",
      "Batch: 9300,train loss is: 0.002229730998714661\n",
      "test loss is 0.0008016702216816513\n",
      "Batch: 9400,train loss is: 0.0005617578219989409\n",
      "test loss is 0.00039692520465545926\n",
      "Batch: 9500,train loss is: 0.0004344182396684914\n",
      "test loss is 0.0005764931340368588\n",
      "Batch: 9600,train loss is: 0.00021689239219011643\n",
      "test loss is 0.0003139057660034678\n",
      "Batch: 9700,train loss is: 0.0005628951501238095\n",
      "test loss is 0.0004522647536566192\n",
      "Batch: 9800,train loss is: 0.00015856138560646683\n",
      "test loss is 0.0006616490358252599\n",
      "Batch: 9900,train loss is: 0.00022349609723349697\n",
      "test loss is 0.0004992553924373778\n",
      "Batch: 10000,train loss is: 0.0007651197450325566\n",
      "test loss is 0.0004890331481729443\n",
      "Batch: 10100,train loss is: 0.0003506944996093779\n",
      "test loss is 0.0004085484420874003\n",
      "Batch: 10200,train loss is: 0.0009156778379856129\n",
      "test loss is 0.00033392584933181234\n",
      "Batch: 10300,train loss is: 0.00034334335723504474\n",
      "test loss is 0.00046286572680142007\n",
      "Batch: 10400,train loss is: 0.00014169236520074623\n",
      "test loss is 0.0005271693236954431\n",
      "Batch: 10500,train loss is: 0.0005948876542956051\n",
      "test loss is 0.0006372219711377082\n",
      "Batch: 10600,train loss is: 0.00032106570773677044\n",
      "test loss is 0.0005038130250351985\n",
      "Batch: 10700,train loss is: 0.0009398082686210943\n",
      "test loss is 0.0006533029948365687\n",
      "Batch: 10800,train loss is: 0.0013202394548153277\n",
      "test loss is 0.003040995518052341\n",
      "Batch: 10900,train loss is: 0.00042449542921704395\n",
      "test loss is 0.0004005735872957564\n",
      "Batch: 11000,train loss is: 0.0006191720184426679\n",
      "test loss is 0.00041688279058273034\n",
      "Batch: 11100,train loss is: 0.0001541293690007164\n",
      "test loss is 0.00028338884901495706\n",
      "Batch: 11200,train loss is: 0.00024417791668578077\n",
      "test loss is 0.0005189089288242843\n",
      "Batch: 11300,train loss is: 0.0009417249233703912\n",
      "test loss is 0.0019778978484161535\n",
      "Batch: 11400,train loss is: 0.0005093048932913451\n",
      "test loss is 0.000501134984684372\n",
      "Batch: 11500,train loss is: 0.00013024837439763704\n",
      "test loss is 0.0005219624027086204\n",
      "Batch: 11600,train loss is: 0.0007277241771519803\n",
      "test loss is 0.00045013251701861597\n",
      "Batch: 11700,train loss is: 0.00028110858086154963\n",
      "test loss is 0.0003132282823776517\n",
      "Batch: 11800,train loss is: 0.0001359500128889986\n",
      "test loss is 0.000319414828088116\n",
      "Batch: 11900,train loss is: 0.0007205067603264004\n",
      "test loss is 0.00031924532304591186\n",
      "Batch: 12000,train loss is: 0.0009020741799598546\n",
      "test loss is 0.0006167298975490931\n",
      "Batch: 12100,train loss is: 0.0012637417970830262\n",
      "test loss is 0.002641729978209654\n",
      "Batch: 12200,train loss is: 0.00017595892289654853\n",
      "test loss is 0.0007992178233110569\n",
      "Batch: 12300,train loss is: 0.000544100391461625\n",
      "test loss is 0.00029784223579557646\n",
      "Batch: 12400,train loss is: 0.0003232751750700356\n",
      "test loss is 0.0005166069960958537\n",
      "Batch: 12500,train loss is: 0.0008681821721513661\n",
      "test loss is 0.0009734429337743421\n",
      "Batch: 12600,train loss is: 0.00026576400185493855\n",
      "test loss is 0.0003195275738296956\n",
      "Batch: 12700,train loss is: 0.00010529275078266318\n",
      "test loss is 0.00024639002101712465\n",
      "Batch: 12800,train loss is: 9.194160887546638e-05\n",
      "test loss is 0.00032126137372875436\n",
      "Batch: 12900,train loss is: 0.00038478726803801\n",
      "test loss is 0.00048340875878104\n",
      "Batch: 13000,train loss is: 0.0004651316690704159\n",
      "test loss is 0.0003129655076780318\n",
      "Batch: 13100,train loss is: 0.000496019075575382\n",
      "test loss is 0.0002320730513960551\n",
      "Batch: 13200,train loss is: 0.00021414245123338853\n",
      "test loss is 0.00029581658640496583\n",
      "Batch: 13300,train loss is: 0.001220937211661754\n",
      "test loss is 0.003027440506463512\n",
      "Batch: 13400,train loss is: 0.000654087236022826\n",
      "test loss is 0.0013876966684405528\n",
      "Batch: 13500,train loss is: 0.000529476778517736\n",
      "test loss is 0.0006414611646499585\n",
      "Batch: 13600,train loss is: 0.002504918456994128\n",
      "test loss is 0.002473672620677363\n",
      "Batch: 13700,train loss is: 0.0004621868316227975\n",
      "test loss is 0.0005080361485741633\n",
      "Batch: 13800,train loss is: 0.0005590376913405931\n",
      "test loss is 0.0005990144765454902\n",
      "Batch: 13900,train loss is: 0.00015482807452732924\n",
      "test loss is 0.000370622434916145\n",
      "Batch: 14000,train loss is: 0.0003339940489184959\n",
      "test loss is 0.0007454145588077053\n",
      "Batch: 14100,train loss is: 0.0005625534733883768\n",
      "test loss is 0.0006131397045544782\n",
      "Batch: 14200,train loss is: 0.00012488674364350612\n",
      "test loss is 0.0002646801228192567\n",
      "Batch: 14300,train loss is: 0.00010460997612227523\n",
      "test loss is 0.0003502090524482321\n",
      "Batch: 14400,train loss is: 0.0003556301928739926\n",
      "test loss is 0.00023410197399689436\n",
      "Batch: 14500,train loss is: 0.0002447554545855374\n",
      "test loss is 0.00025967552284596244\n",
      "Batch: 14600,train loss is: 0.0008189600257863017\n",
      "test loss is 0.0007297519451669825\n",
      "Batch: 14700,train loss is: 0.0012756391857443377\n",
      "test loss is 0.0009373371143168325\n",
      "Batch: 14800,train loss is: 0.00036661216009572053\n",
      "test loss is 0.0004144109470997903\n",
      "Batch: 14900,train loss is: 0.0002623858133888677\n",
      "test loss is 0.0002982277477350979\n",
      "Batch: 15000,train loss is: 0.0005585364132349446\n",
      "test loss is 0.0003555143888761251\n",
      "Batch: 15100,train loss is: 0.00020433083856650935\n",
      "test loss is 0.00033864905162082524\n",
      "Batch: 15200,train loss is: 0.000790808389047698\n",
      "test loss is 0.0003336295225115596\n",
      "Batch: 15300,train loss is: 0.000753392612014169\n",
      "test loss is 0.0004939190645037379\n",
      "Batch: 15400,train loss is: 0.00046704079942984844\n",
      "test loss is 0.0005237369782148414\n",
      "Batch: 15500,train loss is: 0.0005133135268299221\n",
      "test loss is 0.0006184201855069763\n",
      "Batch: 15600,train loss is: 0.004305569459417658\n",
      "test loss is 0.0014885256751155632\n",
      "Batch: 15700,train loss is: 0.00023801964613166828\n",
      "test loss is 0.0005386903683680326\n",
      "Batch: 15800,train loss is: 0.00020147513341458535\n",
      "test loss is 0.0005968241758761408\n",
      "Batch: 15900,train loss is: 0.0008336526056816301\n",
      "test loss is 0.00045020992074334927\n",
      "Batch: 16000,train loss is: 0.0001797503067892635\n",
      "test loss is 0.0002675801965443739\n",
      "Batch: 16100,train loss is: 0.000694657561990653\n",
      "test loss is 0.0003745899173189992\n",
      "Batch: 16200,train loss is: 0.0004706573038010649\n",
      "test loss is 0.0005367882856137758\n",
      "Batch: 16300,train loss is: 0.00010537792543046466\n",
      "test loss is 0.00041458794239990107\n",
      "Batch: 16400,train loss is: 0.00039963915325205967\n",
      "test loss is 0.0003094405968438486\n",
      "Batch: 16500,train loss is: 0.00038718381913574115\n",
      "test loss is 0.0004427636498911212\n",
      "Batch: 16600,train loss is: 0.0001182774916272378\n",
      "test loss is 0.00027998765621240065\n",
      "Batch: 16700,train loss is: 0.0005552022176964235\n",
      "test loss is 0.00040470754736817187\n",
      "Batch: 16800,train loss is: 0.0002054216362506821\n",
      "test loss is 0.00036614063334941934\n",
      "Batch: 16900,train loss is: 0.0011559474203466486\n",
      "test loss is 0.0009144303711255701\n",
      "Batch: 17000,train loss is: 0.0002178457702890332\n",
      "test loss is 0.0004828029843665753\n",
      "Batch: 17100,train loss is: 0.0004998101635800546\n",
      "test loss is 0.0009481669593200295\n",
      "Batch: 17200,train loss is: 0.00044651932959883877\n",
      "test loss is 0.0005158474566736211\n",
      "Batch: 17300,train loss is: 0.00022271171239234239\n",
      "test loss is 0.0003853506699947545\n",
      "Batch: 17400,train loss is: 0.00038949192263454315\n",
      "test loss is 0.00040103820049188903\n",
      "Batch: 17500,train loss is: 0.0005962312008289646\n",
      "test loss is 0.0004387948017088524\n",
      "Batch: 17600,train loss is: 0.0007810625490876838\n",
      "test loss is 0.0011384014795649947\n",
      "Batch: 17700,train loss is: 0.0002447621447302681\n",
      "test loss is 0.0004698884538522286\n",
      "Batch: 17800,train loss is: 0.0006004018263145068\n",
      "test loss is 0.0007762627533779257\n",
      "Batch: 17900,train loss is: 0.0006274948842255378\n",
      "test loss is 0.0003244854103436827\n",
      "Batch: 18000,train loss is: 0.00028342652257880725\n",
      "test loss is 0.00031185679351975013\n",
      "Batch: 18100,train loss is: 0.0003319366321779509\n",
      "test loss is 0.0004498779414551108\n",
      "Batch: 18200,train loss is: 0.00029894187266667145\n",
      "test loss is 0.0004666138557557542\n",
      "Batch: 18300,train loss is: 0.0007035977797131249\n",
      "test loss is 0.00032079377202594593\n",
      "Batch: 18400,train loss is: 0.00034117752001693757\n",
      "test loss is 0.0006897591192826357\n",
      "Batch: 18500,train loss is: 0.0005682748654382236\n",
      "test loss is 0.0003873731792145876\n",
      "Batch: 18600,train loss is: 0.00044603179508576914\n",
      "test loss is 0.0008397045602660607\n",
      "Batch: 18700,train loss is: 0.0004669475309828008\n",
      "test loss is 0.0005883210199887002\n",
      "Batch: 18800,train loss is: 0.00029454196451766853\n",
      "test loss is 0.0003116797170195221\n",
      "Batch: 18900,train loss is: 0.000572368817837535\n",
      "test loss is 0.0003066886648909031\n",
      "Batch: 19000,train loss is: 0.0013962681889237692\n",
      "test loss is 0.0007180845290650229\n",
      "Batch: 19100,train loss is: 0.00046083597605192715\n",
      "test loss is 0.0003149401354243813\n",
      "Batch: 19200,train loss is: 0.0001737271308254544\n",
      "test loss is 0.00031616671744461825\n",
      "Batch: 19300,train loss is: 0.0005856825260342045\n",
      "test loss is 0.0003757407893138656\n",
      "Batch: 19400,train loss is: 0.00022619167457089062\n",
      "test loss is 0.0009163698499510014\n",
      "Batch: 19500,train loss is: 0.0002686966544125829\n",
      "test loss is 0.0003926455669052997\n",
      "Batch: 19600,train loss is: 0.0004691273277269161\n",
      "test loss is 0.0005870085525140003\n",
      "Batch: 19700,train loss is: 0.0002734026979679111\n",
      "test loss is 0.0003029444379244715\n",
      "Batch: 19800,train loss is: 0.0002990087540872739\n",
      "test loss is 0.00026130958094342134\n",
      "Batch: 19900,train loss is: 0.0002734418914388668\n",
      "test loss is 0.0005328849485549172\n",
      "Batch: 20000,train loss is: 0.0005910242042180271\n",
      "test loss is 0.0005185267769976865\n",
      "Batch: 20100,train loss is: 0.0003038034259583166\n",
      "test loss is 0.0005851300579921988\n",
      "Batch: 20200,train loss is: 0.00019708438299534238\n",
      "test loss is 0.00028486935194998927\n",
      "Batch: 20300,train loss is: 0.0007818877315831513\n",
      "test loss is 0.00040464233430257356\n",
      "Batch: 20400,train loss is: 0.0004425750873811709\n",
      "test loss is 0.0010564572835854644\n",
      "Batch: 20500,train loss is: 0.0006168974846869791\n",
      "test loss is 0.0010205284318863324\n",
      "Batch: 20600,train loss is: 0.0010625175990791101\n",
      "test loss is 0.0009047121286602293\n",
      "Batch: 20700,train loss is: 0.0005245116742846911\n",
      "test loss is 0.0004859536847506243\n",
      "Batch: 20800,train loss is: 0.0005819535387871961\n",
      "test loss is 0.00033158994730577747\n",
      "Batch: 20900,train loss is: 0.00022319432363837296\n",
      "test loss is 0.00032021765896345875\n",
      "Batch: 21000,train loss is: 0.0006091396584032107\n",
      "test loss is 0.00036292489400342016\n",
      "Batch: 21100,train loss is: 0.00031391287852950556\n",
      "test loss is 0.0003976225444387387\n",
      "Batch: 21200,train loss is: 0.0007987686907453803\n",
      "test loss is 0.0008168502554565074\n",
      "Batch: 21300,train loss is: 0.0002512606065568879\n",
      "test loss is 0.00043022485687873745\n",
      "Batch: 21400,train loss is: 0.0002680629147749878\n",
      "test loss is 0.0003128112899927428\n",
      "Batch: 21500,train loss is: 0.00022647905827319486\n",
      "test loss is 0.00046707470558002073\n",
      "Batch: 21600,train loss is: 0.00047337857319061953\n",
      "test loss is 0.00054918536706746\n",
      "Batch: 21700,train loss is: 0.00031039742787238923\n",
      "test loss is 0.0005751303162098097\n",
      "Batch: 21800,train loss is: 0.00031864789629039\n",
      "test loss is 0.00025225516106552046\n",
      "Batch: 21900,train loss is: 0.00010220171081030717\n",
      "test loss is 0.00043558245382940765\n",
      "Batch: 22000,train loss is: 0.00043305432523237965\n",
      "test loss is 0.001547501398292281\n",
      "Batch: 22100,train loss is: 0.00019124092439191636\n",
      "test loss is 0.0007315341760394205\n",
      "Batch: 22200,train loss is: 0.0007039170804714175\n",
      "test loss is 0.0007941721277857612\n",
      "Batch: 22300,train loss is: 0.00033264972171507016\n",
      "test loss is 0.000534868119682002\n",
      "Batch: 22400,train loss is: 0.00037300505730326097\n",
      "test loss is 0.00044996960455033903\n",
      "Batch: 22500,train loss is: 0.00030841081324845334\n",
      "test loss is 0.0003664454695283278\n",
      "Batch: 22600,train loss is: 0.0002392751676981761\n",
      "test loss is 0.0003033914989226975\n",
      "Batch: 22700,train loss is: 0.00032090848207473995\n",
      "test loss is 0.0007234518444465527\n",
      "Batch: 22800,train loss is: 0.000705609162146915\n",
      "test loss is 0.0003932966200985268\n",
      "Batch: 22900,train loss is: 0.00014064774623006034\n",
      "test loss is 0.00036545252519312087\n",
      "Batch: 23000,train loss is: 0.0003203782564806212\n",
      "test loss is 0.000255746293996171\n",
      "Batch: 23100,train loss is: 0.0002427290986788711\n",
      "test loss is 0.0005373240874313078\n",
      "Batch: 23200,train loss is: 0.00011080917481239139\n",
      "test loss is 0.0003047246207773123\n",
      "Batch: 23300,train loss is: 0.00044099847657609934\n",
      "test loss is 0.0005466782185802448\n",
      "Batch: 23400,train loss is: 0.0003822459206436233\n",
      "test loss is 0.000385220993404518\n",
      "Batch: 23500,train loss is: 0.0006685452733954509\n",
      "test loss is 0.00035801763030637927\n",
      "Batch: 23600,train loss is: 0.00014244898063065374\n",
      "test loss is 0.0003177809853931548\n",
      "Batch: 23700,train loss is: 0.00031163927976059165\n",
      "test loss is 0.0005149487319150677\n",
      "Batch: 23800,train loss is: 0.000673573687827931\n",
      "test loss is 0.000671069003769558\n",
      "Batch: 23900,train loss is: 0.00045312970831591604\n",
      "test loss is 0.0005324117039769203\n",
      "Batch: 24000,train loss is: 0.00048351729421853884\n",
      "test loss is 0.0005085438860679222\n",
      "Batch: 24100,train loss is: 0.0002090078205823638\n",
      "test loss is 0.0002974080044315777\n",
      "Batch: 24200,train loss is: 0.0003917801483070433\n",
      "test loss is 0.00030372823578300333\n",
      "Batch: 24300,train loss is: 0.0004120582751762174\n",
      "test loss is 0.0004826696674255066\n",
      "Batch: 24400,train loss is: 0.0002983529361862058\n",
      "test loss is 0.00035941380271275607\n",
      "Batch: 24500,train loss is: 0.0015273376592430219\n",
      "test loss is 0.0007249276433949754\n",
      "Batch: 24600,train loss is: 0.00029416291060350844\n",
      "test loss is 0.0012784421559412416\n",
      "Batch: 24700,train loss is: 0.00045256316959649425\n",
      "test loss is 0.0005271834791752925\n",
      "Batch: 24800,train loss is: 0.00025127176621813263\n",
      "test loss is 0.0003661466017647762\n",
      "Batch: 24900,train loss is: 0.0002742621755052934\n",
      "test loss is 0.0003341372144910801\n",
      "Batch: 25000,train loss is: 0.0002576760977499796\n",
      "test loss is 0.0002747431290945248\n",
      "Batch: 25100,train loss is: 0.0005452104051261725\n",
      "test loss is 0.00044723276454150984\n",
      "Batch: 25200,train loss is: 0.0015532852521721449\n",
      "test loss is 0.0009285768324867362\n",
      "Batch: 25300,train loss is: 0.0007373315861642415\n",
      "test loss is 0.0003238240349960977\n",
      "Batch: 25400,train loss is: 0.0006153940391095214\n",
      "test loss is 0.0005540164701114719\n",
      "Batch: 25500,train loss is: 0.00012905923628580494\n",
      "test loss is 0.0002992192811686943\n",
      "Batch: 25600,train loss is: 0.00023901100363499274\n",
      "test loss is 0.0003300994499973232\n",
      "Batch: 25700,train loss is: 0.001022899314951591\n",
      "test loss is 0.00030817556130281255\n",
      "Batch: 25800,train loss is: 0.0009083665528279254\n",
      "test loss is 0.0006889082806948491\n",
      "Batch: 25900,train loss is: 0.00012693334639407325\n",
      "test loss is 0.0004950498744530828\n",
      "Batch: 26000,train loss is: 0.0011823128788458516\n",
      "test loss is 0.0010966215112644291\n",
      "Batch: 26100,train loss is: 0.0002294525028606298\n",
      "test loss is 0.00035371564456931247\n",
      "Batch: 26200,train loss is: 0.00022176861144867873\n",
      "test loss is 0.00036594759888601607\n",
      "Batch: 26300,train loss is: 0.00018908124190397996\n",
      "test loss is 0.00047220778077335844\n",
      "Batch: 26400,train loss is: 0.00016913370975095614\n",
      "test loss is 0.00028050650606142175\n",
      "Batch: 26500,train loss is: 0.0004588101453940922\n",
      "test loss is 0.0007199393267059109\n",
      "Batch: 26600,train loss is: 0.0004537750723190509\n",
      "test loss is 0.00047741202183685435\n",
      "Batch: 26700,train loss is: 0.00023371909005737867\n",
      "test loss is 0.0005130544880428256\n",
      "Batch: 26800,train loss is: 0.00024154503765036714\n",
      "test loss is 0.0006017315253536269\n",
      "Batch: 26900,train loss is: 0.00015980403540700991\n",
      "test loss is 0.0002739416478762806\n",
      "Batch: 27000,train loss is: 0.0010538275259287045\n",
      "test loss is 0.00035635781773222874\n",
      "Batch: 27100,train loss is: 0.00023177853265975466\n",
      "test loss is 0.00031775197312179605\n",
      "Batch: 27200,train loss is: 0.0003253901851604945\n",
      "test loss is 0.00035872529449756593\n",
      "Batch: 27300,train loss is: 0.000389499271942152\n",
      "test loss is 0.0005128599010892477\n",
      "Batch: 27400,train loss is: 0.00045537641662884954\n",
      "test loss is 0.0007340565052334425\n",
      "Batch: 27500,train loss is: 0.00013854718934147922\n",
      "test loss is 0.0005163045849362984\n",
      "Batch: 27600,train loss is: 0.0004011892652695756\n",
      "test loss is 0.0008636636544376955\n",
      "Batch: 27700,train loss is: 0.0003764680287599541\n",
      "test loss is 0.0006245996731167132\n",
      "Batch: 27800,train loss is: 0.0005546959478830142\n",
      "test loss is 0.00122288129942304\n",
      "Batch: 27900,train loss is: 0.0006620813996745036\n",
      "test loss is 0.0003104925888117521\n",
      "Batch: 28000,train loss is: 0.000297184327449488\n",
      "test loss is 0.0003150729717410778\n",
      "Batch: 28100,train loss is: 0.00023309931746413602\n",
      "test loss is 0.00043060243027694643\n",
      "Batch: 28200,train loss is: 0.0001519323540820391\n",
      "test loss is 0.000538250354524011\n",
      "Batch: 28300,train loss is: 0.0004153864809085435\n",
      "test loss is 0.0002339101182074116\n",
      "Batch: 28400,train loss is: 0.000316005237444807\n",
      "test loss is 0.0003333077740346855\n",
      "Batch: 28500,train loss is: 0.0008387461187894853\n",
      "test loss is 0.000580610417323852\n",
      "Batch: 28600,train loss is: 0.0004435409915369209\n",
      "test loss is 0.0003319380868199945\n",
      "Batch: 28700,train loss is: 0.00031492558777551123\n",
      "test loss is 0.0003797508230800624\n",
      "Batch: 28800,train loss is: 0.00047773796171125176\n",
      "test loss is 0.00031716853593864296\n",
      "Batch: 28900,train loss is: 0.000900920161347557\n",
      "test loss is 0.00048598354326103186\n",
      "Batch: 29000,train loss is: 0.00035067636325090203\n",
      "test loss is 0.00038466885873224396\n",
      "Batch: 29100,train loss is: 0.000381766173956449\n",
      "test loss is 0.00048091879033065524\n",
      "Batch: 29200,train loss is: 0.0007295442060385705\n",
      "test loss is 0.001487448796396767\n",
      "Batch: 29300,train loss is: 0.0006731600306876564\n",
      "test loss is 0.0010540371356655396\n",
      "Batch: 29400,train loss is: 0.0009614722264650186\n",
      "test loss is 0.0004097450383902554\n",
      "Batch: 29500,train loss is: 0.0006510994844960063\n",
      "test loss is 0.0008486001118184157\n",
      "Batch: 29600,train loss is: 0.0004169823279322352\n",
      "test loss is 0.00026768918524663535\n",
      "Batch: 29700,train loss is: 0.00014482579813914794\n",
      "test loss is 0.00024928433706042586\n",
      "Batch: 29800,train loss is: 0.0004830334555343656\n",
      "test loss is 0.0003335886318030771\n",
      "Batch: 29900,train loss is: 0.0011423536952254794\n",
      "test loss is 0.000342022441466703\n",
      "Batch: 30000,train loss is: 0.00024635223842259837\n",
      "test loss is 0.0002520181734159956\n",
      "Batch: 30100,train loss is: 0.0004970178925746211\n",
      "test loss is 0.0009579663793920925\n",
      "Batch: 30200,train loss is: 0.00021336435899943428\n",
      "test loss is 0.0002408061402294603\n",
      "Batch: 30300,train loss is: 0.00029751135483180384\n",
      "test loss is 0.000833933679239359\n",
      "Batch: 30400,train loss is: 0.0015997599981356838\n",
      "test loss is 0.0037109120482393624\n",
      "Batch: 30500,train loss is: 0.0004317612374849372\n",
      "test loss is 0.0005470365629107376\n",
      "Batch: 30600,train loss is: 0.0001595429106744364\n",
      "test loss is 0.0003495142383539706\n",
      "Batch: 30700,train loss is: 0.00028482105298998885\n",
      "test loss is 0.0007097248228764851\n",
      "Batch: 30800,train loss is: 0.00018979317752795206\n",
      "test loss is 0.0003056542459642554\n",
      "Batch: 30900,train loss is: 0.00013854103649487638\n",
      "test loss is 0.00040038554071755066\n",
      "Batch: 31000,train loss is: 0.000368820139811218\n",
      "test loss is 0.00043815797418394754\n",
      "Batch: 31100,train loss is: 0.00018939643325326114\n",
      "test loss is 0.0004178037820899899\n",
      "Batch: 31200,train loss is: 0.0002195916972241572\n",
      "test loss is 0.0003340841332092471\n",
      "Batch: 31300,train loss is: 0.00025047977295686536\n",
      "test loss is 0.00041103509100378624\n",
      "Batch: 31400,train loss is: 0.00013637343979790116\n",
      "test loss is 0.0003674955368218804\n",
      "Batch: 31500,train loss is: 0.000337035250211493\n",
      "test loss is 0.0017243073073571888\n",
      "Batch: 31600,train loss is: 0.0013381486561602435\n",
      "test loss is 0.0007185399610823838\n",
      "Batch: 31700,train loss is: 0.0002760941303009577\n",
      "test loss is 0.0005189210451647853\n",
      "Batch: 31800,train loss is: 0.0003992976984490177\n",
      "test loss is 0.00038410293696134476\n",
      "Batch: 31900,train loss is: 0.001341770965528153\n",
      "test loss is 0.00129732304376116\n",
      "Batch: 32000,train loss is: 0.002592244284886144\n",
      "test loss is 0.00040954349495138083\n",
      "Batch: 32100,train loss is: 0.00016530991902401456\n",
      "test loss is 0.00029712744202442894\n",
      "Batch: 32200,train loss is: 0.000301679232086369\n",
      "test loss is 0.0003407586375638962\n",
      "Batch: 32300,train loss is: 0.000379168604496055\n",
      "test loss is 0.00045470570678040283\n",
      "Batch: 32400,train loss is: 0.000142409172922327\n",
      "test loss is 0.0003704032779257866\n",
      "Batch: 32500,train loss is: 0.0006835962949414932\n",
      "test loss is 0.0003372031629597245\n",
      "Batch: 32600,train loss is: 0.00015133002082088799\n",
      "test loss is 0.0003130192596228195\n",
      "Batch: 32700,train loss is: 0.00035843664373380477\n",
      "test loss is 0.0002637286555928154\n",
      "Batch: 32800,train loss is: 0.0002519104607766761\n",
      "test loss is 0.000229952383833664\n",
      "Batch: 32900,train loss is: 0.0001545123756367053\n",
      "test loss is 0.0002972040232162499\n",
      "Batch: 33000,train loss is: 0.00031346239886270716\n",
      "test loss is 0.00040215053403069325\n",
      "Batch: 33100,train loss is: 0.0015382910394475756\n",
      "test loss is 0.0012648352718017469\n",
      "Batch: 33200,train loss is: 0.00014365794491080783\n",
      "test loss is 0.00037376372280116597\n",
      "Batch: 33300,train loss is: 0.00019786259021511412\n",
      "test loss is 0.00026369884606727997\n",
      "Batch: 33400,train loss is: 0.0010020364852991151\n",
      "test loss is 0.0011997148079146013\n",
      "Batch: 33500,train loss is: 0.00046405407510670676\n",
      "test loss is 0.0017286647855262035\n",
      "Batch: 33600,train loss is: 0.00048154098378503995\n",
      "test loss is 0.0004424280846761995\n",
      "Batch: 33700,train loss is: 0.00038898326886809833\n",
      "test loss is 0.0003080468950642678\n",
      "Batch: 33800,train loss is: 0.00041776871176929867\n",
      "test loss is 0.0002810769849684238\n",
      "Batch: 33900,train loss is: 0.0005018127597204625\n",
      "test loss is 0.00025837655837646854\n",
      "-----------------------Epoch: 3----------------------------------\n",
      "Batch: 0,train loss is: 0.0002665835926412641\n",
      "test loss is 0.00037027881034996376\n",
      "Batch: 100,train loss is: 0.000458278489149023\n",
      "test loss is 0.0002418079469430328\n",
      "Batch: 200,train loss is: 0.00015419893436172092\n",
      "test loss is 0.00023158177132550186\n",
      "Batch: 300,train loss is: 0.00014693241773581042\n",
      "test loss is 0.0002479323484233693\n",
      "Batch: 400,train loss is: 0.00035873677099386106\n",
      "test loss is 0.000765995409540037\n",
      "Batch: 500,train loss is: 0.0002580917392753058\n",
      "test loss is 0.000443394683221715\n",
      "Batch: 600,train loss is: 8.280458003460282e-05\n",
      "test loss is 0.00031927620754797545\n",
      "Batch: 700,train loss is: 0.0001539846273359905\n",
      "test loss is 0.0003158858873000411\n",
      "Batch: 800,train loss is: 0.0002767901995962722\n",
      "test loss is 0.00025968745332036755\n",
      "Batch: 900,train loss is: 0.0002742560794463108\n",
      "test loss is 0.0002915377395048152\n",
      "Batch: 1000,train loss is: 0.0008154912175641112\n",
      "test loss is 0.000490716239633928\n",
      "Batch: 1100,train loss is: 0.0002275718826393583\n",
      "test loss is 0.00022993635507182267\n",
      "Batch: 1200,train loss is: 0.00011209334763899765\n",
      "test loss is 0.00044371290756389017\n",
      "Batch: 1300,train loss is: 0.00025186380295945496\n",
      "test loss is 0.00033721625426350597\n",
      "Batch: 1400,train loss is: 0.000282382299908944\n",
      "test loss is 0.0006108435849427975\n",
      "Batch: 1500,train loss is: 0.0003113412979802312\n",
      "test loss is 0.000640568842511189\n",
      "Batch: 1600,train loss is: 0.0007597116987999103\n",
      "test loss is 0.0005301785301658227\n",
      "Batch: 1700,train loss is: 0.00035846002983840346\n",
      "test loss is 0.0003050409947210588\n",
      "Batch: 1800,train loss is: 0.0007358573866013071\n",
      "test loss is 0.00046598531473803075\n",
      "Batch: 1900,train loss is: 0.00045433543165843604\n",
      "test loss is 0.0008762069292805194\n",
      "Batch: 2000,train loss is: 0.0004831967697495209\n",
      "test loss is 0.00042538951532587694\n",
      "Batch: 2100,train loss is: 0.00031344056215553307\n",
      "test loss is 0.00047205427974909056\n",
      "Batch: 2200,train loss is: 0.00024344209964862525\n",
      "test loss is 0.00024579323054845447\n",
      "Batch: 2300,train loss is: 0.00021715107481097906\n",
      "test loss is 0.0002769160319082662\n",
      "Batch: 2400,train loss is: 0.00012130731445290148\n",
      "test loss is 0.00026190287360737747\n",
      "Batch: 2500,train loss is: 0.0011397344527550998\n",
      "test loss is 0.0007844106302762375\n",
      "Batch: 2600,train loss is: 0.00015154853659176757\n",
      "test loss is 0.0003567748000264618\n",
      "Batch: 2700,train loss is: 0.0002907896366788299\n",
      "test loss is 0.0003447248318201819\n",
      "Batch: 2800,train loss is: 0.00014356242234152313\n",
      "test loss is 0.0003363904587869097\n",
      "Batch: 2900,train loss is: 0.000467159924453336\n",
      "test loss is 0.0004510792705765233\n",
      "Batch: 3000,train loss is: 0.0003224304574065083\n",
      "test loss is 0.0004888093669189067\n",
      "Batch: 3100,train loss is: 0.00025166701186969697\n",
      "test loss is 0.0003127222010038704\n",
      "Batch: 3200,train loss is: 0.000877630371501298\n",
      "test loss is 0.0011229238361928316\n",
      "Batch: 3300,train loss is: 0.00021629507878334243\n",
      "test loss is 0.000401378962228077\n",
      "Batch: 3400,train loss is: 0.0008114490923181674\n",
      "test loss is 0.0012477520156428754\n",
      "Batch: 3500,train loss is: 0.0004415371301238322\n",
      "test loss is 0.001211437912653951\n",
      "Batch: 3600,train loss is: 0.0001180627601032309\n",
      "test loss is 0.0008559318735000655\n",
      "Batch: 3700,train loss is: 0.00028182931338275284\n",
      "test loss is 0.0004200672734323307\n",
      "Batch: 3800,train loss is: 0.00029141300362384804\n",
      "test loss is 0.00024388692322054658\n",
      "Batch: 3900,train loss is: 0.00033885046545928664\n",
      "test loss is 0.00038579551896880036\n",
      "Batch: 4000,train loss is: 0.00040144364782276665\n",
      "test loss is 0.000428978359941571\n",
      "Batch: 4100,train loss is: 0.00017506026509836448\n",
      "test loss is 0.0002989133776904794\n",
      "Batch: 4200,train loss is: 0.00025764398360285205\n",
      "test loss is 0.00037704576225323054\n",
      "Batch: 4300,train loss is: 0.0001449236337164184\n",
      "test loss is 0.00025509722342461483\n",
      "Batch: 4400,train loss is: 0.0009465344535778696\n",
      "test loss is 0.0005198302519804975\n",
      "Batch: 4500,train loss is: 0.0002426209152723001\n",
      "test loss is 0.00026722023953810515\n",
      "Batch: 4600,train loss is: 0.0002720579335570861\n",
      "test loss is 0.00032044696171368\n",
      "Batch: 4700,train loss is: 0.00034107590014774886\n",
      "test loss is 0.00027754191374757166\n",
      "Batch: 4800,train loss is: 0.00058469680351821\n",
      "test loss is 0.0002691547197573664\n",
      "Batch: 4900,train loss is: 0.00033934134023974735\n",
      "test loss is 0.000489299660471051\n",
      "Batch: 5000,train loss is: 0.00012653895452042642\n",
      "test loss is 0.0002736668009385578\n",
      "Batch: 5100,train loss is: 0.00020655837828416416\n",
      "test loss is 0.0003891237144458245\n",
      "Batch: 5200,train loss is: 0.000605524513621427\n",
      "test loss is 0.000479620596108623\n",
      "Batch: 5300,train loss is: 0.00039516120296568586\n",
      "test loss is 0.0006999222176721159\n",
      "Batch: 5400,train loss is: 0.00022829531793096652\n",
      "test loss is 0.00030789324950544295\n",
      "Batch: 5500,train loss is: 0.0006586925518222785\n",
      "test loss is 0.0005807152537262995\n",
      "Batch: 5600,train loss is: 0.00028088407859162644\n",
      "test loss is 0.0014581144685591055\n",
      "Batch: 5700,train loss is: 0.0003335811365399581\n",
      "test loss is 0.000400870359664635\n",
      "Batch: 5800,train loss is: 0.00024038715951123994\n",
      "test loss is 0.00024972668684665743\n",
      "Batch: 5900,train loss is: 0.0006538090501597803\n",
      "test loss is 0.00045522734370701716\n",
      "Batch: 6000,train loss is: 0.0011928305575828462\n",
      "test loss is 0.00039593790269971366\n",
      "Batch: 6100,train loss is: 0.00011339190790237455\n",
      "test loss is 0.0003310004560538108\n",
      "Batch: 6200,train loss is: 0.0003820117480314999\n",
      "test loss is 0.00029397591721123126\n",
      "Batch: 6300,train loss is: 0.00037448395618012444\n",
      "test loss is 0.0008137516430458794\n",
      "Batch: 6400,train loss is: 0.0003689960883555859\n",
      "test loss is 0.0005740713095544116\n",
      "Batch: 6500,train loss is: 0.0001775811203095591\n",
      "test loss is 0.0002726673338468877\n",
      "Batch: 6600,train loss is: 0.0009780841909150394\n",
      "test loss is 0.00042202016420935597\n",
      "Batch: 6700,train loss is: 7.250053268099515e-05\n",
      "test loss is 0.0002575195291728475\n",
      "Batch: 6800,train loss is: 0.00044703527488168857\n",
      "test loss is 0.0003533541841899909\n",
      "Batch: 6900,train loss is: 0.0008370570584172444\n",
      "test loss is 0.0002946505606442562\n",
      "Batch: 7000,train loss is: 0.00034879799800979663\n",
      "test loss is 0.0003427968124703721\n",
      "Batch: 7100,train loss is: 0.00022742787181543608\n",
      "test loss is 0.000582316247206596\n",
      "Batch: 7200,train loss is: 0.00010516269557553163\n",
      "test loss is 0.000253564924976921\n",
      "Batch: 7300,train loss is: 0.0006146158686123349\n",
      "test loss is 0.0006495426899595609\n",
      "Batch: 7400,train loss is: 0.00015341669255622276\n",
      "test loss is 0.000486392691263442\n",
      "Batch: 7500,train loss is: 0.00028150820801312374\n",
      "test loss is 0.0003176976171626251\n",
      "Batch: 7600,train loss is: 0.0003835627209140231\n",
      "test loss is 0.0002853029380826212\n",
      "Batch: 7700,train loss is: 0.0007909822280529512\n",
      "test loss is 0.00042412560286011716\n",
      "Batch: 7800,train loss is: 0.0005514055542572952\n",
      "test loss is 0.00044980942334568\n",
      "Batch: 7900,train loss is: 0.00018820323023137892\n",
      "test loss is 0.00028391399058598277\n",
      "Batch: 8000,train loss is: 8.69330606933716e-05\n",
      "test loss is 0.00024001902445485145\n",
      "Batch: 8100,train loss is: 0.00038906885524229003\n",
      "test loss is 0.0007047629093676732\n",
      "Batch: 8200,train loss is: 0.0007025646156364592\n",
      "test loss is 0.00035266519527060936\n",
      "Batch: 8300,train loss is: 0.0001066579630168855\n",
      "test loss is 0.0002972602149354168\n",
      "Batch: 8400,train loss is: 0.0005893185790679663\n",
      "test loss is 0.0008082746731023671\n",
      "Batch: 8500,train loss is: 0.000359744129679025\n",
      "test loss is 0.00038162128697489557\n",
      "Batch: 8600,train loss is: 0.00043462913780639744\n",
      "test loss is 0.000917859933094458\n",
      "Batch: 8700,train loss is: 0.0021979048158955517\n",
      "test loss is 0.0013474403716314195\n",
      "Batch: 8800,train loss is: 0.0012490987755011753\n",
      "test loss is 0.0005298934516502099\n",
      "Batch: 8900,train loss is: 0.0004291602504122601\n",
      "test loss is 0.00038891284717906544\n",
      "Batch: 9000,train loss is: 9.204586838787043e-05\n",
      "test loss is 0.000248787186776986\n",
      "Batch: 9100,train loss is: 0.00014706936754002605\n",
      "test loss is 0.00025416555721739526\n",
      "Batch: 9200,train loss is: 0.0002557858969059541\n",
      "test loss is 0.00031679077523578313\n",
      "Batch: 9300,train loss is: 0.0015938034793556092\n",
      "test loss is 0.0007129431992748109\n",
      "Batch: 9400,train loss is: 0.000490774273409596\n",
      "test loss is 0.00032439843176899837\n",
      "Batch: 9500,train loss is: 0.00041265873200824703\n",
      "test loss is 0.0005077940439798012\n",
      "Batch: 9600,train loss is: 0.00012421361073919806\n",
      "test loss is 0.00024179913791663583\n",
      "Batch: 9700,train loss is: 0.0005108684903547636\n",
      "test loss is 0.0003947685723140989\n",
      "Batch: 9800,train loss is: 0.00019453754592423358\n",
      "test loss is 0.0005487267696152396\n",
      "Batch: 9900,train loss is: 0.000157512447479986\n",
      "test loss is 0.0003790037129799198\n",
      "Batch: 10000,train loss is: 0.0016621448541652068\n",
      "test loss is 0.0005679767977319967\n",
      "Batch: 10100,train loss is: 0.0003947330060574647\n",
      "test loss is 0.00027427119919906613\n",
      "Batch: 10200,train loss is: 0.0005371557420562198\n",
      "test loss is 0.0002812784031603489\n",
      "Batch: 10300,train loss is: 0.00024457771063700605\n",
      "test loss is 0.0002917159091090513\n",
      "Batch: 10400,train loss is: 0.0002368921079216552\n",
      "test loss is 0.0004522161404601187\n",
      "Batch: 10500,train loss is: 0.0007859724507109675\n",
      "test loss is 0.0005624086781995732\n",
      "Batch: 10600,train loss is: 0.00035962123121737036\n",
      "test loss is 0.00046430098460446806\n",
      "Batch: 10700,train loss is: 0.0015006678859297996\n",
      "test loss is 0.0016812572737790292\n",
      "Batch: 10800,train loss is: 0.0005193077694241957\n",
      "test loss is 0.0011315114896641115\n",
      "Batch: 10900,train loss is: 0.0004224805472471296\n",
      "test loss is 0.00046837153059523304\n",
      "Batch: 11000,train loss is: 0.0009172221963181848\n",
      "test loss is 0.0005844697192347587\n",
      "Batch: 11100,train loss is: 0.00015387703096370392\n",
      "test loss is 0.0002744136071370097\n",
      "Batch: 11200,train loss is: 0.00025127723289509346\n",
      "test loss is 0.00031138630399634413\n",
      "Batch: 11300,train loss is: 0.00048329074020814526\n",
      "test loss is 0.001120994881925223\n",
      "Batch: 11400,train loss is: 0.00036934936685817496\n",
      "test loss is 0.0003792560544521965\n",
      "Batch: 11500,train loss is: 0.0003426880378579332\n",
      "test loss is 0.0005862779046387008\n",
      "Batch: 11600,train loss is: 0.0005922386172949789\n",
      "test loss is 0.00033496486957143915\n",
      "Batch: 11700,train loss is: 0.00033987903283692894\n",
      "test loss is 0.0003204500849941531\n",
      "Batch: 11800,train loss is: 0.00014468735440778394\n",
      "test loss is 0.0003708954839561979\n",
      "Batch: 11900,train loss is: 0.0005221648526684985\n",
      "test loss is 0.0003434328449423062\n",
      "Batch: 12000,train loss is: 0.0009072104968469621\n",
      "test loss is 0.0008228954655701471\n",
      "Batch: 12100,train loss is: 0.0017388087475568522\n",
      "test loss is 0.002097400066612688\n",
      "Batch: 12200,train loss is: 0.00015630523228335501\n",
      "test loss is 0.0005910976087675119\n",
      "Batch: 12300,train loss is: 0.00035809026168318915\n",
      "test loss is 0.00024933675800842297\n",
      "Batch: 12400,train loss is: 0.00027217329822361763\n",
      "test loss is 0.0004360798342730728\n",
      "Batch: 12500,train loss is: 0.0008664870252091325\n",
      "test loss is 0.0008909395154854388\n",
      "Batch: 12600,train loss is: 0.00023155422392287574\n",
      "test loss is 0.00028247188370647574\n",
      "Batch: 12700,train loss is: 9.896079836181486e-05\n",
      "test loss is 0.00024631249603017884\n",
      "Batch: 12800,train loss is: 9.711268080646954e-05\n",
      "test loss is 0.00028749476771856045\n",
      "Batch: 12900,train loss is: 0.00029442184939519795\n",
      "test loss is 0.0003208519708592809\n",
      "Batch: 13000,train loss is: 0.0004693428418097209\n",
      "test loss is 0.00031417183314254266\n",
      "Batch: 13100,train loss is: 0.0005599917207544365\n",
      "test loss is 0.00021125589111996887\n",
      "Batch: 13200,train loss is: 0.00019410427692746646\n",
      "test loss is 0.00022765525167280442\n",
      "Batch: 13300,train loss is: 0.0011531529217029234\n",
      "test loss is 0.002807885589491248\n",
      "Batch: 13400,train loss is: 0.0006130608773678999\n",
      "test loss is 0.0009362281128530951\n",
      "Batch: 13500,train loss is: 0.0004624166592018811\n",
      "test loss is 0.0005079177017670969\n",
      "Batch: 13600,train loss is: 0.0026726013641050674\n",
      "test loss is 0.002405411802060446\n",
      "Batch: 13700,train loss is: 0.000409883528301923\n",
      "test loss is 0.0004944118056790456\n",
      "Batch: 13800,train loss is: 0.0004109643267705159\n",
      "test loss is 0.0005271162639964919\n",
      "Batch: 13900,train loss is: 0.00013069502614592514\n",
      "test loss is 0.0003711756374095606\n",
      "Batch: 14000,train loss is: 0.00021374292772497784\n",
      "test loss is 0.0007087188492704425\n",
      "Batch: 14100,train loss is: 0.0005036088185508112\n",
      "test loss is 0.0005108436881928499\n",
      "Batch: 14200,train loss is: 0.00012096663016132021\n",
      "test loss is 0.00023011072801216295\n",
      "Batch: 14300,train loss is: 0.00012359265707961316\n",
      "test loss is 0.00033720887351855873\n",
      "Batch: 14400,train loss is: 0.0003766153865428088\n",
      "test loss is 0.00022307861496315988\n",
      "Batch: 14500,train loss is: 0.0002541997080363165\n",
      "test loss is 0.00025579475614550515\n",
      "Batch: 14600,train loss is: 0.0005937516516319096\n",
      "test loss is 0.0006996644642157699\n",
      "Batch: 14700,train loss is: 0.0006256820115796351\n",
      "test loss is 0.0008284749077709088\n",
      "Batch: 14800,train loss is: 0.0003759358447560793\n",
      "test loss is 0.0003131835269224875\n",
      "Batch: 14900,train loss is: 0.000252490933639681\n",
      "test loss is 0.0002664226078038716\n",
      "Batch: 15000,train loss is: 0.0001881240481856271\n",
      "test loss is 0.000307006242170182\n",
      "Batch: 15100,train loss is: 0.00022225686320439652\n",
      "test loss is 0.00025706664821188084\n",
      "Batch: 15200,train loss is: 0.000571528625725327\n",
      "test loss is 0.0002563083368548784\n",
      "Batch: 15300,train loss is: 0.000572923484050288\n",
      "test loss is 0.0004701807155444555\n",
      "Batch: 15400,train loss is: 0.00035263517521760465\n",
      "test loss is 0.00045701551592783205\n",
      "Batch: 15500,train loss is: 0.000457762226226136\n",
      "test loss is 0.0005567121677350728\n",
      "Batch: 15600,train loss is: 0.004963463780030735\n",
      "test loss is 0.0017889737138583157\n",
      "Batch: 15700,train loss is: 0.0003186880248884992\n",
      "test loss is 0.0005639074411120955\n",
      "Batch: 15800,train loss is: 0.00015091569914250176\n",
      "test loss is 0.0006318439029757123\n",
      "Batch: 15900,train loss is: 0.0006252902931600551\n",
      "test loss is 0.0003862378752972768\n",
      "Batch: 16000,train loss is: 0.00022372511697754156\n",
      "test loss is 0.00026295051879796094\n",
      "Batch: 16100,train loss is: 0.0006214859896152881\n",
      "test loss is 0.00030498275350199336\n",
      "Batch: 16200,train loss is: 0.0003938477915980512\n",
      "test loss is 0.0004345284849031128\n",
      "Batch: 16300,train loss is: 0.00011128349599313236\n",
      "test loss is 0.0003826517167056471\n",
      "Batch: 16400,train loss is: 0.00034866100981727265\n",
      "test loss is 0.00028904467936990866\n",
      "Batch: 16500,train loss is: 0.0002633268836011027\n",
      "test loss is 0.00037471036820034917\n",
      "Batch: 16600,train loss is: 9.81591159540344e-05\n",
      "test loss is 0.0002513229030905656\n",
      "Batch: 16700,train loss is: 0.0007481286870854601\n",
      "test loss is 0.0004503872925207028\n",
      "Batch: 16800,train loss is: 0.00017724624604313488\n",
      "test loss is 0.00029833591944360355\n",
      "Batch: 16900,train loss is: 0.0008297445589414818\n",
      "test loss is 0.0007200609838355138\n",
      "Batch: 17000,train loss is: 0.0002464587027026118\n",
      "test loss is 0.0007249322050719589\n",
      "Batch: 17100,train loss is: 0.0002738652344888368\n",
      "test loss is 0.000803526740169712\n",
      "Batch: 17200,train loss is: 0.000366340491292392\n",
      "test loss is 0.00043879886923284435\n",
      "Batch: 17300,train loss is: 0.00017574503881920907\n",
      "test loss is 0.0002430548133624674\n",
      "Batch: 17400,train loss is: 0.0005084547824669085\n",
      "test loss is 0.0004683536816821286\n",
      "Batch: 17500,train loss is: 0.0005027636420928939\n",
      "test loss is 0.0003431669494630704\n",
      "Batch: 17600,train loss is: 0.0004467414420816611\n",
      "test loss is 0.0008089408035800725\n",
      "Batch: 17700,train loss is: 0.00024081366574985478\n",
      "test loss is 0.0003306490349259722\n",
      "Batch: 17800,train loss is: 0.0004221246976757496\n",
      "test loss is 0.00048769279747106425\n",
      "Batch: 17900,train loss is: 0.0006548578002634762\n",
      "test loss is 0.0002872677548511598\n",
      "Batch: 18000,train loss is: 0.00021816590432326374\n",
      "test loss is 0.0002688974237692269\n",
      "Batch: 18100,train loss is: 0.0004436180523249037\n",
      "test loss is 0.0004519521444948981\n",
      "Batch: 18200,train loss is: 0.00038850447817089114\n",
      "test loss is 0.0005063906359296452\n",
      "Batch: 18300,train loss is: 0.0010043776038683215\n",
      "test loss is 0.0003430526001730685\n",
      "Batch: 18400,train loss is: 0.00030310803403770995\n",
      "test loss is 0.000740488462965695\n",
      "Batch: 18500,train loss is: 0.00047905991264218475\n",
      "test loss is 0.0003760238914044614\n",
      "Batch: 18600,train loss is: 0.00045576691220147226\n",
      "test loss is 0.0007230089183700671\n",
      "Batch: 18700,train loss is: 0.00037399365176691966\n",
      "test loss is 0.0005693933317862183\n",
      "Batch: 18800,train loss is: 0.00026686007842234534\n",
      "test loss is 0.0002912417620523336\n",
      "Batch: 18900,train loss is: 0.0004921651114853053\n",
      "test loss is 0.0002728200997650639\n",
      "Batch: 19000,train loss is: 0.0011207308182867779\n",
      "test loss is 0.0006920411500204377\n",
      "Batch: 19100,train loss is: 0.0003675496173016844\n",
      "test loss is 0.00028709802061270556\n",
      "Batch: 19200,train loss is: 0.00015517801157539067\n",
      "test loss is 0.000303875735050683\n",
      "Batch: 19300,train loss is: 0.0004407805537216761\n",
      "test loss is 0.0003047930140792749\n",
      "Batch: 19400,train loss is: 0.00021744473435223325\n",
      "test loss is 0.0008614220240484502\n",
      "Batch: 19500,train loss is: 0.00035911934004311994\n",
      "test loss is 0.00048113895642537817\n",
      "Batch: 19600,train loss is: 0.0006184345770827143\n",
      "test loss is 0.0005745759149442257\n",
      "Batch: 19700,train loss is: 0.00022781033097917305\n",
      "test loss is 0.00032119353982111836\n",
      "Batch: 19800,train loss is: 0.00028876859837333435\n",
      "test loss is 0.00027516199706327965\n",
      "Batch: 19900,train loss is: 0.0003242816828638908\n",
      "test loss is 0.0003998917708766496\n",
      "Batch: 20000,train loss is: 0.00020316053243148665\n",
      "test loss is 0.00030969293066462217\n",
      "Batch: 20100,train loss is: 0.0002907755973188391\n",
      "test loss is 0.0005709754048254118\n",
      "Batch: 20200,train loss is: 0.00030088930236837777\n",
      "test loss is 0.0003123533308599047\n",
      "Batch: 20300,train loss is: 0.000804599908142952\n",
      "test loss is 0.0003860901249138472\n",
      "Batch: 20400,train loss is: 0.00032048328480968694\n",
      "test loss is 0.0008743391942957011\n",
      "Batch: 20500,train loss is: 0.0010632018152555618\n",
      "test loss is 0.0020353010451621967\n",
      "Batch: 20600,train loss is: 0.0007296585598683859\n",
      "test loss is 0.0007175224139886079\n",
      "Batch: 20700,train loss is: 0.00024495203391248864\n",
      "test loss is 0.000340239675484\n",
      "Batch: 20800,train loss is: 0.0002294954339052756\n",
      "test loss is 0.0002416391543010439\n",
      "Batch: 20900,train loss is: 0.00020111087019930793\n",
      "test loss is 0.00031869730080445806\n",
      "Batch: 21000,train loss is: 0.0006167870733217366\n",
      "test loss is 0.00033753281094275184\n",
      "Batch: 21100,train loss is: 0.0002273379962282658\n",
      "test loss is 0.0003862620637566268\n",
      "Batch: 21200,train loss is: 0.0007700339364299344\n",
      "test loss is 0.0008062879118700253\n",
      "Batch: 21300,train loss is: 0.00017654570631830065\n",
      "test loss is 0.00040743357729018556\n",
      "Batch: 21400,train loss is: 0.0002148843575331985\n",
      "test loss is 0.0002819324576340539\n",
      "Batch: 21500,train loss is: 0.00016266621599274504\n",
      "test loss is 0.00040142912304184437\n",
      "Batch: 21600,train loss is: 0.0004986192336041816\n",
      "test loss is 0.000476340098333103\n",
      "Batch: 21700,train loss is: 0.0002922450664739494\n",
      "test loss is 0.0005920252885346623\n",
      "Batch: 21800,train loss is: 0.0002346655939818816\n",
      "test loss is 0.0002502740169414076\n",
      "Batch: 21900,train loss is: 0.0001233413304033519\n",
      "test loss is 0.0004337300452924993\n",
      "Batch: 22000,train loss is: 0.0003018147434398849\n",
      "test loss is 0.0014633830188192922\n",
      "Batch: 22100,train loss is: 0.00016323104572938426\n",
      "test loss is 0.0008045491411325803\n",
      "Batch: 22200,train loss is: 0.0003688036470998468\n",
      "test loss is 0.0005039681784514253\n",
      "Batch: 22300,train loss is: 0.0002043623418212654\n",
      "test loss is 0.00036821052002217907\n",
      "Batch: 22400,train loss is: 0.0002819601099283756\n",
      "test loss is 0.00041788548859067535\n",
      "Batch: 22500,train loss is: 0.00030000486701427806\n",
      "test loss is 0.0003391351111285694\n",
      "Batch: 22600,train loss is: 0.00023141605901947507\n",
      "test loss is 0.00027486242763929123\n",
      "Batch: 22700,train loss is: 0.00016492457131524763\n",
      "test loss is 0.0005255756810412303\n",
      "Batch: 22800,train loss is: 0.000562889208347178\n",
      "test loss is 0.0003435072684945672\n",
      "Batch: 22900,train loss is: 0.00019106024621359403\n",
      "test loss is 0.0003918367227665829\n",
      "Batch: 23000,train loss is: 0.0002161363750388406\n",
      "test loss is 0.00021446966602377715\n",
      "Batch: 23100,train loss is: 0.00023970046369435413\n",
      "test loss is 0.0006970359548139895\n",
      "Batch: 23200,train loss is: 0.00010183515923567627\n",
      "test loss is 0.0002697837095286741\n",
      "Batch: 23300,train loss is: 0.00030524593600116334\n",
      "test loss is 0.00040778960153386355\n",
      "Batch: 23400,train loss is: 0.0003729723577379643\n",
      "test loss is 0.0003334590127071454\n",
      "Batch: 23500,train loss is: 0.000687045427313692\n",
      "test loss is 0.0003167612548443341\n",
      "Batch: 23600,train loss is: 0.00015257891529432634\n",
      "test loss is 0.0002946251217307943\n",
      "Batch: 23700,train loss is: 0.00029989277061789465\n",
      "test loss is 0.0004332272433715566\n",
      "Batch: 23800,train loss is: 0.000692618644450912\n",
      "test loss is 0.0007053279642202033\n",
      "Batch: 23900,train loss is: 0.0004541479535009439\n",
      "test loss is 0.0006583895376157485\n",
      "Batch: 24000,train loss is: 0.0006547530759189955\n",
      "test loss is 0.00047725083323234843\n",
      "Batch: 24100,train loss is: 0.00022781942850013737\n",
      "test loss is 0.00032304164768814215\n",
      "Batch: 24200,train loss is: 0.0003629487254921431\n",
      "test loss is 0.0002549190714152161\n",
      "Batch: 24300,train loss is: 0.00024536082424115663\n",
      "test loss is 0.00043520579955143643\n",
      "Batch: 24400,train loss is: 0.0002980257047950167\n",
      "test loss is 0.00029605807050235236\n",
      "Batch: 24500,train loss is: 0.0012976123142400806\n",
      "test loss is 0.0005839072937770649\n",
      "Batch: 24600,train loss is: 0.00025985111558501423\n",
      "test loss is 0.0011000231065526008\n",
      "Batch: 24700,train loss is: 0.00030242796085835205\n",
      "test loss is 0.00040077049454359217\n",
      "Batch: 24800,train loss is: 0.00022502156526106763\n",
      "test loss is 0.0003259743090205869\n",
      "Batch: 24900,train loss is: 0.00025954221608732916\n",
      "test loss is 0.00034420612992224614\n",
      "Batch: 25000,train loss is: 0.0002540765495923517\n",
      "test loss is 0.0002704140062860135\n",
      "Batch: 25100,train loss is: 0.0004732710513953285\n",
      "test loss is 0.00042752681433502683\n",
      "Batch: 25200,train loss is: 0.0011993902656131656\n",
      "test loss is 0.0006674919105180284\n",
      "Batch: 25300,train loss is: 0.000606594315588104\n",
      "test loss is 0.00031959116014478164\n",
      "Batch: 25400,train loss is: 0.000549684576790433\n",
      "test loss is 0.0004648362681787379\n",
      "Batch: 25500,train loss is: 0.00013461015416879165\n",
      "test loss is 0.00026618635137071905\n",
      "Batch: 25600,train loss is: 0.0002453981391573804\n",
      "test loss is 0.00031354265620524947\n",
      "Batch: 25700,train loss is: 0.0009449523081280965\n",
      "test loss is 0.00028017917429803215\n",
      "Batch: 25800,train loss is: 0.0008513928525453609\n",
      "test loss is 0.0006038763816446358\n",
      "Batch: 25900,train loss is: 0.00010606570266645616\n",
      "test loss is 0.000453350690651772\n",
      "Batch: 26000,train loss is: 0.0010911312868581672\n",
      "test loss is 0.0009374776510180743\n",
      "Batch: 26100,train loss is: 0.000244341072838472\n",
      "test loss is 0.00030751214200251307\n",
      "Batch: 26200,train loss is: 0.00019963123321991633\n",
      "test loss is 0.0003690210275107193\n",
      "Batch: 26300,train loss is: 0.0001593758985231718\n",
      "test loss is 0.00048346605110815346\n",
      "Batch: 26400,train loss is: 0.0001610616852396045\n",
      "test loss is 0.0002698705985557143\n",
      "Batch: 26500,train loss is: 0.0004379913077347863\n",
      "test loss is 0.0006433486438988704\n",
      "Batch: 26600,train loss is: 0.00027394173936706945\n",
      "test loss is 0.00037834145958088835\n",
      "Batch: 26700,train loss is: 0.00019394346302737693\n",
      "test loss is 0.0005321700375658779\n",
      "Batch: 26800,train loss is: 0.00036334116866786677\n",
      "test loss is 0.0005358108575787594\n",
      "Batch: 26900,train loss is: 0.00014558598396799985\n",
      "test loss is 0.00024136535831742378\n",
      "Batch: 27000,train loss is: 0.0008678156797824794\n",
      "test loss is 0.0003254890987717817\n",
      "Batch: 27100,train loss is: 0.00020834975098932862\n",
      "test loss is 0.0002856601047273767\n",
      "Batch: 27200,train loss is: 0.00015970855454447457\n",
      "test loss is 0.0003450472743425349\n",
      "Batch: 27300,train loss is: 0.00020880292193641996\n",
      "test loss is 0.0003617666972742626\n",
      "Batch: 27400,train loss is: 0.0005117344195327685\n",
      "test loss is 0.0006213269022683454\n",
      "Batch: 27500,train loss is: 0.00015719940113879052\n",
      "test loss is 0.00044923011585527075\n",
      "Batch: 27600,train loss is: 0.0003251960026860586\n",
      "test loss is 0.0007430428800279263\n",
      "Batch: 27700,train loss is: 0.0003908822160660994\n",
      "test loss is 0.000516951443765435\n",
      "Batch: 27800,train loss is: 0.00037263269620043136\n",
      "test loss is 0.0009423848556465424\n",
      "Batch: 27900,train loss is: 0.0005847221286239091\n",
      "test loss is 0.0003038108963665391\n",
      "Batch: 28000,train loss is: 0.00040907616744699716\n",
      "test loss is 0.00029304586426260975\n",
      "Batch: 28100,train loss is: 0.00026047023381387425\n",
      "test loss is 0.00042615683447845033\n",
      "Batch: 28200,train loss is: 0.00014744521293856417\n",
      "test loss is 0.0004295551143273369\n",
      "Batch: 28300,train loss is: 0.00039389212622522473\n",
      "test loss is 0.0002226383976645094\n",
      "Batch: 28400,train loss is: 0.00023974591592581943\n",
      "test loss is 0.0002492006966583643\n",
      "Batch: 28500,train loss is: 0.0006148089241544794\n",
      "test loss is 0.0005004489051501879\n",
      "Batch: 28600,train loss is: 0.00031326052293970276\n",
      "test loss is 0.0003315720214771278\n",
      "Batch: 28700,train loss is: 0.0003506077798926165\n",
      "test loss is 0.0003941779056064868\n",
      "Batch: 28800,train loss is: 0.0006917246769873146\n",
      "test loss is 0.00032083530116233265\n",
      "Batch: 28900,train loss is: 0.0011111780271968656\n",
      "test loss is 0.0005668799686590232\n",
      "Batch: 29000,train loss is: 0.00028378193987341443\n",
      "test loss is 0.0004072365139955751\n",
      "Batch: 29100,train loss is: 0.00033919910131265465\n",
      "test loss is 0.0005546265711650247\n",
      "Batch: 29200,train loss is: 0.0005596990935534437\n",
      "test loss is 0.0011632518915582828\n",
      "Batch: 29300,train loss is: 0.000733684557587097\n",
      "test loss is 0.0008284153174251288\n",
      "Batch: 29400,train loss is: 0.0006189008441066766\n",
      "test loss is 0.0003422358068647398\n",
      "Batch: 29500,train loss is: 0.0006428908839702394\n",
      "test loss is 0.0006794579176257682\n",
      "Batch: 29600,train loss is: 0.0003183352450230872\n",
      "test loss is 0.00023625583481519372\n",
      "Batch: 29700,train loss is: 0.00014628072434883207\n",
      "test loss is 0.000265363506128963\n",
      "Batch: 29800,train loss is: 0.0005253229910398438\n",
      "test loss is 0.0002959386630044759\n",
      "Batch: 29900,train loss is: 0.0010981010857570328\n",
      "test loss is 0.00031984269302844754\n",
      "Batch: 30000,train loss is: 0.0002442174028300522\n",
      "test loss is 0.0002548037487265066\n",
      "Batch: 30100,train loss is: 0.0005363186303544334\n",
      "test loss is 0.001078138443997532\n",
      "Batch: 30200,train loss is: 0.00024131500561877186\n",
      "test loss is 0.00023546947497415265\n",
      "Batch: 30300,train loss is: 0.0005163034712379933\n",
      "test loss is 0.001442276078536387\n",
      "Batch: 30400,train loss is: 0.0012199991415831042\n",
      "test loss is 0.0033073038816052395\n",
      "Batch: 30500,train loss is: 0.0003463495246142082\n",
      "test loss is 0.0004853204589957783\n",
      "Batch: 30600,train loss is: 0.00017405947259269817\n",
      "test loss is 0.0003828108683291775\n",
      "Batch: 30700,train loss is: 0.00023906181291280418\n",
      "test loss is 0.0006483415606789761\n",
      "Batch: 30800,train loss is: 0.0001041436343504288\n",
      "test loss is 0.00023999664007984072\n",
      "Batch: 30900,train loss is: 0.00011993631716063551\n",
      "test loss is 0.00033364719880655446\n",
      "Batch: 31000,train loss is: 0.00032923577255843556\n",
      "test loss is 0.000457569965531866\n",
      "Batch: 31100,train loss is: 0.0001774637529880462\n",
      "test loss is 0.00041298382393970207\n",
      "Batch: 31200,train loss is: 0.0002268235574001974\n",
      "test loss is 0.0003287529170403534\n",
      "Batch: 31300,train loss is: 0.00022289802386260598\n",
      "test loss is 0.0005385568746483459\n",
      "Batch: 31400,train loss is: 0.00015417258824470894\n",
      "test loss is 0.0004147352342156174\n",
      "Batch: 31500,train loss is: 0.00032159873306220136\n",
      "test loss is 0.0012625013962024934\n",
      "Batch: 31600,train loss is: 0.001011768879164102\n",
      "test loss is 0.0005704905451714805\n",
      "Batch: 31700,train loss is: 0.00016732301066067014\n",
      "test loss is 0.00042836244605575543\n",
      "Batch: 31800,train loss is: 0.00036091724093209593\n",
      "test loss is 0.0003465238026220728\n",
      "Batch: 31900,train loss is: 0.001511229935337852\n",
      "test loss is 0.0011757758560310674\n",
      "Batch: 32000,train loss is: 0.002385999612171467\n",
      "test loss is 0.0003813091992100095\n",
      "Batch: 32100,train loss is: 0.00015301415749158226\n",
      "test loss is 0.00025844517367900065\n",
      "Batch: 32200,train loss is: 0.0002739932157196065\n",
      "test loss is 0.00032684692234419337\n",
      "Batch: 32300,train loss is: 0.00033773688274563694\n",
      "test loss is 0.0003918837755389128\n",
      "Batch: 32400,train loss is: 0.00010839302095805508\n",
      "test loss is 0.0003203244714625007\n",
      "Batch: 32500,train loss is: 0.0006059618768056218\n",
      "test loss is 0.0003220672628903052\n",
      "Batch: 32600,train loss is: 0.0001071353828289785\n",
      "test loss is 0.0002925404183331444\n",
      "Batch: 32700,train loss is: 0.00037387017176490134\n",
      "test loss is 0.00023683502523697442\n",
      "Batch: 32800,train loss is: 0.00028144419740817547\n",
      "test loss is 0.00022887373746177612\n",
      "Batch: 32900,train loss is: 0.0002626180709678362\n",
      "test loss is 0.00024859279180940874\n",
      "Batch: 33000,train loss is: 0.0002423606582148489\n",
      "test loss is 0.0003056286011423314\n",
      "Batch: 33100,train loss is: 0.0016421618766818053\n",
      "test loss is 0.0012280331640971579\n",
      "Batch: 33200,train loss is: 0.00013100941276554743\n",
      "test loss is 0.0003698580691058711\n",
      "Batch: 33300,train loss is: 0.00026685732207325886\n",
      "test loss is 0.0002544117557277243\n",
      "Batch: 33400,train loss is: 0.0005144596195203808\n",
      "test loss is 0.001003365704969038\n",
      "Batch: 33500,train loss is: 0.00031829581940104784\n",
      "test loss is 0.001600168225888197\n",
      "Batch: 33600,train loss is: 0.00041510890581338586\n",
      "test loss is 0.0003908074113264041\n",
      "Batch: 33700,train loss is: 0.00034024956670974764\n",
      "test loss is 0.000298834695379197\n",
      "Batch: 33800,train loss is: 0.0004759832956032028\n",
      "test loss is 0.0002996937331632955\n",
      "Batch: 33900,train loss is: 0.0005379937854357535\n",
      "test loss is 0.00024411537022946443\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model(loss_MSE,optim_Adam,model,data_loader,train_data,test_data,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Epoch: 0----------------------------------\n",
      "Batch: 0,train loss is: 0.00025688925011882187\n",
      "test loss is 0.00035571089084992456\n",
      "Batch: 100,train loss is: 0.00027952857599471674\n",
      "test loss is 0.00019175619235531441\n",
      "Batch: 200,train loss is: 0.000136786337436899\n",
      "test loss is 0.0001808134941055253\n",
      "Batch: 300,train loss is: 0.00013592883215825634\n",
      "test loss is 0.00019425373996039797\n",
      "Batch: 400,train loss is: 0.00011882415054616123\n",
      "test loss is 0.00018194607443947777\n",
      "Batch: 500,train loss is: 7.378989437010583e-05\n",
      "test loss is 0.00017806670378682694\n",
      "Batch: 600,train loss is: 5.040661251237643e-05\n",
      "test loss is 0.00016399011623908773\n",
      "Batch: 700,train loss is: 0.00011671261820995356\n",
      "test loss is 0.00016941308688413488\n",
      "Batch: 800,train loss is: 0.00012028922001346587\n",
      "test loss is 0.00016630646802270622\n",
      "Batch: 900,train loss is: 0.00010351479340031418\n",
      "test loss is 0.0001723720350342117\n",
      "Batch: 1000,train loss is: 0.00010394250851545853\n",
      "test loss is 0.0001626858443479317\n",
      "Batch: 1100,train loss is: 0.0001475378997056903\n",
      "test loss is 0.0001757571912164085\n",
      "Batch: 1200,train loss is: 9.190951088960795e-05\n",
      "test loss is 0.00016314413874648522\n",
      "Batch: 1300,train loss is: 0.00010762184484113067\n",
      "test loss is 0.00016654396376413323\n",
      "Batch: 1400,train loss is: 9.57025642504956e-05\n",
      "test loss is 0.0001613969315792314\n",
      "Batch: 1500,train loss is: 0.00013036860700219743\n",
      "test loss is 0.00015904498591004098\n",
      "Batch: 1600,train loss is: 0.0001074433596681894\n",
      "test loss is 0.0001593899694604938\n",
      "Batch: 1700,train loss is: 0.0002230508472234525\n",
      "test loss is 0.00015570352277671442\n",
      "Batch: 1800,train loss is: 9.98720267641296e-05\n",
      "test loss is 0.0001641809221443581\n",
      "Batch: 1900,train loss is: 0.00012643929038441283\n",
      "test loss is 0.00018966284431505613\n",
      "Batch: 2000,train loss is: 0.00012271240423660678\n",
      "test loss is 0.0001561106551708417\n",
      "Batch: 2100,train loss is: 0.00011939274649146246\n",
      "test loss is 0.000175155890659136\n",
      "Batch: 2200,train loss is: 0.00013605531585705782\n",
      "test loss is 0.0001583529634938563\n",
      "Batch: 2300,train loss is: 8.327329196059467e-05\n",
      "test loss is 0.00015463064812115557\n",
      "Batch: 2400,train loss is: 9.214056636742347e-05\n",
      "test loss is 0.0001585185129820362\n",
      "Batch: 2500,train loss is: 0.0001744622400458816\n",
      "test loss is 0.0001598653657548449\n",
      "Batch: 2600,train loss is: 9.297586215144907e-05\n",
      "test loss is 0.00016256315635766923\n",
      "Batch: 2700,train loss is: 0.00010965901985869192\n",
      "test loss is 0.0001606243677765718\n",
      "Batch: 2800,train loss is: 0.00014413934509988427\n",
      "test loss is 0.00015893952806640614\n",
      "Batch: 2900,train loss is: 0.00016644000482040117\n",
      "test loss is 0.00015713163878061371\n",
      "Batch: 3000,train loss is: 0.00011503104489138249\n",
      "test loss is 0.0001635302295946174\n",
      "Batch: 3100,train loss is: 7.795467479469731e-05\n",
      "test loss is 0.0001534833118823226\n",
      "Batch: 3200,train loss is: 0.0002075766768010634\n",
      "test loss is 0.00015709801047754396\n",
      "Batch: 3300,train loss is: 0.0001355055511091636\n",
      "test loss is 0.0001542864641001204\n",
      "Batch: 3400,train loss is: 0.00012657590726498664\n",
      "test loss is 0.00017044233755414266\n",
      "Batch: 3500,train loss is: 5.1897698519333156e-05\n",
      "test loss is 0.00015676670827367438\n",
      "Batch: 3600,train loss is: 7.384999936214374e-05\n",
      "test loss is 0.00015734803325341985\n",
      "Batch: 3700,train loss is: 0.00011078768930225381\n",
      "test loss is 0.0001538003915515005\n",
      "Batch: 3800,train loss is: 0.00012795408001720174\n",
      "test loss is 0.00015846997032384344\n",
      "Batch: 3900,train loss is: 0.00019747022582745943\n",
      "test loss is 0.0001621515260729152\n",
      "Batch: 4000,train loss is: 7.121183680126955e-05\n",
      "test loss is 0.00015510122676842366\n",
      "Batch: 4100,train loss is: 0.00014268641930522422\n",
      "test loss is 0.00015820179870829377\n",
      "Batch: 4200,train loss is: 0.0001615296332484246\n",
      "test loss is 0.00016230644795253317\n",
      "Batch: 4300,train loss is: 0.00011914316141970397\n",
      "test loss is 0.00016441645207736826\n",
      "Batch: 4400,train loss is: 0.00010353524367050388\n",
      "test loss is 0.00015316692194081342\n",
      "Batch: 4500,train loss is: 0.00010721680546469272\n",
      "test loss is 0.000154026034195069\n",
      "Batch: 4600,train loss is: 9.84879258211656e-05\n",
      "test loss is 0.00015548943017178567\n",
      "Batch: 4700,train loss is: 0.00017408888463176964\n",
      "test loss is 0.00015284686189314037\n",
      "Batch: 4800,train loss is: 0.00032020347497993936\n",
      "test loss is 0.00015493336262853758\n",
      "Batch: 4900,train loss is: 0.00011859190970019574\n",
      "test loss is 0.00016807874650484305\n",
      "Batch: 5000,train loss is: 0.0001044387935839442\n",
      "test loss is 0.0001509169659288976\n",
      "Batch: 5100,train loss is: 0.00014361970234510536\n",
      "test loss is 0.00016093597097405817\n",
      "Batch: 5200,train loss is: 6.709015370604157e-05\n",
      "test loss is 0.00015509600767377594\n",
      "Batch: 5300,train loss is: 0.00012915719841479292\n",
      "test loss is 0.00018251948938136087\n",
      "Batch: 5400,train loss is: 0.00020159383764980262\n",
      "test loss is 0.00015156988412849763\n",
      "Batch: 5500,train loss is: 0.00010031990632111583\n",
      "test loss is 0.0001546278198462717\n",
      "Batch: 5600,train loss is: 0.0001666836440564513\n",
      "test loss is 0.00015389864623868666\n",
      "Batch: 5700,train loss is: 0.00017429305288605936\n",
      "test loss is 0.00015732658868176412\n",
      "Batch: 5800,train loss is: 0.00012736998750540618\n",
      "test loss is 0.00016211872984174547\n",
      "Batch: 5900,train loss is: 0.0004009129242440278\n",
      "test loss is 0.00015357349559544828\n",
      "Batch: 6000,train loss is: 0.00016373899340707714\n",
      "test loss is 0.0001534021807131343\n",
      "Batch: 6100,train loss is: 0.00010214837265300128\n",
      "test loss is 0.00016835116368671911\n",
      "Batch: 6200,train loss is: 0.00010505543444184017\n",
      "test loss is 0.00014799564665550046\n",
      "Batch: 6300,train loss is: 0.00020318583619375016\n",
      "test loss is 0.00015094744585647722\n",
      "Batch: 6400,train loss is: 0.00010101953697156853\n",
      "test loss is 0.00015417196487301326\n",
      "Batch: 6500,train loss is: 9.198751268715511e-05\n",
      "test loss is 0.00015984960630060596\n",
      "Batch: 6600,train loss is: 0.00034279868092044547\n",
      "test loss is 0.0001476085087136489\n",
      "Batch: 6700,train loss is: 6.206327723163368e-05\n",
      "test loss is 0.00015651875648666687\n",
      "Batch: 6800,train loss is: 0.00017226584051076998\n",
      "test loss is 0.00014950322129512328\n",
      "Batch: 6900,train loss is: 0.00010554297855993629\n",
      "test loss is 0.0001539946160965576\n",
      "Batch: 7000,train loss is: 0.00017933705183187428\n",
      "test loss is 0.00014908561443793947\n",
      "Batch: 7100,train loss is: 0.0001247225701022124\n",
      "test loss is 0.00015391353754272774\n",
      "Batch: 7200,train loss is: 6.984253434018932e-05\n",
      "test loss is 0.00016219824847730252\n",
      "Batch: 7300,train loss is: 0.0003466366960844032\n",
      "test loss is 0.0001562804711065899\n",
      "Batch: 7400,train loss is: 9.766043732587363e-05\n",
      "test loss is 0.00016511104370743411\n",
      "Batch: 7500,train loss is: 0.0003034243604700002\n",
      "test loss is 0.0001623463833402442\n",
      "Batch: 7600,train loss is: 0.00020197054739257283\n",
      "test loss is 0.00015635832143828476\n",
      "Batch: 7700,train loss is: 0.00021039629275091824\n",
      "test loss is 0.0001574133601728256\n",
      "Batch: 7800,train loss is: 7.5028917432035e-05\n",
      "test loss is 0.0001477666989166719\n",
      "Batch: 7900,train loss is: 7.691268957344022e-05\n",
      "test loss is 0.00015549769084571735\n",
      "Batch: 8000,train loss is: 5.8979542931940894e-05\n",
      "test loss is 0.00014878933968461065\n",
      "Batch: 8100,train loss is: 0.000267926481164374\n",
      "test loss is 0.00016004832553800493\n",
      "Batch: 8200,train loss is: 0.00018253521133248986\n",
      "test loss is 0.00016540459005177227\n",
      "Batch: 8300,train loss is: 0.0001048813796325243\n",
      "test loss is 0.0001674507685395005\n",
      "Batch: 8400,train loss is: 0.00012113526898287501\n",
      "test loss is 0.00014816777924690243\n",
      "Batch: 8500,train loss is: 0.00014259203738505174\n",
      "test loss is 0.0001486124403988891\n",
      "Batch: 8600,train loss is: 0.00015129851655783181\n",
      "test loss is 0.00016262318528079277\n",
      "Batch: 8700,train loss is: 0.00011808818175515735\n",
      "test loss is 0.00017766088199481456\n",
      "Batch: 8800,train loss is: 0.00017242828791100864\n",
      "test loss is 0.00015058101733969427\n",
      "Batch: 8900,train loss is: 0.0002032711520284661\n",
      "test loss is 0.00014788165223945704\n",
      "Batch: 9000,train loss is: 8.21808525311714e-05\n",
      "test loss is 0.000150673480724956\n",
      "Batch: 9100,train loss is: 8.85081361705789e-05\n",
      "test loss is 0.00014899584581106113\n",
      "Batch: 9200,train loss is: 0.00012049634206341501\n",
      "test loss is 0.00015027078489879777\n",
      "Batch: 9300,train loss is: 0.0001701028217227551\n",
      "test loss is 0.0001695383586992133\n",
      "Batch: 9400,train loss is: 0.00011682783020224751\n",
      "test loss is 0.0001532304320429353\n",
      "Batch: 9500,train loss is: 8.073084872965284e-05\n",
      "test loss is 0.00014643478590831784\n",
      "Batch: 9600,train loss is: 8.727851699417768e-05\n",
      "test loss is 0.00015556114877085907\n",
      "Batch: 9700,train loss is: 7.529935562383642e-05\n",
      "test loss is 0.00014744478370654578\n",
      "Batch: 9800,train loss is: 6.499091513335095e-05\n",
      "test loss is 0.00015460707434585339\n",
      "Batch: 9900,train loss is: 7.226267595358573e-05\n",
      "test loss is 0.00016148812401195614\n",
      "Batch: 10000,train loss is: 9.802628175671876e-05\n",
      "test loss is 0.0001518940460677578\n",
      "Batch: 10100,train loss is: 0.0002501802602621187\n",
      "test loss is 0.00015522024683237834\n",
      "Batch: 10200,train loss is: 0.00035744806951058073\n",
      "test loss is 0.0001486720386811819\n",
      "Batch: 10300,train loss is: 9.705483579097658e-05\n",
      "test loss is 0.00014815735965414525\n",
      "Batch: 10400,train loss is: 7.673460669154601e-05\n",
      "test loss is 0.00014865345413367075\n",
      "Batch: 10500,train loss is: 0.00024887139954590885\n",
      "test loss is 0.00015138456646277064\n",
      "Batch: 10600,train loss is: 9.970921805085884e-05\n",
      "test loss is 0.00015101418501174409\n",
      "Batch: 10700,train loss is: 0.00013194081554389304\n",
      "test loss is 0.00014818172372986575\n",
      "Batch: 10800,train loss is: 0.00019580531032453813\n",
      "test loss is 0.00014867639107807483\n",
      "Batch: 10900,train loss is: 0.0001076261506296219\n",
      "test loss is 0.00014882726132867577\n",
      "Batch: 11000,train loss is: 0.00011822266787205483\n",
      "test loss is 0.00014758378756958972\n",
      "Batch: 11100,train loss is: 0.00010890520463529801\n",
      "test loss is 0.000163161253500713\n",
      "Batch: 11200,train loss is: 0.00012723532845711804\n",
      "test loss is 0.00014896249521446597\n",
      "Batch: 11300,train loss is: 9.556472274317182e-05\n",
      "test loss is 0.0001498141830910113\n",
      "Batch: 11400,train loss is: 0.00019956996083945347\n",
      "test loss is 0.0001620906560419024\n",
      "Batch: 11500,train loss is: 7.876185621350384e-05\n",
      "test loss is 0.00014670233198134803\n",
      "Batch: 11600,train loss is: 0.00021579973862689058\n",
      "test loss is 0.00014920033130080918\n",
      "Batch: 11700,train loss is: 0.00010358162531964716\n",
      "test loss is 0.00014861616488217794\n",
      "Batch: 11800,train loss is: 0.00010381503932295239\n",
      "test loss is 0.00015140638692375024\n",
      "Batch: 11900,train loss is: 0.0001339246664948885\n",
      "test loss is 0.00014904827769775407\n",
      "Batch: 12000,train loss is: 0.00014669889395414024\n",
      "test loss is 0.00014946614539863466\n",
      "Batch: 12100,train loss is: 0.0008573948497900826\n",
      "test loss is 0.00015890353371641867\n",
      "Batch: 12200,train loss is: 0.00011533796886274166\n",
      "test loss is 0.00014723133044403425\n",
      "Batch: 12300,train loss is: 0.00012457434969581936\n",
      "test loss is 0.00015258963699950294\n",
      "Batch: 12400,train loss is: 0.00011917603618754275\n",
      "test loss is 0.00015356914824080734\n",
      "Batch: 12500,train loss is: 0.0002926175872201957\n",
      "test loss is 0.00014706115860087842\n",
      "Batch: 12600,train loss is: 8.916859773942676e-05\n",
      "test loss is 0.00014608761671470124\n",
      "Batch: 12700,train loss is: 9.235879542813929e-05\n",
      "test loss is 0.0001532737967322574\n",
      "Batch: 12800,train loss is: 8.011504512317161e-05\n",
      "test loss is 0.00014690524358237036\n",
      "Batch: 12900,train loss is: 0.0001564674294586732\n",
      "test loss is 0.0001471232150738671\n",
      "Batch: 13000,train loss is: 0.00011748960483405437\n",
      "test loss is 0.0001607886624393498\n",
      "Batch: 13100,train loss is: 0.00025726516259130236\n",
      "test loss is 0.00014588455225332544\n",
      "Batch: 13200,train loss is: 0.00012087034510526142\n",
      "test loss is 0.000143441759090807\n",
      "Batch: 13300,train loss is: 8.847635864125112e-05\n",
      "test loss is 0.00017573297942057695\n",
      "Batch: 13400,train loss is: 0.00011297688275316374\n",
      "test loss is 0.00015740603683953632\n",
      "Batch: 13500,train loss is: 9.088052745296345e-05\n",
      "test loss is 0.00014931300468771732\n",
      "Batch: 13600,train loss is: 0.00011082541404675402\n",
      "test loss is 0.00014636825053575275\n",
      "Batch: 13700,train loss is: 0.00012487088624843834\n",
      "test loss is 0.00014696667403351824\n",
      "Batch: 13800,train loss is: 0.00014548871600870506\n",
      "test loss is 0.00015467362590654656\n",
      "Batch: 13900,train loss is: 6.878504634564431e-05\n",
      "test loss is 0.00014881513305298926\n",
      "Batch: 14000,train loss is: 0.00010740846943239463\n",
      "test loss is 0.0001479877711821726\n",
      "Batch: 14100,train loss is: 0.00012010075390742631\n",
      "test loss is 0.00017302167728662236\n",
      "Batch: 14200,train loss is: 6.0745627657886035e-05\n",
      "test loss is 0.00014702780215345956\n",
      "Batch: 14300,train loss is: 7.008496167776315e-05\n",
      "test loss is 0.00014627419736314436\n",
      "Batch: 14400,train loss is: 0.00018274521160123594\n",
      "test loss is 0.0001432286509770892\n",
      "Batch: 14500,train loss is: 6.655113694096683e-05\n",
      "test loss is 0.00014423659927831247\n",
      "Batch: 14600,train loss is: 0.00025534169179907725\n",
      "test loss is 0.00014718497807846804\n",
      "Batch: 14700,train loss is: 8.259856016594738e-05\n",
      "test loss is 0.00014494064721194957\n",
      "Batch: 14800,train loss is: 0.0001801558813311294\n",
      "test loss is 0.00014871522764726392\n",
      "Batch: 14900,train loss is: 0.00011321989116777655\n",
      "test loss is 0.00014651411496203952\n",
      "Batch: 15000,train loss is: 9.988822221909683e-05\n",
      "test loss is 0.0001637410871780807\n",
      "Batch: 15100,train loss is: 0.00019913293492451989\n",
      "test loss is 0.00015016592041364182\n",
      "Batch: 15200,train loss is: 0.00021158639170826277\n",
      "test loss is 0.00014306467528263752\n",
      "Batch: 15300,train loss is: 0.00017613751776868777\n",
      "test loss is 0.0001428272673927041\n",
      "Batch: 15400,train loss is: 0.00010973719664279058\n",
      "test loss is 0.00015167011257635646\n",
      "Batch: 15500,train loss is: 0.00019122771955101025\n",
      "test loss is 0.0001574025822373057\n",
      "Batch: 15600,train loss is: 0.00023745682989815422\n",
      "test loss is 0.0001491353970642559\n",
      "Batch: 15700,train loss is: 0.00010510120469735917\n",
      "test loss is 0.00016442588810377384\n",
      "Batch: 15800,train loss is: 0.00011173100474464937\n",
      "test loss is 0.00014660973026901265\n",
      "Batch: 15900,train loss is: 0.0002984649567993453\n",
      "test loss is 0.00014802501529720123\n",
      "Batch: 16000,train loss is: 0.00012305537660122653\n",
      "test loss is 0.00014821986470001545\n",
      "Batch: 16100,train loss is: 0.0001511155899824343\n",
      "test loss is 0.00014256611881552947\n",
      "Batch: 16200,train loss is: 7.747354444898016e-05\n",
      "test loss is 0.0001505605976986164\n",
      "Batch: 16300,train loss is: 0.00011384946246724642\n",
      "test loss is 0.00014951470512506845\n",
      "Batch: 16400,train loss is: 0.00016468834405235656\n",
      "test loss is 0.00014552513967680162\n",
      "Batch: 16500,train loss is: 0.0001609343288089141\n",
      "test loss is 0.00014923754015712086\n",
      "Batch: 16600,train loss is: 7.164506641275062e-05\n",
      "test loss is 0.0001487403710745251\n",
      "Batch: 16700,train loss is: 0.00020607460087249935\n",
      "test loss is 0.0001492040083148264\n",
      "Batch: 16800,train loss is: 0.00015154107405786275\n",
      "test loss is 0.00015190455559212717\n",
      "Batch: 16900,train loss is: 0.0001377902101446478\n",
      "test loss is 0.0001520062807709549\n",
      "Batch: 17000,train loss is: 0.00015868274629712243\n",
      "test loss is 0.00014656413653664146\n",
      "Batch: 17100,train loss is: 0.000128578959020244\n",
      "test loss is 0.00014398417094851917\n",
      "Batch: 17200,train loss is: 0.00011877030640196403\n",
      "test loss is 0.00015506152247549905\n",
      "Batch: 17300,train loss is: 0.00014915974360445335\n",
      "test loss is 0.0001458567433170945\n",
      "Batch: 17400,train loss is: 6.96615076390164e-05\n",
      "test loss is 0.0001487648124312956\n",
      "Batch: 17500,train loss is: 0.00011515607917342505\n",
      "test loss is 0.00014412497719681533\n",
      "Batch: 17600,train loss is: 7.789824918401204e-05\n",
      "test loss is 0.0001479293183003252\n",
      "Batch: 17700,train loss is: 0.00014311383760369406\n",
      "test loss is 0.00014257570077480107\n",
      "Batch: 17800,train loss is: 0.00021667730733298114\n",
      "test loss is 0.0001525953787645293\n",
      "Batch: 17900,train loss is: 0.0001112857719467905\n",
      "test loss is 0.00014303071497955675\n",
      "Batch: 18000,train loss is: 9.972963408937649e-05\n",
      "test loss is 0.00014885014318278395\n",
      "Batch: 18100,train loss is: 0.00014116072243576915\n",
      "test loss is 0.00014345811978351767\n",
      "Batch: 18200,train loss is: 0.0001127066252033378\n",
      "test loss is 0.00014101634054589887\n",
      "Batch: 18300,train loss is: 0.00020219674627547013\n",
      "test loss is 0.00014596417018939678\n",
      "Batch: 18400,train loss is: 0.00012093642463574465\n",
      "test loss is 0.00014151731180715084\n",
      "Batch: 18500,train loss is: 0.00010238837786245061\n",
      "test loss is 0.00014616171928794782\n",
      "Batch: 18600,train loss is: 9.364832388683873e-05\n",
      "test loss is 0.0001491535713132594\n",
      "Batch: 18700,train loss is: 0.0001696181419855068\n",
      "test loss is 0.00015183432638703938\n",
      "Batch: 18800,train loss is: 0.00014274570375825198\n",
      "test loss is 0.0001467019002035882\n",
      "Batch: 18900,train loss is: 0.0002625370805918155\n",
      "test loss is 0.00015898702410598937\n",
      "Batch: 19000,train loss is: 7.906961907565983e-05\n",
      "test loss is 0.0001551671166847015\n",
      "Batch: 19100,train loss is: 7.484830768267281e-05\n",
      "test loss is 0.00017823101689234508\n",
      "Batch: 19200,train loss is: 0.00011818854449771498\n",
      "test loss is 0.0001548491078735254\n",
      "Batch: 19300,train loss is: 0.00013858358083687172\n",
      "test loss is 0.00014719307890205167\n",
      "Batch: 19400,train loss is: 7.95070323914503e-05\n",
      "test loss is 0.00014364314690643663\n",
      "Batch: 19500,train loss is: 0.00010719873556157913\n",
      "test loss is 0.00015355783731202193\n",
      "Batch: 19600,train loss is: 7.058855914857114e-05\n",
      "test loss is 0.00014873405844516237\n",
      "Batch: 19700,train loss is: 0.00016380504014169118\n",
      "test loss is 0.00014446659757168197\n",
      "Batch: 19800,train loss is: 0.00012505129388072997\n",
      "test loss is 0.00014210599746639188\n",
      "Batch: 19900,train loss is: 8.378603250226259e-05\n",
      "test loss is 0.00015177752213609295\n",
      "Batch: 20000,train loss is: 0.00016306820329184404\n",
      "test loss is 0.0001483588649948849\n",
      "Batch: 20100,train loss is: 0.0001150271078529324\n",
      "test loss is 0.00014761355174243724\n",
      "Batch: 20200,train loss is: 9.057150543848002e-05\n",
      "test loss is 0.00014176568655229429\n",
      "Batch: 20300,train loss is: 7.660017253760996e-05\n",
      "test loss is 0.000144807675589154\n",
      "Batch: 20400,train loss is: 0.0001474675986548501\n",
      "test loss is 0.00023195782212387484\n",
      "Batch: 20500,train loss is: 9.492306741635693e-05\n",
      "test loss is 0.0001421581635599028\n",
      "Batch: 20600,train loss is: 0.00026685892963066295\n",
      "test loss is 0.00014766689550209817\n",
      "Batch: 20700,train loss is: 0.00013397608511939139\n",
      "test loss is 0.00014221967824038626\n",
      "Batch: 20800,train loss is: 6.351087787396281e-05\n",
      "test loss is 0.00014613938913161136\n",
      "Batch: 20900,train loss is: 7.340809166357942e-05\n",
      "test loss is 0.00014338485401082623\n",
      "Batch: 21000,train loss is: 5.822130968796643e-05\n",
      "test loss is 0.0001430459934262781\n",
      "Batch: 21100,train loss is: 0.00010798368804898275\n",
      "test loss is 0.00014375671179672357\n",
      "Batch: 21200,train loss is: 0.00012463558509649963\n",
      "test loss is 0.00015160158236134156\n",
      "Batch: 21300,train loss is: 7.478856595087163e-05\n",
      "test loss is 0.00014505478247034642\n",
      "Batch: 21400,train loss is: 0.00018873236920562895\n",
      "test loss is 0.00015274721471517398\n",
      "Batch: 21500,train loss is: 9.136393660201586e-05\n",
      "test loss is 0.00014600352201339677\n",
      "Batch: 21600,train loss is: 0.00014049505625504368\n",
      "test loss is 0.00014142043356381374\n",
      "Batch: 21700,train loss is: 7.757334749431658e-05\n",
      "test loss is 0.00014668179693899365\n",
      "Batch: 21800,train loss is: 0.00016318144868239322\n",
      "test loss is 0.00015150559033034412\n",
      "Batch: 21900,train loss is: 8.2577873327743e-05\n",
      "test loss is 0.00014483380465472233\n",
      "Batch: 22000,train loss is: 0.00013956526854663607\n",
      "test loss is 0.00014687326665315727\n",
      "Batch: 22100,train loss is: 9.84282721038599e-05\n",
      "test loss is 0.0001411662744036231\n",
      "Batch: 22200,train loss is: 8.495295833463072e-05\n",
      "test loss is 0.00014176360728171164\n",
      "Batch: 22300,train loss is: 0.00012287869371697656\n",
      "test loss is 0.00015295527800915162\n",
      "Batch: 22400,train loss is: 0.00011056626787204112\n",
      "test loss is 0.00014803038227052947\n",
      "Batch: 22500,train loss is: 0.00013220994753532472\n",
      "test loss is 0.00014122431150580958\n",
      "Batch: 22600,train loss is: 0.00011386935058990938\n",
      "test loss is 0.00015014500497396457\n",
      "Batch: 22700,train loss is: 7.585841991314885e-05\n",
      "test loss is 0.00014511860364260464\n",
      "Batch: 22800,train loss is: 0.00012989689145859095\n",
      "test loss is 0.000141394828692094\n",
      "Batch: 22900,train loss is: 0.00011445272212205689\n",
      "test loss is 0.00014448456505609613\n",
      "Batch: 23000,train loss is: 0.0001963546964742248\n",
      "test loss is 0.00014516790169173113\n",
      "Batch: 23100,train loss is: 0.00010523788433903442\n",
      "test loss is 0.00014080848090625432\n",
      "Batch: 23200,train loss is: 7.5899400536234e-05\n",
      "test loss is 0.00014059999896097388\n",
      "Batch: 23300,train loss is: 9.58175599964563e-05\n",
      "test loss is 0.00014238749102392853\n",
      "Batch: 23400,train loss is: 0.00011764337369859293\n",
      "test loss is 0.00014059281077485864\n",
      "Batch: 23500,train loss is: 0.00020087267469497637\n",
      "test loss is 0.0001409934497418597\n",
      "Batch: 23600,train loss is: 8.584217937163066e-05\n",
      "test loss is 0.00014376120413902825\n",
      "Batch: 23700,train loss is: 0.0001315822199654277\n",
      "test loss is 0.00015957674514716766\n",
      "Batch: 23800,train loss is: 0.00021045402102941037\n",
      "test loss is 0.0001481932438824064\n",
      "Batch: 23900,train loss is: 0.0002915672373475182\n",
      "test loss is 0.0001411561325478772\n",
      "Batch: 24000,train loss is: 0.00012133119560115875\n",
      "test loss is 0.00015048668633479265\n",
      "Batch: 24100,train loss is: 0.00011159789112958704\n",
      "test loss is 0.0001472908026064266\n",
      "Batch: 24200,train loss is: 0.00020369657676014104\n",
      "test loss is 0.00014419962513737662\n",
      "Batch: 24300,train loss is: 0.00012986300656348858\n",
      "test loss is 0.00018816237323472434\n",
      "Batch: 24400,train loss is: 0.0001655663802140106\n",
      "test loss is 0.0001440918670423647\n",
      "Batch: 24500,train loss is: 0.0001983798672652139\n",
      "test loss is 0.0001456645778800883\n",
      "Batch: 24600,train loss is: 8.492197040229172e-05\n",
      "test loss is 0.00015730219252804687\n",
      "Batch: 24700,train loss is: 6.371806152153543e-05\n",
      "test loss is 0.0001393347191435796\n",
      "Batch: 24800,train loss is: 0.00024843694773000883\n",
      "test loss is 0.00014778414711478496\n",
      "Batch: 24900,train loss is: 9.067170778073092e-05\n",
      "test loss is 0.00016144114597725382\n",
      "Batch: 25000,train loss is: 0.00015659070034206745\n",
      "test loss is 0.0001410674871842675\n",
      "Batch: 25100,train loss is: 7.744200225497981e-05\n",
      "test loss is 0.0001472018826244282\n",
      "Batch: 25200,train loss is: 0.00012571845572289367\n",
      "test loss is 0.00013933725847700866\n",
      "Batch: 25300,train loss is: 0.00018805755693324366\n",
      "test loss is 0.00014327750491217758\n",
      "Batch: 25400,train loss is: 0.0001327797567158041\n",
      "test loss is 0.0001455156485743916\n",
      "Batch: 25500,train loss is: 8.979946870697187e-05\n",
      "test loss is 0.00014335089259779373\n",
      "Batch: 25600,train loss is: 0.00016476036515723929\n",
      "test loss is 0.00016023757894653818\n",
      "Batch: 25700,train loss is: 0.00018329552313658517\n",
      "test loss is 0.00014545349817422461\n",
      "Batch: 25800,train loss is: 0.00010898512557378004\n",
      "test loss is 0.00013990129253991086\n",
      "Batch: 25900,train loss is: 9.324826800966313e-05\n",
      "test loss is 0.0001444988214151081\n",
      "Batch: 26000,train loss is: 0.0001606108018382271\n",
      "test loss is 0.00014513923325166957\n",
      "Batch: 26100,train loss is: 0.00013499305110660766\n",
      "test loss is 0.00014674261870454514\n",
      "Batch: 26200,train loss is: 0.00019544271710101563\n",
      "test loss is 0.00014018217274147144\n",
      "Batch: 26300,train loss is: 0.00015741185332228264\n",
      "test loss is 0.00016138858133735957\n",
      "Batch: 26400,train loss is: 7.22711996700693e-05\n",
      "test loss is 0.00014704386343830948\n",
      "Batch: 26500,train loss is: 0.00012192077170390213\n",
      "test loss is 0.00014994735370719954\n",
      "Batch: 26600,train loss is: 0.00021544553829141102\n",
      "test loss is 0.0001400267647153168\n",
      "Batch: 26700,train loss is: 0.00011435301288205951\n",
      "test loss is 0.0001619114540877409\n",
      "Batch: 26800,train loss is: 0.00014092305306879276\n",
      "test loss is 0.000141397694131661\n",
      "Batch: 26900,train loss is: 7.319526665492292e-05\n",
      "test loss is 0.00014083836741225606\n",
      "Batch: 27000,train loss is: 0.00012539817882932348\n",
      "test loss is 0.00014759150870753777\n",
      "Batch: 27100,train loss is: 0.00015570720787577884\n",
      "test loss is 0.00015156833145712616\n",
      "Batch: 27200,train loss is: 0.00013429081362750382\n",
      "test loss is 0.00014337017445328226\n",
      "Batch: 27300,train loss is: 6.487572915494963e-05\n",
      "test loss is 0.0001443716850417043\n",
      "Batch: 27400,train loss is: 0.0001445323042511756\n",
      "test loss is 0.0001445198700189956\n",
      "Batch: 27500,train loss is: 0.0001137340969072777\n",
      "test loss is 0.0001385543253226023\n",
      "Batch: 27600,train loss is: 9.694910721993e-05\n",
      "test loss is 0.00014618497451955508\n",
      "Batch: 27700,train loss is: 0.000334309262159363\n",
      "test loss is 0.00015049754033161038\n",
      "Batch: 27800,train loss is: 0.00013438157206788304\n",
      "test loss is 0.00014982845881142732\n",
      "Batch: 27900,train loss is: 0.00013363286648662752\n",
      "test loss is 0.00014199663997023734\n",
      "Batch: 28000,train loss is: 0.00010355212756507458\n",
      "test loss is 0.0001480705216339498\n",
      "Batch: 28100,train loss is: 0.00016317226973007755\n",
      "test loss is 0.0001472394013596674\n",
      "Batch: 28200,train loss is: 0.00011298647829183872\n",
      "test loss is 0.00014720431103530855\n",
      "Batch: 28300,train loss is: 0.0004230284618737624\n",
      "test loss is 0.00015216522136035645\n",
      "Batch: 28400,train loss is: 0.00011961911873425677\n",
      "test loss is 0.0001384052585839705\n",
      "Batch: 28500,train loss is: 0.00018923685858979707\n",
      "test loss is 0.00014971746977511844\n",
      "Batch: 28600,train loss is: 0.0001397102831888271\n",
      "test loss is 0.00014602659358304911\n",
      "Batch: 28700,train loss is: 0.00011419015731510841\n",
      "test loss is 0.0001429500775233764\n",
      "Batch: 28800,train loss is: 0.00020589909795839072\n",
      "test loss is 0.00014080956368919036\n",
      "Batch: 28900,train loss is: 0.00012547731434028192\n",
      "test loss is 0.0001588467915507406\n",
      "Batch: 29000,train loss is: 0.0001390900547065982\n",
      "test loss is 0.00014961580469279468\n",
      "Batch: 29100,train loss is: 0.00010186055055739408\n",
      "test loss is 0.00015005421910104243\n",
      "Batch: 29200,train loss is: 0.00012767703775801512\n",
      "test loss is 0.0001683624930843532\n",
      "Batch: 29300,train loss is: 0.00023534951619603165\n",
      "test loss is 0.00014709823905120683\n",
      "Batch: 29400,train loss is: 0.00017381438408214196\n",
      "test loss is 0.00014968012518700686\n",
      "Batch: 29500,train loss is: 0.0001534380144625869\n",
      "test loss is 0.00013830068581825155\n",
      "Batch: 29600,train loss is: 0.00017686654751212602\n",
      "test loss is 0.00014955019161285937\n",
      "Batch: 29700,train loss is: 7.36171775520671e-05\n",
      "test loss is 0.0001409857126021487\n",
      "Batch: 29800,train loss is: 0.00023581204198518335\n",
      "test loss is 0.0001454102680509553\n",
      "Batch: 29900,train loss is: 0.0003995273218552439\n",
      "test loss is 0.0001421253550466624\n",
      "Batch: 30000,train loss is: 0.00011240212838144556\n",
      "test loss is 0.00014055829208364635\n",
      "Batch: 30100,train loss is: 0.0001959651193161972\n",
      "test loss is 0.00017010778701149416\n",
      "Batch: 30200,train loss is: 9.784889369280936e-05\n",
      "test loss is 0.00014077808795305729\n",
      "Batch: 30300,train loss is: 0.00010418303861162002\n",
      "test loss is 0.00014171403913769995\n",
      "Batch: 30400,train loss is: 6.440840961818075e-05\n",
      "test loss is 0.00014883836971545895\n",
      "Batch: 30500,train loss is: 0.0001399378301948199\n",
      "test loss is 0.00016885419608810075\n",
      "Batch: 30600,train loss is: 9.646979606548919e-05\n",
      "test loss is 0.0001427014104334907\n",
      "Batch: 30700,train loss is: 9.78738982605541e-05\n",
      "test loss is 0.00014118126078090004\n",
      "Batch: 30800,train loss is: 6.304694644185977e-05\n",
      "test loss is 0.00014751292845053776\n",
      "Batch: 30900,train loss is: 9.062315557152299e-05\n",
      "test loss is 0.00014308108747727408\n",
      "Batch: 31000,train loss is: 8.561055620191164e-05\n",
      "test loss is 0.00014164858006208687\n",
      "Batch: 31100,train loss is: 0.00012243534136720572\n",
      "test loss is 0.000186919876319556\n",
      "Batch: 31200,train loss is: 0.00011500069288382148\n",
      "test loss is 0.00014482053795657342\n",
      "Batch: 31300,train loss is: 0.00012993349971617674\n",
      "test loss is 0.00014869893588756637\n",
      "Batch: 31400,train loss is: 8.153027726546593e-05\n",
      "test loss is 0.00014515162641644074\n",
      "Batch: 31500,train loss is: 0.0001399170721805563\n",
      "test loss is 0.00014659041633039276\n",
      "Batch: 31600,train loss is: 0.0001589763489456817\n",
      "test loss is 0.00014347751861373383\n",
      "Batch: 31700,train loss is: 8.070068662549509e-05\n",
      "test loss is 0.00014715859342799253\n",
      "Batch: 31800,train loss is: 0.00021359104288091852\n",
      "test loss is 0.00013965456163041575\n",
      "Batch: 31900,train loss is: 0.00017331116711155012\n",
      "test loss is 0.00014198054795632893\n",
      "Batch: 32000,train loss is: 0.0005876667008972427\n",
      "test loss is 0.00015202517104766455\n",
      "Batch: 32100,train loss is: 6.648510301091891e-05\n",
      "test loss is 0.000138570634846578\n",
      "Batch: 32200,train loss is: 9.074447552491305e-05\n",
      "test loss is 0.00014307879101062903\n",
      "Batch: 32300,train loss is: 0.00010419140947027189\n",
      "test loss is 0.00013752435115966738\n",
      "Batch: 32400,train loss is: 7.231015043783838e-05\n",
      "test loss is 0.00013900323201628654\n",
      "Batch: 32500,train loss is: 0.0001572433511208775\n",
      "test loss is 0.00013936771047321641\n",
      "Batch: 32600,train loss is: 0.00010390649173953996\n",
      "test loss is 0.00016231422496526283\n",
      "Batch: 32700,train loss is: 6.777323951231e-05\n",
      "test loss is 0.0001429922920420752\n",
      "Batch: 32800,train loss is: 0.00014840953472775427\n",
      "test loss is 0.00016268085924039127\n",
      "Batch: 32900,train loss is: 0.00011800132196056778\n",
      "test loss is 0.00013737309336957833\n",
      "Batch: 33000,train loss is: 7.824330046553828e-05\n",
      "test loss is 0.00014239762805295334\n",
      "Batch: 33100,train loss is: 7.686724418461804e-05\n",
      "test loss is 0.00015087286721176141\n",
      "Batch: 33200,train loss is: 6.550358488626456e-05\n",
      "test loss is 0.00013797657766400931\n",
      "Batch: 33300,train loss is: 9.484755403069176e-05\n",
      "test loss is 0.00014566403265444753\n",
      "Batch: 33400,train loss is: 0.00010904505154986707\n",
      "test loss is 0.00014189021225973186\n",
      "Batch: 33500,train loss is: 0.00010348760907494973\n",
      "test loss is 0.00015149363974595092\n",
      "Batch: 33600,train loss is: 8.863197992223445e-05\n",
      "test loss is 0.00015190875272060275\n",
      "Batch: 33700,train loss is: 0.00012148757223653887\n",
      "test loss is 0.00014930890260660801\n",
      "Batch: 33800,train loss is: 0.00015980017603709246\n",
      "test loss is 0.00013972337754230486\n",
      "Batch: 33900,train loss is: 8.260169786176611e-05\n",
      "test loss is 0.00013853306900305163\n",
      "-----------------------Epoch: 1----------------------------------\n",
      "Batch: 0,train loss is: 0.00018092449387266692\n",
      "test loss is 0.0001463515108280267\n",
      "Batch: 100,train loss is: 0.00024864748906538466\n",
      "test loss is 0.00015253086721767024\n",
      "Batch: 200,train loss is: 0.0001224751334523439\n",
      "test loss is 0.00013974151450752982\n",
      "Batch: 300,train loss is: 0.0001070309896873991\n",
      "test loss is 0.00014320462493264442\n",
      "Batch: 400,train loss is: 0.00010487511108380683\n",
      "test loss is 0.00014534226071082843\n",
      "Batch: 500,train loss is: 8.292952925852725e-05\n",
      "test loss is 0.0001543276650498879\n",
      "Batch: 600,train loss is: 4.7420566477029655e-05\n",
      "test loss is 0.0001427746823281529\n",
      "Batch: 700,train loss is: 0.00010587586502064384\n",
      "test loss is 0.00015730644782357628\n",
      "Batch: 800,train loss is: 0.00014915796493488828\n",
      "test loss is 0.00013808780364145761\n",
      "Batch: 900,train loss is: 9.787136970245154e-05\n",
      "test loss is 0.00014953238205005898\n",
      "Batch: 1000,train loss is: 0.00011037078201938977\n",
      "test loss is 0.0001451183563459504\n",
      "Batch: 1100,train loss is: 0.00024694499206279305\n",
      "test loss is 0.00014903140714346062\n",
      "Batch: 1200,train loss is: 7.330411274415932e-05\n",
      "test loss is 0.00013970815032538715\n",
      "Batch: 1300,train loss is: 9.616100615733612e-05\n",
      "test loss is 0.00014464659269567334\n",
      "Batch: 1400,train loss is: 9.022827922389141e-05\n",
      "test loss is 0.00014235316101391714\n",
      "Batch: 1500,train loss is: 0.00011623280385705676\n",
      "test loss is 0.0001439202481990965\n",
      "Batch: 1600,train loss is: 9.685680152384871e-05\n",
      "test loss is 0.00013620330837233255\n",
      "Batch: 1700,train loss is: 0.00015110361591022238\n",
      "test loss is 0.00013869185848317954\n",
      "Batch: 1800,train loss is: 8.030670525991189e-05\n",
      "test loss is 0.00013695619282687554\n",
      "Batch: 1900,train loss is: 0.00011317851646540233\n",
      "test loss is 0.00016372274793598264\n",
      "Batch: 2000,train loss is: 0.00011384046475690194\n",
      "test loss is 0.0001401284376555431\n",
      "Batch: 2100,train loss is: 0.00010336192535040615\n",
      "test loss is 0.00015114991535094524\n",
      "Batch: 2200,train loss is: 0.00012546338818982596\n",
      "test loss is 0.00014281146939136363\n",
      "Batch: 2300,train loss is: 8.12574018931113e-05\n",
      "test loss is 0.00013644620062006543\n",
      "Batch: 2400,train loss is: 9.216655414331183e-05\n",
      "test loss is 0.00013899699758542644\n",
      "Batch: 2500,train loss is: 0.00015533971173973453\n",
      "test loss is 0.00014040124569696572\n",
      "Batch: 2600,train loss is: 8.242530359767647e-05\n",
      "test loss is 0.00013934573397678612\n",
      "Batch: 2700,train loss is: 0.00011375349512837436\n",
      "test loss is 0.0001388546305086121\n",
      "Batch: 2800,train loss is: 0.00012767563338337364\n",
      "test loss is 0.00014093678507726387\n",
      "Batch: 2900,train loss is: 0.00014228270836355946\n",
      "test loss is 0.00015484214203295862\n",
      "Batch: 3000,train loss is: 0.00010945557276584695\n",
      "test loss is 0.00014652330997619493\n",
      "Batch: 3100,train loss is: 6.673535979058746e-05\n",
      "test loss is 0.0001362657595369364\n",
      "Batch: 3200,train loss is: 0.00018709502160640736\n",
      "test loss is 0.00013961472240109842\n",
      "Batch: 3300,train loss is: 0.00011384050953784193\n",
      "test loss is 0.00013770195565457875\n",
      "Batch: 3400,train loss is: 0.00011625282305278788\n",
      "test loss is 0.00016745415645231444\n",
      "Batch: 3500,train loss is: 5.2572049413996194e-05\n",
      "test loss is 0.00014005623413961305\n",
      "Batch: 3600,train loss is: 7.214478813589142e-05\n",
      "test loss is 0.00014229960543579105\n",
      "Batch: 3700,train loss is: 0.00010914337022618663\n",
      "test loss is 0.00013858469168207636\n",
      "Batch: 3800,train loss is: 0.00011517596405165392\n",
      "test loss is 0.00014100127824498123\n",
      "Batch: 3900,train loss is: 0.00016815834667988178\n",
      "test loss is 0.00014920976564441207\n",
      "Batch: 4000,train loss is: 6.37133882450308e-05\n",
      "test loss is 0.0001383617964002631\n",
      "Batch: 4100,train loss is: 0.00013335022816580622\n",
      "test loss is 0.00014368277602087206\n",
      "Batch: 4200,train loss is: 0.00014054141675250506\n",
      "test loss is 0.00013982673685435218\n",
      "Batch: 4300,train loss is: 9.40004590056435e-05\n",
      "test loss is 0.00014570076058380193\n",
      "Batch: 4400,train loss is: 9.347677681347071e-05\n",
      "test loss is 0.00013892012933378387\n",
      "Batch: 4500,train loss is: 0.00010280511830399793\n",
      "test loss is 0.0001390476338827364\n",
      "Batch: 4600,train loss is: 8.93130271719613e-05\n",
      "test loss is 0.00014061645938782277\n",
      "Batch: 4700,train loss is: 0.00015732949153879816\n",
      "test loss is 0.00013740936769067374\n",
      "Batch: 4800,train loss is: 0.00027579126860427625\n",
      "test loss is 0.00014091209521735015\n",
      "Batch: 4900,train loss is: 0.0001042494558404119\n",
      "test loss is 0.00015796461587735025\n",
      "Batch: 5000,train loss is: 9.32969249931889e-05\n",
      "test loss is 0.0001364104936509737\n",
      "Batch: 5100,train loss is: 0.00010828013191843725\n",
      "test loss is 0.00014008504666356895\n",
      "Batch: 5200,train loss is: 5.674355274038016e-05\n",
      "test loss is 0.0001378129008072558\n",
      "Batch: 5300,train loss is: 0.0001348753674594365\n",
      "test loss is 0.00017832222417500865\n",
      "Batch: 5400,train loss is: 0.00019070523092140754\n",
      "test loss is 0.00013810222518980184\n",
      "Batch: 5500,train loss is: 8.501674569670331e-05\n",
      "test loss is 0.00013790766953704911\n",
      "Batch: 5600,train loss is: 0.000168727362700756\n",
      "test loss is 0.00013904512083843525\n",
      "Batch: 5700,train loss is: 0.00015136986659537813\n",
      "test loss is 0.00014706135798802852\n",
      "Batch: 5800,train loss is: 0.00011992652375084683\n",
      "test loss is 0.0001457901508599331\n",
      "Batch: 5900,train loss is: 0.00035989358302020445\n",
      "test loss is 0.00014099925541647795\n",
      "Batch: 6000,train loss is: 0.00016471412915777545\n",
      "test loss is 0.00014031519736831737\n",
      "Batch: 6100,train loss is: 0.00010454250770222731\n",
      "test loss is 0.0001597970126857764\n",
      "Batch: 6200,train loss is: 9.91407058663565e-05\n",
      "test loss is 0.0001347094543985329\n",
      "Batch: 6300,train loss is: 0.00017211438784594827\n",
      "test loss is 0.00013744211740203242\n",
      "Batch: 6400,train loss is: 9.868718683221627e-05\n",
      "test loss is 0.00013927850635575827\n",
      "Batch: 6500,train loss is: 8.52501886363869e-05\n",
      "test loss is 0.00014512601957243932\n",
      "Batch: 6600,train loss is: 0.000292785059322101\n",
      "test loss is 0.0001353303309038235\n",
      "Batch: 6700,train loss is: 5.605125915258385e-05\n",
      "test loss is 0.00014263340708221825\n",
      "Batch: 6800,train loss is: 0.00015794018038472728\n",
      "test loss is 0.00013562626617898748\n",
      "Batch: 6900,train loss is: 8.374899845479172e-05\n",
      "test loss is 0.0001359862646236249\n",
      "Batch: 7000,train loss is: 0.0001408262140333798\n",
      "test loss is 0.00013704486038091269\n",
      "Batch: 7100,train loss is: 0.0001159831934433582\n",
      "test loss is 0.00014012628891836687\n",
      "Batch: 7200,train loss is: 6.47455677481398e-05\n",
      "test loss is 0.00014568807253111124\n",
      "Batch: 7300,train loss is: 0.00030795464043202467\n",
      "test loss is 0.00014244253368007605\n",
      "Batch: 7400,train loss is: 8.953052838300523e-05\n",
      "test loss is 0.00015233800272395275\n",
      "Batch: 7500,train loss is: 0.0002343912048533468\n",
      "test loss is 0.00014562996538163489\n",
      "Batch: 7600,train loss is: 0.0001822627968689275\n",
      "test loss is 0.00014228423664527207\n",
      "Batch: 7700,train loss is: 0.0001949728029902567\n",
      "test loss is 0.00014311643619488047\n",
      "Batch: 7800,train loss is: 6.957463680482383e-05\n",
      "test loss is 0.0001371998562629523\n",
      "Batch: 7900,train loss is: 7.228245224106577e-05\n",
      "test loss is 0.0001413856212853312\n",
      "Batch: 8000,train loss is: 5.68428329021265e-05\n",
      "test loss is 0.00013874592461160558\n",
      "Batch: 8100,train loss is: 0.00024976109299571707\n",
      "test loss is 0.00014528063069214913\n",
      "Batch: 8200,train loss is: 0.0001528039685619249\n",
      "test loss is 0.00015087732561596518\n",
      "Batch: 8300,train loss is: 9.049422727476917e-05\n",
      "test loss is 0.0001470767757834673\n",
      "Batch: 8400,train loss is: 0.00012356443025018083\n",
      "test loss is 0.00013569041665546918\n",
      "Batch: 8500,train loss is: 0.00012944397569256093\n",
      "test loss is 0.00013673010187158007\n",
      "Batch: 8600,train loss is: 0.00014585197197201948\n",
      "test loss is 0.00014689677617797146\n",
      "Batch: 8700,train loss is: 0.00011392082736082623\n",
      "test loss is 0.00015456956269213425\n",
      "Batch: 8800,train loss is: 0.00017445717009697894\n",
      "test loss is 0.00013772733510894602\n",
      "Batch: 8900,train loss is: 0.0002085916155735814\n",
      "test loss is 0.0001341422189624168\n",
      "Batch: 9000,train loss is: 7.73840216680846e-05\n",
      "test loss is 0.0001377913160182186\n",
      "Batch: 9100,train loss is: 7.45578285881918e-05\n",
      "test loss is 0.00013362108781040524\n",
      "Batch: 9200,train loss is: 0.00011294881597963431\n",
      "test loss is 0.0001362688338712877\n",
      "Batch: 9300,train loss is: 0.0001445094210734938\n",
      "test loss is 0.0001557212801150133\n",
      "Batch: 9400,train loss is: 0.00010057152751477322\n",
      "test loss is 0.00014104475197818136\n",
      "Batch: 9500,train loss is: 7.321127619293704e-05\n",
      "test loss is 0.00013420196069833945\n",
      "Batch: 9600,train loss is: 7.759186408724964e-05\n",
      "test loss is 0.00014104243540771566\n",
      "Batch: 9700,train loss is: 6.987314215428841e-05\n",
      "test loss is 0.00013451623997347482\n",
      "Batch: 9800,train loss is: 5.91248928852621e-05\n",
      "test loss is 0.00014502954847188536\n",
      "Batch: 9900,train loss is: 6.456573951534638e-05\n",
      "test loss is 0.00014967848240604427\n",
      "Batch: 10000,train loss is: 9.5761850474963e-05\n",
      "test loss is 0.00013990942962448209\n",
      "Batch: 10100,train loss is: 0.0002279262800404234\n",
      "test loss is 0.00014671773500115544\n",
      "Batch: 10200,train loss is: 0.00030467983149510164\n",
      "test loss is 0.00013635220087868428\n",
      "Batch: 10300,train loss is: 8.472792440081532e-05\n",
      "test loss is 0.00013630621178382007\n",
      "Batch: 10400,train loss is: 6.549513429551999e-05\n",
      "test loss is 0.00013691692347422366\n",
      "Batch: 10500,train loss is: 0.00021943643052019227\n",
      "test loss is 0.00013996601224918568\n",
      "Batch: 10600,train loss is: 9.217936258111442e-05\n",
      "test loss is 0.00013740532679838016\n",
      "Batch: 10700,train loss is: 0.00012355764866929698\n",
      "test loss is 0.00013732019262517982\n",
      "Batch: 10800,train loss is: 0.00018524581951368617\n",
      "test loss is 0.0001363797821979924\n",
      "Batch: 10900,train loss is: 0.00011335623827720369\n",
      "test loss is 0.0001412699890572521\n",
      "Batch: 11000,train loss is: 0.00010844620684295071\n",
      "test loss is 0.00013750774504817226\n",
      "Batch: 11100,train loss is: 0.00010807570002842693\n",
      "test loss is 0.00014725125487750218\n",
      "Batch: 11200,train loss is: 0.00012412397350660597\n",
      "test loss is 0.0001367651928580306\n",
      "Batch: 11300,train loss is: 9.067297152418796e-05\n",
      "test loss is 0.00013810322260060596\n",
      "Batch: 11400,train loss is: 0.00018210315319875425\n",
      "test loss is 0.00014604359265299832\n",
      "Batch: 11500,train loss is: 7.915068676568235e-05\n",
      "test loss is 0.00013887516872226874\n",
      "Batch: 11600,train loss is: 0.00022101567252113955\n",
      "test loss is 0.0001410968500815282\n",
      "Batch: 11700,train loss is: 9.314853815692107e-05\n",
      "test loss is 0.00013601131190529481\n",
      "Batch: 11800,train loss is: 9.714670930586426e-05\n",
      "test loss is 0.0001417968792629934\n",
      "Batch: 11900,train loss is: 0.00012592102062316372\n",
      "test loss is 0.00013800576137051648\n",
      "Batch: 12000,train loss is: 0.00014218362982090966\n",
      "test loss is 0.00013824886485023\n",
      "Batch: 12100,train loss is: 0.0007632798644871401\n",
      "test loss is 0.00014546240420821774\n",
      "Batch: 12200,train loss is: 0.00010577190886529383\n",
      "test loss is 0.00013720002665124664\n",
      "Batch: 12300,train loss is: 0.00010616206619115881\n",
      "test loss is 0.00013859970900541855\n",
      "Batch: 12400,train loss is: 0.00010775152144094032\n",
      "test loss is 0.00014022040794935776\n",
      "Batch: 12500,train loss is: 0.0002615303664805207\n",
      "test loss is 0.0001362984173157189\n",
      "Batch: 12600,train loss is: 7.730098591940399e-05\n",
      "test loss is 0.00013513108201628643\n",
      "Batch: 12700,train loss is: 7.612271559539812e-05\n",
      "test loss is 0.00013696618325593498\n",
      "Batch: 12800,train loss is: 7.431525460450573e-05\n",
      "test loss is 0.0001363898571089273\n",
      "Batch: 12900,train loss is: 0.0001389562195506194\n",
      "test loss is 0.00013486560799383257\n",
      "Batch: 13000,train loss is: 0.00010402057997653954\n",
      "test loss is 0.00015059978884251593\n",
      "Batch: 13100,train loss is: 0.00023110887626129475\n",
      "test loss is 0.00013656272072417794\n",
      "Batch: 13200,train loss is: 0.00011568165968993555\n",
      "test loss is 0.0001328525352208165\n",
      "Batch: 13300,train loss is: 8.142687062586744e-05\n",
      "test loss is 0.00016279230152872092\n",
      "Batch: 13400,train loss is: 0.00010569671095197389\n",
      "test loss is 0.00014699667763914267\n",
      "Batch: 13500,train loss is: 8.96900720160852e-05\n",
      "test loss is 0.00014337847222585434\n",
      "Batch: 13600,train loss is: 0.00010530713749621133\n",
      "test loss is 0.0001353740983985293\n",
      "Batch: 13700,train loss is: 0.00011355460016766039\n",
      "test loss is 0.00013639244108462246\n",
      "Batch: 13800,train loss is: 0.00013397136672748956\n",
      "test loss is 0.00014086546461164674\n",
      "Batch: 13900,train loss is: 6.708661488527e-05\n",
      "test loss is 0.0001399848164422195\n",
      "Batch: 14000,train loss is: 9.581062574378131e-05\n",
      "test loss is 0.0001362290710528007\n",
      "Batch: 14100,train loss is: 0.0001138815851294743\n",
      "test loss is 0.00016278842274947817\n",
      "Batch: 14200,train loss is: 5.646770347281871e-05\n",
      "test loss is 0.0001360883080816085\n",
      "Batch: 14300,train loss is: 6.467730897137816e-05\n",
      "test loss is 0.00013562526341501447\n",
      "Batch: 14400,train loss is: 0.00015536469500329898\n",
      "test loss is 0.00013285925177855963\n",
      "Batch: 14500,train loss is: 5.9540594554305815e-05\n",
      "test loss is 0.0001337827764990485\n",
      "Batch: 14600,train loss is: 0.0002596499968896464\n",
      "test loss is 0.00013582088446677023\n",
      "Batch: 14700,train loss is: 7.938943626572048e-05\n",
      "test loss is 0.00013448856920758131\n",
      "Batch: 14800,train loss is: 0.00015777298414764194\n",
      "test loss is 0.00013578840866321383\n",
      "Batch: 14900,train loss is: 0.00010154234816812952\n",
      "test loss is 0.00013717050442349786\n",
      "Batch: 15000,train loss is: 8.696964146905761e-05\n",
      "test loss is 0.0001528020779625639\n",
      "Batch: 15100,train loss is: 0.0001921709048669669\n",
      "test loss is 0.00013807362673047357\n",
      "Batch: 15200,train loss is: 0.00018036597119799603\n",
      "test loss is 0.00013227508396705323\n",
      "Batch: 15300,train loss is: 0.0001626663124254465\n",
      "test loss is 0.00013225737485396378\n",
      "Batch: 15400,train loss is: 9.915899007794082e-05\n",
      "test loss is 0.0001402176269090856\n",
      "Batch: 15500,train loss is: 0.0001812234271014382\n",
      "test loss is 0.00014582615973739567\n",
      "Batch: 15600,train loss is: 0.0002316201916525556\n",
      "test loss is 0.0001401004012867817\n",
      "Batch: 15700,train loss is: 9.947886335438988e-05\n",
      "test loss is 0.00015094945042637601\n",
      "Batch: 15800,train loss is: 0.00010003715726783456\n",
      "test loss is 0.00013669086680381405\n",
      "Batch: 15900,train loss is: 0.0002642463118473685\n",
      "test loss is 0.000137006557961119\n",
      "Batch: 16000,train loss is: 0.00010677121487558937\n",
      "test loss is 0.00013741774690580712\n",
      "Batch: 16100,train loss is: 0.00015471401143109074\n",
      "test loss is 0.0001323637678956951\n",
      "Batch: 16200,train loss is: 6.809345160641567e-05\n",
      "test loss is 0.00013976063576623477\n",
      "Batch: 16300,train loss is: 0.00010671134496605868\n",
      "test loss is 0.0001393057998105532\n",
      "Batch: 16400,train loss is: 0.00014573042350964763\n",
      "test loss is 0.00013569637981208338\n",
      "Batch: 16500,train loss is: 0.00014869893088926105\n",
      "test loss is 0.0001402987091217874\n",
      "Batch: 16600,train loss is: 7.199741774056208e-05\n",
      "test loss is 0.000140121794362552\n",
      "Batch: 16700,train loss is: 0.00018812658469361286\n",
      "test loss is 0.0001406785038293179\n",
      "Batch: 16800,train loss is: 0.00013888872915617363\n",
      "test loss is 0.00014083354074749259\n",
      "Batch: 16900,train loss is: 0.00013102724178271562\n",
      "test loss is 0.0001460652814253749\n",
      "Batch: 17000,train loss is: 0.00014177694486284937\n",
      "test loss is 0.00013675916598856718\n",
      "Batch: 17100,train loss is: 0.0001299411869488798\n",
      "test loss is 0.00013494930937118656\n",
      "Batch: 17200,train loss is: 0.00010370060840371615\n",
      "test loss is 0.0001421162113530665\n",
      "Batch: 17300,train loss is: 0.00013186712031424381\n",
      "test loss is 0.0001375592032549888\n",
      "Batch: 17400,train loss is: 6.213336250998828e-05\n",
      "test loss is 0.00013955244386759517\n",
      "Batch: 17500,train loss is: 0.00010205388313740994\n",
      "test loss is 0.0001340311574750737\n",
      "Batch: 17600,train loss is: 7.575880171331173e-05\n",
      "test loss is 0.0001374003272907276\n",
      "Batch: 17700,train loss is: 0.00013027418800221538\n",
      "test loss is 0.00013270732580352405\n",
      "Batch: 17800,train loss is: 0.00020062881697913\n",
      "test loss is 0.0001429320732307407\n",
      "Batch: 17900,train loss is: 9.532490469484898e-05\n",
      "test loss is 0.00013356414786525346\n",
      "Batch: 18000,train loss is: 9.375810877180068e-05\n",
      "test loss is 0.0001384761787443097\n",
      "Batch: 18100,train loss is: 0.00013247540362195612\n",
      "test loss is 0.00013440397822533887\n",
      "Batch: 18200,train loss is: 9.609447856580148e-05\n",
      "test loss is 0.00013198651613495293\n",
      "Batch: 18300,train loss is: 0.00020839750806626276\n",
      "test loss is 0.0001373975542834075\n",
      "Batch: 18400,train loss is: 0.00010713001760762494\n",
      "test loss is 0.00013118525193709767\n",
      "Batch: 18500,train loss is: 9.583787097330213e-05\n",
      "test loss is 0.00013562957974547037\n",
      "Batch: 18600,train loss is: 9.038302880569073e-05\n",
      "test loss is 0.00013787078040569038\n",
      "Batch: 18700,train loss is: 0.0001559224053032161\n",
      "test loss is 0.00014044441258062425\n",
      "Batch: 18800,train loss is: 0.00013180512555269227\n",
      "test loss is 0.00013777715258046666\n",
      "Batch: 18900,train loss is: 0.00023694300572239796\n",
      "test loss is 0.00014788420166800018\n",
      "Batch: 19000,train loss is: 7.553864588097854e-05\n",
      "test loss is 0.0001456985234480158\n",
      "Batch: 19100,train loss is: 7.468594761885072e-05\n",
      "test loss is 0.00017214676816395313\n",
      "Batch: 19200,train loss is: 0.0001026496651762084\n",
      "test loss is 0.00014471850674078733\n",
      "Batch: 19300,train loss is: 0.00013549270796900517\n",
      "test loss is 0.00013846766828494933\n",
      "Batch: 19400,train loss is: 7.349995858036287e-05\n",
      "test loss is 0.0001344735867113709\n",
      "Batch: 19500,train loss is: 0.00010308319634579473\n",
      "test loss is 0.00014348032384662736\n",
      "Batch: 19600,train loss is: 6.797214445686228e-05\n",
      "test loss is 0.00013769358140881252\n",
      "Batch: 19700,train loss is: 0.00015764865862482908\n",
      "test loss is 0.0001340762213042555\n",
      "Batch: 19800,train loss is: 0.00012044180775267862\n",
      "test loss is 0.00013347725428187866\n",
      "Batch: 19900,train loss is: 8.269902981804533e-05\n",
      "test loss is 0.0001413749323270221\n",
      "Batch: 20000,train loss is: 0.0001558293645934996\n",
      "test loss is 0.00013926766989023017\n",
      "Batch: 20100,train loss is: 0.0001158253301473371\n",
      "test loss is 0.00013806625121461574\n",
      "Batch: 20200,train loss is: 8.381072125353507e-05\n",
      "test loss is 0.0001319440844391429\n",
      "Batch: 20300,train loss is: 7.271286742131424e-05\n",
      "test loss is 0.0001351236228643889\n",
      "Batch: 20400,train loss is: 0.00013561758853370315\n",
      "test loss is 0.0002183113251087257\n",
      "Batch: 20500,train loss is: 8.950637202953301e-05\n",
      "test loss is 0.00013280550932633268\n",
      "Batch: 20600,train loss is: 0.000243450971355903\n",
      "test loss is 0.0001399296020510331\n",
      "Batch: 20700,train loss is: 0.00013102748673476732\n",
      "test loss is 0.00013419917444767302\n",
      "Batch: 20800,train loss is: 5.9669904948432075e-05\n",
      "test loss is 0.0001364386894355438\n",
      "Batch: 20900,train loss is: 6.247641589264994e-05\n",
      "test loss is 0.0001344554830013019\n",
      "Batch: 21000,train loss is: 5.612757391528373e-05\n",
      "test loss is 0.00013356984299457337\n",
      "Batch: 21100,train loss is: 0.00010045467273399137\n",
      "test loss is 0.00013449303223440253\n",
      "Batch: 21200,train loss is: 0.0001140805160032168\n",
      "test loss is 0.0001430697741810042\n",
      "Batch: 21300,train loss is: 6.800810071991969e-05\n",
      "test loss is 0.00013436977346509074\n",
      "Batch: 21400,train loss is: 0.00016887705623582034\n",
      "test loss is 0.00014060665888756667\n",
      "Batch: 21500,train loss is: 8.48737620512195e-05\n",
      "test loss is 0.00013506316793016637\n",
      "Batch: 21600,train loss is: 0.00013751299849564842\n",
      "test loss is 0.00013226361095410377\n",
      "Batch: 21700,train loss is: 7.148725209724765e-05\n",
      "test loss is 0.00013796861258365687\n",
      "Batch: 21800,train loss is: 0.00015421906268848917\n",
      "test loss is 0.00014209116697637674\n",
      "Batch: 21900,train loss is: 7.732478987135348e-05\n",
      "test loss is 0.0001354231902751511\n",
      "Batch: 22000,train loss is: 0.00013742885555610807\n",
      "test loss is 0.00013953995143567929\n",
      "Batch: 22100,train loss is: 9.388904664060317e-05\n",
      "test loss is 0.00013244773170901014\n",
      "Batch: 22200,train loss is: 8.054842366296638e-05\n",
      "test loss is 0.0001323279884140518\n",
      "Batch: 22300,train loss is: 0.00011304365784770081\n",
      "test loss is 0.00014217679209910561\n",
      "Batch: 22400,train loss is: 0.00010446248979598258\n",
      "test loss is 0.00014003760571839967\n",
      "Batch: 22500,train loss is: 0.00012173824261085414\n",
      "test loss is 0.00013253503912900888\n",
      "Batch: 22600,train loss is: 0.00010905126902239183\n",
      "test loss is 0.00014401242573801082\n",
      "Batch: 22700,train loss is: 7.139409875739298e-05\n",
      "test loss is 0.00013421590723937166\n",
      "Batch: 22800,train loss is: 0.00011742966367718762\n",
      "test loss is 0.00013227939706134424\n",
      "Batch: 22900,train loss is: 0.00011142918933568749\n",
      "test loss is 0.00013444647432786158\n",
      "Batch: 23000,train loss is: 0.00019425435389402277\n",
      "test loss is 0.0001342453946795703\n",
      "Batch: 23100,train loss is: 9.705162465177581e-05\n",
      "test loss is 0.0001320094289079983\n",
      "Batch: 23200,train loss is: 7.09081693918265e-05\n",
      "test loss is 0.0001332825531527195\n",
      "Batch: 23300,train loss is: 9.57716856358683e-05\n",
      "test loss is 0.00013597252700947186\n",
      "Batch: 23400,train loss is: 0.00011087568977351084\n",
      "test loss is 0.00013173758147496314\n",
      "Batch: 23500,train loss is: 0.00018111301701243142\n",
      "test loss is 0.00013313735454378503\n",
      "Batch: 23600,train loss is: 8.20598127823717e-05\n",
      "test loss is 0.00013439746254922616\n",
      "Batch: 23700,train loss is: 0.00011974422007922966\n",
      "test loss is 0.00015312010412429404\n",
      "Batch: 23800,train loss is: 0.00019799402530376153\n",
      "test loss is 0.0001389468035161737\n",
      "Batch: 23900,train loss is: 0.00027431839131170343\n",
      "test loss is 0.00013171724750932906\n",
      "Batch: 24000,train loss is: 0.00010694327891526124\n",
      "test loss is 0.00014221488055601438\n",
      "Batch: 24100,train loss is: 0.00010868825868811612\n",
      "test loss is 0.00013757270665210288\n",
      "Batch: 24200,train loss is: 0.00018499727559663157\n",
      "test loss is 0.00013457312110703304\n",
      "Batch: 24300,train loss is: 0.0001041795562429523\n",
      "test loss is 0.00016703171935745535\n",
      "Batch: 24400,train loss is: 0.00015090486707612988\n",
      "test loss is 0.00013436637278424094\n",
      "Batch: 24500,train loss is: 0.00019603332001971163\n",
      "test loss is 0.00013495255655106072\n",
      "Batch: 24600,train loss is: 8.338726341252426e-05\n",
      "test loss is 0.0001500336684794848\n",
      "Batch: 24700,train loss is: 5.9701499457249386e-05\n",
      "test loss is 0.00013040289006510943\n",
      "Batch: 24800,train loss is: 0.00023661089351641212\n",
      "test loss is 0.00013975616972589246\n",
      "Batch: 24900,train loss is: 8.451631445537351e-05\n",
      "test loss is 0.0001499887427518536\n",
      "Batch: 25000,train loss is: 0.00013999133906588223\n",
      "test loss is 0.0001321175959395961\n",
      "Batch: 25100,train loss is: 7.797441558897154e-05\n",
      "test loss is 0.00013881663667728684\n",
      "Batch: 25200,train loss is: 0.00012403788681836274\n",
      "test loss is 0.0001313808792079898\n",
      "Batch: 25300,train loss is: 0.00017697656287192753\n",
      "test loss is 0.00013360703351237264\n",
      "Batch: 25400,train loss is: 0.00012143341036832474\n",
      "test loss is 0.0001359251461167947\n",
      "Batch: 25500,train loss is: 8.330348095340873e-05\n",
      "test loss is 0.00013366610642418994\n",
      "Batch: 25600,train loss is: 0.00015531822024754768\n",
      "test loss is 0.00015114241430379304\n",
      "Batch: 25700,train loss is: 0.00017808258630330932\n",
      "test loss is 0.00013845741949908427\n",
      "Batch: 25800,train loss is: 0.00010508174472758611\n",
      "test loss is 0.00013132845880455612\n",
      "Batch: 25900,train loss is: 8.93074988758951e-05\n",
      "test loss is 0.00013563170087058722\n",
      "Batch: 26000,train loss is: 0.00015255333357418505\n",
      "test loss is 0.000136203052353586\n",
      "Batch: 26100,train loss is: 0.00012593947236023325\n",
      "test loss is 0.00013967561623409768\n",
      "Batch: 26200,train loss is: 0.0001836892933942776\n",
      "test loss is 0.0001317706335838824\n",
      "Batch: 26300,train loss is: 0.00014916104155750188\n",
      "test loss is 0.00015344063735507507\n",
      "Batch: 26400,train loss is: 6.48378302102789e-05\n",
      "test loss is 0.0001368884511856865\n",
      "Batch: 26500,train loss is: 0.00010712775834913523\n",
      "test loss is 0.00013716184417392476\n",
      "Batch: 26600,train loss is: 0.00019733716919420323\n",
      "test loss is 0.0001310670417099096\n",
      "Batch: 26700,train loss is: 0.0001080112967688421\n",
      "test loss is 0.00015218874939212503\n",
      "Batch: 26800,train loss is: 0.00012432045664225965\n",
      "test loss is 0.00013270268516378552\n",
      "Batch: 26900,train loss is: 7.079114774585542e-05\n",
      "test loss is 0.000131925591662915\n",
      "Batch: 27000,train loss is: 0.00011711520520870126\n",
      "test loss is 0.0001392513911677446\n",
      "Batch: 27100,train loss is: 0.00014795166905341687\n",
      "test loss is 0.00014302097609802524\n",
      "Batch: 27200,train loss is: 0.00012029263572666004\n",
      "test loss is 0.00013463201532401772\n",
      "Batch: 27300,train loss is: 6.068418883368549e-05\n",
      "test loss is 0.00013361633567208128\n",
      "Batch: 27400,train loss is: 0.00013119795479168183\n",
      "test loss is 0.0001357170100972775\n",
      "Batch: 27500,train loss is: 0.00011749030853685213\n",
      "test loss is 0.0001314209817132725\n",
      "Batch: 27600,train loss is: 9.530254016364799e-05\n",
      "test loss is 0.0001390123640218728\n",
      "Batch: 27700,train loss is: 0.00033795328709667325\n",
      "test loss is 0.00013955396984423823\n",
      "Batch: 27800,train loss is: 0.00013276818211381647\n",
      "test loss is 0.00014148048654184854\n",
      "Batch: 27900,train loss is: 0.00012383599948971951\n",
      "test loss is 0.00013385060084826288\n",
      "Batch: 28000,train loss is: 0.00010246949661499258\n",
      "test loss is 0.00014089815304401036\n",
      "Batch: 28100,train loss is: 0.0001693697817803607\n",
      "test loss is 0.0001381514167183467\n",
      "Batch: 28200,train loss is: 0.0001095271441397826\n",
      "test loss is 0.00013695603974677812\n",
      "Batch: 28300,train loss is: 0.00040194054187578946\n",
      "test loss is 0.00014113555433034627\n",
      "Batch: 28400,train loss is: 0.00010577188181414678\n",
      "test loss is 0.0001293341787947662\n",
      "Batch: 28500,train loss is: 0.00018711445094921598\n",
      "test loss is 0.000140390645161623\n",
      "Batch: 28600,train loss is: 0.0001321303853382122\n",
      "test loss is 0.0001363430747150178\n",
      "Batch: 28700,train loss is: 0.00010266021226056489\n",
      "test loss is 0.0001342704909024018\n",
      "Batch: 28800,train loss is: 0.00019506794643018552\n",
      "test loss is 0.0001331519463646911\n",
      "Batch: 28900,train loss is: 0.0001212506904856192\n",
      "test loss is 0.00015050041417189093\n",
      "Batch: 29000,train loss is: 0.00012475409445576125\n",
      "test loss is 0.00013829074364994583\n",
      "Batch: 29100,train loss is: 9.37377106154235e-05\n",
      "test loss is 0.00014004112095249125\n",
      "Batch: 29200,train loss is: 0.00012534026887556034\n",
      "test loss is 0.00016245199429277624\n",
      "Batch: 29300,train loss is: 0.0002302925665780355\n",
      "test loss is 0.00013887030160445504\n",
      "Batch: 29400,train loss is: 0.00016910073858449198\n",
      "test loss is 0.00013850062673416273\n",
      "Batch: 29500,train loss is: 0.00013758929011073673\n",
      "test loss is 0.00012988230279731739\n",
      "Batch: 29600,train loss is: 0.00016429368357076077\n",
      "test loss is 0.00014055179464777888\n",
      "Batch: 29700,train loss is: 6.930765695965078e-05\n",
      "test loss is 0.00013231179241814738\n",
      "Batch: 29800,train loss is: 0.0002326187612543568\n",
      "test loss is 0.00013693225159957075\n",
      "Batch: 29900,train loss is: 0.0004031558291080135\n",
      "test loss is 0.00013336876951425332\n",
      "Batch: 30000,train loss is: 0.00010481822584841709\n",
      "test loss is 0.00013170183194775593\n",
      "Batch: 30100,train loss is: 0.00020325185652929692\n",
      "test loss is 0.00015916930009935283\n",
      "Batch: 30200,train loss is: 9.117111551241772e-05\n",
      "test loss is 0.0001312322874361568\n",
      "Batch: 30300,train loss is: 9.4654757222427e-05\n",
      "test loss is 0.00013232724553719513\n",
      "Batch: 30400,train loss is: 6.150878616027592e-05\n",
      "test loss is 0.00014277258653955984\n",
      "Batch: 30500,train loss is: 0.00012778599089909398\n",
      "test loss is 0.00015909395445473927\n",
      "Batch: 30600,train loss is: 9.241429251183801e-05\n",
      "test loss is 0.00013378348612558692\n",
      "Batch: 30700,train loss is: 9.58418513571712e-05\n",
      "test loss is 0.00013320449271616853\n",
      "Batch: 30800,train loss is: 5.866805043578432e-05\n",
      "test loss is 0.0001397960555541893\n",
      "Batch: 30900,train loss is: 8.815691766971222e-05\n",
      "test loss is 0.00013404384729833222\n",
      "Batch: 31000,train loss is: 7.962437517045955e-05\n",
      "test loss is 0.00013229457797301917\n",
      "Batch: 31100,train loss is: 0.00011127908261136003\n",
      "test loss is 0.00017346363704799848\n",
      "Batch: 31200,train loss is: 0.00010946972782182166\n",
      "test loss is 0.0001364124323905471\n",
      "Batch: 31300,train loss is: 0.00012390978762032783\n",
      "test loss is 0.00013991404306165376\n",
      "Batch: 31400,train loss is: 7.83180783924317e-05\n",
      "test loss is 0.00013530030616563658\n",
      "Batch: 31500,train loss is: 0.00012781923230069638\n",
      "test loss is 0.00013857058529915202\n",
      "Batch: 31600,train loss is: 0.00014744505804155567\n",
      "test loss is 0.00013574754723555085\n",
      "Batch: 31700,train loss is: 7.991029850025793e-05\n",
      "test loss is 0.00014137507544740183\n",
      "Batch: 31800,train loss is: 0.00020544239988794122\n",
      "test loss is 0.0001316671799416375\n",
      "Batch: 31900,train loss is: 0.00017061747323939697\n",
      "test loss is 0.00013403089778068455\n",
      "Batch: 32000,train loss is: 0.0005306562940741707\n",
      "test loss is 0.0001434173457787235\n",
      "Batch: 32100,train loss is: 6.325090615064626e-05\n",
      "test loss is 0.0001303233110809232\n",
      "Batch: 32200,train loss is: 9.20654209096222e-05\n",
      "test loss is 0.00013414332490946463\n",
      "Batch: 32300,train loss is: 0.00010002918352167404\n",
      "test loss is 0.00013040124229271176\n",
      "Batch: 32400,train loss is: 6.603037802224693e-05\n",
      "test loss is 0.00013036481775454065\n",
      "Batch: 32500,train loss is: 0.00014069469586144525\n",
      "test loss is 0.00013115724127664538\n",
      "Batch: 32600,train loss is: 9.735214570883729e-05\n",
      "test loss is 0.00015012269286177697\n",
      "Batch: 32700,train loss is: 6.0366515314181366e-05\n",
      "test loss is 0.0001338191424573995\n",
      "Batch: 32800,train loss is: 0.00014278413528817107\n",
      "test loss is 0.00015488989064809327\n",
      "Batch: 32900,train loss is: 0.00011111106468163567\n",
      "test loss is 0.00012930176150944845\n",
      "Batch: 33000,train loss is: 7.00366150307356e-05\n",
      "test loss is 0.00013449270531785002\n",
      "Batch: 33100,train loss is: 7.657816132360081e-05\n",
      "test loss is 0.00014217642246274837\n",
      "Batch: 33200,train loss is: 6.062350954572718e-05\n",
      "test loss is 0.00012969019783849788\n",
      "Batch: 33300,train loss is: 8.857869070681498e-05\n",
      "test loss is 0.00013428492288545944\n",
      "Batch: 33400,train loss is: 0.00010745360470138959\n",
      "test loss is 0.00013339392059836895\n",
      "Batch: 33500,train loss is: 9.600481683575023e-05\n",
      "test loss is 0.00014280217885412458\n",
      "Batch: 33600,train loss is: 8.023202255888449e-05\n",
      "test loss is 0.0001425952187328306\n",
      "Batch: 33700,train loss is: 0.00011444615731190376\n",
      "test loss is 0.00013889846100744872\n",
      "Batch: 33800,train loss is: 0.00015829581890152087\n",
      "test loss is 0.00013010182767475123\n",
      "Batch: 33900,train loss is: 7.702931429388085e-05\n",
      "test loss is 0.00013184574043010526\n",
      "-----------------------Epoch: 2----------------------------------\n",
      "Batch: 0,train loss is: 0.00015737924654609102\n",
      "test loss is 0.0001385163600079242\n",
      "Batch: 100,train loss is: 0.0002445370773044806\n",
      "test loss is 0.00014546275928895927\n",
      "Batch: 200,train loss is: 0.00011921153936618852\n",
      "test loss is 0.00013056531182856213\n",
      "Batch: 300,train loss is: 0.00010550139334740367\n",
      "test loss is 0.00013679989718166707\n",
      "Batch: 400,train loss is: 9.570895807300308e-05\n",
      "test loss is 0.00013745772549278231\n",
      "Batch: 500,train loss is: 8.012312010132184e-05\n",
      "test loss is 0.00014411940825828455\n",
      "Batch: 600,train loss is: 4.8106470957280075e-05\n",
      "test loss is 0.00013640336689146402\n",
      "Batch: 700,train loss is: 9.788783812206703e-05\n",
      "test loss is 0.00014931083788219594\n",
      "Batch: 800,train loss is: 0.00015189215525176117\n",
      "test loss is 0.00013009265973821938\n",
      "Batch: 900,train loss is: 9.431026774308955e-05\n",
      "test loss is 0.00014116380934326483\n",
      "Batch: 1000,train loss is: 0.00010588962126580428\n",
      "test loss is 0.00013703992247796743\n",
      "Batch: 1100,train loss is: 0.00023730672097383102\n",
      "test loss is 0.00014150050087632387\n",
      "Batch: 1200,train loss is: 6.533559691567848e-05\n",
      "test loss is 0.00013140479388133708\n",
      "Batch: 1300,train loss is: 9.137296022934663e-05\n",
      "test loss is 0.00013668439217711054\n",
      "Batch: 1400,train loss is: 8.655874363940175e-05\n",
      "test loss is 0.00013424025946723707\n",
      "Batch: 1500,train loss is: 0.00011069674111541093\n",
      "test loss is 0.00013502888186422832\n",
      "Batch: 1600,train loss is: 9.144154703137402e-05\n",
      "test loss is 0.00012777101420272826\n",
      "Batch: 1700,train loss is: 0.0001415347912336106\n",
      "test loss is 0.00013011119653325324\n",
      "Batch: 1800,train loss is: 7.695417597054165e-05\n",
      "test loss is 0.0001299021939974494\n",
      "Batch: 1900,train loss is: 0.00011201862762094325\n",
      "test loss is 0.00015691231368469448\n",
      "Batch: 2000,train loss is: 0.00010842503590548679\n",
      "test loss is 0.00013231630628227013\n",
      "Batch: 2100,train loss is: 0.00010586175434259705\n",
      "test loss is 0.00014480073608314582\n",
      "Batch: 2200,train loss is: 0.00010990046002129946\n",
      "test loss is 0.00013504119449922892\n",
      "Batch: 2300,train loss is: 7.449574117553467e-05\n",
      "test loss is 0.00012868664181671152\n",
      "Batch: 2400,train loss is: 8.497951354548611e-05\n",
      "test loss is 0.00013140605956457637\n",
      "Batch: 2500,train loss is: 0.00013729630182766984\n",
      "test loss is 0.00013525115216970606\n",
      "Batch: 2600,train loss is: 7.880265332288392e-05\n",
      "test loss is 0.00013193421683706648\n",
      "Batch: 2700,train loss is: 0.00011160903082166761\n",
      "test loss is 0.00013054964759522754\n",
      "Batch: 2800,train loss is: 0.00011877671862103418\n",
      "test loss is 0.00013274529089351623\n",
      "Batch: 2900,train loss is: 0.00013777558169791255\n",
      "test loss is 0.000150569841264566\n",
      "Batch: 3000,train loss is: 0.0001064544464132121\n",
      "test loss is 0.00013785473311279757\n",
      "Batch: 3100,train loss is: 6.539184043154391e-05\n",
      "test loss is 0.00012804481700791172\n",
      "Batch: 3200,train loss is: 0.0001785977136108871\n",
      "test loss is 0.00013121518249151838\n",
      "Batch: 3300,train loss is: 0.0001113085021390123\n",
      "test loss is 0.00012917604615093263\n",
      "Batch: 3400,train loss is: 0.00010956993635236155\n",
      "test loss is 0.00016155965003729426\n",
      "Batch: 3500,train loss is: 5.168537479975084e-05\n",
      "test loss is 0.0001323264289261518\n",
      "Batch: 3600,train loss is: 6.953275771342365e-05\n",
      "test loss is 0.0001347094964677356\n",
      "Batch: 3700,train loss is: 9.9294230944623e-05\n",
      "test loss is 0.00013097689982871522\n",
      "Batch: 3800,train loss is: 0.00010593014131923096\n",
      "test loss is 0.0001326043458040614\n",
      "Batch: 3900,train loss is: 0.0001544128819571776\n",
      "test loss is 0.00014146745756499294\n",
      "Batch: 4000,train loss is: 5.8177150079541814e-05\n",
      "test loss is 0.00013051934645825407\n",
      "Batch: 4100,train loss is: 0.00012779238336330052\n",
      "test loss is 0.00013501364735032798\n",
      "Batch: 4200,train loss is: 0.0001363169364712273\n",
      "test loss is 0.00013163997445496368\n",
      "Batch: 4300,train loss is: 8.753365829820325e-05\n",
      "test loss is 0.00013800394959365356\n",
      "Batch: 4400,train loss is: 8.876540875661323e-05\n",
      "test loss is 0.0001315936459323398\n",
      "Batch: 4500,train loss is: 9.589378739283393e-05\n",
      "test loss is 0.0001310422881520342\n",
      "Batch: 4600,train loss is: 8.293174067643376e-05\n",
      "test loss is 0.00013212798751433772\n",
      "Batch: 4700,train loss is: 0.00014560283974236613\n",
      "test loss is 0.00012943000277027632\n",
      "Batch: 4800,train loss is: 0.0002564609139292807\n",
      "test loss is 0.00013266582039523286\n",
      "Batch: 4900,train loss is: 9.345910606666241e-05\n",
      "test loss is 0.00014912316142671786\n",
      "Batch: 5000,train loss is: 8.610884898838518e-05\n",
      "test loss is 0.00012904269770305706\n",
      "Batch: 5100,train loss is: 9.533617464923995e-05\n",
      "test loss is 0.00013168378567938023\n",
      "Batch: 5200,train loss is: 5.341489218578638e-05\n",
      "test loss is 0.00012922312621178506\n",
      "Batch: 5300,train loss is: 0.0001306408176879298\n",
      "test loss is 0.0001726321433881098\n",
      "Batch: 5400,train loss is: 0.00018171774397543022\n",
      "test loss is 0.00013052614306067342\n",
      "Batch: 5500,train loss is: 8.244548000425421e-05\n",
      "test loss is 0.00013084683510609411\n",
      "Batch: 5600,train loss is: 0.00016781029591711336\n",
      "test loss is 0.00013061874977267379\n",
      "Batch: 5700,train loss is: 0.0001415933514743779\n",
      "test loss is 0.00013933167062911298\n",
      "Batch: 5800,train loss is: 0.00011360305606734147\n",
      "test loss is 0.0001365171542144384\n",
      "Batch: 5900,train loss is: 0.00035085292144081684\n",
      "test loss is 0.00013302395193429527\n",
      "Batch: 6000,train loss is: 0.0001618777617769202\n",
      "test loss is 0.00013254452684227896\n",
      "Batch: 6100,train loss is: 0.00010595730383800646\n",
      "test loss is 0.00015395147241448537\n",
      "Batch: 6200,train loss is: 9.40668911603289e-05\n",
      "test loss is 0.00012672050010443695\n",
      "Batch: 6300,train loss is: 0.0001584227490797818\n",
      "test loss is 0.00013016153695992854\n",
      "Batch: 6400,train loss is: 9.637685217564521e-05\n",
      "test loss is 0.0001309374225496198\n",
      "Batch: 6500,train loss is: 8.064933261585602e-05\n",
      "test loss is 0.0001365102495874941\n",
      "Batch: 6600,train loss is: 0.0002733144276255088\n",
      "test loss is 0.0001280321981679411\n",
      "Batch: 6700,train loss is: 5.348331893517237e-05\n",
      "test loss is 0.000133506108682386\n",
      "Batch: 6800,train loss is: 0.00015167094380857282\n",
      "test loss is 0.00012854163181057814\n",
      "Batch: 6900,train loss is: 7.979229968417362e-05\n",
      "test loss is 0.00012746781645199036\n",
      "Batch: 7000,train loss is: 0.00012766602001167005\n",
      "test loss is 0.0001299717364857854\n",
      "Batch: 7100,train loss is: 0.00010711361257246212\n",
      "test loss is 0.0001322106392560781\n",
      "Batch: 7200,train loss is: 6.173356300851919e-05\n",
      "test loss is 0.00013671316147427457\n",
      "Batch: 7300,train loss is: 0.0002930549235121701\n",
      "test loss is 0.00013495011579021143\n",
      "Batch: 7400,train loss is: 8.180255429122437e-05\n",
      "test loss is 0.0001435768620683869\n",
      "Batch: 7500,train loss is: 0.00021488809654411157\n",
      "test loss is 0.00013723775070618308\n",
      "Batch: 7600,train loss is: 0.00016225276273105216\n",
      "test loss is 0.00013428939829754154\n",
      "Batch: 7700,train loss is: 0.00018828020888347723\n",
      "test loss is 0.00013503394959484425\n",
      "Batch: 7800,train loss is: 6.357842370397571e-05\n",
      "test loss is 0.00013014423104256692\n",
      "Batch: 7900,train loss is: 7.26426444104033e-05\n",
      "test loss is 0.00013373059916756453\n",
      "Batch: 8000,train loss is: 5.3854031937320176e-05\n",
      "test loss is 0.0001328561055066411\n",
      "Batch: 8100,train loss is: 0.00024551658521689587\n",
      "test loss is 0.00013725368053048146\n",
      "Batch: 8200,train loss is: 0.00014173261190668235\n",
      "test loss is 0.00014217684526416185\n",
      "Batch: 8300,train loss is: 8.549213171244546e-05\n",
      "test loss is 0.00013703306525240754\n",
      "Batch: 8400,train loss is: 0.00011893849276058996\n",
      "test loss is 0.00012841638798131874\n",
      "Batch: 8500,train loss is: 0.00012416686507280654\n",
      "test loss is 0.00012966951065883947\n",
      "Batch: 8600,train loss is: 0.00013934353345442237\n",
      "test loss is 0.00013967466602723275\n",
      "Batch: 8700,train loss is: 0.00011095548365822204\n",
      "test loss is 0.00014501966787633906\n",
      "Batch: 8800,train loss is: 0.00017133988925884157\n",
      "test loss is 0.00013012955858941952\n",
      "Batch: 8900,train loss is: 0.00020536324225848738\n",
      "test loss is 0.00012766756380294745\n",
      "Batch: 9000,train loss is: 7.6066651972122e-05\n",
      "test loss is 0.00013039586845418354\n",
      "Batch: 9100,train loss is: 7.117014761584554e-05\n",
      "test loss is 0.00012608718990350228\n",
      "Batch: 9200,train loss is: 0.00011053968067833438\n",
      "test loss is 0.00012849313796549556\n",
      "Batch: 9300,train loss is: 0.00013526699784368525\n",
      "test loss is 0.00014754214899555446\n",
      "Batch: 9400,train loss is: 9.029699540239102e-05\n",
      "test loss is 0.00013310343092981186\n",
      "Batch: 9500,train loss is: 6.862511261802983e-05\n",
      "test loss is 0.0001269550196444689\n",
      "Batch: 9600,train loss is: 7.078136471562735e-05\n",
      "test loss is 0.00013182721700167574\n",
      "Batch: 9700,train loss is: 6.468503976248247e-05\n",
      "test loss is 0.00012688209682212224\n",
      "Batch: 9800,train loss is: 5.7209651790198475e-05\n",
      "test loss is 0.00013787681626665325\n",
      "Batch: 9900,train loss is: 6.075725762415922e-05\n",
      "test loss is 0.00014116927136160668\n",
      "Batch: 10000,train loss is: 9.283113715713932e-05\n",
      "test loss is 0.0001315055537510238\n",
      "Batch: 10100,train loss is: 0.00021990148758197736\n",
      "test loss is 0.00013998687824600818\n",
      "Batch: 10200,train loss is: 0.0002843362239516042\n",
      "test loss is 0.00012833776654974332\n",
      "Batch: 10300,train loss is: 7.93682941801651e-05\n",
      "test loss is 0.00012881196323331987\n",
      "Batch: 10400,train loss is: 5.9944109352192334e-05\n",
      "test loss is 0.00012940692974189167\n",
      "Batch: 10500,train loss is: 0.00019866346248122608\n",
      "test loss is 0.0001326470792066719\n",
      "Batch: 10600,train loss is: 8.644695972402014e-05\n",
      "test loss is 0.0001296798700804933\n",
      "Batch: 10700,train loss is: 0.00011809316838872779\n",
      "test loss is 0.00013020306301255892\n",
      "Batch: 10800,train loss is: 0.00017262650657972356\n",
      "test loss is 0.0001290371536269008\n",
      "Batch: 10900,train loss is: 0.00011163940415846091\n",
      "test loss is 0.0001365008524738959\n",
      "Batch: 11000,train loss is: 0.00010007638636834664\n",
      "test loss is 0.00012986020691227703\n",
      "Batch: 11100,train loss is: 0.00010073350849252394\n",
      "test loss is 0.0001384551764169787\n",
      "Batch: 11200,train loss is: 0.00012328863073142728\n",
      "test loss is 0.00012883092177227388\n",
      "Batch: 11300,train loss is: 8.423533438552239e-05\n",
      "test loss is 0.00013062189418766137\n",
      "Batch: 11400,train loss is: 0.00016657675797882308\n",
      "test loss is 0.00013762690550649922\n",
      "Batch: 11500,train loss is: 7.61315397997854e-05\n",
      "test loss is 0.00013222655978065867\n",
      "Batch: 11600,train loss is: 0.00020961056345651953\n",
      "test loss is 0.00013357803816358669\n",
      "Batch: 11700,train loss is: 8.761120296801029e-05\n",
      "test loss is 0.0001282040435965715\n",
      "Batch: 11800,train loss is: 9.445950505505947e-05\n",
      "test loss is 0.0001351748918042636\n",
      "Batch: 11900,train loss is: 0.00012125756011979837\n",
      "test loss is 0.0001310773425529136\n",
      "Batch: 12000,train loss is: 0.00013621925622760684\n",
      "test loss is 0.00013104177901057516\n",
      "Batch: 12100,train loss is: 0.0007127990976217877\n",
      "test loss is 0.0001376985779374503\n",
      "Batch: 12200,train loss is: 9.518175223729689e-05\n",
      "test loss is 0.0001301106817690607\n",
      "Batch: 12300,train loss is: 9.963747675604504e-05\n",
      "test loss is 0.00012988693069799497\n",
      "Batch: 12400,train loss is: 9.857716885305913e-05\n",
      "test loss is 0.00013102372532784861\n",
      "Batch: 12500,train loss is: 0.0002513913852537253\n",
      "test loss is 0.00012856555323576482\n",
      "Batch: 12600,train loss is: 7.033429558982542e-05\n",
      "test loss is 0.00012763208284982615\n",
      "Batch: 12700,train loss is: 6.718854551721981e-05\n",
      "test loss is 0.00012748778549026498\n",
      "Batch: 12800,train loss is: 7.056812204752053e-05\n",
      "test loss is 0.00012903210231621857\n",
      "Batch: 12900,train loss is: 0.0001290882087514978\n",
      "test loss is 0.00012726404728379475\n",
      "Batch: 13000,train loss is: 9.673887296804472e-05\n",
      "test loss is 0.00014206028154240907\n",
      "Batch: 13100,train loss is: 0.00021282811123809797\n",
      "test loss is 0.0001305733318442631\n",
      "Batch: 13200,train loss is: 0.00011356367898856788\n",
      "test loss is 0.000126084745653173\n",
      "Batch: 13300,train loss is: 7.795183750680497e-05\n",
      "test loss is 0.00015595706461504698\n",
      "Batch: 13400,train loss is: 9.756246109997703e-05\n",
      "test loss is 0.00013831090484898976\n",
      "Batch: 13500,train loss is: 8.678312763462458e-05\n",
      "test loss is 0.00013773941463343604\n",
      "Batch: 13600,train loss is: 0.00010284202585118929\n",
      "test loss is 0.0001281674042337487\n",
      "Batch: 13700,train loss is: 0.00010913965344588986\n",
      "test loss is 0.00012898956418771356\n",
      "Batch: 13800,train loss is: 0.00012369064450877535\n",
      "test loss is 0.00013259828884027003\n",
      "Batch: 13900,train loss is: 6.67371609291973e-05\n",
      "test loss is 0.0001346239808744496\n",
      "Batch: 14000,train loss is: 9.06016562811816e-05\n",
      "test loss is 0.00012902142050810785\n",
      "Batch: 14100,train loss is: 0.00010668742608865963\n",
      "test loss is 0.0001557857323868463\n",
      "Batch: 14200,train loss is: 5.357014200028497e-05\n",
      "test loss is 0.00012877882040556243\n",
      "Batch: 14300,train loss is: 5.972626748276402e-05\n",
      "test loss is 0.00012830297830413844\n",
      "Batch: 14400,train loss is: 0.000144374624680043\n",
      "test loss is 0.000125927876857616\n",
      "Batch: 14500,train loss is: 5.53215603713778e-05\n",
      "test loss is 0.00012647879099786475\n",
      "Batch: 14600,train loss is: 0.0002454484990987943\n",
      "test loss is 0.0001283505746718999\n",
      "Batch: 14700,train loss is: 7.706292457883368e-05\n",
      "test loss is 0.00012730667416078386\n",
      "Batch: 14800,train loss is: 0.0001511224990441272\n",
      "test loss is 0.00012755696426562912\n",
      "Batch: 14900,train loss is: 9.360773280199649e-05\n",
      "test loss is 0.00013010839731164223\n",
      "Batch: 15000,train loss is: 7.856113478121464e-05\n",
      "test loss is 0.00014475989360497048\n",
      "Batch: 15100,train loss is: 0.00018312599709078932\n",
      "test loss is 0.00013087862158933324\n",
      "Batch: 15200,train loss is: 0.00016338583705797065\n",
      "test loss is 0.00012509647911233877\n",
      "Batch: 15300,train loss is: 0.0001506708020898695\n",
      "test loss is 0.00012496411117471833\n",
      "Batch: 15400,train loss is: 9.460441933517508e-05\n",
      "test loss is 0.00013256803725292726\n",
      "Batch: 15500,train loss is: 0.0001766007733926142\n",
      "test loss is 0.000137409702828384\n",
      "Batch: 15600,train loss is: 0.00022307092914794724\n",
      "test loss is 0.0001338552800332732\n",
      "Batch: 15700,train loss is: 0.00010020071718700908\n",
      "test loss is 0.0001412042550742603\n",
      "Batch: 15800,train loss is: 9.091889207694661e-05\n",
      "test loss is 0.0001293584205764122\n",
      "Batch: 15900,train loss is: 0.0002478335334658053\n",
      "test loss is 0.00012945023984751276\n",
      "Batch: 16000,train loss is: 9.715481394086975e-05\n",
      "test loss is 0.00013003643964416194\n",
      "Batch: 16100,train loss is: 0.00015229662447798954\n",
      "test loss is 0.00012549932574810717\n",
      "Batch: 16200,train loss is: 6.0967172478982486e-05\n",
      "test loss is 0.00013261604364145517\n",
      "Batch: 16300,train loss is: 0.00010271094618307056\n",
      "test loss is 0.00013213113173077463\n",
      "Batch: 16400,train loss is: 0.00013900228507241082\n",
      "test loss is 0.00012843053629021662\n",
      "Batch: 16500,train loss is: 0.00014051463114311637\n",
      "test loss is 0.00013435928872371144\n",
      "Batch: 16600,train loss is: 7.281178562605022e-05\n",
      "test loss is 0.00013221601330082032\n",
      "Batch: 16700,train loss is: 0.0001724657162912313\n",
      "test loss is 0.0001350442436661655\n",
      "Batch: 16800,train loss is: 0.0001297828231199518\n",
      "test loss is 0.00013255213801950064\n",
      "Batch: 16900,train loss is: 0.00012591699934847317\n",
      "test loss is 0.00014084468219813142\n",
      "Batch: 17000,train loss is: 0.00013122025744183378\n",
      "test loss is 0.0001297783289545801\n",
      "Batch: 17100,train loss is: 0.0001278972025627616\n",
      "test loss is 0.00012843993474106864\n",
      "Batch: 17200,train loss is: 9.745306969933114e-05\n",
      "test loss is 0.00013392170192541054\n",
      "Batch: 17300,train loss is: 0.00011913602067760127\n",
      "test loss is 0.0001318993950002602\n",
      "Batch: 17400,train loss is: 5.846447773368371e-05\n",
      "test loss is 0.00013326769704299392\n",
      "Batch: 17500,train loss is: 9.315881503230124e-05\n",
      "test loss is 0.00012720247393989726\n",
      "Batch: 17600,train loss is: 7.378304830482433e-05\n",
      "test loss is 0.00013035188464866697\n",
      "Batch: 17700,train loss is: 0.00012166724672205485\n",
      "test loss is 0.00012606020588695356\n",
      "Batch: 17800,train loss is: 0.00019857081679580026\n",
      "test loss is 0.00013627069141432873\n",
      "Batch: 17900,train loss is: 8.922745466947296e-05\n",
      "test loss is 0.00012745378519534573\n",
      "Batch: 18000,train loss is: 8.988914282373837e-05\n",
      "test loss is 0.00013070187954353153\n",
      "Batch: 18100,train loss is: 0.0001253667378312717\n",
      "test loss is 0.00012692626090082172\n",
      "Batch: 18200,train loss is: 8.970318803700859e-05\n",
      "test loss is 0.00012572012015276419\n",
      "Batch: 18300,train loss is: 0.00021004939257353078\n",
      "test loss is 0.00013074823265790057\n",
      "Batch: 18400,train loss is: 9.835475858678464e-05\n",
      "test loss is 0.00012435905506748808\n",
      "Batch: 18500,train loss is: 9.030088186631051e-05\n",
      "test loss is 0.0001288718692992911\n",
      "Batch: 18600,train loss is: 8.69877430439419e-05\n",
      "test loss is 0.00012992866704887308\n",
      "Batch: 18700,train loss is: 0.00014718794452035644\n",
      "test loss is 0.00013189834176740144\n",
      "Batch: 18800,train loss is: 0.0001233246436155552\n",
      "test loss is 0.00013112217509490026\n",
      "Batch: 18900,train loss is: 0.0002234371284889717\n",
      "test loss is 0.00013953162056598982\n",
      "Batch: 19000,train loss is: 7.281661621355217e-05\n",
      "test loss is 0.00013735005832472407\n",
      "Batch: 19100,train loss is: 7.162767393952867e-05\n",
      "test loss is 0.00016583883501950194\n",
      "Batch: 19200,train loss is: 9.430102666122126e-05\n",
      "test loss is 0.00013639941456370869\n",
      "Batch: 19300,train loss is: 0.00013006975520637662\n",
      "test loss is 0.00013365604830201634\n",
      "Batch: 19400,train loss is: 7.063023148574158e-05\n",
      "test loss is 0.00012824110212889916\n",
      "Batch: 19500,train loss is: 0.0001006224961505504\n",
      "test loss is 0.00013708327981124942\n",
      "Batch: 19600,train loss is: 6.496803673072243e-05\n",
      "test loss is 0.00013105183168847992\n",
      "Batch: 19700,train loss is: 0.00014924900065510277\n",
      "test loss is 0.00012691512287765647\n",
      "Batch: 19800,train loss is: 0.00011525537450166807\n",
      "test loss is 0.00012673265442695865\n",
      "Batch: 19900,train loss is: 7.87837304242473e-05\n",
      "test loss is 0.0001341619368984376\n",
      "Batch: 20000,train loss is: 0.0001493810625664776\n",
      "test loss is 0.00013214104844864284\n",
      "Batch: 20100,train loss is: 0.00011782558301379809\n",
      "test loss is 0.00013073634955441534\n",
      "Batch: 20200,train loss is: 7.581455437825065e-05\n",
      "test loss is 0.00012539631612100516\n",
      "Batch: 20300,train loss is: 7.066820021661398e-05\n",
      "test loss is 0.00012777864190994013\n",
      "Batch: 20400,train loss is: 0.0001291609027382462\n",
      "test loss is 0.0002051416252242331\n",
      "Batch: 20500,train loss is: 8.617858436593098e-05\n",
      "test loss is 0.00012635496767120206\n",
      "Batch: 20600,train loss is: 0.00022855510621621686\n",
      "test loss is 0.0001340361751184618\n",
      "Batch: 20700,train loss is: 0.0001297205999711718\n",
      "test loss is 0.00012901493219127873\n",
      "Batch: 20800,train loss is: 5.716780585330685e-05\n",
      "test loss is 0.0001291915361637767\n",
      "Batch: 20900,train loss is: 5.6399195470614806e-05\n",
      "test loss is 0.00012865944813738364\n",
      "Batch: 21000,train loss is: 5.54645068752294e-05\n",
      "test loss is 0.00012699997404964358\n",
      "Batch: 21100,train loss is: 9.492722241302126e-05\n",
      "test loss is 0.00012840553895662954\n",
      "Batch: 21200,train loss is: 0.00010466076566667395\n",
      "test loss is 0.00013640051719143444\n",
      "Batch: 21300,train loss is: 6.238234507952947e-05\n",
      "test loss is 0.00012686064640324656\n",
      "Batch: 21400,train loss is: 0.00015706183538994969\n",
      "test loss is 0.0001316507894940577\n",
      "Batch: 21500,train loss is: 8.129828125967041e-05\n",
      "test loss is 0.00012725481497655327\n",
      "Batch: 21600,train loss is: 0.00013270881018687884\n",
      "test loss is 0.00012600857135302903\n",
      "Batch: 21700,train loss is: 6.9397045690027e-05\n",
      "test loss is 0.0001319982923916491\n",
      "Batch: 21800,train loss is: 0.00014595283087405744\n",
      "test loss is 0.00013422158553651935\n",
      "Batch: 21900,train loss is: 7.344373435712793e-05\n",
      "test loss is 0.0001288654601498225\n",
      "Batch: 22000,train loss is: 0.0001339531574392634\n",
      "test loss is 0.00013369832732640634\n",
      "Batch: 22100,train loss is: 8.768825037199374e-05\n",
      "test loss is 0.00012589864605082026\n",
      "Batch: 22200,train loss is: 7.621663520117736e-05\n",
      "test loss is 0.0001258285249357016\n",
      "Batch: 22300,train loss is: 0.00010647347392109969\n",
      "test loss is 0.0001347199059476741\n",
      "Batch: 22400,train loss is: 9.925509714482446e-05\n",
      "test loss is 0.0001336264595012715\n",
      "Batch: 22500,train loss is: 0.00011436984086781081\n",
      "test loss is 0.0001261915651499229\n",
      "Batch: 22600,train loss is: 0.00010276775429363343\n",
      "test loss is 0.00013808033139291724\n",
      "Batch: 22700,train loss is: 6.804988942235521e-05\n",
      "test loss is 0.00012663738671404493\n",
      "Batch: 22800,train loss is: 0.00011034487570437245\n",
      "test loss is 0.00012594702318367935\n",
      "Batch: 22900,train loss is: 0.00010729919286281393\n",
      "test loss is 0.0001270617863750705\n",
      "Batch: 23000,train loss is: 0.00019161906757566647\n",
      "test loss is 0.0001263550860266351\n",
      "Batch: 23100,train loss is: 8.947454811699129e-05\n",
      "test loss is 0.00012565193472848267\n",
      "Batch: 23200,train loss is: 6.868125959561593e-05\n",
      "test loss is 0.0001280458591026985\n",
      "Batch: 23300,train loss is: 9.508254734239715e-05\n",
      "test loss is 0.00013064273102948737\n",
      "Batch: 23400,train loss is: 0.00010750092716708048\n",
      "test loss is 0.00012523766728501698\n",
      "Batch: 23500,train loss is: 0.0001714080576398689\n",
      "test loss is 0.00012713611226631189\n",
      "Batch: 23600,train loss is: 7.96449909672774e-05\n",
      "test loss is 0.00012687470707706185\n",
      "Batch: 23700,train loss is: 0.00010996038864532295\n",
      "test loss is 0.0001474693592191238\n",
      "Batch: 23800,train loss is: 0.0001864774702147658\n",
      "test loss is 0.00013185164477463673\n",
      "Batch: 23900,train loss is: 0.0002611498922123163\n",
      "test loss is 0.00012448059559225902\n",
      "Batch: 24000,train loss is: 9.820892935839676e-05\n",
      "test loss is 0.00013570716879128297\n",
      "Batch: 24100,train loss is: 0.00010582196915182952\n",
      "test loss is 0.00013073944963815616\n",
      "Batch: 24200,train loss is: 0.00017556078597622077\n",
      "test loss is 0.00012699696221213347\n",
      "Batch: 24300,train loss is: 9.302307453611181e-05\n",
      "test loss is 0.00015445488228543413\n",
      "Batch: 24400,train loss is: 0.00013859444966037355\n",
      "test loss is 0.0001270672249425689\n",
      "Batch: 24500,train loss is: 0.0001782718298924026\n",
      "test loss is 0.00012639755540436702\n",
      "Batch: 24600,train loss is: 8.204219768167356e-05\n",
      "test loss is 0.00014477223155291355\n",
      "Batch: 24700,train loss is: 5.6203506151898305e-05\n",
      "test loss is 0.00012375260812195248\n",
      "Batch: 24800,train loss is: 0.00022133908451218623\n",
      "test loss is 0.00013383715915847358\n",
      "Batch: 24900,train loss is: 7.933919204076256e-05\n",
      "test loss is 0.00014188940416381016\n",
      "Batch: 25000,train loss is: 0.00013053268073668573\n",
      "test loss is 0.00012522963701431142\n",
      "Batch: 25100,train loss is: 7.72473305454226e-05\n",
      "test loss is 0.00013136809227211172\n",
      "Batch: 25200,train loss is: 0.00012207744375122089\n",
      "test loss is 0.00012513815290204246\n",
      "Batch: 25300,train loss is: 0.00016665989257255294\n",
      "test loss is 0.00012718099618834935\n",
      "Batch: 25400,train loss is: 0.0001148119675244831\n",
      "test loss is 0.0001283794635444463\n",
      "Batch: 25500,train loss is: 7.860951130452681e-05\n",
      "test loss is 0.000126542632813037\n",
      "Batch: 25600,train loss is: 0.00015139761542577455\n",
      "test loss is 0.00014473690587812847\n",
      "Batch: 25700,train loss is: 0.0001728782313502943\n",
      "test loss is 0.00013288574388326424\n",
      "Batch: 25800,train loss is: 0.00010002446083074908\n",
      "test loss is 0.00012469129264725107\n",
      "Batch: 25900,train loss is: 8.766494551514202e-05\n",
      "test loss is 0.0001289192127198115\n",
      "Batch: 26000,train loss is: 0.00014565659041675382\n",
      "test loss is 0.0001294125284186224\n",
      "Batch: 26100,train loss is: 0.00011975643499646422\n",
      "test loss is 0.0001342957486324798\n",
      "Batch: 26200,train loss is: 0.00017395546276529377\n",
      "test loss is 0.0001255898329526505\n",
      "Batch: 26300,train loss is: 0.00014250063120819413\n",
      "test loss is 0.00014611619353635343\n",
      "Batch: 26400,train loss is: 6.018868166116587e-05\n",
      "test loss is 0.00012961367365245301\n",
      "Batch: 26500,train loss is: 9.582956307991074e-05\n",
      "test loss is 0.00012805423817708803\n",
      "Batch: 26600,train loss is: 0.00019380462320196446\n",
      "test loss is 0.00012445630403949814\n",
      "Batch: 26700,train loss is: 0.00010098929395390491\n",
      "test loss is 0.00014355380447798356\n",
      "Batch: 26800,train loss is: 0.00011269476812461453\n",
      "test loss is 0.0001262271941957523\n",
      "Batch: 26900,train loss is: 6.707536324002605e-05\n",
      "test loss is 0.00012554212673315644\n",
      "Batch: 27000,train loss is: 0.0001126732059577564\n",
      "test loss is 0.00013327706296575937\n",
      "Batch: 27100,train loss is: 0.00014008190909290945\n",
      "test loss is 0.00013625943140344423\n",
      "Batch: 27200,train loss is: 0.00011319255045268923\n",
      "test loss is 0.00012783967799486712\n",
      "Batch: 27300,train loss is: 5.845084155007899e-05\n",
      "test loss is 0.0001264301858714424\n",
      "Batch: 27400,train loss is: 0.0001255456292420262\n",
      "test loss is 0.0001291939212323549\n",
      "Batch: 27500,train loss is: 0.00011799882291617793\n",
      "test loss is 0.00012574223822578364\n",
      "Batch: 27600,train loss is: 9.65556388125996e-05\n",
      "test loss is 0.00013338727115953206\n",
      "Batch: 27700,train loss is: 0.00032529051827808614\n",
      "test loss is 0.00013162826549871\n",
      "Batch: 27800,train loss is: 0.0001267451912409458\n",
      "test loss is 0.00013529014690725344\n",
      "Batch: 27900,train loss is: 0.00011655552895748701\n",
      "test loss is 0.00012743621385535633\n",
      "Batch: 28000,train loss is: 0.0001013302316079032\n",
      "test loss is 0.0001349861711334286\n",
      "Batch: 28100,train loss is: 0.00017399821275625632\n",
      "test loss is 0.00013104841723603246\n",
      "Batch: 28200,train loss is: 0.00010924537485712124\n",
      "test loss is 0.00012980572799382333\n",
      "Batch: 28300,train loss is: 0.0003832475659513626\n",
      "test loss is 0.0001321090497716046\n",
      "Batch: 28400,train loss is: 9.596079080610594e-05\n",
      "test loss is 0.00012260546027609306\n",
      "Batch: 28500,train loss is: 0.00018880232523544314\n",
      "test loss is 0.0001323665147234525\n",
      "Batch: 28600,train loss is: 0.000131736080928686\n",
      "test loss is 0.000129413842252497\n",
      "Batch: 28700,train loss is: 9.416113075576801e-05\n",
      "test loss is 0.0001278780686703671\n",
      "Batch: 28800,train loss is: 0.00018434237961124233\n",
      "test loss is 0.0001269007430869239\n",
      "Batch: 28900,train loss is: 0.00011721445368069824\n",
      "test loss is 0.0001436520647664508\n",
      "Batch: 29000,train loss is: 0.0001144486871248089\n",
      "test loss is 0.00013049046479690182\n",
      "Batch: 29100,train loss is: 8.736485673720982e-05\n",
      "test loss is 0.00013262909708686147\n",
      "Batch: 29200,train loss is: 0.00011857009951496707\n",
      "test loss is 0.00015617256102529116\n",
      "Batch: 29300,train loss is: 0.00022339883832476394\n",
      "test loss is 0.00013223651648909234\n",
      "Batch: 29400,train loss is: 0.0001596474473984639\n",
      "test loss is 0.00013000219014915485\n",
      "Batch: 29500,train loss is: 0.00012949598375461264\n",
      "test loss is 0.00012366267780493544\n",
      "Batch: 29600,train loss is: 0.00015134479466155632\n",
      "test loss is 0.00013182610163872365\n",
      "Batch: 29700,train loss is: 6.658127108397851e-05\n",
      "test loss is 0.00012604827797757783\n",
      "Batch: 29800,train loss is: 0.00023381677313925165\n",
      "test loss is 0.0001313753002613765\n",
      "Batch: 29900,train loss is: 0.00039692920094350676\n",
      "test loss is 0.0001266768047385718\n",
      "Batch: 30000,train loss is: 9.862312961296699e-05\n",
      "test loss is 0.00012491433023919613\n",
      "Batch: 30100,train loss is: 0.00020752648792027282\n",
      "test loss is 0.00014949186947039683\n",
      "Batch: 30200,train loss is: 8.753999192025541e-05\n",
      "test loss is 0.00012452293917258628\n",
      "Batch: 30300,train loss is: 8.763577738140042e-05\n",
      "test loss is 0.00012542044107819144\n",
      "Batch: 30400,train loss is: 5.926856202284687e-05\n",
      "test loss is 0.0001375501616889997\n",
      "Batch: 30500,train loss is: 0.00012066979871772848\n",
      "test loss is 0.00015026371458167013\n",
      "Batch: 30600,train loss is: 8.852792629584363e-05\n",
      "test loss is 0.00012739516799739568\n",
      "Batch: 30700,train loss is: 9.392956661771678e-05\n",
      "test loss is 0.00012716746234838996\n",
      "Batch: 30800,train loss is: 5.5119256460518856e-05\n",
      "test loss is 0.00013366234554548854\n",
      "Batch: 30900,train loss is: 8.541019180442903e-05\n",
      "test loss is 0.0001270046674422911\n",
      "Batch: 31000,train loss is: 7.201491997551418e-05\n",
      "test loss is 0.00012531813874324695\n",
      "Batch: 31100,train loss is: 0.00010302088830751645\n",
      "test loss is 0.0001627293438877859\n",
      "Batch: 31200,train loss is: 0.00010640405740123198\n",
      "test loss is 0.00012937823584686911\n",
      "Batch: 31300,train loss is: 0.00011805372136259165\n",
      "test loss is 0.0001327142517737833\n",
      "Batch: 31400,train loss is: 7.558466598689005e-05\n",
      "test loss is 0.0001274906266277265\n",
      "Batch: 31500,train loss is: 0.00012119790668557932\n",
      "test loss is 0.0001325825012608916\n",
      "Batch: 31600,train loss is: 0.0001364338746427437\n",
      "test loss is 0.00012941392261283288\n",
      "Batch: 31700,train loss is: 7.865771736481942e-05\n",
      "test loss is 0.00013684958072807146\n",
      "Batch: 31800,train loss is: 0.00019702959790589584\n",
      "test loss is 0.0001255559182190531\n",
      "Batch: 31900,train loss is: 0.0001661797606420335\n",
      "test loss is 0.00012832709485648374\n",
      "Batch: 32000,train loss is: 0.00048198627170275627\n",
      "test loss is 0.0001361630512606472\n",
      "Batch: 32100,train loss is: 5.977483179688848e-05\n",
      "test loss is 0.00012418700888221893\n",
      "Batch: 32200,train loss is: 9.193252204554865e-05\n",
      "test loss is 0.0001273830473630443\n",
      "Batch: 32300,train loss is: 9.63054785219039e-05\n",
      "test loss is 0.00012501826009094088\n",
      "Batch: 32400,train loss is: 6.131797285502524e-05\n",
      "test loss is 0.00012363789997745704\n",
      "Batch: 32500,train loss is: 0.00012933095663716343\n",
      "test loss is 0.0001245136078261755\n",
      "Batch: 32600,train loss is: 9.339479344895549e-05\n",
      "test loss is 0.00014099107852427108\n",
      "Batch: 32700,train loss is: 5.561282114223031e-05\n",
      "test loss is 0.0001265317369390139\n",
      "Batch: 32800,train loss is: 0.00013798056551821172\n",
      "test loss is 0.00014880324637876077\n",
      "Batch: 32900,train loss is: 0.00010591902858717158\n",
      "test loss is 0.0001230788097433516\n",
      "Batch: 33000,train loss is: 6.404284750599279e-05\n",
      "test loss is 0.00012792405552862842\n",
      "Batch: 33100,train loss is: 7.720736308635118e-05\n",
      "test loss is 0.00013608247198264287\n",
      "Batch: 33200,train loss is: 5.766544185863233e-05\n",
      "test loss is 0.00012326799788927152\n",
      "Batch: 33300,train loss is: 8.318181344444658e-05\n",
      "test loss is 0.00012637428486494468\n",
      "Batch: 33400,train loss is: 0.00010644102011444653\n",
      "test loss is 0.0001265442108682425\n",
      "Batch: 33500,train loss is: 9.271230216949724e-05\n",
      "test loss is 0.00013717076881645293\n",
      "Batch: 33600,train loss is: 7.364078278637604e-05\n",
      "test loss is 0.0001354993298108189\n",
      "Batch: 33700,train loss is: 0.00010839895789096203\n",
      "test loss is 0.00013092887143211938\n",
      "Batch: 33800,train loss is: 0.00015309805881060258\n",
      "test loss is 0.0001228061347326558\n",
      "Batch: 33900,train loss is: 7.352598421219911e-05\n",
      "test loss is 0.00012600563567593726\n",
      "-----------------------Epoch: 3----------------------------------\n",
      "Batch: 0,train loss is: 0.0001420398759798438\n",
      "test loss is 0.00013203864660382936\n",
      "Batch: 100,train loss is: 0.0002397472694552871\n",
      "test loss is 0.00013860434006009832\n",
      "Batch: 200,train loss is: 0.00011461227258762526\n",
      "test loss is 0.0001235754408320965\n",
      "Batch: 300,train loss is: 0.00010401489081743295\n",
      "test loss is 0.00013131748918234574\n",
      "Batch: 400,train loss is: 9.05671382025465e-05\n",
      "test loss is 0.00013081806878403326\n",
      "Batch: 500,train loss is: 7.71373682389193e-05\n",
      "test loss is 0.00013573755353470373\n",
      "Batch: 600,train loss is: 4.637769974097894e-05\n",
      "test loss is 0.00013126522622541528\n",
      "Batch: 700,train loss is: 9.305362720011147e-05\n",
      "test loss is 0.0001429598085743474\n",
      "Batch: 800,train loss is: 0.00015046650102726764\n",
      "test loss is 0.00012369845227882384\n",
      "Batch: 900,train loss is: 9.063451570855622e-05\n",
      "test loss is 0.0001342690625248333\n",
      "Batch: 1000,train loss is: 9.94030192130553e-05\n",
      "test loss is 0.0001307289781817044\n",
      "Batch: 1100,train loss is: 0.00023011137446968854\n",
      "test loss is 0.00013578598197008213\n",
      "Batch: 1200,train loss is: 6.012077540216386e-05\n",
      "test loss is 0.00012513386449698605\n",
      "Batch: 1300,train loss is: 8.75801643509895e-05\n",
      "test loss is 0.00013032942298967718\n",
      "Batch: 1400,train loss is: 8.3153093889093e-05\n",
      "test loss is 0.00012777162218244643\n",
      "Batch: 1500,train loss is: 0.00010909035734814986\n",
      "test loss is 0.00012784791126850656\n",
      "Batch: 1600,train loss is: 8.897383170821004e-05\n",
      "test loss is 0.00012153365502580727\n",
      "Batch: 1700,train loss is: 0.00013273319834629932\n",
      "test loss is 0.0001235791239451559\n",
      "Batch: 1800,train loss is: 7.505181228010708e-05\n",
      "test loss is 0.00012429729314539375\n",
      "Batch: 1900,train loss is: 0.00010977811886902175\n",
      "test loss is 0.00015121456269808083\n",
      "Batch: 2000,train loss is: 0.00010500526885016681\n",
      "test loss is 0.00012578180610386188\n",
      "Batch: 2100,train loss is: 0.00011102198455308983\n",
      "test loss is 0.00013954983456291928\n",
      "Batch: 2200,train loss is: 9.994125604314003e-05\n",
      "test loss is 0.00012854473712505508\n",
      "Batch: 2300,train loss is: 6.785877674539165e-05\n",
      "test loss is 0.0001225319896658196\n",
      "Batch: 2400,train loss is: 7.973372231904322e-05\n",
      "test loss is 0.00012533354881449034\n",
      "Batch: 2500,train loss is: 0.00012505166626924373\n",
      "test loss is 0.00013022962699330424\n",
      "Batch: 2600,train loss is: 7.536102556647533e-05\n",
      "test loss is 0.00012611048692123402\n",
      "Batch: 2700,train loss is: 0.0001095255713580294\n",
      "test loss is 0.00012415454923865143\n",
      "Batch: 2800,train loss is: 0.00011089287655626443\n",
      "test loss is 0.00012624758913600632\n",
      "Batch: 2900,train loss is: 0.00013326180862609333\n",
      "test loss is 0.00014469522492862588\n",
      "Batch: 3000,train loss is: 0.00010264142256707296\n",
      "test loss is 0.00013075541725258093\n",
      "Batch: 3100,train loss is: 6.415747683769081e-05\n",
      "test loss is 0.0001218025182510074\n",
      "Batch: 3200,train loss is: 0.0001701928742434981\n",
      "test loss is 0.00012527796191686425\n",
      "Batch: 3300,train loss is: 0.00010921921957095606\n",
      "test loss is 0.00012256565520266108\n",
      "Batch: 3400,train loss is: 0.00010581873824479785\n",
      "test loss is 0.00015831822296601724\n",
      "Batch: 3500,train loss is: 5.1622047617175056e-05\n",
      "test loss is 0.00012652196149201927\n",
      "Batch: 3600,train loss is: 6.659212850472852e-05\n",
      "test loss is 0.00012876813934981\n",
      "Batch: 3700,train loss is: 9.27585613850488e-05\n",
      "test loss is 0.00012504587495427717\n",
      "Batch: 3800,train loss is: 9.689494601590698e-05\n",
      "test loss is 0.000126005150941463\n",
      "Batch: 3900,train loss is: 0.00014505642830974866\n",
      "test loss is 0.00013470355935475803\n",
      "Batch: 4000,train loss is: 5.521493122002294e-05\n",
      "test loss is 0.0001246696190930847\n",
      "Batch: 4100,train loss is: 0.00011705770033935354\n",
      "test loss is 0.0001279454440095506\n",
      "Batch: 4200,train loss is: 0.0001309859386024755\n",
      "test loss is 0.00012557387472459614\n",
      "Batch: 4300,train loss is: 8.41839175453133e-05\n",
      "test loss is 0.00013226252165850216\n",
      "Batch: 4400,train loss is: 8.535643049451955e-05\n",
      "test loss is 0.00012603785019443933\n",
      "Batch: 4500,train loss is: 9.137612155212831e-05\n",
      "test loss is 0.00012456247410843694\n",
      "Batch: 4600,train loss is: 7.77158479742826e-05\n",
      "test loss is 0.00012572453667076137\n",
      "Batch: 4700,train loss is: 0.00013803390962972348\n",
      "test loss is 0.00012353079165998443\n",
      "Batch: 4800,train loss is: 0.00024221226404972924\n",
      "test loss is 0.0001258979834687067\n",
      "Batch: 4900,train loss is: 8.502910555489684e-05\n",
      "test loss is 0.00014158822897798805\n",
      "Batch: 5000,train loss is: 7.929995015210248e-05\n",
      "test loss is 0.00012340882712562676\n",
      "Batch: 5100,train loss is: 8.723112963279422e-05\n",
      "test loss is 0.00012605447174558973\n",
      "Batch: 5200,train loss is: 5.0494181032527715e-05\n",
      "test loss is 0.0001224304286610273\n",
      "Batch: 5300,train loss is: 0.0001265067656827226\n",
      "test loss is 0.0001690596421485103\n",
      "Batch: 5400,train loss is: 0.0001755900996791904\n",
      "test loss is 0.00012454624816391992\n",
      "Batch: 5500,train loss is: 8.000578854074024e-05\n",
      "test loss is 0.0001257711283498612\n",
      "Batch: 5600,train loss is: 0.0001655564525674173\n",
      "test loss is 0.00012422177829394003\n",
      "Batch: 5700,train loss is: 0.00013390051171648\n",
      "test loss is 0.00013349762524747847\n",
      "Batch: 5800,train loss is: 0.00010870038770173397\n",
      "test loss is 0.00012933220796846936\n",
      "Batch: 5900,train loss is: 0.0003441568731414203\n",
      "test loss is 0.00012646719252512793\n",
      "Batch: 6000,train loss is: 0.00015588157469475857\n",
      "test loss is 0.00012623443071651635\n",
      "Batch: 6100,train loss is: 0.00010444507081613673\n",
      "test loss is 0.00014926850515737132\n",
      "Batch: 6200,train loss is: 9.124577343363014e-05\n",
      "test loss is 0.00012074975938133724\n",
      "Batch: 6300,train loss is: 0.00015055316651361815\n",
      "test loss is 0.00012428727813850542\n",
      "Batch: 6400,train loss is: 9.342207178515848e-05\n",
      "test loss is 0.0001241407044839475\n",
      "Batch: 6500,train loss is: 7.58727541929788e-05\n",
      "test loss is 0.00012977291276076257\n",
      "Batch: 6600,train loss is: 0.0002576489008154062\n",
      "test loss is 0.00012222916648276536\n",
      "Batch: 6700,train loss is: 5.158652509286454e-05\n",
      "test loss is 0.0001262089911119303\n",
      "Batch: 6800,train loss is: 0.0001435528912315111\n",
      "test loss is 0.0001234212704682656\n",
      "Batch: 6900,train loss is: 7.745069712702958e-05\n",
      "test loss is 0.00012142147161293657\n",
      "Batch: 7000,train loss is: 0.00011876462236867141\n",
      "test loss is 0.00012430854749862345\n",
      "Batch: 7100,train loss is: 0.0001015910915690814\n",
      "test loss is 0.00012529405896547435\n",
      "Batch: 7200,train loss is: 5.92304033418388e-05\n",
      "test loss is 0.00012893213748167115\n",
      "Batch: 7300,train loss is: 0.0002778380058795257\n",
      "test loss is 0.00012826589308645163\n",
      "Batch: 7400,train loss is: 7.614872577702063e-05\n",
      "test loss is 0.00013636217099371984\n",
      "Batch: 7500,train loss is: 0.00020537126181822222\n",
      "test loss is 0.00013141483020748194\n",
      "Batch: 7600,train loss is: 0.00014889384499393828\n",
      "test loss is 0.00012803094776322712\n",
      "Batch: 7700,train loss is: 0.0001831217185390475\n",
      "test loss is 0.00012865281600866343\n",
      "Batch: 7800,train loss is: 5.927477371560918e-05\n",
      "test loss is 0.00012489314447082706\n",
      "Batch: 7900,train loss is: 7.289607228387032e-05\n",
      "test loss is 0.00012721367034344163\n",
      "Batch: 8000,train loss is: 5.116733576090372e-05\n",
      "test loss is 0.00012849214292189336\n",
      "Batch: 8100,train loss is: 0.0002401802139086324\n",
      "test loss is 0.00013074455534944785\n",
      "Batch: 8200,train loss is: 0.00013290995256481225\n",
      "test loss is 0.00013511636506420398\n",
      "Batch: 8300,train loss is: 8.295267146802448e-05\n",
      "test loss is 0.00013000090467273377\n",
      "Batch: 8400,train loss is: 0.00011506712639929045\n",
      "test loss is 0.000122754810135757\n",
      "Batch: 8500,train loss is: 0.00012404152750544953\n",
      "test loss is 0.00012371805798114233\n",
      "Batch: 8600,train loss is: 0.0001341440541608167\n",
      "test loss is 0.00013286277694729744\n",
      "Batch: 8700,train loss is: 0.00010734882773461768\n",
      "test loss is 0.0001379692617189825\n",
      "Batch: 8800,train loss is: 0.0001632804508639639\n",
      "test loss is 0.0001242491445930484\n",
      "Batch: 8900,train loss is: 0.00020060655849495637\n",
      "test loss is 0.0001230220582997086\n",
      "Batch: 9000,train loss is: 7.5113246943254e-05\n",
      "test loss is 0.0001242870560317296\n",
      "Batch: 9100,train loss is: 6.846953658302944e-05\n",
      "test loss is 0.0001204316584794931\n",
      "Batch: 9200,train loss is: 0.00010524641843530666\n",
      "test loss is 0.00012251661163235517\n",
      "Batch: 9300,train loss is: 0.00012774377486325582\n",
      "test loss is 0.00014052293183909175\n",
      "Batch: 9400,train loss is: 8.451645165424637e-05\n",
      "test loss is 0.00012691007220703486\n",
      "Batch: 9500,train loss is: 6.40073455886124e-05\n",
      "test loss is 0.00012134844474052937\n",
      "Batch: 9600,train loss is: 6.54742589850303e-05\n",
      "test loss is 0.00012406591686944265\n",
      "Batch: 9700,train loss is: 5.973435533259681e-05\n",
      "test loss is 0.00012075412367263156\n",
      "Batch: 9800,train loss is: 5.580672385022566e-05\n",
      "test loss is 0.00013181994960114612\n",
      "Batch: 9900,train loss is: 5.7589216857753834e-05\n",
      "test loss is 0.00013400985196976154\n",
      "Batch: 10000,train loss is: 8.840099284914824e-05\n",
      "test loss is 0.00012510324400252608\n",
      "Batch: 10100,train loss is: 0.0002126224733686642\n",
      "test loss is 0.00013333908405863598\n",
      "Batch: 10200,train loss is: 0.0002623194164841988\n",
      "test loss is 0.00012213503188836832\n",
      "Batch: 10300,train loss is: 7.65555820928604e-05\n",
      "test loss is 0.0001229018182970616\n",
      "Batch: 10400,train loss is: 5.7072558926963135e-05\n",
      "test loss is 0.0001236121174400417\n",
      "Batch: 10500,train loss is: 0.00017903587533687652\n",
      "test loss is 0.00012665716491065852\n",
      "Batch: 10600,train loss is: 8.044349969191609e-05\n",
      "test loss is 0.00012403465489106667\n",
      "Batch: 10700,train loss is: 0.00011167551099265743\n",
      "test loss is 0.0001244237598903265\n",
      "Batch: 10800,train loss is: 0.00016218978825284586\n",
      "test loss is 0.00012297230821507998\n",
      "Batch: 10900,train loss is: 0.00011068240941355775\n",
      "test loss is 0.0001317619789324882\n",
      "Batch: 11000,train loss is: 9.321381828185478e-05\n",
      "test loss is 0.0001238525378990225\n",
      "Batch: 11100,train loss is: 9.191588003622367e-05\n",
      "test loss is 0.00013059197428528003\n",
      "Batch: 11200,train loss is: 0.00012240154988180405\n",
      "test loss is 0.00012269487120049127\n",
      "Batch: 11300,train loss is: 7.814109876982611e-05\n",
      "test loss is 0.00012407648322395633\n",
      "Batch: 11400,train loss is: 0.0001557661243709093\n",
      "test loss is 0.00013037927870797236\n",
      "Batch: 11500,train loss is: 7.405217454897628e-05\n",
      "test loss is 0.00012582044974849367\n",
      "Batch: 11600,train loss is: 0.00019825684971990022\n",
      "test loss is 0.00012734170345079013\n",
      "Batch: 11700,train loss is: 8.114615859731955e-05\n",
      "test loss is 0.00012185013727044971\n",
      "Batch: 11800,train loss is: 9.023628766679676e-05\n",
      "test loss is 0.00012879489322530142\n",
      "Batch: 11900,train loss is: 0.00011682909825371928\n",
      "test loss is 0.00012526105206627704\n",
      "Batch: 12000,train loss is: 0.00013163181239879076\n",
      "test loss is 0.00012590332325669693\n",
      "Batch: 12100,train loss is: 0.0006670720471695098\n",
      "test loss is 0.00013144512733722388\n",
      "Batch: 12200,train loss is: 8.879246035401192e-05\n",
      "test loss is 0.00012441094830678296\n",
      "Batch: 12300,train loss is: 9.535484838676879e-05\n",
      "test loss is 0.00012376150700688023\n",
      "Batch: 12400,train loss is: 9.297270841911478e-05\n",
      "test loss is 0.0001237760846033498\n",
      "Batch: 12500,train loss is: 0.000234722663716177\n",
      "test loss is 0.00012252971985706953\n",
      "Batch: 12600,train loss is: 6.741901198666088e-05\n",
      "test loss is 0.000121783353573374\n",
      "Batch: 12700,train loss is: 6.105957696941102e-05\n",
      "test loss is 0.00012071368793116019\n",
      "Batch: 12800,train loss is: 6.808736155873866e-05\n",
      "test loss is 0.0001228914751783402\n",
      "Batch: 12900,train loss is: 0.0001226618225864255\n",
      "test loss is 0.00012110295350159679\n",
      "Batch: 13000,train loss is: 9.241642102224295e-05\n",
      "test loss is 0.00013511914010460032\n",
      "Batch: 13100,train loss is: 0.00020120813382344907\n",
      "test loss is 0.00012558726636673003\n",
      "Batch: 13200,train loss is: 0.000113809430032791\n",
      "test loss is 0.00012059134995353463\n",
      "Batch: 13300,train loss is: 7.62775317951977e-05\n",
      "test loss is 0.0001508684064421053\n",
      "Batch: 13400,train loss is: 9.017996318784545e-05\n",
      "test loss is 0.00013053833977344403\n",
      "Batch: 13500,train loss is: 8.45306552964536e-05\n",
      "test loss is 0.00013241893761422585\n",
      "Batch: 13600,train loss is: 0.00010149741015714784\n",
      "test loss is 0.00012209272577719122\n",
      "Batch: 13700,train loss is: 0.00010269577833394758\n",
      "test loss is 0.00012281249058515985\n",
      "Batch: 13800,train loss is: 0.00011594181669267248\n",
      "test loss is 0.0001261496812818152\n",
      "Batch: 13900,train loss is: 6.581459852227321e-05\n",
      "test loss is 0.00012982497267174592\n",
      "Batch: 14000,train loss is: 8.623949869247215e-05\n",
      "test loss is 0.00012326045515729416\n",
      "Batch: 14100,train loss is: 0.00010068885486326624\n",
      "test loss is 0.0001497648165158825\n",
      "Batch: 14200,train loss is: 5.1405730870161924e-05\n",
      "test loss is 0.00012291033065830195\n",
      "Batch: 14300,train loss is: 5.632838154013371e-05\n",
      "test loss is 0.00012262169786131293\n",
      "Batch: 14400,train loss is: 0.00013750837081378102\n",
      "test loss is 0.00012011279764782492\n",
      "Batch: 14500,train loss is: 5.438926207131365e-05\n",
      "test loss is 0.00012040609677779903\n",
      "Batch: 14600,train loss is: 0.00022749907390132984\n",
      "test loss is 0.00012253920543594548\n",
      "Batch: 14700,train loss is: 7.374343652735291e-05\n",
      "test loss is 0.0001216536171796472\n",
      "Batch: 14800,train loss is: 0.00014412689402934912\n",
      "test loss is 0.00012093152493716977\n",
      "Batch: 14900,train loss is: 8.56039444666432e-05\n",
      "test loss is 0.00012383803432667651\n",
      "Batch: 15000,train loss is: 7.265512017196003e-05\n",
      "test loss is 0.0001373189748865169\n",
      "Batch: 15100,train loss is: 0.0001759767565678018\n",
      "test loss is 0.00012501511971746887\n",
      "Batch: 15200,train loss is: 0.00015162025643857194\n",
      "test loss is 0.00011946565145260568\n",
      "Batch: 15300,train loss is: 0.00014312161220829688\n",
      "test loss is 0.00011937566541051663\n",
      "Batch: 15400,train loss is: 9.352723989443025e-05\n",
      "test loss is 0.0001269967414637624\n",
      "Batch: 15500,train loss is: 0.00017334147290665078\n",
      "test loss is 0.00013109940771017202\n",
      "Batch: 15600,train loss is: 0.00021391343971207617\n",
      "test loss is 0.0001291274000500904\n",
      "Batch: 15700,train loss is: 9.838330497752e-05\n",
      "test loss is 0.00013357045487636212\n",
      "Batch: 15800,train loss is: 8.388728635925214e-05\n",
      "test loss is 0.00012349040291466287\n",
      "Batch: 15900,train loss is: 0.00023767673684512016\n",
      "test loss is 0.00012325429088919432\n",
      "Batch: 16000,train loss is: 9.101442777580183e-05\n",
      "test loss is 0.0001240649519152165\n",
      "Batch: 16100,train loss is: 0.00014877466379174025\n",
      "test loss is 0.00012022416386703437\n",
      "Batch: 16200,train loss is: 5.612423676538593e-05\n",
      "test loss is 0.00012701845518146878\n",
      "Batch: 16300,train loss is: 0.00010012966602773546\n",
      "test loss is 0.00012713934148817483\n",
      "Batch: 16400,train loss is: 0.00013212201868122823\n",
      "test loss is 0.00012286892279679768\n",
      "Batch: 16500,train loss is: 0.00013402405580824006\n",
      "test loss is 0.00012999043483119524\n",
      "Batch: 16600,train loss is: 7.128026373942167e-05\n",
      "test loss is 0.00012516069990497371\n",
      "Batch: 16700,train loss is: 0.00016090513214254263\n",
      "test loss is 0.0001300286378227118\n",
      "Batch: 16800,train loss is: 0.000125211135455616\n",
      "test loss is 0.00012587629500392138\n",
      "Batch: 16900,train loss is: 0.00012030697122761215\n",
      "test loss is 0.00013675328409118274\n",
      "Batch: 17000,train loss is: 0.0001223961379699796\n",
      "test loss is 0.00012379789149964844\n",
      "Batch: 17100,train loss is: 0.00012698325843140572\n",
      "test loss is 0.00012322656973128528\n",
      "Batch: 17200,train loss is: 9.503215201399407e-05\n",
      "test loss is 0.00012756216589197993\n",
      "Batch: 17300,train loss is: 0.00011035797556872812\n",
      "test loss is 0.00012677269876567047\n",
      "Batch: 17400,train loss is: 5.539293781876848e-05\n",
      "test loss is 0.00012814362134907592\n",
      "Batch: 17500,train loss is: 8.611964173865486e-05\n",
      "test loss is 0.00012159596932290268\n",
      "Batch: 17600,train loss is: 7.148480770810687e-05\n",
      "test loss is 0.0001240642002628108\n",
      "Batch: 17700,train loss is: 0.00011412271204258288\n",
      "test loss is 0.00012048699066524492\n",
      "Batch: 17800,train loss is: 0.00019037256378175732\n",
      "test loss is 0.00013089494792100804\n",
      "Batch: 17900,train loss is: 8.416187271938716e-05\n",
      "test loss is 0.00012238054795671148\n",
      "Batch: 18000,train loss is: 8.76189024768947e-05\n",
      "test loss is 0.00012460378961535898\n",
      "Batch: 18100,train loss is: 0.00012016546261707018\n",
      "test loss is 0.00012110592354507249\n",
      "Batch: 18200,train loss is: 8.573848844247575e-05\n",
      "test loss is 0.00012043218952493637\n",
      "Batch: 18300,train loss is: 0.00021039056561915868\n",
      "test loss is 0.00012514403377951246\n",
      "Batch: 18400,train loss is: 9.287272288108549e-05\n",
      "test loss is 0.00011871818115657818\n",
      "Batch: 18500,train loss is: 8.597391220936063e-05\n",
      "test loss is 0.00012345994408596237\n",
      "Batch: 18600,train loss is: 8.303740061457062e-05\n",
      "test loss is 0.00012414402732013855\n",
      "Batch: 18700,train loss is: 0.00014148361825640319\n",
      "test loss is 0.00012502937619293826\n",
      "Batch: 18800,train loss is: 0.00011316304571562409\n",
      "test loss is 0.00012507955263544785\n",
      "Batch: 18900,train loss is: 0.00021331699922818372\n",
      "test loss is 0.0001326047489103793\n",
      "Batch: 19000,train loss is: 6.852564507219119e-05\n",
      "test loss is 0.0001299541716091058\n",
      "Batch: 19100,train loss is: 6.951583085483348e-05\n",
      "test loss is 0.00016125094430192655\n",
      "Batch: 19200,train loss is: 8.848022932069062e-05\n",
      "test loss is 0.00012969834216498415\n",
      "Batch: 19300,train loss is: 0.00012567184351616452\n",
      "test loss is 0.00012891390816065858\n",
      "Batch: 19400,train loss is: 6.862954131049549e-05\n",
      "test loss is 0.0001230831164221515\n",
      "Batch: 19500,train loss is: 9.897361164306254e-05\n",
      "test loss is 0.0001317320570466138\n",
      "Batch: 19600,train loss is: 6.175071913752168e-05\n",
      "test loss is 0.00012562720023991892\n",
      "Batch: 19700,train loss is: 0.00014217594571815047\n",
      "test loss is 0.00012074369771047454\n",
      "Batch: 19800,train loss is: 0.00011024861145590013\n",
      "test loss is 0.00012138286634205095\n",
      "Batch: 19900,train loss is: 7.563869740632209e-05\n",
      "test loss is 0.00012841734032631907\n",
      "Batch: 20000,train loss is: 0.00014683199452336914\n",
      "test loss is 0.0001264555354490674\n",
      "Batch: 20100,train loss is: 0.00011862621889833961\n",
      "test loss is 0.00012482912860534588\n",
      "Batch: 20200,train loss is: 7.063958659642946e-05\n",
      "test loss is 0.00012020631850367909\n",
      "Batch: 20300,train loss is: 6.827058609467512e-05\n",
      "test loss is 0.00012177697064369122\n",
      "Batch: 20400,train loss is: 0.00012648570413369712\n",
      "test loss is 0.000196721511162007\n",
      "Batch: 20500,train loss is: 8.303398358891653e-05\n",
      "test loss is 0.00012132496902032502\n",
      "Batch: 20600,train loss is: 0.00021427294384776885\n",
      "test loss is 0.00012977740736628163\n",
      "Batch: 20700,train loss is: 0.0001271345227743726\n",
      "test loss is 0.00012497739984921612\n",
      "Batch: 20800,train loss is: 5.6569340431166296e-05\n",
      "test loss is 0.0001231655747135859\n",
      "Batch: 20900,train loss is: 5.3319652093752255e-05\n",
      "test loss is 0.00012369229363127908\n",
      "Batch: 21000,train loss is: 5.6690060778479594e-05\n",
      "test loss is 0.00012171801296539062\n",
      "Batch: 21100,train loss is: 9.055238860584233e-05\n",
      "test loss is 0.00012347051445931354\n",
      "Batch: 21200,train loss is: 9.859981367793011e-05\n",
      "test loss is 0.00013135107992500028\n",
      "Batch: 21300,train loss is: 5.833038472678212e-05\n",
      "test loss is 0.00012091212650979693\n",
      "Batch: 21400,train loss is: 0.00014840896422775655\n",
      "test loss is 0.0001247973269790062\n",
      "Batch: 21500,train loss is: 7.751043110187273e-05\n",
      "test loss is 0.00012102065721296083\n",
      "Batch: 21600,train loss is: 0.00012379279388031658\n",
      "test loss is 0.0001207624003185393\n",
      "Batch: 21700,train loss is: 6.665744574569652e-05\n",
      "test loss is 0.00012726845892678691\n",
      "Batch: 21800,train loss is: 0.00014057038733968064\n",
      "test loss is 0.00012704021464580908\n",
      "Batch: 21900,train loss is: 6.928810403828271e-05\n",
      "test loss is 0.00012324977715388364\n",
      "Batch: 22000,train loss is: 0.0001294562399890583\n",
      "test loss is 0.00012894380978006688\n",
      "Batch: 22100,train loss is: 8.390845171312881e-05\n",
      "test loss is 0.00012078743572838558\n",
      "Batch: 22200,train loss is: 7.275842236631659e-05\n",
      "test loss is 0.00012059687088131107\n",
      "Batch: 22300,train loss is: 0.00010055151708992716\n",
      "test loss is 0.00012854904463311564\n",
      "Batch: 22400,train loss is: 9.469918761533781e-05\n",
      "test loss is 0.00012830457428766062\n",
      "Batch: 22500,train loss is: 0.00011094213191957481\n",
      "test loss is 0.0001215573888797428\n",
      "Batch: 22600,train loss is: 9.734605514530866e-05\n",
      "test loss is 0.00013268934518998612\n",
      "Batch: 22700,train loss is: 6.512833517851013e-05\n",
      "test loss is 0.00012086347175045033\n",
      "Batch: 22800,train loss is: 0.00010511095648983056\n",
      "test loss is 0.00012064074495173505\n",
      "Batch: 22900,train loss is: 0.00010371960307016628\n",
      "test loss is 0.00012127739735031868\n",
      "Batch: 23000,train loss is: 0.00018781451493913017\n",
      "test loss is 0.00012005290385858233\n",
      "Batch: 23100,train loss is: 8.412589509658898e-05\n",
      "test loss is 0.00012041943061370347\n",
      "Batch: 23200,train loss is: 6.759147114476206e-05\n",
      "test loss is 0.0001235207437946245\n",
      "Batch: 23300,train loss is: 9.335638289981441e-05\n",
      "test loss is 0.00012520708429463413\n",
      "Batch: 23400,train loss is: 0.000103861300923932\n",
      "test loss is 0.00011974080278368781\n",
      "Batch: 23500,train loss is: 0.00017184620013766563\n",
      "test loss is 0.00012169912501379771\n",
      "Batch: 23600,train loss is: 7.767367811667872e-05\n",
      "test loss is 0.00012109691674646306\n",
      "Batch: 23700,train loss is: 0.00010321774990067687\n",
      "test loss is 0.00014327775027118167\n",
      "Batch: 23800,train loss is: 0.00017991688534492697\n",
      "test loss is 0.00012602862559514042\n",
      "Batch: 23900,train loss is: 0.0002534196480554687\n",
      "test loss is 0.00011882602795750249\n",
      "Batch: 24000,train loss is: 9.195070501027707e-05\n",
      "test loss is 0.00013025224357003182\n",
      "Batch: 24100,train loss is: 0.00010231583889693357\n",
      "test loss is 0.00012545538405653615\n",
      "Batch: 24200,train loss is: 0.0001710148091089861\n",
      "test loss is 0.00012106399752592094\n",
      "Batch: 24300,train loss is: 8.509253482848005e-05\n",
      "test loss is 0.00014576096381681085\n",
      "Batch: 24400,train loss is: 0.00012937975637406488\n",
      "test loss is 0.00012112181764281594\n",
      "Batch: 24500,train loss is: 0.00015848768719933497\n",
      "test loss is 0.00011984991071188046\n",
      "Batch: 24600,train loss is: 8.193816428280984e-05\n",
      "test loss is 0.00013890191781364947\n",
      "Batch: 24700,train loss is: 5.319068929421086e-05\n",
      "test loss is 0.00011839767570196019\n",
      "Batch: 24800,train loss is: 0.0002096895344770395\n",
      "test loss is 0.0001283784380976883\n",
      "Batch: 24900,train loss is: 7.344431049916669e-05\n",
      "test loss is 0.0001357754688652151\n",
      "Batch: 25000,train loss is: 0.00012468784563184084\n",
      "test loss is 0.00011981720261914563\n",
      "Batch: 25100,train loss is: 7.585749668512834e-05\n",
      "test loss is 0.00012523950723691387\n",
      "Batch: 25200,train loss is: 0.00011967956488499928\n",
      "test loss is 0.00012025118197328846\n",
      "Batch: 25300,train loss is: 0.00015792618206427025\n",
      "test loss is 0.00012136934553249094\n",
      "Batch: 25400,train loss is: 0.00010849160194823923\n",
      "test loss is 0.0001228031762609059\n",
      "Batch: 25500,train loss is: 7.362820048296853e-05\n",
      "test loss is 0.00012101342386043433\n",
      "Batch: 25600,train loss is: 0.0001509891795110857\n",
      "test loss is 0.00013958303383231754\n",
      "Batch: 25700,train loss is: 0.0001670718430284735\n",
      "test loss is 0.00012797864404179768\n",
      "Batch: 25800,train loss is: 9.652317889090348e-05\n",
      "test loss is 0.00011915639754597598\n",
      "Batch: 25900,train loss is: 8.509179324104828e-05\n",
      "test loss is 0.00012362517808671148\n",
      "Batch: 26000,train loss is: 0.0001424716757533978\n",
      "test loss is 0.00012380816587649808\n",
      "Batch: 26100,train loss is: 0.00011410966923562697\n",
      "test loss is 0.00013004916372172623\n",
      "Batch: 26200,train loss is: 0.00016617149427631102\n",
      "test loss is 0.00012060432372355156\n",
      "Batch: 26300,train loss is: 0.00013508022084995422\n",
      "test loss is 0.0001395937737887447\n",
      "Batch: 26400,train loss is: 5.565977110795746e-05\n",
      "test loss is 0.00012304735114955858\n",
      "Batch: 26500,train loss is: 8.91986918167983e-05\n",
      "test loss is 0.00012113846693311033\n",
      "Batch: 26600,train loss is: 0.00018988690598802205\n",
      "test loss is 0.00011922947824818138\n",
      "Batch: 26700,train loss is: 9.717901335129388e-05\n",
      "test loss is 0.00013693124889084323\n",
      "Batch: 26800,train loss is: 0.00010602300181508584\n",
      "test loss is 0.00012085013713286886\n",
      "Batch: 26900,train loss is: 6.411347845419426e-05\n",
      "test loss is 0.0001202050784159059\n",
      "Batch: 27000,train loss is: 0.00010849187367576313\n",
      "test loss is 0.00012812347869047239\n",
      "Batch: 27100,train loss is: 0.00013283606380620077\n",
      "test loss is 0.00012988045472210727\n",
      "Batch: 27200,train loss is: 0.00010779322522605913\n",
      "test loss is 0.00012184257771193667\n",
      "Batch: 27300,train loss is: 5.7501020081406255e-05\n",
      "test loss is 0.00012069272316193182\n",
      "Batch: 27400,train loss is: 0.00012102649001043166\n",
      "test loss is 0.00012405020726175665\n",
      "Batch: 27500,train loss is: 0.00011667317071325354\n",
      "test loss is 0.00012113400312906817\n",
      "Batch: 27600,train loss is: 9.825925830893726e-05\n",
      "test loss is 0.00012851869733698944\n",
      "Batch: 27700,train loss is: 0.0003045469931529435\n",
      "test loss is 0.00012506224335069092\n",
      "Batch: 27800,train loss is: 0.00011918019851030313\n",
      "test loss is 0.0001311147119060467\n",
      "Batch: 27900,train loss is: 0.00011045909961657792\n",
      "test loss is 0.00012202836248011693\n",
      "Batch: 28000,train loss is: 9.979588834943231e-05\n",
      "test loss is 0.0001298503057498873\n",
      "Batch: 28100,train loss is: 0.0001730226300901402\n",
      "test loss is 0.00012505519109026652\n",
      "Batch: 28200,train loss is: 0.00010981396017463854\n",
      "test loss is 0.00012394383799881664\n",
      "Batch: 28300,train loss is: 0.00036070180478928784\n",
      "test loss is 0.0001249440106669869\n",
      "Batch: 28400,train loss is: 8.967196360220022e-05\n",
      "test loss is 0.0001172694310515036\n",
      "Batch: 28500,train loss is: 0.00018708835440644744\n",
      "test loss is 0.0001255738750917057\n",
      "Batch: 28600,train loss is: 0.00012974517022112115\n",
      "test loss is 0.00012381847537353902\n",
      "Batch: 28700,train loss is: 8.556905289449956e-05\n",
      "test loss is 0.00012279400506571338\n",
      "Batch: 28800,train loss is: 0.00017636872967939375\n",
      "test loss is 0.00012185793552369633\n",
      "Batch: 28900,train loss is: 0.00011809531025375598\n",
      "test loss is 0.00013768274368788826\n",
      "Batch: 29000,train loss is: 0.00010797051385781071\n",
      "test loss is 0.00012422388168012842\n",
      "Batch: 29100,train loss is: 8.185828684699429e-05\n",
      "test loss is 0.00012619721326384588\n",
      "Batch: 29200,train loss is: 0.00011619015119826414\n",
      "test loss is 0.00015177330851236527\n",
      "Batch: 29300,train loss is: 0.00021790121088842621\n",
      "test loss is 0.00012655355902490008\n",
      "Batch: 29400,train loss is: 0.00015418512720631147\n",
      "test loss is 0.0001233844748627098\n",
      "Batch: 29500,train loss is: 0.00012258525981831135\n",
      "test loss is 0.00011836020961164844\n",
      "Batch: 29600,train loss is: 0.00014264272576444917\n",
      "test loss is 0.00012498520448815157\n",
      "Batch: 29700,train loss is: 6.40597026914555e-05\n",
      "test loss is 0.00012065378423446606\n",
      "Batch: 29800,train loss is: 0.0002301555635394306\n",
      "test loss is 0.00012646021760561783\n",
      "Batch: 29900,train loss is: 0.000386496512737512\n",
      "test loss is 0.00012085644361785485\n",
      "Batch: 30000,train loss is: 9.388489822405735e-05\n",
      "test loss is 0.00011944293009500426\n",
      "Batch: 30100,train loss is: 0.00020745974294444244\n",
      "test loss is 0.00014161860863673895\n",
      "Batch: 30200,train loss is: 8.441592928614635e-05\n",
      "test loss is 0.00011917678811608174\n",
      "Batch: 30300,train loss is: 8.224245563597595e-05\n",
      "test loss is 0.00011971731669817641\n",
      "Batch: 30400,train loss is: 5.735936456023142e-05\n",
      "test loss is 0.00013367155940048668\n",
      "Batch: 30500,train loss is: 0.00011250704640738182\n",
      "test loss is 0.000142667205679277\n",
      "Batch: 30600,train loss is: 8.523080508015324e-05\n",
      "test loss is 0.00012234815624580908\n",
      "Batch: 30700,train loss is: 9.223372621228057e-05\n",
      "test loss is 0.00012212208757353063\n",
      "Batch: 30800,train loss is: 5.1261403497057993e-05\n",
      "test loss is 0.00012826013105664323\n",
      "Batch: 30900,train loss is: 8.170905889940053e-05\n",
      "test loss is 0.00012115026690769296\n",
      "Batch: 31000,train loss is: 6.716662279606934e-05\n",
      "test loss is 0.00011965348730215124\n",
      "Batch: 31100,train loss is: 9.574255889304092e-05\n",
      "test loss is 0.00015279243710149994\n",
      "Batch: 31200,train loss is: 0.00010106896006524661\n",
      "test loss is 0.0001235208891266987\n",
      "Batch: 31300,train loss is: 0.00011497659031960923\n",
      "test loss is 0.0001270914799974963\n",
      "Batch: 31400,train loss is: 7.211057643498862e-05\n",
      "test loss is 0.00012116783948318937\n",
      "Batch: 31500,train loss is: 0.00011598824935495577\n",
      "test loss is 0.00012756604026059503\n",
      "Batch: 31600,train loss is: 0.00012786294907058744\n",
      "test loss is 0.00012435039136804554\n",
      "Batch: 31700,train loss is: 7.607946318362321e-05\n",
      "test loss is 0.00013352503843102446\n",
      "Batch: 31800,train loss is: 0.0001887133987159628\n",
      "test loss is 0.00012058266853015631\n",
      "Batch: 31900,train loss is: 0.00016259453551268772\n",
      "test loss is 0.0001237147284069178\n",
      "Batch: 32000,train loss is: 0.00044544457123976277\n",
      "test loss is 0.0001300077328982323\n",
      "Batch: 32100,train loss is: 5.77426580562624e-05\n",
      "test loss is 0.00011884857275833285\n",
      "Batch: 32200,train loss is: 9.140187768257026e-05\n",
      "test loss is 0.00012188871962620838\n",
      "Batch: 32300,train loss is: 9.37485115872244e-05\n",
      "test loss is 0.00012051678295592513\n",
      "Batch: 32400,train loss is: 5.659801421731604e-05\n",
      "test loss is 0.00011817307473010076\n",
      "Batch: 32500,train loss is: 0.00011596712654410515\n",
      "test loss is 0.00011899667127544189\n",
      "Batch: 32600,train loss is: 8.754192803543983e-05\n",
      "test loss is 0.00013290994186428934\n",
      "Batch: 32700,train loss is: 5.226540447222267e-05\n",
      "test loss is 0.00012066272069401524\n",
      "Batch: 32800,train loss is: 0.00013402684373492628\n",
      "test loss is 0.00014392005052481268\n",
      "Batch: 32900,train loss is: 0.00010011490346069719\n",
      "test loss is 0.0001179253169428463\n",
      "Batch: 33000,train loss is: 5.8007274713910564e-05\n",
      "test loss is 0.00012271088952065066\n",
      "Batch: 33100,train loss is: 7.737723147275692e-05\n",
      "test loss is 0.00013035423581822292\n",
      "Batch: 33200,train loss is: 5.442379717401509e-05\n",
      "test loss is 0.00011770288407713053\n",
      "Batch: 33300,train loss is: 7.914141402139984e-05\n",
      "test loss is 0.00012028889393823076\n",
      "Batch: 33400,train loss is: 0.00010549873150470959\n",
      "test loss is 0.00012087855772311981\n",
      "Batch: 33500,train loss is: 8.862357090502e-05\n",
      "test loss is 0.0001328744562361889\n",
      "Batch: 33600,train loss is: 6.810031138862969e-05\n",
      "test loss is 0.0001300753781977441\n",
      "Batch: 33700,train loss is: 0.00010393575998246766\n",
      "test loss is 0.00012461199678417875\n",
      "Batch: 33800,train loss is: 0.00014708683177974306\n",
      "test loss is 0.00011708885582411182\n",
      "Batch: 33900,train loss is: 7.05841733781101e-05\n",
      "test loss is 0.00012052218656337851\n",
      "-----------------------Epoch: 4----------------------------------\n",
      "Batch: 0,train loss is: 0.0001321777021278194\n",
      "test loss is 0.0001267053843095548\n",
      "Batch: 100,train loss is: 0.00023251469577360276\n",
      "test loss is 0.00013329478003586503\n",
      "Batch: 200,train loss is: 0.00010942039249935771\n",
      "test loss is 0.00011797347000247303\n",
      "Batch: 300,train loss is: 0.00010302293142783737\n",
      "test loss is 0.00012677005201738149\n",
      "Batch: 400,train loss is: 8.685683188526783e-05\n",
      "test loss is 0.00012529181863923326\n",
      "Batch: 500,train loss is: 7.353074110449225e-05\n",
      "test loss is 0.00012924058209152987\n",
      "Batch: 600,train loss is: 4.465324382729113e-05\n",
      "test loss is 0.00012709498047615278\n",
      "Batch: 700,train loss is: 8.913170130693798e-05\n",
      "test loss is 0.00013672233955074793\n",
      "Batch: 800,train loss is: 0.00014729129376867946\n",
      "test loss is 0.00011830210195850775\n",
      "Batch: 900,train loss is: 8.677296845116911e-05\n",
      "test loss is 0.00012870214484285452\n",
      "Batch: 1000,train loss is: 9.361913668446312e-05\n",
      "test loss is 0.00012520608836054042\n",
      "Batch: 1100,train loss is: 0.0002222480197876664\n",
      "test loss is 0.00013022651193362767\n",
      "Batch: 1200,train loss is: 5.515273663221158e-05\n",
      "test loss is 0.0001195857197558007\n",
      "Batch: 1300,train loss is: 8.448352988121468e-05\n",
      "test loss is 0.0001250008478269699\n",
      "Batch: 1400,train loss is: 8.102160792453062e-05\n",
      "test loss is 0.00012217257243823522\n",
      "Batch: 1500,train loss is: 0.00010706933178675116\n",
      "test loss is 0.00012120595050515367\n",
      "Batch: 1600,train loss is: 8.662388138719901e-05\n",
      "test loss is 0.00011632296698426289\n",
      "Batch: 1700,train loss is: 0.00013031042154219228\n",
      "test loss is 0.00011803490991759262\n",
      "Batch: 1800,train loss is: 7.234751744519706e-05\n",
      "test loss is 0.00011939659642655468\n",
      "Batch: 1900,train loss is: 0.0001095646967563035\n",
      "test loss is 0.00014683084601695325\n",
      "Batch: 2000,train loss is: 0.00010339236963505835\n",
      "test loss is 0.00012031241558339259\n",
      "Batch: 2100,train loss is: 0.00011405045433236742\n",
      "test loss is 0.00013481950490002375\n",
      "Batch: 2200,train loss is: 9.425012790997798e-05\n",
      "test loss is 0.00012258477189682724\n",
      "Batch: 2300,train loss is: 6.444753500156092e-05\n",
      "test loss is 0.00011738169783057741\n",
      "Batch: 2400,train loss is: 7.453571485794265e-05\n",
      "test loss is 0.0001195976838730542\n",
      "Batch: 2500,train loss is: 0.00011570905539855219\n",
      "test loss is 0.00012544742401958192\n",
      "Batch: 2600,train loss is: 7.229749059904674e-05\n",
      "test loss is 0.00012141543189487985\n",
      "Batch: 2700,train loss is: 0.00010392055089153948\n",
      "test loss is 0.000118860573033397\n",
      "Batch: 2800,train loss is: 0.0001046645812063531\n",
      "test loss is 0.0001208703102446409\n",
      "Batch: 2900,train loss is: 0.00012884705120509178\n",
      "test loss is 0.0001386862308685527\n",
      "Batch: 3000,train loss is: 0.00010087856912155962\n",
      "test loss is 0.00012494031768536131\n",
      "Batch: 3100,train loss is: 6.397864441246263e-05\n",
      "test loss is 0.00011687576982206338\n",
      "Batch: 3200,train loss is: 0.00016354043938185726\n",
      "test loss is 0.00012040383242687162\n",
      "Batch: 3300,train loss is: 0.00010597431553287405\n",
      "test loss is 0.00011701240750951159\n",
      "Batch: 3400,train loss is: 0.00010543936333159104\n",
      "test loss is 0.00015617745743843708\n",
      "Batch: 3500,train loss is: 5.144038378436244e-05\n",
      "test loss is 0.00012151884047393746\n",
      "Batch: 3600,train loss is: 6.36084928157836e-05\n",
      "test loss is 0.0001229712657394339\n",
      "Batch: 3700,train loss is: 8.711386197027103e-05\n",
      "test loss is 0.00012006952917912467\n",
      "Batch: 3800,train loss is: 9.02575666874092e-05\n",
      "test loss is 0.00012075946630862347\n",
      "Batch: 3900,train loss is: 0.00013832293540812213\n",
      "test loss is 0.00012870941941508458\n",
      "Batch: 4000,train loss is: 5.20387131177415e-05\n",
      "test loss is 0.00011993121710569223\n",
      "Batch: 4100,train loss is: 0.00010609386989880428\n",
      "test loss is 0.00012205144250760192\n",
      "Batch: 4200,train loss is: 0.00012550453606545979\n",
      "test loss is 0.00012039021682484686\n",
      "Batch: 4300,train loss is: 8.15019580621179e-05\n",
      "test loss is 0.00012737475589370224\n",
      "Batch: 4400,train loss is: 8.242384371348649e-05\n",
      "test loss is 0.00012120994542334852\n",
      "Batch: 4500,train loss is: 8.759573758555172e-05\n",
      "test loss is 0.00011888220644085045\n",
      "Batch: 4600,train loss is: 7.30694333529353e-05\n",
      "test loss is 0.00012035633210509809\n",
      "Batch: 4700,train loss is: 0.0001320602289503732\n",
      "test loss is 0.00011852088109096409\n",
      "Batch: 4800,train loss is: 0.00023276293185393897\n",
      "test loss is 0.00012030217726560398\n",
      "Batch: 4900,train loss is: 7.785872187919222e-05\n",
      "test loss is 0.00013481007479318566\n",
      "Batch: 5000,train loss is: 7.346497881992957e-05\n",
      "test loss is 0.00011853279061855941\n",
      "Batch: 5100,train loss is: 8.083989263736283e-05\n",
      "test loss is 0.00012138250021429521\n",
      "Batch: 5200,train loss is: 4.793997755432539e-05\n",
      "test loss is 0.00011685708261184358\n",
      "Batch: 5300,train loss is: 0.00012158293310555418\n",
      "test loss is 0.00016477830275111677\n",
      "Batch: 5400,train loss is: 0.00016942280496305668\n",
      "test loss is 0.0001191548944188842\n",
      "Batch: 5500,train loss is: 7.688754521405833e-05\n",
      "test loss is 0.00012112550672706998\n",
      "Batch: 5600,train loss is: 0.00016023022235346244\n",
      "test loss is 0.0001186675807649638\n",
      "Batch: 5700,train loss is: 0.00012654571652082064\n",
      "test loss is 0.00012802526249533345\n",
      "Batch: 5800,train loss is: 0.00010349238851946799\n",
      "test loss is 0.0001236378717647195\n",
      "Batch: 5900,train loss is: 0.0003352615376035146\n",
      "test loss is 0.00012108588194913362\n",
      "Batch: 6000,train loss is: 0.0001475895981169959\n",
      "test loss is 0.00012131059681631363\n",
      "Batch: 6100,train loss is: 0.00010189817772790733\n",
      "test loss is 0.00014543121574873367\n",
      "Batch: 6200,train loss is: 8.663229193603376e-05\n",
      "test loss is 0.00011574050518872764\n",
      "Batch: 6300,train loss is: 0.0001423870037668883\n",
      "test loss is 0.00011962318003414775\n",
      "Batch: 6400,train loss is: 9.160333930525861e-05\n",
      "test loss is 0.00011861333116224576\n",
      "Batch: 6500,train loss is: 7.23126690083974e-05\n",
      "test loss is 0.00012389438381297426\n",
      "Batch: 6600,train loss is: 0.0002470301916601934\n",
      "test loss is 0.00011713624139519256\n",
      "Batch: 6700,train loss is: 5.024714279063224e-05\n",
      "test loss is 0.00012004036247409617\n",
      "Batch: 6800,train loss is: 0.00013842089770630889\n",
      "test loss is 0.00011920080783199388\n",
      "Batch: 6900,train loss is: 7.468621558719441e-05\n",
      "test loss is 0.00011665427133869237\n",
      "Batch: 7000,train loss is: 0.00011036964345006913\n",
      "test loss is 0.00011990710394403608\n",
      "Batch: 7100,train loss is: 9.717314401199618e-05\n",
      "test loss is 0.00011978712888858406\n",
      "Batch: 7200,train loss is: 5.6900073339616593e-05\n",
      "test loss is 0.00012231875267349287\n",
      "Batch: 7300,train loss is: 0.0002666607492535384\n",
      "test loss is 0.00012346174937281538\n",
      "Batch: 7400,train loss is: 7.19131155783493e-05\n",
      "test loss is 0.00012943848836323714\n",
      "Batch: 7500,train loss is: 0.0001966390821330282\n",
      "test loss is 0.00012658303242838243\n",
      "Batch: 7600,train loss is: 0.00013631943752601998\n",
      "test loss is 0.0001226325507838262\n",
      "Batch: 7700,train loss is: 0.00017602534159628024\n",
      "test loss is 0.00012326585316723137\n",
      "Batch: 7800,train loss is: 5.5359750404705066e-05\n",
      "test loss is 0.00012005920128951713\n",
      "Batch: 7900,train loss is: 7.44669661785581e-05\n",
      "test loss is 0.00012168519109588666\n",
      "Batch: 8000,train loss is: 4.870777457916691e-05\n",
      "test loss is 0.00012464671757457443\n",
      "Batch: 8100,train loss is: 0.0002343964067147197\n",
      "test loss is 0.00012523891988732753\n",
      "Batch: 8200,train loss is: 0.000126810936742232\n",
      "test loss is 0.00012933788931480494\n",
      "Batch: 8300,train loss is: 8.137604271110485e-05\n",
      "test loss is 0.00012441077118547083\n",
      "Batch: 8400,train loss is: 0.00011093598774481222\n",
      "test loss is 0.00011807912064519524\n",
      "Batch: 8500,train loss is: 0.0001226357516199441\n",
      "test loss is 0.00011868125786940194\n",
      "Batch: 8600,train loss is: 0.00012833893409414074\n",
      "test loss is 0.0001281152264273749\n",
      "Batch: 8700,train loss is: 0.00010268206974886415\n",
      "test loss is 0.0001326601801333731\n",
      "Batch: 8800,train loss is: 0.0001595987834384828\n",
      "test loss is 0.00011934857622149994\n",
      "Batch: 8900,train loss is: 0.00019562501642027196\n",
      "test loss is 0.00011919760071000454\n",
      "Batch: 9000,train loss is: 7.327511440612928e-05\n",
      "test loss is 0.0001190553642989305\n",
      "Batch: 9100,train loss is: 6.62005091277786e-05\n",
      "test loss is 0.00011578075729456157\n",
      "Batch: 9200,train loss is: 0.00010374089794177822\n",
      "test loss is 0.00011761375555786404\n",
      "Batch: 9300,train loss is: 0.00012138383091801857\n",
      "test loss is 0.0001338566652117805\n",
      "Batch: 9400,train loss is: 7.981227188395734e-05\n",
      "test loss is 0.00012159142725914157\n",
      "Batch: 9500,train loss is: 6.095212750839425e-05\n",
      "test loss is 0.00011651672440699536\n",
      "Batch: 9600,train loss is: 6.2919290208408e-05\n",
      "test loss is 0.00011811339104005967\n",
      "Batch: 9700,train loss is: 5.6884547490653534e-05\n",
      "test loss is 0.00011546344118865319\n",
      "Batch: 9800,train loss is: 5.444417107006572e-05\n",
      "test loss is 0.00012682823329428022\n",
      "Batch: 9900,train loss is: 5.510479212596135e-05\n",
      "test loss is 0.0001284022112290784\n",
      "Batch: 10000,train loss is: 8.399115248622443e-05\n",
      "test loss is 0.00011964829287661801\n",
      "Batch: 10100,train loss is: 0.00020396879149220203\n",
      "test loss is 0.00012827231023292252\n",
      "Batch: 10200,train loss is: 0.0002402162352555407\n",
      "test loss is 0.00011713046404030047\n",
      "Batch: 10300,train loss is: 7.407228609029725e-05\n",
      "test loss is 0.00011795523366357733\n",
      "Batch: 10400,train loss is: 5.476193350629235e-05\n",
      "test loss is 0.00011847446955918218\n",
      "Batch: 10500,train loss is: 0.00016885393650321485\n",
      "test loss is 0.00012119553373501672\n",
      "Batch: 10600,train loss is: 7.467975786828989e-05\n",
      "test loss is 0.00011973870793532272\n",
      "Batch: 10700,train loss is: 0.00010826572582292521\n",
      "test loss is 0.00011948839201365706\n",
      "Batch: 10800,train loss is: 0.00015325188079226334\n",
      "test loss is 0.0001178860868590981\n",
      "Batch: 10900,train loss is: 0.00011022854026611234\n",
      "test loss is 0.0001273581156721171\n",
      "Batch: 11000,train loss is: 8.94405933834618e-05\n",
      "test loss is 0.00011860508137372115\n",
      "Batch: 11100,train loss is: 8.453096470671183e-05\n",
      "test loss is 0.0001238949444692162\n",
      "Batch: 11200,train loss is: 0.00012075722839325254\n",
      "test loss is 0.0001178536932171627\n",
      "Batch: 11300,train loss is: 7.270886428921911e-05\n",
      "test loss is 0.00011863658367628624\n",
      "Batch: 11400,train loss is: 0.00015083320928452866\n",
      "test loss is 0.00012466141917024778\n",
      "Batch: 11500,train loss is: 7.157394163015e-05\n",
      "test loss is 0.00011965535902533955\n",
      "Batch: 11600,train loss is: 0.00018971935740716672\n",
      "test loss is 0.00012159966152394028\n",
      "Batch: 11700,train loss is: 7.704934908143032e-05\n",
      "test loss is 0.0001165709172100798\n",
      "Batch: 11800,train loss is: 8.760558699358834e-05\n",
      "test loss is 0.00012408049755773367\n",
      "Batch: 11900,train loss is: 0.00011196665639537479\n",
      "test loss is 0.0001204837943778328\n",
      "Batch: 12000,train loss is: 0.00012681473398683753\n",
      "test loss is 0.00012118200316459819\n",
      "Batch: 12100,train loss is: 0.0006294696671551697\n",
      "test loss is 0.00012620190757583943\n",
      "Batch: 12200,train loss is: 8.222546894851695e-05\n",
      "test loss is 0.00011942572311904467\n",
      "Batch: 12300,train loss is: 9.174146489316132e-05\n",
      "test loss is 0.00011892449685648075\n",
      "Batch: 12400,train loss is: 8.887528633579139e-05\n",
      "test loss is 0.00011797649425356427\n",
      "Batch: 12500,train loss is: 0.00022002308673958808\n",
      "test loss is 0.00011730886788676618\n",
      "Batch: 12600,train loss is: 6.43374666633866e-05\n",
      "test loss is 0.00011677359224245079\n",
      "Batch: 12700,train loss is: 5.647345573413711e-05\n",
      "test loss is 0.00011601751781752108\n",
      "Batch: 12800,train loss is: 6.605974103095154e-05\n",
      "test loss is 0.00011746730817561571\n",
      "Batch: 12900,train loss is: 0.0001200185752951085\n",
      "test loss is 0.0001161053318780468\n",
      "Batch: 13000,train loss is: 8.811249399856773e-05\n",
      "test loss is 0.00012921498618033176\n",
      "Batch: 13100,train loss is: 0.0001883559661530714\n",
      "test loss is 0.00012144472344442918\n",
      "Batch: 13200,train loss is: 0.00011269239243151372\n",
      "test loss is 0.00011587149495043512\n",
      "Batch: 13300,train loss is: 7.435577501600984e-05\n",
      "test loss is 0.0001466065357164888\n",
      "Batch: 13400,train loss is: 8.52385076706995e-05\n",
      "test loss is 0.0001239920003050231\n",
      "Batch: 13500,train loss is: 8.272318784564837e-05\n",
      "test loss is 0.00012763998219191054\n",
      "Batch: 13600,train loss is: 9.997145415640376e-05\n",
      "test loss is 0.00011700193672362429\n",
      "Batch: 13700,train loss is: 0.00010209235830209948\n",
      "test loss is 0.0001176391708869093\n",
      "Batch: 13800,train loss is: 0.00010879566250588983\n",
      "test loss is 0.00012101867389626934\n",
      "Batch: 13900,train loss is: 6.462056287415885e-05\n",
      "test loss is 0.00012542285893568878\n",
      "Batch: 14000,train loss is: 8.144155040895037e-05\n",
      "test loss is 0.00011852949785160009\n",
      "Batch: 14100,train loss is: 9.658452182384719e-05\n",
      "test loss is 0.0001451508587309949\n",
      "Batch: 14200,train loss is: 5.04092269450771e-05\n",
      "test loss is 0.00011776789934735887\n",
      "Batch: 14300,train loss is: 5.429402705691111e-05\n",
      "test loss is 0.0001178656845421925\n",
      "Batch: 14400,train loss is: 0.0001330146461264575\n",
      "test loss is 0.00011515564809779293\n",
      "Batch: 14500,train loss is: 5.3972293417447895e-05\n",
      "test loss is 0.00011532425624587602\n",
      "Batch: 14600,train loss is: 0.0002106652346642066\n",
      "test loss is 0.00011777721201035902\n",
      "Batch: 14700,train loss is: 7.086050347747088e-05\n",
      "test loss is 0.00011697358295638836\n",
      "Batch: 14800,train loss is: 0.0001352117652368079\n",
      "test loss is 0.00011562521229734918\n",
      "Batch: 14900,train loss is: 7.951192401652031e-05\n",
      "test loss is 0.00011853734079912661\n",
      "Batch: 15000,train loss is: 6.969295745087806e-05\n",
      "test loss is 0.00013099567922215784\n",
      "Batch: 15100,train loss is: 0.0001673376487293291\n",
      "test loss is 0.0001198853657408165\n",
      "Batch: 15200,train loss is: 0.0001395892277003842\n",
      "test loss is 0.00011462418918779631\n",
      "Batch: 15300,train loss is: 0.00013792445311117688\n",
      "test loss is 0.0001145971650142683\n",
      "Batch: 15400,train loss is: 8.992118318148071e-05\n",
      "test loss is 0.000121819421526537\n",
      "Batch: 15500,train loss is: 0.00017159602106596575\n",
      "test loss is 0.00012517909019948387\n",
      "Batch: 15600,train loss is: 0.00020808615434185954\n",
      "test loss is 0.0001250171032176021\n",
      "Batch: 15700,train loss is: 9.338135603010962e-05\n",
      "test loss is 0.0001272156628906138\n",
      "Batch: 15800,train loss is: 7.715858422990582e-05\n",
      "test loss is 0.00011864642675437797\n",
      "Batch: 15900,train loss is: 0.00022987354220031905\n",
      "test loss is 0.0001181341052324466\n",
      "Batch: 16000,train loss is: 8.559594401895079e-05\n",
      "test loss is 0.00011953700152938139\n",
      "Batch: 16100,train loss is: 0.0001444460467764191\n",
      "test loss is 0.00011578822069709829\n",
      "Batch: 16200,train loss is: 5.242040721201546e-05\n",
      "test loss is 0.00012247830173202899\n",
      "Batch: 16300,train loss is: 9.951756764339055e-05\n",
      "test loss is 0.0001224878329963759\n",
      "Batch: 16400,train loss is: 0.0001256417755267613\n",
      "test loss is 0.00011816252294721839\n",
      "Batch: 16500,train loss is: 0.00012942662894781118\n",
      "test loss is 0.0001265586848269318\n",
      "Batch: 16600,train loss is: 6.991290979173987e-05\n",
      "test loss is 0.00011948826043158606\n",
      "Batch: 16700,train loss is: 0.00014921275734609748\n",
      "test loss is 0.0001256860991279919\n",
      "Batch: 16800,train loss is: 0.00012039844781993362\n",
      "test loss is 0.00012014106460741954\n",
      "Batch: 16900,train loss is: 0.00011683977203588075\n",
      "test loss is 0.00013203288001289488\n",
      "Batch: 17000,train loss is: 0.00011304168456742045\n",
      "test loss is 0.00011880383248023708\n",
      "Batch: 17100,train loss is: 0.0001238397630668299\n",
      "test loss is 0.00011863770487468727\n",
      "Batch: 17200,train loss is: 9.356624566427213e-05\n",
      "test loss is 0.00012210685630497528\n",
      "Batch: 17300,train loss is: 0.0001024584158153503\n",
      "test loss is 0.00012225232241768697\n",
      "Batch: 17400,train loss is: 5.36914886581078e-05\n",
      "test loss is 0.00012366468342595236\n",
      "Batch: 17500,train loss is: 7.976856913520758e-05\n",
      "test loss is 0.00011690033555431194\n",
      "Batch: 17600,train loss is: 6.960862030306528e-05\n",
      "test loss is 0.00011884085405224995\n",
      "Batch: 17700,train loss is: 0.00010719391393277221\n",
      "test loss is 0.00011568304436995638\n",
      "Batch: 17800,train loss is: 0.00018162347806660587\n",
      "test loss is 0.00012567214456619483\n",
      "Batch: 17900,train loss is: 8.109336331273861e-05\n",
      "test loss is 0.00011802532847937945\n",
      "Batch: 18000,train loss is: 8.441282104490583e-05\n",
      "test loss is 0.00011896020977375131\n",
      "Batch: 18100,train loss is: 0.00011567278863298773\n",
      "test loss is 0.00011570659926263627\n",
      "Batch: 18200,train loss is: 8.178919917038038e-05\n",
      "test loss is 0.00011606775764050665\n",
      "Batch: 18300,train loss is: 0.00021009142896155316\n",
      "test loss is 0.00012070077086422348\n",
      "Batch: 18400,train loss is: 8.901123648848317e-05\n",
      "test loss is 0.00011391505408058055\n",
      "Batch: 18500,train loss is: 8.137307279242142e-05\n",
      "test loss is 0.00011886029097331567\n",
      "Batch: 18600,train loss is: 7.875629214986832e-05\n",
      "test loss is 0.00011926562042776437\n",
      "Batch: 18700,train loss is: 0.00013653386528103697\n",
      "test loss is 0.00011921009522338964\n",
      "Batch: 18800,train loss is: 0.00010458398719379219\n",
      "test loss is 0.00011943016374094475\n",
      "Batch: 18900,train loss is: 0.00020445015306042158\n",
      "test loss is 0.00012653841087561536\n",
      "Batch: 19000,train loss is: 6.496680282732297e-05\n",
      "test loss is 0.0001237923801171212\n",
      "Batch: 19100,train loss is: 6.81519733984491e-05\n",
      "test loss is 0.00015650135460097955\n",
      "Batch: 19200,train loss is: 8.401659444950503e-05\n",
      "test loss is 0.00012352040923583075\n",
      "Batch: 19300,train loss is: 0.00012237342495254614\n",
      "test loss is 0.00012445889683269047\n",
      "Batch: 19400,train loss is: 6.720399913076137e-05\n",
      "test loss is 0.00011842781115196692\n",
      "Batch: 19500,train loss is: 9.796151780336618e-05\n",
      "test loss is 0.00012704992694834876\n",
      "Batch: 19600,train loss is: 5.8626195964092945e-05\n",
      "test loss is 0.00012072426188520874\n",
      "Batch: 19700,train loss is: 0.0001354872405363346\n",
      "test loss is 0.00011537997664989466\n",
      "Batch: 19800,train loss is: 0.00010529382689546123\n",
      "test loss is 0.00011683561145239005\n",
      "Batch: 19900,train loss is: 7.180032275526559e-05\n",
      "test loss is 0.00012305645429919163\n",
      "Batch: 20000,train loss is: 0.00014251197901409143\n",
      "test loss is 0.00012132957994003929\n",
      "Batch: 20100,train loss is: 0.00011939106476849552\n",
      "test loss is 0.0001193633366995646\n",
      "Batch: 20200,train loss is: 6.531920007230408e-05\n",
      "test loss is 0.0001158956285048176\n",
      "Batch: 20300,train loss is: 6.661635649230426e-05\n",
      "test loss is 0.00011649607556741636\n",
      "Batch: 20400,train loss is: 0.00012760926242193286\n",
      "test loss is 0.00019245534832804792\n",
      "Batch: 20500,train loss is: 7.948451111004138e-05\n",
      "test loss is 0.00011667143166123388\n",
      "Batch: 20600,train loss is: 0.00020518918729998644\n",
      "test loss is 0.00012503470878519405\n",
      "Batch: 20700,train loss is: 0.0001233407664426974\n",
      "test loss is 0.00012206525037773278\n",
      "Batch: 20800,train loss is: 5.666135777234646e-05\n",
      "test loss is 0.00011800327455395004\n",
      "Batch: 20900,train loss is: 5.133417501480596e-05\n",
      "test loss is 0.00011988479525794037\n",
      "Batch: 21000,train loss is: 5.736570037838703e-05\n",
      "test loss is 0.0001175206444430742\n",
      "Batch: 21100,train loss is: 8.56563668693651e-05\n",
      "test loss is 0.00011909620261375005\n",
      "Batch: 21200,train loss is: 9.355922367013507e-05\n",
      "test loss is 0.0001261501235038509\n",
      "Batch: 21300,train loss is: 5.429287010471437e-05\n",
      "test loss is 0.00011588906435220278\n",
      "Batch: 21400,train loss is: 0.0001411966792266604\n",
      "test loss is 0.00011890347960002724\n",
      "Batch: 21500,train loss is: 7.506210744940736e-05\n",
      "test loss is 0.0001155728508201477\n",
      "Batch: 21600,train loss is: 0.00011837875913918867\n",
      "test loss is 0.00011622235478390409\n",
      "Batch: 21700,train loss is: 6.587449328397969e-05\n",
      "test loss is 0.0001232259037853487\n",
      "Batch: 21800,train loss is: 0.00013651190313508218\n",
      "test loss is 0.00012095414881036138\n",
      "Batch: 21900,train loss is: 6.591639247424792e-05\n",
      "test loss is 0.0001183386663924946\n",
      "Batch: 22000,train loss is: 0.00012716833466739892\n",
      "test loss is 0.0001247619613453574\n",
      "Batch: 22100,train loss is: 7.913155132278877e-05\n",
      "test loss is 0.0001162438641511465\n",
      "Batch: 22200,train loss is: 7.035557694693466e-05\n",
      "test loss is 0.0001161905278078745\n",
      "Batch: 22300,train loss is: 9.758846653561505e-05\n",
      "test loss is 0.0001227386015801248\n",
      "Batch: 22400,train loss is: 9.151464150133651e-05\n",
      "test loss is 0.00012355788632409463\n",
      "Batch: 22500,train loss is: 0.0001085162757027434\n",
      "test loss is 0.00011749938248908044\n",
      "Batch: 22600,train loss is: 9.373374603707607e-05\n",
      "test loss is 0.00012805511257560294\n",
      "Batch: 22700,train loss is: 6.336562119749786e-05\n",
      "test loss is 0.00011618125004479496\n",
      "Batch: 22800,train loss is: 0.0001010803230718605\n",
      "test loss is 0.00011631864017679284\n",
      "Batch: 22900,train loss is: 0.00010011251026784914\n",
      "test loss is 0.00011650211998794257\n",
      "Batch: 23000,train loss is: 0.0001829765910379611\n",
      "test loss is 0.000114778738992066\n",
      "Batch: 23100,train loss is: 8.101056157481311e-05\n",
      "test loss is 0.00011583652146734877\n",
      "Batch: 23200,train loss is: 6.59718270616257e-05\n",
      "test loss is 0.00011945599355508731\n",
      "Batch: 23300,train loss is: 9.086241436560101e-05\n",
      "test loss is 0.00012000102268958348\n",
      "Batch: 23400,train loss is: 0.00010014601035347346\n",
      "test loss is 0.000115089294071831\n",
      "Batch: 23500,train loss is: 0.0001742768336213951\n",
      "test loss is 0.00011696817617676146\n",
      "Batch: 23600,train loss is: 7.476520351379645e-05\n",
      "test loss is 0.00011584291446422229\n",
      "Batch: 23700,train loss is: 9.84097647431159e-05\n",
      "test loss is 0.00013944348647476946\n",
      "Batch: 23800,train loss is: 0.00017292141022788894\n",
      "test loss is 0.0001211865957651486\n",
      "Batch: 23900,train loss is: 0.0002479762066668832\n",
      "test loss is 0.00011418027474232067\n",
      "Batch: 24000,train loss is: 8.700567064209206e-05\n",
      "test loss is 0.00012570474087074464\n",
      "Batch: 24100,train loss is: 9.982249913987407e-05\n",
      "test loss is 0.00012066141875062202\n",
      "Batch: 24200,train loss is: 0.0001631977427022868\n",
      "test loss is 0.00011596673212134951\n",
      "Batch: 24300,train loss is: 7.759226000343889e-05\n",
      "test loss is 0.00013845606036269479\n",
      "Batch: 24400,train loss is: 0.00012009546267439137\n",
      "test loss is 0.00011642370314077972\n",
      "Batch: 24500,train loss is: 0.00014619390427730197\n",
      "test loss is 0.00011474064721681263\n",
      "Batch: 24600,train loss is: 8.129216728057797e-05\n",
      "test loss is 0.00013373924877966472\n",
      "Batch: 24700,train loss is: 5.1505419067570185e-05\n",
      "test loss is 0.0001139853042987795\n",
      "Batch: 24800,train loss is: 0.0001979330288451298\n",
      "test loss is 0.0001234824522006277\n",
      "Batch: 24900,train loss is: 6.934294455909708e-05\n",
      "test loss is 0.00012924401001385052\n",
      "Batch: 25000,train loss is: 0.0001184444217409787\n",
      "test loss is 0.00011503957329522006\n",
      "Batch: 25100,train loss is: 7.532937057850793e-05\n",
      "test loss is 0.00012023677834468604\n",
      "Batch: 25200,train loss is: 0.00011819333093385599\n",
      "test loss is 0.00011568990193385769\n",
      "Batch: 25300,train loss is: 0.0001500759481538057\n",
      "test loss is 0.00011655385345618217\n",
      "Batch: 25400,train loss is: 0.00010283682820078065\n",
      "test loss is 0.00011767928465132857\n",
      "Batch: 25500,train loss is: 6.853065263707495e-05\n",
      "test loss is 0.00011645245781323698\n",
      "Batch: 25600,train loss is: 0.00014932632660171136\n",
      "test loss is 0.00013474720015552215\n",
      "Batch: 25700,train loss is: 0.00016782941271659924\n",
      "test loss is 0.00012366367036978693\n",
      "Batch: 25800,train loss is: 9.278751697768097e-05\n",
      "test loss is 0.00011448549450635161\n",
      "Batch: 25900,train loss is: 8.17316837227655e-05\n",
      "test loss is 0.0001189900069556641\n",
      "Batch: 26000,train loss is: 0.00014149831643914455\n",
      "test loss is 0.00011858982543937482\n",
      "Batch: 26100,train loss is: 0.00010881211348575331\n",
      "test loss is 0.00012595435594501227\n",
      "Batch: 26200,train loss is: 0.0001600025429826656\n",
      "test loss is 0.00011590728370207698\n",
      "Batch: 26300,train loss is: 0.0001286925419560413\n",
      "test loss is 0.00013339208314412526\n",
      "Batch: 26400,train loss is: 5.1448293081428185e-05\n",
      "test loss is 0.000117072279760198\n",
      "Batch: 26500,train loss is: 8.290805831970615e-05\n",
      "test loss is 0.00011527014225315457\n",
      "Batch: 26600,train loss is: 0.00018752635015131938\n",
      "test loss is 0.00011480859448834614\n",
      "Batch: 26700,train loss is: 9.420010009356583e-05\n",
      "test loss is 0.00013188680896990206\n",
      "Batch: 26800,train loss is: 0.00010316799689881343\n",
      "test loss is 0.00011638852675882318\n",
      "Batch: 26900,train loss is: 6.188207179326751e-05\n",
      "test loss is 0.00011561354315966891\n",
      "Batch: 27000,train loss is: 0.0001049322059099122\n",
      "test loss is 0.00012318706661009558\n",
      "Batch: 27100,train loss is: 0.00012785717169290354\n",
      "test loss is 0.00012510823425462665\n",
      "Batch: 27200,train loss is: 0.00010251499254883526\n",
      "test loss is 0.00011685384399436951\n",
      "Batch: 27300,train loss is: 5.7561246816631735e-05\n",
      "test loss is 0.00011587037549872562\n",
      "Batch: 27400,train loss is: 0.00011777050781911207\n",
      "test loss is 0.00011954711254940306\n",
      "Batch: 27500,train loss is: 0.00011446312643298832\n",
      "test loss is 0.00011711416641348143\n",
      "Batch: 27600,train loss is: 9.822741469268496e-05\n",
      "test loss is 0.0001244702504985776\n",
      "Batch: 27700,train loss is: 0.00029027402793753947\n",
      "test loss is 0.00011955157324904027\n",
      "Batch: 27800,train loss is: 0.00011029099076086131\n",
      "test loss is 0.00012624188845751212\n",
      "Batch: 27900,train loss is: 0.00010626137840422646\n",
      "test loss is 0.00011718805441448007\n",
      "Batch: 28000,train loss is: 9.686362129122608e-05\n",
      "test loss is 0.00012503588392233361\n",
      "Batch: 28100,train loss is: 0.00016626574521176845\n",
      "test loss is 0.00011977196709765458\n",
      "Batch: 28200,train loss is: 0.00011055663870927409\n",
      "test loss is 0.00011901212372720588\n",
      "Batch: 28300,train loss is: 0.00033929943934273336\n",
      "test loss is 0.00011894347996025106\n",
      "Batch: 28400,train loss is: 8.545491930283835e-05\n",
      "test loss is 0.0001128791848565536\n",
      "Batch: 28500,train loss is: 0.00018349340987193778\n",
      "test loss is 0.00012020161369257135\n",
      "Batch: 28600,train loss is: 0.00012685920165895651\n",
      "test loss is 0.0001190474283279406\n",
      "Batch: 28700,train loss is: 7.95991421021253e-05\n",
      "test loss is 0.00011880370422131086\n",
      "Batch: 28800,train loss is: 0.00017033832554283867\n",
      "test loss is 0.0001173844005651454\n",
      "Batch: 28900,train loss is: 0.00011872814582545111\n",
      "test loss is 0.00013274677495833206\n",
      "Batch: 29000,train loss is: 0.00010175143894713277\n",
      "test loss is 0.00011899384254029298\n",
      "Batch: 29100,train loss is: 7.810868883997606e-05\n",
      "test loss is 0.00012145875908437614\n",
      "Batch: 29200,train loss is: 0.00011385716415603217\n",
      "test loss is 0.00014853582624250482\n",
      "Batch: 29300,train loss is: 0.0002111098652103975\n",
      "test loss is 0.00012194894082614048\n",
      "Batch: 29400,train loss is: 0.00014684080479505874\n",
      "test loss is 0.00011757720815308844\n",
      "Batch: 29500,train loss is: 0.00011998351202998488\n",
      "test loss is 0.00011384005463750088\n",
      "Batch: 29600,train loss is: 0.00013642429683381142\n",
      "test loss is 0.00011924294584467149\n",
      "Batch: 29700,train loss is: 6.110124784007874e-05\n",
      "test loss is 0.00011592872918468788\n",
      "Batch: 29800,train loss is: 0.00022543903896820602\n",
      "test loss is 0.0001218848500929622\n",
      "Batch: 29900,train loss is: 0.0003746748253777699\n",
      "test loss is 0.00011578933311253304\n",
      "Batch: 30000,train loss is: 8.933248793658133e-05\n",
      "test loss is 0.00011478868626604189\n",
      "Batch: 30100,train loss is: 0.0002065423562079227\n",
      "test loss is 0.00013521401155426287\n",
      "Batch: 30200,train loss is: 8.13402558016667e-05\n",
      "test loss is 0.0001147976506351062\n",
      "Batch: 30300,train loss is: 7.72749644195673e-05\n",
      "test loss is 0.00011547722294552893\n",
      "Batch: 30400,train loss is: 5.678778632278961e-05\n",
      "test loss is 0.0001294977159668926\n",
      "Batch: 30500,train loss is: 0.00010652496926889302\n",
      "test loss is 0.0001361184648759388\n",
      "Batch: 30600,train loss is: 8.251452982700532e-05\n",
      "test loss is 0.00011830176100440842\n",
      "Batch: 30700,train loss is: 9.044129483509422e-05\n",
      "test loss is 0.00011787446400759702\n",
      "Batch: 30800,train loss is: 4.770974040999983e-05\n",
      "test loss is 0.0001233002597135055\n",
      "Batch: 30900,train loss is: 7.916344324527187e-05\n",
      "test loss is 0.00011652938094578381\n",
      "Batch: 31000,train loss is: 6.231930645857606e-05\n",
      "test loss is 0.00011485328729471226\n",
      "Batch: 31100,train loss is: 9.126483913136602e-05\n",
      "test loss is 0.00014531755162738993\n",
      "Batch: 31200,train loss is: 9.951132563738315e-05\n",
      "test loss is 0.0001181982116455284\n",
      "Batch: 31300,train loss is: 0.00011135516118760723\n",
      "test loss is 0.00012234125033691332\n",
      "Batch: 31400,train loss is: 6.860026315655223e-05\n",
      "test loss is 0.00011554094635815434\n",
      "Batch: 31500,train loss is: 0.00011290065870516603\n",
      "test loss is 0.00012296397727160578\n",
      "Batch: 31600,train loss is: 0.00011946118586311323\n",
      "test loss is 0.00012020122627031341\n",
      "Batch: 31700,train loss is: 7.286908318819583e-05\n",
      "test loss is 0.00013036314271643314\n",
      "Batch: 31800,train loss is: 0.00018379783201784823\n",
      "test loss is 0.00011635322308996523\n",
      "Batch: 31900,train loss is: 0.00015815011395849192\n",
      "test loss is 0.00011948585045190395\n",
      "Batch: 32000,train loss is: 0.00041410125702067243\n",
      "test loss is 0.00012463625438729632\n",
      "Batch: 32100,train loss is: 5.5466407344481815e-05\n",
      "test loss is 0.00011443348208867313\n",
      "Batch: 32200,train loss is: 8.989322151872654e-05\n",
      "test loss is 0.00011716577374749811\n",
      "Batch: 32300,train loss is: 9.040893704155061e-05\n",
      "test loss is 0.0001168871372325904\n",
      "Batch: 32400,train loss is: 5.320340729879486e-05\n",
      "test loss is 0.00011340646629280145\n",
      "Batch: 32500,train loss is: 0.00010348277121881083\n",
      "test loss is 0.0001142228418045531\n",
      "Batch: 32600,train loss is: 8.284706058235635e-05\n",
      "test loss is 0.0001258466239329038\n",
      "Batch: 32700,train loss is: 4.958501581410761e-05\n",
      "test loss is 0.0001154645173444151\n",
      "Batch: 32800,train loss is: 0.00013200314817602446\n",
      "test loss is 0.00013884581941064471\n",
      "Batch: 32900,train loss is: 9.430379031480102e-05\n",
      "test loss is 0.00011318500067391621\n",
      "Batch: 33000,train loss is: 5.2532218862300216e-05\n",
      "test loss is 0.00011796862410139878\n",
      "Batch: 33100,train loss is: 7.64799051080188e-05\n",
      "test loss is 0.00012556486121404793\n",
      "Batch: 33200,train loss is: 5.0745358833992274e-05\n",
      "test loss is 0.0001128249260798401\n",
      "Batch: 33300,train loss is: 7.605751599450561e-05\n",
      "test loss is 0.00011487338763511573\n",
      "Batch: 33400,train loss is: 0.00010404856017707861\n",
      "test loss is 0.00011597339731901876\n",
      "Batch: 33500,train loss is: 8.601087633230404e-05\n",
      "test loss is 0.00012923572665997932\n",
      "Batch: 33600,train loss is: 6.38948007571376e-05\n",
      "test loss is 0.0001254059572400739\n",
      "Batch: 33700,train loss is: 9.953805954774313e-05\n",
      "test loss is 0.00011919770304445516\n",
      "Batch: 33800,train loss is: 0.00014146108542577356\n",
      "test loss is 0.00011225409446448077\n",
      "Batch: 33900,train loss is: 6.922435216203228e-05\n",
      "test loss is 0.00011564599521423731\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optim_Adam = torch.optim.Adam(model.parameters(),lr= 0.00001)\n",
    "train_model(loss_MSE,optim_Adam,model,data_loader,train_data,test_data,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'../../Data/YU/rflatBergomi_pointwise88.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "initial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
