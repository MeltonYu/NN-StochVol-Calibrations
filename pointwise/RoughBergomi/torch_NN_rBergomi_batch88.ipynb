{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "f = gzip.GzipFile(r\"../../Data/rBergomiTrainSet.txt.gz\", \"r\")\n",
    "dat=np.load(f)\n",
    "xx=dat[:,:4]\n",
    "yy=dat[:,4:]\n",
    "strikes=np.array([0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4,1.5 ])\n",
    "maturities=np.array([0.1,0.3,0.6,0.9,1.2,1.5,1.8,2.0 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    xx, yy, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_and_expand(a,x,y):\n",
    "    # use choose and where !\n",
    "    n = len(x)*len(y)\n",
    "    a_index = np.arange(len(a))%n\n",
    "    \n",
    "    \n",
    "    x_index = a_index//len(y)\n",
    "    y_index = a_index%len(y)\n",
    "    \n",
    "    x_added = np.choose(x_index,x.reshape(-1,1)).reshape(-1,1)\n",
    "    y_added = np.choose(y_index,y.reshape(-1,1)).reshape(-1,1)\n",
    "    \n",
    "    return np.hstack([a,x_added,y_added])\n",
    "\n",
    "y_train,y_test = y_train.reshape(-1,8,11),y_test.reshape(-1,8,11)\n",
    "x_train,x_test = np.repeat(x_train, 8*11,axis=0),np.repeat(x_test, 8*11,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test=append_and_expand(x_train,maturities,strikes),append_and_expand(x_test,maturities,strikes)\n",
    "y_train,y_test = y_train.reshape(-1).reshape(-1,1), y_test.reshape(-1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "scale_y=  StandardScaler()\n",
    "\n",
    "def ytransform(y_train,y_test):\n",
    "    return [scale_y.fit_transform(y_train),scale_y.transform(y_test)]\n",
    "\n",
    "def yinversetransform(y):\n",
    "    return scale_y.inverse_transform(y)\n",
    "\n",
    "# Upper and lower bounds used in the training set\n",
    "ub=np.array([0.16,4,-0.1,0.5,2.0,1.5])\n",
    "lb=np.array([0.01,0.3,-0.95,0.025,0.1,0.5])\n",
    "\n",
    "def myscale(x):\n",
    "    return (x - (ub+lb)*0.5)*2/(ub-lb)\n",
    "def myinverse(x):\n",
    "    return x*(ub-lb)*0.5+(ub+lb)*0.5\n",
    "\n",
    "x_train_transform = myscale(x_train)\n",
    "x_test_transform = myscale(x_test)\n",
    "[y_train_transform,y_test_transform] = ytransform(y_train,y_test)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"device is {device}\")\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(x_train_transform).to(device=device),\n",
    "                                               torch.from_numpy(y_train_transform).to(device=device))\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.from_numpy(x_test_transform).to(device=device),\n",
    "                                              torch.from_numpy(y_test_transform).to(device=device))\n",
    "\n",
    "\n",
    "train_data = (torch.from_numpy(x_train_transform).to(device=device),torch.from_numpy(y_train_transform).to(device=device))\n",
    "test_data = (torch.from_numpy(x_test_transform).to(device=device),torch.from_numpy(y_test_transform).to(device=device))\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(train_dataset,batch_size =88,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')  # Add the parent directory to the Python path\n",
    "\n",
    "from torch_NN.nn import ResNN_pricing\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "hyperparas = {'input_dim':6,'hidden_dim':64,'hidden_nums':10,'output_dim':1,'block_layer_nums':3}\n",
    "\n",
    "model = ResNN_pricing(hyperparas=hyperparas).to(device=device,dtype=torch.float64)\n",
    "\n",
    "loss_MSE = nn.MSELoss()\n",
    "optim_Adam = torch.optim.Adam(model.parameters(),lr= 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Epoch: 0----------------------------------\n",
      "Batch: 0,train loss is: 1.1867440781734666\n",
      "test loss is 1.585635669494096\n",
      "Batch: 100,train loss is: 0.6773497952116992\n",
      "test loss is 0.32458406005914753\n",
      "Batch: 200,train loss is: 0.034135306484978756\n",
      "test loss is 0.15046069978581594\n",
      "Batch: 300,train loss is: 0.07667411687015212\n",
      "test loss is 0.11057684892713483\n",
      "Batch: 400,train loss is: 0.022256620923481407\n",
      "test loss is 0.08499628935544178\n",
      "Batch: 500,train loss is: 0.06972948235296496\n",
      "test loss is 0.07448597434448725\n",
      "Batch: 600,train loss is: 0.04439371790959378\n",
      "test loss is 0.06766715548970133\n",
      "Batch: 700,train loss is: 0.014705220567723753\n",
      "test loss is 0.06047685814233061\n",
      "Batch: 800,train loss is: 0.018446212647522896\n",
      "test loss is 0.05024565259915624\n",
      "Batch: 900,train loss is: 0.01925664573481065\n",
      "test loss is 0.044208270534145326\n",
      "Batch: 1000,train loss is: 0.01584588421485247\n",
      "test loss is 0.03908852071657136\n",
      "Batch: 1100,train loss is: 0.15745152681234323\n",
      "test loss is 0.058267611695708946\n",
      "Batch: 1200,train loss is: 0.02515394456443618\n",
      "test loss is 0.03992677226647138\n",
      "Batch: 1300,train loss is: 0.015501982801591305\n",
      "test loss is 0.04173093877875674\n",
      "Batch: 1400,train loss is: 0.022549560852670625\n",
      "test loss is 0.03265360980801054\n",
      "Batch: 1500,train loss is: 0.01764094217748401\n",
      "test loss is 0.05269232298248963\n",
      "Batch: 1600,train loss is: 0.022179078378900863\n",
      "test loss is 0.058422860731968757\n",
      "Batch: 1700,train loss is: 0.04926681870858018\n",
      "test loss is 0.026229773981154936\n",
      "Batch: 1800,train loss is: 0.006591639017800401\n",
      "test loss is 0.03724845649802589\n",
      "Batch: 1900,train loss is: 0.028722683097994757\n",
      "test loss is 0.0426521737848596\n",
      "Batch: 2000,train loss is: 0.01123823052600804\n",
      "test loss is 0.021100034840881753\n",
      "Batch: 2100,train loss is: 0.027187177674068728\n",
      "test loss is 0.028952357935735425\n",
      "Batch: 2200,train loss is: 0.010398069970045852\n",
      "test loss is 0.01997649875687389\n",
      "Batch: 2300,train loss is: 0.03566407498297683\n",
      "test loss is 0.022351825044588575\n",
      "Batch: 2400,train loss is: 0.0181385934298336\n",
      "test loss is 0.017936459908013187\n",
      "Batch: 2500,train loss is: 0.02870948735415088\n",
      "test loss is 0.02066732432244897\n",
      "Batch: 2600,train loss is: 0.028318021849415283\n",
      "test loss is 0.020944863740942923\n",
      "Batch: 2700,train loss is: 0.011569196649834416\n",
      "test loss is 0.017231460582236704\n",
      "Batch: 2800,train loss is: 0.0072759021678454276\n",
      "test loss is 0.02170903992021791\n",
      "Batch: 2900,train loss is: 0.009518623738671739\n",
      "test loss is 0.015452675933115608\n",
      "Batch: 3000,train loss is: 0.010530568150212877\n",
      "test loss is 0.01578046611300332\n",
      "Batch: 3100,train loss is: 0.037306475087009544\n",
      "test loss is 0.018580134975925713\n",
      "Batch: 3200,train loss is: 0.015745860493630862\n",
      "test loss is 0.03641539004887731\n",
      "Batch: 3300,train loss is: 0.011433854503232331\n",
      "test loss is 0.01947982296291223\n",
      "Batch: 3400,train loss is: 0.020008084596353955\n",
      "test loss is 0.018441371097251896\n",
      "Batch: 3500,train loss is: 0.00800588495898427\n",
      "test loss is 0.013685853822993015\n",
      "Batch: 3600,train loss is: 0.01677154525532184\n",
      "test loss is 0.022226270869926706\n",
      "Batch: 3700,train loss is: 0.025012778715354175\n",
      "test loss is 0.015158552455162835\n",
      "Batch: 3800,train loss is: 0.009581293745803996\n",
      "test loss is 0.014085681401470127\n",
      "Batch: 3900,train loss is: 0.048386319943242544\n",
      "test loss is 0.024301870222294002\n",
      "Batch: 4000,train loss is: 0.01968164104721942\n",
      "test loss is 0.03998047151372476\n",
      "Batch: 4100,train loss is: 0.007181534528265986\n",
      "test loss is 0.010934893041952881\n",
      "Batch: 4200,train loss is: 0.030200443388352215\n",
      "test loss is 0.01642645972453496\n",
      "Batch: 4300,train loss is: 0.0051704976112724125\n",
      "test loss is 0.011691992157012271\n",
      "Batch: 4400,train loss is: 0.003231089520248487\n",
      "test loss is 0.012159271885424575\n",
      "Batch: 4500,train loss is: 0.009597576300555423\n",
      "test loss is 0.01701684107844699\n",
      "Batch: 4600,train loss is: 0.010512923152677033\n",
      "test loss is 0.017015806183611773\n",
      "Batch: 4700,train loss is: 0.02333935110034412\n",
      "test loss is 0.012965531748550212\n",
      "Batch: 4800,train loss is: 0.02455249576472618\n",
      "test loss is 0.016675500360450433\n",
      "Batch: 4900,train loss is: 0.00960205651346263\n",
      "test loss is 0.011293598765613562\n",
      "Batch: 5000,train loss is: 0.0107106325886173\n",
      "test loss is 0.01032388172003391\n",
      "Batch: 5100,train loss is: 0.016020642281756963\n",
      "test loss is 0.012973789654648219\n",
      "Batch: 5200,train loss is: 0.0025724543922815953\n",
      "test loss is 0.009208752644735698\n",
      "Batch: 5300,train loss is: 0.006006291134504307\n",
      "test loss is 0.03731710872113474\n",
      "Batch: 5400,train loss is: 0.03516687562807441\n",
      "test loss is 0.010908582428248602\n",
      "Batch: 5500,train loss is: 0.022912222041568984\n",
      "test loss is 0.01442475311476721\n",
      "Batch: 5600,train loss is: 0.017592452149025817\n",
      "test loss is 0.011932120975996567\n",
      "Batch: 5700,train loss is: 0.012211684334075665\n",
      "test loss is 0.010470511728850612\n",
      "Batch: 5800,train loss is: 0.004874621350649421\n",
      "test loss is 0.011030135922043341\n",
      "Batch: 5900,train loss is: 0.02579419914239902\n",
      "test loss is 0.013124701551029563\n",
      "Batch: 6000,train loss is: 0.02974532368299829\n",
      "test loss is 0.01721067926043569\n",
      "Batch: 6100,train loss is: 0.00687473239464143\n",
      "test loss is 0.01763401762096057\n",
      "Batch: 6200,train loss is: 0.008121741179659907\n",
      "test loss is 0.009511320986947638\n",
      "Batch: 6300,train loss is: 0.01061842332322717\n",
      "test loss is 0.01969538223657942\n",
      "Batch: 6400,train loss is: 0.006216695385867196\n",
      "test loss is 0.009928295895293503\n",
      "Batch: 6500,train loss is: 0.0026229882236470377\n",
      "test loss is 0.007880981100295968\n",
      "Batch: 6600,train loss is: 0.0438215954096552\n",
      "test loss is 0.009129767045894582\n",
      "Batch: 6700,train loss is: 0.004256541152894844\n",
      "test loss is 0.007833232721300841\n",
      "Batch: 6800,train loss is: 0.01381312291586823\n",
      "test loss is 0.010587390402019685\n",
      "Batch: 6900,train loss is: 0.07931306876160663\n",
      "test loss is 0.012478139333480971\n",
      "Batch: 7000,train loss is: 0.021943015711492028\n",
      "test loss is 0.014199482669595621\n",
      "Batch: 7100,train loss is: 0.006560863862570518\n",
      "test loss is 0.0126131994341783\n",
      "Batch: 7200,train loss is: 0.0023495676996379165\n",
      "test loss is 0.008213265828241096\n",
      "Batch: 7300,train loss is: 0.0140481122794057\n",
      "test loss is 0.008127914654455412\n",
      "Batch: 7400,train loss is: 0.0041066168729843545\n",
      "test loss is 0.010486396205026508\n",
      "Batch: 7500,train loss is: 0.006061880757594067\n",
      "test loss is 0.01844013726417975\n",
      "Batch: 7600,train loss is: 0.01678293913610311\n",
      "test loss is 0.01399761253690323\n",
      "Batch: 7700,train loss is: 0.013388439263119717\n",
      "test loss is 0.007667174979110211\n",
      "Batch: 7800,train loss is: 0.004838487245539708\n",
      "test loss is 0.006676124834175408\n",
      "Batch: 7900,train loss is: 0.0026511264621680423\n",
      "test loss is 0.00866984715587393\n",
      "Batch: 8000,train loss is: 0.018781730880942038\n",
      "test loss is 0.021411995102100494\n",
      "Batch: 8100,train loss is: 0.010205566812130973\n",
      "test loss is 0.012141829021264287\n",
      "Batch: 8200,train loss is: 0.011215194256330921\n",
      "test loss is 0.009364597645586876\n",
      "Batch: 8300,train loss is: 0.010261068843639668\n",
      "test loss is 0.013285728299314792\n",
      "Batch: 8400,train loss is: 0.01894293390710958\n",
      "test loss is 0.011266790235127055\n",
      "Batch: 8500,train loss is: 0.005546267307238616\n",
      "test loss is 0.008413552887544301\n",
      "Batch: 8600,train loss is: 0.016722409439258942\n",
      "test loss is 0.012146037170380526\n",
      "Batch: 8700,train loss is: 0.006472036701345698\n",
      "test loss is 0.010291819061358216\n",
      "Batch: 8800,train loss is: 0.008520151430293086\n",
      "test loss is 0.006308827907071409\n",
      "Batch: 8900,train loss is: 0.007214437653815652\n",
      "test loss is 0.007856627128223647\n",
      "Batch: 9000,train loss is: 0.003378160178329946\n",
      "test loss is 0.008840916728550585\n",
      "Batch: 9100,train loss is: 0.012496085186597543\n",
      "test loss is 0.01215140672920729\n",
      "Batch: 9200,train loss is: 0.0121666354681717\n",
      "test loss is 0.012958641542199457\n",
      "Batch: 9300,train loss is: 0.006025191248225265\n",
      "test loss is 0.00591919826742603\n",
      "Batch: 9400,train loss is: 0.009577557954224441\n",
      "test loss is 0.005457588040967659\n",
      "Batch: 9500,train loss is: 0.0025419831160093382\n",
      "test loss is 0.007743380257730199\n",
      "Batch: 9600,train loss is: 0.00295357007493717\n",
      "test loss is 0.005947242130993972\n",
      "Batch: 9700,train loss is: 0.007962122249960329\n",
      "test loss is 0.009848242282173236\n",
      "Batch: 9800,train loss is: 0.0021915014320155845\n",
      "test loss is 0.007046228248448126\n",
      "Batch: 9900,train loss is: 0.019194602192432113\n",
      "test loss is 0.029830556138678903\n",
      "Batch: 10000,train loss is: 0.06437968083070689\n",
      "test loss is 0.02645359220354238\n",
      "Batch: 10100,train loss is: 0.02712862560532101\n",
      "test loss is 0.010376576861535232\n",
      "Batch: 10200,train loss is: 0.06955304043661314\n",
      "test loss is 0.006091356622922336\n",
      "Batch: 10300,train loss is: 0.0034414521173003945\n",
      "test loss is 0.007147747314128421\n",
      "Batch: 10400,train loss is: 0.007354801496927659\n",
      "test loss is 0.00854492910192714\n",
      "Batch: 10500,train loss is: 0.010464976361969143\n",
      "test loss is 0.006305922905714127\n",
      "Batch: 10600,train loss is: 0.003939871707272928\n",
      "test loss is 0.005427166849262457\n",
      "Batch: 10700,train loss is: 0.012000256836732564\n",
      "test loss is 0.008426988277456344\n",
      "Batch: 10800,train loss is: 0.005963540003196632\n",
      "test loss is 0.012474092522189742\n",
      "Batch: 10900,train loss is: 0.00779682095976876\n",
      "test loss is 0.007444464292146613\n",
      "Batch: 11000,train loss is: 0.0040999461841140115\n",
      "test loss is 0.004480682767197284\n",
      "Batch: 11100,train loss is: 0.007669269073281323\n",
      "test loss is 0.009934114736783675\n",
      "Batch: 11200,train loss is: 0.018867776249174493\n",
      "test loss is 0.005597768313009809\n",
      "Batch: 11300,train loss is: 0.004005881856791427\n",
      "test loss is 0.006234394574761558\n",
      "Batch: 11400,train loss is: 0.030763770668711984\n",
      "test loss is 0.008938021380381187\n",
      "Batch: 11500,train loss is: 0.015509968091898817\n",
      "test loss is 0.00928726265337031\n",
      "Batch: 11600,train loss is: 0.017986300853563893\n",
      "test loss is 0.01764181141094979\n",
      "Batch: 11700,train loss is: 0.005352900042375733\n",
      "test loss is 0.008692510562182506\n",
      "Batch: 11800,train loss is: 0.004408627456857481\n",
      "test loss is 0.0060524084569039665\n",
      "Batch: 11900,train loss is: 0.005080642613559472\n",
      "test loss is 0.004558207895096055\n",
      "Batch: 12000,train loss is: 0.009213841531312867\n",
      "test loss is 0.007583901446102898\n",
      "Batch: 12100,train loss is: 0.03474182686053932\n",
      "test loss is 0.007136121169102888\n",
      "Batch: 12200,train loss is: 0.02106853706884118\n",
      "test loss is 0.08063216098001\n",
      "Batch: 12300,train loss is: 0.004810955586439643\n",
      "test loss is 0.00573175317633342\n",
      "Batch: 12400,train loss is: 0.004176995072456883\n",
      "test loss is 0.005037182842791484\n",
      "Batch: 12500,train loss is: 0.008521223222980441\n",
      "test loss is 0.00551995150042957\n",
      "Batch: 12600,train loss is: 0.00229641360186992\n",
      "test loss is 0.004075044761763683\n",
      "Batch: 12700,train loss is: 0.002416190713226557\n",
      "test loss is 0.0043766559703687915\n",
      "Batch: 12800,train loss is: 0.0014571535919832763\n",
      "test loss is 0.0055348460228139655\n",
      "Batch: 12900,train loss is: 0.007165540066072817\n",
      "test loss is 0.019025347097136346\n",
      "Batch: 13000,train loss is: 0.004018050551203139\n",
      "test loss is 0.007484605211952166\n",
      "Batch: 13100,train loss is: 0.005076391687537408\n",
      "test loss is 0.0070195093315739395\n",
      "Batch: 13200,train loss is: 0.006628755384904052\n",
      "test loss is 0.005430098435143225\n",
      "Batch: 13300,train loss is: 0.0017947568068980302\n",
      "test loss is 0.009724184082809569\n",
      "Batch: 13400,train loss is: 0.006395783554913911\n",
      "test loss is 0.0063147890965036734\n",
      "Batch: 13500,train loss is: 0.024787154225578988\n",
      "test loss is 0.02298061225961074\n",
      "Batch: 13600,train loss is: 0.00909881562249369\n",
      "test loss is 0.025924982457572472\n",
      "Batch: 13700,train loss is: 0.005147589017609428\n",
      "test loss is 0.006657570499503916\n",
      "Batch: 13800,train loss is: 0.0018890968471696548\n",
      "test loss is 0.005126879596383741\n",
      "Batch: 13900,train loss is: 0.001865624129137707\n",
      "test loss is 0.004786601272031298\n",
      "Batch: 14000,train loss is: 0.0027819563818888204\n",
      "test loss is 0.007813403054058608\n",
      "Batch: 14100,train loss is: 0.005179108693657346\n",
      "test loss is 0.0060974582360405075\n",
      "Batch: 14200,train loss is: 0.003518162261215779\n",
      "test loss is 0.003794679803449168\n",
      "Batch: 14300,train loss is: 0.007491971948838355\n",
      "test loss is 0.0071499386602318325\n",
      "Batch: 14400,train loss is: 0.0021958900729051834\n",
      "test loss is 0.010200632059781728\n",
      "Batch: 14500,train loss is: 0.002398614178939076\n",
      "test loss is 0.004955848906264374\n",
      "Batch: 14600,train loss is: 0.01042210464312442\n",
      "test loss is 0.011235052335282823\n",
      "Batch: 14700,train loss is: 0.0035756311801621475\n",
      "test loss is 0.010750604084602277\n",
      "Batch: 14800,train loss is: 0.006132402002335945\n",
      "test loss is 0.00933559643993585\n",
      "Batch: 14900,train loss is: 0.010025759156388711\n",
      "test loss is 0.009043370440198007\n",
      "Batch: 15000,train loss is: 0.005775576119749099\n",
      "test loss is 0.008337131498756806\n",
      "Batch: 15100,train loss is: 0.01399474207644621\n",
      "test loss is 0.006451076455069927\n",
      "Batch: 15200,train loss is: 0.007721131082019041\n",
      "test loss is 0.005558830566974565\n",
      "Batch: 15300,train loss is: 0.0038654050873057517\n",
      "test loss is 0.0042412406246686105\n",
      "Batch: 15400,train loss is: 0.004080769183457254\n",
      "test loss is 0.007660759984973152\n",
      "Batch: 15500,train loss is: 0.009565093008359879\n",
      "test loss is 0.00916153513930281\n",
      "Batch: 15600,train loss is: 0.00886881940092309\n",
      "test loss is 0.005918299864824738\n",
      "Batch: 15700,train loss is: 0.0023456734021996294\n",
      "test loss is 0.006983832962455004\n",
      "Batch: 15800,train loss is: 0.0078052580942252195\n",
      "test loss is 0.006168699752833068\n",
      "Batch: 15900,train loss is: 0.0017508598266848118\n",
      "test loss is 0.0035523391928469626\n",
      "Batch: 16000,train loss is: 0.005101208322241546\n",
      "test loss is 0.006065842699021355\n",
      "Batch: 16100,train loss is: 0.002084866644335043\n",
      "test loss is 0.003937588901345185\n",
      "Batch: 16200,train loss is: 0.0021619327931801497\n",
      "test loss is 0.005050609500293693\n",
      "Batch: 16300,train loss is: 0.004380056005252763\n",
      "test loss is 0.0034129408780748196\n",
      "Batch: 16400,train loss is: 0.0013523428014830298\n",
      "test loss is 0.003189868795382134\n",
      "Batch: 16500,train loss is: 0.007819582489403242\n",
      "test loss is 0.009850601862601414\n",
      "Batch: 16600,train loss is: 0.0027267894573057877\n",
      "test loss is 0.006402730266694931\n",
      "Batch: 16700,train loss is: 0.005511037083530248\n",
      "test loss is 0.00557322010643103\n",
      "Batch: 16800,train loss is: 0.005886239728641742\n",
      "test loss is 0.005645598433967514\n",
      "Batch: 16900,train loss is: 0.008415924663184506\n",
      "test loss is 0.008006110304957384\n",
      "Batch: 17000,train loss is: 0.0081903662058514\n",
      "test loss is 0.008714869554532238\n",
      "Batch: 17100,train loss is: 0.011451660289625808\n",
      "test loss is 0.008113585961222391\n",
      "Batch: 17200,train loss is: 0.0019092608300072282\n",
      "test loss is 0.003981304611133356\n",
      "Batch: 17300,train loss is: 0.001701668343436858\n",
      "test loss is 0.004906322757935753\n",
      "Batch: 17400,train loss is: 0.004336150634999518\n",
      "test loss is 0.008281479180702272\n",
      "Batch: 17500,train loss is: 0.011407910298081879\n",
      "test loss is 0.006788165131364434\n",
      "Batch: 17600,train loss is: 0.007161619738427016\n",
      "test loss is 0.005678764003584137\n",
      "Batch: 17700,train loss is: 0.0037952228553592136\n",
      "test loss is 0.0037733001651510233\n",
      "Batch: 17800,train loss is: 0.009258762288945864\n",
      "test loss is 0.0049979874254516814\n",
      "Batch: 17900,train loss is: 0.012377999124056292\n",
      "test loss is 0.00763624156047355\n",
      "Batch: 18000,train loss is: 0.005212488309187442\n",
      "test loss is 0.007520488662702726\n",
      "Batch: 18100,train loss is: 0.005773692607133207\n",
      "test loss is 0.003680965025583279\n",
      "Batch: 18200,train loss is: 0.0033015864371886416\n",
      "test loss is 0.004866611106626775\n",
      "Batch: 18300,train loss is: 0.02432409702918419\n",
      "test loss is 0.004957734776147776\n",
      "Batch: 18400,train loss is: 0.0016348129855116758\n",
      "test loss is 0.0033428297543463266\n",
      "Batch: 18500,train loss is: 0.007983093562911504\n",
      "test loss is 0.018580340590212446\n",
      "Batch: 18600,train loss is: 0.001893672800653062\n",
      "test loss is 0.004882935213038629\n",
      "Batch: 18700,train loss is: 0.0024515055474955217\n",
      "test loss is 0.005301222312810873\n",
      "Batch: 18800,train loss is: 0.003188286366980184\n",
      "test loss is 0.002875140166027647\n",
      "Batch: 18900,train loss is: 0.0024954632722156223\n",
      "test loss is 0.004079596604850239\n",
      "Batch: 19000,train loss is: 0.0030260387434025812\n",
      "test loss is 0.0034706893000244934\n",
      "Batch: 19100,train loss is: 0.005093580956855473\n",
      "test loss is 0.00600237847766129\n",
      "Batch: 19200,train loss is: 0.005056780263644506\n",
      "test loss is 0.0041775227937372655\n",
      "Batch: 19300,train loss is: 0.005790069283540579\n",
      "test loss is 0.015697213657010815\n",
      "Batch: 19400,train loss is: 0.0018711609553054224\n",
      "test loss is 0.005105625383094381\n",
      "Batch: 19500,train loss is: 0.0032742556933657877\n",
      "test loss is 0.005231898193169086\n",
      "Batch: 19600,train loss is: 0.0025646340014135872\n",
      "test loss is 0.003273756149460892\n",
      "Batch: 19700,train loss is: 0.01723638055767254\n",
      "test loss is 0.00994996377117719\n",
      "Batch: 19800,train loss is: 0.002805720897142955\n",
      "test loss is 0.002750019688124815\n",
      "Batch: 19900,train loss is: 0.0062761254977965\n",
      "test loss is 0.005999005011011639\n",
      "Batch: 20000,train loss is: 0.0024455595315547463\n",
      "test loss is 0.002931233530669721\n",
      "Batch: 20100,train loss is: 0.0036116193046067087\n",
      "test loss is 0.004650519489402737\n",
      "Batch: 20200,train loss is: 0.002342046567476216\n",
      "test loss is 0.003378111860453763\n",
      "Batch: 20300,train loss is: 0.0030467237324293002\n",
      "test loss is 0.008684193436034559\n",
      "Batch: 20400,train loss is: 0.00470540312076257\n",
      "test loss is 0.010244869368994118\n",
      "Batch: 20500,train loss is: 0.001670584392538033\n",
      "test loss is 0.0038938890295484507\n",
      "Batch: 20600,train loss is: 0.00833773832531793\n",
      "test loss is 0.004473500508274515\n",
      "Batch: 20700,train loss is: 0.0038500372662438583\n",
      "test loss is 0.008284045487178931\n",
      "Batch: 20800,train loss is: 0.006766054909047741\n",
      "test loss is 0.005771197241239315\n",
      "Batch: 20900,train loss is: 0.005253937090325881\n",
      "test loss is 0.004853969967621001\n",
      "Batch: 21000,train loss is: 0.0030389188289521372\n",
      "test loss is 0.005779612515804278\n",
      "Batch: 21100,train loss is: 0.004686897823896446\n",
      "test loss is 0.004839295327177401\n",
      "Batch: 21200,train loss is: 0.0007902510143117491\n",
      "test loss is 0.004457359380944298\n",
      "Batch: 21300,train loss is: 0.0028381457195504065\n",
      "test loss is 0.0037421593206772275\n",
      "Batch: 21400,train loss is: 0.009531707143518673\n",
      "test loss is 0.007805315199110761\n",
      "Batch: 21500,train loss is: 0.002027585501980609\n",
      "test loss is 0.005190404961578848\n",
      "Batch: 21600,train loss is: 0.002190438516167728\n",
      "test loss is 0.004404973979590962\n",
      "Batch: 21700,train loss is: 0.001507938381373987\n",
      "test loss is 0.005801960667206561\n",
      "Batch: 21800,train loss is: 0.008015551001007221\n",
      "test loss is 0.004391890264484381\n",
      "Batch: 21900,train loss is: 0.0035277271210343185\n",
      "test loss is 0.0040036114607202205\n",
      "Batch: 22000,train loss is: 0.001322517880354662\n",
      "test loss is 0.005323577475936926\n",
      "Batch: 22100,train loss is: 0.003855127994074757\n",
      "test loss is 0.006495388073153356\n",
      "Batch: 22200,train loss is: 0.011182151012896796\n",
      "test loss is 0.006820127045997057\n",
      "Batch: 22300,train loss is: 0.010881854256872037\n",
      "test loss is 0.016579781461985504\n",
      "Batch: 22400,train loss is: 0.0022458920519024005\n",
      "test loss is 0.0057647676871363995\n",
      "Batch: 22500,train loss is: 0.007748173484291507\n",
      "test loss is 0.005625700374668439\n",
      "Batch: 22600,train loss is: 0.008426858639537146\n",
      "test loss is 0.005557227815076911\n",
      "Batch: 22700,train loss is: 0.004828369831851543\n",
      "test loss is 0.004690242958903652\n",
      "Batch: 22800,train loss is: 0.005237652848137562\n",
      "test loss is 0.003983690237912626\n",
      "Batch: 22900,train loss is: 0.003881222748495437\n",
      "test loss is 0.004626020465513747\n",
      "Batch: 23000,train loss is: 0.012469884207587389\n",
      "test loss is 0.0035624519350115403\n",
      "Batch: 23100,train loss is: 0.0017865774495733847\n",
      "test loss is 0.002716441207543773\n",
      "Batch: 23200,train loss is: 0.004232307608575828\n",
      "test loss is 0.0043631770353803724\n",
      "Batch: 23300,train loss is: 0.0014463273086462497\n",
      "test loss is 0.0026563730585680264\n",
      "Batch: 23400,train loss is: 0.0011701219124410132\n",
      "test loss is 0.004413693288284832\n",
      "Batch: 23500,train loss is: 0.012385987746677884\n",
      "test loss is 0.0036633145355381156\n",
      "Batch: 23600,train loss is: 0.00390183575473958\n",
      "test loss is 0.0034749038164822044\n",
      "Batch: 23700,train loss is: 0.0014788533126715686\n",
      "test loss is 0.0032234291126569063\n",
      "Batch: 23800,train loss is: 0.006279547107955477\n",
      "test loss is 0.003049724961075474\n",
      "Batch: 23900,train loss is: 0.008405975707003209\n",
      "test loss is 0.0038429692695308403\n",
      "Batch: 24000,train loss is: 0.008616670203883726\n",
      "test loss is 0.005845030691056322\n",
      "Batch: 24100,train loss is: 0.0024088787575328127\n",
      "test loss is 0.003525113486490907\n",
      "Batch: 24200,train loss is: 0.00415499068345858\n",
      "test loss is 0.003676549811755794\n",
      "Batch: 24300,train loss is: 0.006885080948750299\n",
      "test loss is 0.0032349474657436994\n",
      "Batch: 24400,train loss is: 0.01592496012120731\n",
      "test loss is 0.003398423137263116\n",
      "Batch: 24500,train loss is: 0.012592671406880264\n",
      "test loss is 0.0059951236789324\n",
      "Batch: 24600,train loss is: 0.001670203481211859\n",
      "test loss is 0.003117419633014987\n",
      "Batch: 24700,train loss is: 0.002242264357595361\n",
      "test loss is 0.003225923062777029\n",
      "Batch: 24800,train loss is: 0.0018581849567637773\n",
      "test loss is 0.0023243180881258436\n",
      "Batch: 24900,train loss is: 0.0020936107081622273\n",
      "test loss is 0.002766437195049268\n",
      "Batch: 25000,train loss is: 0.002381534276622545\n",
      "test loss is 0.009826152868338803\n",
      "Batch: 25100,train loss is: 0.0008094283058477536\n",
      "test loss is 0.007625983009887328\n",
      "Batch: 25200,train loss is: 0.004084812940520205\n",
      "test loss is 0.004049280186415046\n",
      "Batch: 25300,train loss is: 0.006128758057242273\n",
      "test loss is 0.003544015523260126\n",
      "Batch: 25400,train loss is: 0.001799817634439898\n",
      "test loss is 0.0033062018594112626\n",
      "Batch: 25500,train loss is: 0.0012864322584915876\n",
      "test loss is 0.0024615875657591725\n",
      "Batch: 25600,train loss is: 0.005489001530215317\n",
      "test loss is 0.003445714197121911\n",
      "Batch: 25700,train loss is: 0.013286965477096988\n",
      "test loss is 0.0023161274618696873\n",
      "Batch: 25800,train loss is: 0.001467319384023903\n",
      "test loss is 0.002805530800953648\n",
      "Batch: 25900,train loss is: 0.0011744721406902203\n",
      "test loss is 0.004514240636032317\n",
      "Batch: 26000,train loss is: 0.002207306347874588\n",
      "test loss is 0.007568643435724768\n",
      "Batch: 26100,train loss is: 0.0022321861379745035\n",
      "test loss is 0.00267014006949256\n",
      "Batch: 26200,train loss is: 0.005165444992569355\n",
      "test loss is 0.0031332350006882726\n",
      "Batch: 26300,train loss is: 0.007432342754382035\n",
      "test loss is 0.0030709291707851794\n",
      "Batch: 26400,train loss is: 0.001830870560916921\n",
      "test loss is 0.0033782535493943064\n",
      "Batch: 26500,train loss is: 0.00534669775120063\n",
      "test loss is 0.0036727585480692965\n",
      "Batch: 26600,train loss is: 0.0016892577645576097\n",
      "test loss is 0.0023518187341759157\n",
      "Batch: 26700,train loss is: 0.0012041508981892237\n",
      "test loss is 0.00353207411845437\n",
      "Batch: 26800,train loss is: 0.00468129947415308\n",
      "test loss is 0.0030662612100756325\n",
      "Batch: 26900,train loss is: 0.0023067814140349434\n",
      "test loss is 0.0035332480999667884\n",
      "Batch: 27000,train loss is: 0.0024359771923947353\n",
      "test loss is 0.0024123482037296216\n",
      "Batch: 27100,train loss is: 0.002744034774547089\n",
      "test loss is 0.007118281480563351\n",
      "Batch: 27200,train loss is: 0.0024205948307277955\n",
      "test loss is 0.0036752206838272677\n",
      "Batch: 27300,train loss is: 0.0020266618744801098\n",
      "test loss is 0.003832824293886475\n",
      "Batch: 27400,train loss is: 0.010315658896038605\n",
      "test loss is 0.0040242205362637316\n",
      "Batch: 27500,train loss is: 0.004858585268668225\n",
      "test loss is 0.00501051305656397\n",
      "Batch: 27600,train loss is: 0.008442520772054263\n",
      "test loss is 0.0047982196199328414\n",
      "Batch: 27700,train loss is: 0.0028298151883889714\n",
      "test loss is 0.00363707578872224\n",
      "Batch: 27800,train loss is: 0.005187081631165123\n",
      "test loss is 0.00745583279183954\n",
      "Batch: 27900,train loss is: 0.005692153676738513\n",
      "test loss is 0.0042403764030646066\n",
      "Batch: 28000,train loss is: 0.0015508947874570311\n",
      "test loss is 0.003977882414161182\n",
      "Batch: 28100,train loss is: 0.01105493048395581\n",
      "test loss is 0.0061251423625941865\n",
      "Batch: 28200,train loss is: 0.004320233165594955\n",
      "test loss is 0.006606018089032175\n",
      "Batch: 28300,train loss is: 0.035530662515172944\n",
      "test loss is 0.004353679481746845\n",
      "Batch: 28400,train loss is: 0.002219672796957717\n",
      "test loss is 0.0024889034823809522\n",
      "Batch: 28500,train loss is: 0.004871367665775278\n",
      "test loss is 0.003156399102955138\n",
      "Batch: 28600,train loss is: 0.002190781072684568\n",
      "test loss is 0.0025661480922787713\n",
      "Batch: 28700,train loss is: 0.0029648139816578463\n",
      "test loss is 0.006042272395471837\n",
      "Batch: 28800,train loss is: 0.009624558022579222\n",
      "test loss is 0.002622019089704209\n",
      "Batch: 28900,train loss is: 0.002065315893255073\n",
      "test loss is 0.002783457933262993\n",
      "Batch: 29000,train loss is: 0.003443623864543795\n",
      "test loss is 0.006370721438470494\n",
      "Batch: 29100,train loss is: 0.006933780145102216\n",
      "test loss is 0.02366705099349127\n",
      "Batch: 29200,train loss is: 0.0038576285712580796\n",
      "test loss is 0.004855794474560578\n",
      "Batch: 29300,train loss is: 0.003943076360110784\n",
      "test loss is 0.0026047453353658183\n",
      "Batch: 29400,train loss is: 0.003685010014736727\n",
      "test loss is 0.0032331590274916975\n",
      "Batch: 29500,train loss is: 0.0009943743098414974\n",
      "test loss is 0.0024549038842420663\n",
      "Batch: 29600,train loss is: 0.0063401661826506224\n",
      "test loss is 0.0030740352474756477\n",
      "Batch: 29700,train loss is: 0.0010118550403820743\n",
      "test loss is 0.00489249217480611\n",
      "Batch: 29800,train loss is: 0.0057497178866715015\n",
      "test loss is 0.002384971804703254\n",
      "Batch: 29900,train loss is: 0.004818432587221277\n",
      "test loss is 0.0019112898096027378\n",
      "Batch: 30000,train loss is: 0.0014742996476092014\n",
      "test loss is 0.001951763898411759\n",
      "Batch: 30100,train loss is: 0.002728920266180901\n",
      "test loss is 0.02617407481027028\n",
      "Batch: 30200,train loss is: 0.001607936628204933\n",
      "test loss is 0.0029040341331092105\n",
      "Batch: 30300,train loss is: 0.00192841819748833\n",
      "test loss is 0.005432850148932864\n",
      "Batch: 30400,train loss is: 0.0021596312781062325\n",
      "test loss is 0.0030724624470984782\n",
      "Batch: 30500,train loss is: 0.002822966730541526\n",
      "test loss is 0.0022072097239602484\n",
      "Batch: 30600,train loss is: 0.000976821244015393\n",
      "test loss is 0.0034853015407771393\n",
      "Batch: 30700,train loss is: 0.001226700326776863\n",
      "test loss is 0.005121977601353298\n",
      "Batch: 30800,train loss is: 0.0028040612967612797\n",
      "test loss is 0.0028289173979195504\n",
      "Batch: 30900,train loss is: 0.003895227932822935\n",
      "test loss is 0.004799635427320196\n",
      "Batch: 31000,train loss is: 0.003074855762653389\n",
      "test loss is 0.0044219905030815545\n",
      "Batch: 31100,train loss is: 0.0014303304054588168\n",
      "test loss is 0.0023484896394611538\n",
      "Batch: 31200,train loss is: 0.00184028609527523\n",
      "test loss is 0.0030120699874710173\n",
      "Batch: 31300,train loss is: 0.0016283062314014932\n",
      "test loss is 0.0029687056468416123\n",
      "Batch: 31400,train loss is: 0.023511459300528205\n",
      "test loss is 0.012273114176064127\n",
      "Batch: 31500,train loss is: 0.002020541970292651\n",
      "test loss is 0.004083842009574021\n",
      "Batch: 31600,train loss is: 0.002500964389644199\n",
      "test loss is 0.0029676065216332204\n",
      "Batch: 31700,train loss is: 0.0008317221008876252\n",
      "test loss is 0.002220775305702315\n",
      "Batch: 31800,train loss is: 0.012011342408947422\n",
      "test loss is 0.00628490430389356\n",
      "Batch: 31900,train loss is: 0.0010977306639233587\n",
      "test loss is 0.0021544001945007054\n",
      "Batch: 32000,train loss is: 0.02277831043976879\n",
      "test loss is 0.0020145082078766225\n",
      "Batch: 32100,train loss is: 0.0008482556469974929\n",
      "test loss is 0.0022979521208041395\n",
      "Batch: 32200,train loss is: 0.002273949005877604\n",
      "test loss is 0.0023674425600899974\n",
      "Batch: 32300,train loss is: 0.0009777096074983468\n",
      "test loss is 0.0030839668833720457\n",
      "Batch: 32400,train loss is: 0.0010179726754533443\n",
      "test loss is 0.0023052457518058468\n",
      "Batch: 32500,train loss is: 0.0026682040982801514\n",
      "test loss is 0.0025689173059081413\n",
      "Batch: 32600,train loss is: 0.002135500687023515\n",
      "test loss is 0.00217197476908454\n",
      "Batch: 32700,train loss is: 0.002295918707365882\n",
      "test loss is 0.0036704051504873794\n",
      "Batch: 32800,train loss is: 0.002547194399663591\n",
      "test loss is 0.0021360494180828085\n",
      "Batch: 32900,train loss is: 0.0031859894483303145\n",
      "test loss is 0.001962373066674186\n",
      "Batch: 33000,train loss is: 0.0014925774677748767\n",
      "test loss is 0.0018628137161462556\n",
      "Batch: 33100,train loss is: 0.0029357378205679863\n",
      "test loss is 0.0026852213573466616\n",
      "Batch: 33200,train loss is: 0.0012408164970353324\n",
      "test loss is 0.0023266800735124\n",
      "Batch: 33300,train loss is: 0.0005062936072070796\n",
      "test loss is 0.002069509901042587\n",
      "Batch: 33400,train loss is: 0.0021553212904941903\n",
      "test loss is 0.00774351427410437\n",
      "Batch: 33500,train loss is: 0.008695100792864834\n",
      "test loss is 0.007887041881124856\n",
      "Batch: 33600,train loss is: 0.0005301323418806409\n",
      "test loss is 0.005964977672907377\n",
      "Batch: 33700,train loss is: 0.0013460645452425205\n",
      "test loss is 0.00284746726977939\n",
      "Batch: 33800,train loss is: 0.009989519686713645\n",
      "test loss is 0.0026699699021341507\n",
      "Batch: 33900,train loss is: 0.0060889028771737175\n",
      "test loss is 0.002628055836698152\n",
      "-----------------------Epoch: 1----------------------------------\n",
      "Batch: 0,train loss is: 0.002346332081956094\n",
      "test loss is 0.003158962649616535\n",
      "Batch: 100,train loss is: 0.008583128666286325\n",
      "test loss is 0.0028895879428770237\n",
      "Batch: 200,train loss is: 0.0022492512789186745\n",
      "test loss is 0.0027876567597043796\n",
      "Batch: 300,train loss is: 0.0021137689360322216\n",
      "test loss is 0.005767839431314327\n",
      "Batch: 400,train loss is: 0.0015154343838681178\n",
      "test loss is 0.0030224810017652694\n",
      "Batch: 500,train loss is: 0.0052346492424524185\n",
      "test loss is 0.005628548700259969\n",
      "Batch: 600,train loss is: 0.0028442279676698505\n",
      "test loss is 0.007836388959739198\n",
      "Batch: 700,train loss is: 0.0016794023979606546\n",
      "test loss is 0.0024968121024761575\n",
      "Batch: 800,train loss is: 0.0008450908574429268\n",
      "test loss is 0.002399531610715308\n",
      "Batch: 900,train loss is: 0.0026785342742820773\n",
      "test loss is 0.0019083711834113563\n",
      "Batch: 1000,train loss is: 0.0025988093831060704\n",
      "test loss is 0.0019517039935659163\n",
      "Batch: 1100,train loss is: 0.006904129670094991\n",
      "test loss is 0.00348426092113642\n",
      "Batch: 1200,train loss is: 0.0031324338291367063\n",
      "test loss is 0.006865565236468489\n",
      "Batch: 1300,train loss is: 0.009100842969256227\n",
      "test loss is 0.005766405945197868\n",
      "Batch: 1400,train loss is: 0.0008384333724692344\n",
      "test loss is 0.0019333653891936225\n",
      "Batch: 1500,train loss is: 0.0016594454270980879\n",
      "test loss is 0.006072527222563174\n",
      "Batch: 1600,train loss is: 0.00430602958535961\n",
      "test loss is 0.002427863981335787\n",
      "Batch: 1700,train loss is: 0.0016351049659343011\n",
      "test loss is 0.002015980218261196\n",
      "Batch: 1800,train loss is: 0.0017006774619652094\n",
      "test loss is 0.0019036508369414847\n",
      "Batch: 1900,train loss is: 0.0024065389952724887\n",
      "test loss is 0.005060613477244908\n",
      "Batch: 2000,train loss is: 0.004202242183400843\n",
      "test loss is 0.0027161949492032177\n",
      "Batch: 2100,train loss is: 0.00269917605997172\n",
      "test loss is 0.002630406257552001\n",
      "Batch: 2200,train loss is: 0.003456648834394931\n",
      "test loss is 0.0024463077906000563\n",
      "Batch: 2300,train loss is: 0.0013903598922389013\n",
      "test loss is 0.0021889796267776595\n",
      "Batch: 2400,train loss is: 0.0008627235285397328\n",
      "test loss is 0.002385928995208903\n",
      "Batch: 2500,train loss is: 0.0009414541328335278\n",
      "test loss is 0.0018607470426413564\n",
      "Batch: 2600,train loss is: 0.001077108536221693\n",
      "test loss is 0.001967347283371173\n",
      "Batch: 2700,train loss is: 0.0007332786856423627\n",
      "test loss is 0.0018020318000733057\n",
      "Batch: 2800,train loss is: 0.0012047143005465798\n",
      "test loss is 0.0026094831330455027\n",
      "Batch: 2900,train loss is: 0.0011118444830835032\n",
      "test loss is 0.0018718651502433513\n",
      "Batch: 3000,train loss is: 0.0022753720242008964\n",
      "test loss is 0.0026087343293580614\n",
      "Batch: 3100,train loss is: 0.0014698154807097801\n",
      "test loss is 0.0017276388825996404\n",
      "Batch: 3200,train loss is: 0.01559969484770977\n",
      "test loss is 0.0029333155227490755\n",
      "Batch: 3300,train loss is: 0.0038100760090436364\n",
      "test loss is 0.003936550494660903\n",
      "Batch: 3400,train loss is: 0.0019820158190413115\n",
      "test loss is 0.004000161585259085\n",
      "Batch: 3500,train loss is: 0.0021856072915774796\n",
      "test loss is 0.003385909954109909\n",
      "Batch: 3600,train loss is: 0.0009296468089614627\n",
      "test loss is 0.003666990130630906\n",
      "Batch: 3700,train loss is: 0.000836938804092668\n",
      "test loss is 0.004281545901638006\n",
      "Batch: 3800,train loss is: 0.002712887354940989\n",
      "test loss is 0.0023861924999469043\n",
      "Batch: 3900,train loss is: 0.0011967954743876753\n",
      "test loss is 0.0022851935489283533\n",
      "Batch: 4000,train loss is: 0.0006403726741762704\n",
      "test loss is 0.0017732201228936463\n",
      "Batch: 4100,train loss is: 0.0019182741535982758\n",
      "test loss is 0.0015224716563725842\n",
      "Batch: 4200,train loss is: 0.0029530078263096345\n",
      "test loss is 0.0022400233324880218\n",
      "Batch: 4300,train loss is: 0.0011154468017637684\n",
      "test loss is 0.0027021677465599715\n",
      "Batch: 4400,train loss is: 0.0008339575056790042\n",
      "test loss is 0.0034880521709277582\n",
      "Batch: 4500,train loss is: 0.002966330063089606\n",
      "test loss is 0.003044266399690047\n",
      "Batch: 4600,train loss is: 0.0024081184769055683\n",
      "test loss is 0.004709438482059001\n",
      "Batch: 4700,train loss is: 0.00217054642381927\n",
      "test loss is 0.0018687071340527065\n",
      "Batch: 4800,train loss is: 0.00953096473695988\n",
      "test loss is 0.006127805022437903\n",
      "Batch: 4900,train loss is: 0.009019388370774538\n",
      "test loss is 0.006444105644929174\n",
      "Batch: 5000,train loss is: 0.0010288124457941636\n",
      "test loss is 0.001843691392348326\n",
      "Batch: 5100,train loss is: 0.0026258867450269156\n",
      "test loss is 0.0033018599728165737\n",
      "Batch: 5200,train loss is: 0.0008668855408113861\n",
      "test loss is 0.00274994800461449\n",
      "Batch: 5300,train loss is: 0.020588712755294274\n",
      "test loss is 0.008246236158542997\n",
      "Batch: 5400,train loss is: 0.004642312175874782\n",
      "test loss is 0.0056191584453182414\n",
      "Batch: 5500,train loss is: 0.0021221355108883328\n",
      "test loss is 0.0029099212008801985\n",
      "Batch: 5600,train loss is: 0.004340782910117509\n",
      "test loss is 0.002713601452982721\n",
      "Batch: 5700,train loss is: 0.000891541753982042\n",
      "test loss is 0.0019085867127082467\n",
      "Batch: 5800,train loss is: 0.0018339849219579274\n",
      "test loss is 0.0023796977718813896\n",
      "Batch: 5900,train loss is: 0.005792669608005992\n",
      "test loss is 0.0022954025711784364\n",
      "Batch: 6000,train loss is: 0.00907101271308387\n",
      "test loss is 0.00445565657012787\n",
      "Batch: 6100,train loss is: 0.001281218183127519\n",
      "test loss is 0.0017966135776229127\n",
      "Batch: 6200,train loss is: 0.0010435084139704459\n",
      "test loss is 0.0017136451287977994\n",
      "Batch: 6300,train loss is: 0.002503571244491062\n",
      "test loss is 0.0024412008192269664\n",
      "Batch: 6400,train loss is: 0.0024593685223369423\n",
      "test loss is 0.0023539648484882515\n",
      "Batch: 6500,train loss is: 0.0026757935990810314\n",
      "test loss is 0.0018229495643274996\n",
      "Batch: 6600,train loss is: 0.00841107754205662\n",
      "test loss is 0.0031437801441951776\n",
      "Batch: 6700,train loss is: 0.0010292270632206878\n",
      "test loss is 0.0021843815278327814\n",
      "Batch: 6800,train loss is: 0.002134931917466234\n",
      "test loss is 0.0020313351240400387\n",
      "Batch: 6900,train loss is: 0.0058923115746288435\n",
      "test loss is 0.002037349394341418\n",
      "Batch: 7000,train loss is: 0.0022458209310337062\n",
      "test loss is 0.00237555230492604\n",
      "Batch: 7100,train loss is: 0.013183109856756746\n",
      "test loss is 0.011699322436637306\n",
      "Batch: 7200,train loss is: 0.001068177401022791\n",
      "test loss is 0.004030290878253803\n",
      "Batch: 7300,train loss is: 0.002737698259182421\n",
      "test loss is 0.001992504225279432\n",
      "Batch: 7400,train loss is: 0.002304253419294541\n",
      "test loss is 0.003267002521521335\n",
      "Batch: 7500,train loss is: 0.001381590483104797\n",
      "test loss is 0.003400209696286343\n",
      "Batch: 7600,train loss is: 0.004270420605272862\n",
      "test loss is 0.004161298868323249\n",
      "Batch: 7700,train loss is: 0.0015533962733506582\n",
      "test loss is 0.001963747557325121\n",
      "Batch: 7800,train loss is: 0.0008778989388577281\n",
      "test loss is 0.002442449827589748\n",
      "Batch: 7900,train loss is: 0.0011524721808801907\n",
      "test loss is 0.0014885852127197656\n",
      "Batch: 8000,train loss is: 0.0010696933228661358\n",
      "test loss is 0.0026885188212104833\n",
      "Batch: 8100,train loss is: 0.002265485841547006\n",
      "test loss is 0.0017689332819720463\n",
      "Batch: 8200,train loss is: 0.004681408366817893\n",
      "test loss is 0.002793193975829091\n",
      "Batch: 8300,train loss is: 0.0012482806168132687\n",
      "test loss is 0.0028835140190006927\n",
      "Batch: 8400,train loss is: 0.003676006566128126\n",
      "test loss is 0.0026260752068813973\n",
      "Batch: 8500,train loss is: 0.0019207447553063963\n",
      "test loss is 0.0017753255317139716\n",
      "Batch: 8600,train loss is: 0.0024465463009226652\n",
      "test loss is 0.004907254336198858\n",
      "Batch: 8700,train loss is: 0.003778240450098211\n",
      "test loss is 0.004536492488312168\n",
      "Batch: 8800,train loss is: 0.0016248251978570759\n",
      "test loss is 0.001972950375501718\n",
      "Batch: 8900,train loss is: 0.0016730437984921755\n",
      "test loss is 0.0017332347375223467\n",
      "Batch: 9000,train loss is: 0.0009383013322066271\n",
      "test loss is 0.0024409781126911102\n",
      "Batch: 9100,train loss is: 0.005335131286548162\n",
      "test loss is 0.004102654364985388\n",
      "Batch: 9200,train loss is: 0.0027554808255519044\n",
      "test loss is 0.005559177279775901\n",
      "Batch: 9300,train loss is: 0.002404517970266418\n",
      "test loss is 0.0016194438995686563\n",
      "Batch: 9400,train loss is: 0.0010992150262920627\n",
      "test loss is 0.0019026442220881942\n",
      "Batch: 9500,train loss is: 0.0018541231247331776\n",
      "test loss is 0.0037599629108084572\n",
      "Batch: 9600,train loss is: 0.0018076319136047167\n",
      "test loss is 0.002632111491992478\n",
      "Batch: 9700,train loss is: 0.0013384341469321194\n",
      "test loss is 0.0018407967699001378\n",
      "Batch: 9800,train loss is: 0.0008423951418458878\n",
      "test loss is 0.0018287494846775337\n",
      "Batch: 9900,train loss is: 0.0008184741230274045\n",
      "test loss is 0.002185718000490269\n",
      "Batch: 10000,train loss is: 0.0020281842709788038\n",
      "test loss is 0.0020164551643062872\n",
      "Batch: 10100,train loss is: 0.004656835255036464\n",
      "test loss is 0.0031653449615019398\n",
      "Batch: 10200,train loss is: 0.011344364345841516\n",
      "test loss is 0.0022229529863180843\n",
      "Batch: 10300,train loss is: 0.03625467303312009\n",
      "test loss is 0.030681448230468495\n",
      "Batch: 10400,train loss is: 0.0010602666774458481\n",
      "test loss is 0.0026580268824769305\n",
      "Batch: 10500,train loss is: 0.004210406586250448\n",
      "test loss is 0.0026844123426198745\n",
      "Batch: 10600,train loss is: 0.0028575842401797834\n",
      "test loss is 0.0030867529410218376\n",
      "Batch: 10700,train loss is: 0.0019123081908076274\n",
      "test loss is 0.0024369570696283595\n",
      "Batch: 10800,train loss is: 0.002446252764522572\n",
      "test loss is 0.007254544567586198\n",
      "Batch: 10900,train loss is: 0.0022740975173977758\n",
      "test loss is 0.0039941063939388744\n",
      "Batch: 11000,train loss is: 0.0011633432567616565\n",
      "test loss is 0.0014842024974992644\n",
      "Batch: 11100,train loss is: 0.0020814453335303114\n",
      "test loss is 0.0028941019570936587\n",
      "Batch: 11200,train loss is: 0.001740084825647891\n",
      "test loss is 0.0016105604675467435\n",
      "Batch: 11300,train loss is: 0.0014118018387887991\n",
      "test loss is 0.002978576970933137\n",
      "Batch: 11400,train loss is: 0.0054481380329697475\n",
      "test loss is 0.0025334396826937515\n",
      "Batch: 11500,train loss is: 0.0005935101467891967\n",
      "test loss is 0.003040061091832345\n",
      "Batch: 11600,train loss is: 0.009744401416368086\n",
      "test loss is 0.002763010382220607\n",
      "Batch: 11700,train loss is: 0.0009198767197588875\n",
      "test loss is 0.0019257120297808638\n",
      "Batch: 11800,train loss is: 0.001072647770079079\n",
      "test loss is 0.0012843755135529395\n",
      "Batch: 11900,train loss is: 0.0013064451423323742\n",
      "test loss is 0.002198293202393173\n",
      "Batch: 12000,train loss is: 0.0015871409567403236\n",
      "test loss is 0.00198369373148128\n",
      "Batch: 12100,train loss is: 0.036287928989399254\n",
      "test loss is 0.0027686813675528777\n",
      "Batch: 12200,train loss is: 0.0017566746318945369\n",
      "test loss is 0.011074726794049748\n",
      "Batch: 12300,train loss is: 0.003222629934218724\n",
      "test loss is 0.0017929018588476904\n",
      "Batch: 12400,train loss is: 0.009090154595820549\n",
      "test loss is 0.004143156349320839\n",
      "Batch: 12500,train loss is: 0.0034924056043906717\n",
      "test loss is 0.002024895361031255\n",
      "Batch: 12600,train loss is: 0.0015983318578656884\n",
      "test loss is 0.0014431405519448248\n",
      "Batch: 12700,train loss is: 0.0007003003176250879\n",
      "test loss is 0.0014157946794582583\n",
      "Batch: 12800,train loss is: 0.0007587862927487983\n",
      "test loss is 0.002520798066137451\n",
      "Batch: 12900,train loss is: 0.0022993752829142673\n",
      "test loss is 0.0027129510337460376\n",
      "Batch: 13000,train loss is: 0.0017843278849511623\n",
      "test loss is 0.0019707396690567406\n",
      "Batch: 13100,train loss is: 0.0032315111297280355\n",
      "test loss is 0.0014669026038385338\n",
      "Batch: 13200,train loss is: 0.0014220109580050854\n",
      "test loss is 0.00183839681033659\n",
      "Batch: 13300,train loss is: 0.002896792363987459\n",
      "test loss is 0.007195058563209395\n",
      "Batch: 13400,train loss is: 0.002196808046590269\n",
      "test loss is 0.0020126374671750173\n",
      "Batch: 13500,train loss is: 0.00555482340161467\n",
      "test loss is 0.005870751222799348\n",
      "Batch: 13600,train loss is: 0.009919307519076447\n",
      "test loss is 0.017759824136977857\n",
      "Batch: 13700,train loss is: 0.002017028490122112\n",
      "test loss is 0.0022862528439093394\n",
      "Batch: 13800,train loss is: 0.0019792686326067944\n",
      "test loss is 0.0019787744126468\n",
      "Batch: 13900,train loss is: 0.0007273187255266632\n",
      "test loss is 0.0015453808506624151\n",
      "Batch: 14000,train loss is: 0.0008918667496708912\n",
      "test loss is 0.0020147459643165386\n",
      "Batch: 14100,train loss is: 0.0014922436061940408\n",
      "test loss is 0.0022971379960692287\n",
      "Batch: 14200,train loss is: 0.0010118297243792943\n",
      "test loss is 0.0014952046411778064\n",
      "Batch: 14300,train loss is: 0.0008869852490792764\n",
      "test loss is 0.0019057863187794443\n",
      "Batch: 14400,train loss is: 0.001245410577490678\n",
      "test loss is 0.004856951321585731\n",
      "Batch: 14500,train loss is: 0.003613043995086053\n",
      "test loss is 0.002803133665908704\n",
      "Batch: 14600,train loss is: 0.0035538500565724974\n",
      "test loss is 0.00466049173569842\n",
      "Batch: 14700,train loss is: 0.001195627789152283\n",
      "test loss is 0.0019620198310677122\n",
      "Batch: 14800,train loss is: 0.004689823679124772\n",
      "test loss is 0.0028917214911940922\n",
      "Batch: 14900,train loss is: 0.0035647308750800335\n",
      "test loss is 0.0017597411444086265\n",
      "Batch: 15000,train loss is: 0.0014423653091182737\n",
      "test loss is 0.003207725982055846\n",
      "Batch: 15100,train loss is: 0.003349109011149408\n",
      "test loss is 0.0018117022416153743\n",
      "Batch: 15200,train loss is: 0.0027475037522911856\n",
      "test loss is 0.0020820810893714876\n",
      "Batch: 15300,train loss is: 0.0011284505697463948\n",
      "test loss is 0.0013863621913782972\n",
      "Batch: 15400,train loss is: 0.0013986895543759749\n",
      "test loss is 0.0039321682433049576\n",
      "Batch: 15500,train loss is: 0.002350693903241023\n",
      "test loss is 0.002455356559083913\n",
      "Batch: 15600,train loss is: 0.003995771494780056\n",
      "test loss is 0.002806546279709119\n",
      "Batch: 15700,train loss is: 0.005346052041708144\n",
      "test loss is 0.00379558035894898\n",
      "Batch: 15800,train loss is: 0.0013875469291812126\n",
      "test loss is 0.0019564661505510885\n",
      "Batch: 15900,train loss is: 0.001375539305394187\n",
      "test loss is 0.0019294558819958811\n",
      "Batch: 16000,train loss is: 0.0024293954640516855\n",
      "test loss is 0.0014625387150344383\n",
      "Batch: 16100,train loss is: 0.0048477760258104075\n",
      "test loss is 0.0017220066217643445\n",
      "Batch: 16200,train loss is: 0.0007226930717197475\n",
      "test loss is 0.0017199653351043407\n",
      "Batch: 16300,train loss is: 0.0012235744772479485\n",
      "test loss is 0.0012980773511296648\n",
      "Batch: 16400,train loss is: 0.0007052826774265538\n",
      "test loss is 0.0012141387483951065\n",
      "Batch: 16500,train loss is: 0.0056202110104641625\n",
      "test loss is 0.005574345964073852\n",
      "Batch: 16600,train loss is: 0.0013436002960160347\n",
      "test loss is 0.0018253416518509756\n",
      "Batch: 16700,train loss is: 0.004998511136001663\n",
      "test loss is 0.0033426972208138927\n",
      "Batch: 16800,train loss is: 0.001782314168587564\n",
      "test loss is 0.002459942283120958\n",
      "Batch: 16900,train loss is: 0.0032341141090468464\n",
      "test loss is 0.0049217753412371\n",
      "Batch: 17000,train loss is: 0.0016704605272170566\n",
      "test loss is 0.002825460905022045\n",
      "Batch: 17100,train loss is: 0.0015412137652647988\n",
      "test loss is 0.002312662818765271\n",
      "Batch: 17200,train loss is: 0.0013770168038060185\n",
      "test loss is 0.001412423350390257\n",
      "Batch: 17300,train loss is: 0.0026723853608800544\n",
      "test loss is 0.0017719284210529669\n",
      "Batch: 17400,train loss is: 0.0023379784348380186\n",
      "test loss is 0.0027716925507537365\n",
      "Batch: 17500,train loss is: 0.0019367128054726475\n",
      "test loss is 0.0017425971106100953\n",
      "Batch: 17600,train loss is: 0.0023498477606372194\n",
      "test loss is 0.0023641969511405507\n",
      "Batch: 17700,train loss is: 0.0009670250271298412\n",
      "test loss is 0.0016034415603972394\n",
      "Batch: 17800,train loss is: 0.0023745424730505353\n",
      "test loss is 0.00276226423848441\n",
      "Batch: 17900,train loss is: 0.007645646777526177\n",
      "test loss is 0.0034220848111814663\n",
      "Batch: 18000,train loss is: 0.0033697503339168186\n",
      "test loss is 0.0029227599957850382\n",
      "Batch: 18100,train loss is: 0.004385504428483377\n",
      "test loss is 0.0027151927376522977\n",
      "Batch: 18200,train loss is: 0.0015376725937334501\n",
      "test loss is 0.001819300187432351\n",
      "Batch: 18300,train loss is: 0.003506176708298668\n",
      "test loss is 0.002513537716284675\n",
      "Batch: 18400,train loss is: 0.0006560651439119939\n",
      "test loss is 0.0015247391334466244\n",
      "Batch: 18500,train loss is: 0.007267369215216835\n",
      "test loss is 0.005453687503839464\n",
      "Batch: 18600,train loss is: 0.0011897480224238148\n",
      "test loss is 0.0018160735809364093\n",
      "Batch: 18700,train loss is: 0.0007540517465471619\n",
      "test loss is 0.0016090516202805156\n",
      "Batch: 18800,train loss is: 0.0009338549590379069\n",
      "test loss is 0.0015323058885050905\n",
      "Batch: 18900,train loss is: 0.0029701319451328726\n",
      "test loss is 0.0015123074261154047\n",
      "Batch: 19000,train loss is: 0.0007197320738256118\n",
      "test loss is 0.001419368728986728\n",
      "Batch: 19100,train loss is: 0.002080997765016798\n",
      "test loss is 0.0021656529174250826\n",
      "Batch: 19200,train loss is: 0.0012990721396837887\n",
      "test loss is 0.0020532076531230847\n",
      "Batch: 19300,train loss is: 0.004338859518507887\n",
      "test loss is 0.006216793853387235\n",
      "Batch: 19400,train loss is: 0.0017940377769185493\n",
      "test loss is 0.002984185445812883\n",
      "Batch: 19500,train loss is: 0.0014511248596192365\n",
      "test loss is 0.0027114491914432544\n",
      "Batch: 19600,train loss is: 0.003254011269544265\n",
      "test loss is 0.0023317834166548487\n",
      "Batch: 19700,train loss is: 0.004313757115873221\n",
      "test loss is 0.002990578117783299\n",
      "Batch: 19800,train loss is: 0.0010312435650604564\n",
      "test loss is 0.001161077979206205\n",
      "Batch: 19900,train loss is: 0.001838373069859631\n",
      "test loss is 0.001869682768986329\n",
      "Batch: 20000,train loss is: 0.001705911760921403\n",
      "test loss is 0.0015922613515103928\n",
      "Batch: 20100,train loss is: 0.0009825328967340912\n",
      "test loss is 0.0014721801744574945\n",
      "Batch: 20200,train loss is: 0.0015902287626273603\n",
      "test loss is 0.001959156401937718\n",
      "Batch: 20300,train loss is: 0.0025365503601515933\n",
      "test loss is 0.0051032871134174445\n",
      "Batch: 20400,train loss is: 0.002983052829773372\n",
      "test loss is 0.007359311324659607\n",
      "Batch: 20500,train loss is: 0.001995754807887694\n",
      "test loss is 0.002343657722417888\n",
      "Batch: 20600,train loss is: 0.0014309377887211203\n",
      "test loss is 0.0014863942953666038\n",
      "Batch: 20700,train loss is: 0.0022869365440964223\n",
      "test loss is 0.003494013930111419\n",
      "Batch: 20800,train loss is: 0.004565158979969575\n",
      "test loss is 0.001797316813420952\n",
      "Batch: 20900,train loss is: 0.0008013592391624055\n",
      "test loss is 0.002048642225728585\n",
      "Batch: 21000,train loss is: 0.0008358589290132615\n",
      "test loss is 0.0017159242018418798\n",
      "Batch: 21100,train loss is: 0.0016273247201885012\n",
      "test loss is 0.0011327727874740217\n",
      "Batch: 21200,train loss is: 0.0004234521743818497\n",
      "test loss is 0.0015377889265796249\n",
      "Batch: 21300,train loss is: 0.0010043083690934288\n",
      "test loss is 0.0012089119181840709\n",
      "Batch: 21400,train loss is: 0.001195016941795964\n",
      "test loss is 0.0011423983881339427\n",
      "Batch: 21500,train loss is: 0.0006293477775256869\n",
      "test loss is 0.0017890462344233677\n",
      "Batch: 21600,train loss is: 0.0015141588624236369\n",
      "test loss is 0.0014522441147824714\n",
      "Batch: 21700,train loss is: 0.0005809583135480262\n",
      "test loss is 0.0018215499914290482\n",
      "Batch: 21800,train loss is: 0.002282015721992938\n",
      "test loss is 0.001418812020747487\n",
      "Batch: 21900,train loss is: 0.002354120353459142\n",
      "test loss is 0.0014859560838962004\n",
      "Batch: 22000,train loss is: 0.0008601123806958558\n",
      "test loss is 0.005024058181596652\n",
      "Batch: 22100,train loss is: 0.0011018211790957855\n",
      "test loss is 0.0024271908095269026\n",
      "Batch: 22200,train loss is: 0.005812900411299931\n",
      "test loss is 0.0031081072920994086\n",
      "Batch: 22300,train loss is: 0.005694315600995983\n",
      "test loss is 0.007169007922439211\n",
      "Batch: 22400,train loss is: 0.0011101753592669618\n",
      "test loss is 0.0027495719486617695\n",
      "Batch: 22500,train loss is: 0.0020438625099844106\n",
      "test loss is 0.0016500669419782516\n",
      "Batch: 22600,train loss is: 0.0032401098047816373\n",
      "test loss is 0.0017539588968976472\n",
      "Batch: 22700,train loss is: 0.0021318595046467366\n",
      "test loss is 0.0023013601850313944\n",
      "Batch: 22800,train loss is: 0.0029110857638632148\n",
      "test loss is 0.002460711037571323\n",
      "Batch: 22900,train loss is: 0.0013332900477469795\n",
      "test loss is 0.0027371977832581374\n",
      "Batch: 23000,train loss is: 0.005164520766458307\n",
      "test loss is 0.001628371637194854\n",
      "Batch: 23100,train loss is: 0.0008929297200928792\n",
      "test loss is 0.0013431891187342538\n",
      "Batch: 23200,train loss is: 0.0008997925871831679\n",
      "test loss is 0.0017385275483740616\n",
      "Batch: 23300,train loss is: 0.0008041283533770533\n",
      "test loss is 0.001443448726696745\n",
      "Batch: 23400,train loss is: 0.000591700507189477\n",
      "test loss is 0.0019505032405460183\n",
      "Batch: 23500,train loss is: 0.005646619917241862\n",
      "test loss is 0.0018080681969890088\n",
      "Batch: 23600,train loss is: 0.0013748272172099572\n",
      "test loss is 0.0018157053338244274\n",
      "Batch: 23700,train loss is: 0.0011121671935529374\n",
      "test loss is 0.0019151155577441295\n",
      "Batch: 23800,train loss is: 0.003894398855226592\n",
      "test loss is 0.0022218899158314947\n",
      "Batch: 23900,train loss is: 0.004981876040451268\n",
      "test loss is 0.002527644313265478\n",
      "Batch: 24000,train loss is: 0.00333223687472424\n",
      "test loss is 0.0026879411251993267\n",
      "Batch: 24100,train loss is: 0.0009179970699031176\n",
      "test loss is 0.0015248145022100216\n",
      "Batch: 24200,train loss is: 0.005050752124748367\n",
      "test loss is 0.0012196714644368155\n",
      "Batch: 24300,train loss is: 0.0036324684039529724\n",
      "test loss is 0.0017225700635754902\n",
      "Batch: 24400,train loss is: 0.008455929309620639\n",
      "test loss is 0.004131560744369285\n",
      "Batch: 24500,train loss is: 0.0034902525295967914\n",
      "test loss is 0.002244930805802672\n",
      "Batch: 24600,train loss is: 0.0010639991010244396\n",
      "test loss is 0.0016878649777370429\n",
      "Batch: 24700,train loss is: 0.0008134058440186032\n",
      "test loss is 0.001609506673131514\n",
      "Batch: 24800,train loss is: 0.0028147099406697165\n",
      "test loss is 0.0030876412528328956\n",
      "Batch: 24900,train loss is: 0.0012231797081408903\n",
      "test loss is 0.006979320174852358\n",
      "Batch: 25000,train loss is: 0.0029831757130708023\n",
      "test loss is 0.005408157515591804\n",
      "Batch: 25100,train loss is: 0.0003884403494719354\n",
      "test loss is 0.0034270773919255156\n",
      "Batch: 25200,train loss is: 0.001967698835837264\n",
      "test loss is 0.002340918705102739\n",
      "Batch: 25300,train loss is: 0.0021866524883219014\n",
      "test loss is 0.0018779747281233367\n",
      "Batch: 25400,train loss is: 0.0015300354153218747\n",
      "test loss is 0.0015742952725031855\n",
      "Batch: 25500,train loss is: 0.000723210793961701\n",
      "test loss is 0.0010968823542820089\n",
      "Batch: 25600,train loss is: 0.0025501755033824473\n",
      "test loss is 0.001800011177097554\n",
      "Batch: 25700,train loss is: 0.0020170033409393955\n",
      "test loss is 0.000999913506517477\n",
      "Batch: 25800,train loss is: 0.0010064938138266088\n",
      "test loss is 0.0011999689311975126\n",
      "Batch: 25900,train loss is: 0.0009914356580179745\n",
      "test loss is 0.0013747294177308281\n",
      "Batch: 26000,train loss is: 0.0013366701201816265\n",
      "test loss is 0.005982619543805527\n",
      "Batch: 26100,train loss is: 0.0011237839162992413\n",
      "test loss is 0.0016697283497039016\n",
      "Batch: 26200,train loss is: 0.001824345217772822\n",
      "test loss is 0.001226843820669754\n",
      "Batch: 26300,train loss is: 0.001762506225807626\n",
      "test loss is 0.001451308708615099\n",
      "Batch: 26400,train loss is: 0.0014308460716163233\n",
      "test loss is 0.001668040950309489\n",
      "Batch: 26500,train loss is: 0.0011511869617444935\n",
      "test loss is 0.001633962821783635\n",
      "Batch: 26600,train loss is: 0.005576006602093222\n",
      "test loss is 0.002567378944200908\n",
      "Batch: 26700,train loss is: 0.0005632324647856223\n",
      "test loss is 0.0013145412003824533\n",
      "Batch: 26800,train loss is: 0.001180874158110356\n",
      "test loss is 0.001238179471251209\n",
      "Batch: 26900,train loss is: 0.0006146420412809476\n",
      "test loss is 0.00247181030766683\n",
      "Batch: 27000,train loss is: 0.001306662119677787\n",
      "test loss is 0.0014037645187335208\n",
      "Batch: 27100,train loss is: 0.0013016755077446243\n",
      "test loss is 0.0021680296864230045\n",
      "Batch: 27200,train loss is: 0.0022001318761742067\n",
      "test loss is 0.0018311819191853356\n",
      "Batch: 27300,train loss is: 0.003945549104006088\n",
      "test loss is 0.007334442605406647\n",
      "Batch: 27400,train loss is: 0.0033112107630100655\n",
      "test loss is 0.004108213578177056\n",
      "Batch: 27500,train loss is: 0.0020422112127467026\n",
      "test loss is 0.0017393430403402882\n",
      "Batch: 27600,train loss is: 0.0038725474455939693\n",
      "test loss is 0.0025209052164145433\n",
      "Batch: 27700,train loss is: 0.004372185861515556\n",
      "test loss is 0.002921919068239316\n",
      "Batch: 27800,train loss is: 0.001867758316939593\n",
      "test loss is 0.004288854318382417\n",
      "Batch: 27900,train loss is: 0.0015950010230369044\n",
      "test loss is 0.0011069443639797614\n",
      "Batch: 28000,train loss is: 0.0010207324184383527\n",
      "test loss is 0.0027965497951410894\n",
      "Batch: 28100,train loss is: 0.003997245174732571\n",
      "test loss is 0.0015536754639827316\n",
      "Batch: 28200,train loss is: 0.0014862375950934368\n",
      "test loss is 0.0021802932752263892\n",
      "Batch: 28300,train loss is: 0.0069321316194891095\n",
      "test loss is 0.0010926385037978237\n",
      "Batch: 28400,train loss is: 0.003193111976888987\n",
      "test loss is 0.0014941846701187712\n",
      "Batch: 28500,train loss is: 0.0016857086363142337\n",
      "test loss is 0.001287774353635706\n",
      "Batch: 28600,train loss is: 0.0008770538943374854\n",
      "test loss is 0.0015558217085495632\n",
      "Batch: 28700,train loss is: 0.0015971713661530537\n",
      "test loss is 0.004799407990480468\n",
      "Batch: 28800,train loss is: 0.004931318355803078\n",
      "test loss is 0.001521874470758249\n",
      "Batch: 28900,train loss is: 0.0006887625046436503\n",
      "test loss is 0.0016183373251369483\n",
      "Batch: 29000,train loss is: 0.0017463607045013587\n",
      "test loss is 0.003200251225054134\n",
      "Batch: 29100,train loss is: 0.005781284365317391\n",
      "test loss is 0.015842566262600666\n",
      "Batch: 29200,train loss is: 0.0008314410800427193\n",
      "test loss is 0.0017887436403529104\n",
      "Batch: 29300,train loss is: 0.0020716973782678935\n",
      "test loss is 0.0016136032829409924\n",
      "Batch: 29400,train loss is: 0.0014064683060189805\n",
      "test loss is 0.003872401777880901\n",
      "Batch: 29500,train loss is: 0.0013954241354205575\n",
      "test loss is 0.002581349899376624\n",
      "Batch: 29600,train loss is: 0.003999270294792021\n",
      "test loss is 0.0018488252372694791\n",
      "Batch: 29700,train loss is: 0.0004502334677027538\n",
      "test loss is 0.001579740164821918\n",
      "Batch: 29800,train loss is: 0.002292858499991149\n",
      "test loss is 0.0010876021163724661\n",
      "Batch: 29900,train loss is: 0.0024769903158485046\n",
      "test loss is 0.0011370951093326496\n",
      "Batch: 30000,train loss is: 0.0009178662217427162\n",
      "test loss is 0.0009093325217501393\n",
      "Batch: 30100,train loss is: 0.0008463206930973468\n",
      "test loss is 0.005700188428830152\n",
      "Batch: 30200,train loss is: 0.0006860709538388351\n",
      "test loss is 0.0014285781353088388\n",
      "Batch: 30300,train loss is: 0.000533211756378447\n",
      "test loss is 0.002158758744756558\n",
      "Batch: 30400,train loss is: 0.0028786562865055166\n",
      "test loss is 0.004879015634746493\n",
      "Batch: 30500,train loss is: 0.001341968014426609\n",
      "test loss is 0.0013374050982156796\n",
      "Batch: 30600,train loss is: 0.0008230133217261362\n",
      "test loss is 0.002632322972167527\n",
      "Batch: 30700,train loss is: 0.0006027335472748014\n",
      "test loss is 0.00214256421807964\n",
      "Batch: 30800,train loss is: 0.000972720286529809\n",
      "test loss is 0.0013297974817194256\n",
      "Batch: 30900,train loss is: 0.0013486382111619045\n",
      "test loss is 0.0017748104851117315\n",
      "Batch: 31000,train loss is: 0.0014311936764780657\n",
      "test loss is 0.0014928303098488804\n",
      "Batch: 31100,train loss is: 0.000586557887620111\n",
      "test loss is 0.0015208590772085315\n",
      "Batch: 31200,train loss is: 0.0015096713624958545\n",
      "test loss is 0.0016074508965053637\n",
      "Batch: 31300,train loss is: 0.0006186721500651355\n",
      "test loss is 0.0022416423302200343\n",
      "Batch: 31400,train loss is: 0.003688749190399792\n",
      "test loss is 0.006786223149352195\n",
      "Batch: 31500,train loss is: 0.0014156795797881732\n",
      "test loss is 0.001839557564050561\n",
      "Batch: 31600,train loss is: 0.0010594290032976514\n",
      "test loss is 0.0014448186929003454\n",
      "Batch: 31700,train loss is: 0.00039777777477180675\n",
      "test loss is 0.0011127551804946815\n",
      "Batch: 31800,train loss is: 0.0025725934252179504\n",
      "test loss is 0.0027489926378374756\n",
      "Batch: 31900,train loss is: 0.00030089226876381327\n",
      "test loss is 0.0010618641862102521\n",
      "Batch: 32000,train loss is: 0.009849045783200984\n",
      "test loss is 0.0009371676828971973\n",
      "Batch: 32100,train loss is: 0.0006189685610616054\n",
      "test loss is 0.001256742130715233\n",
      "Batch: 32200,train loss is: 0.0005057854208730069\n",
      "test loss is 0.001166729539882559\n",
      "Batch: 32300,train loss is: 0.0006028475134916247\n",
      "test loss is 0.0017035980633074263\n",
      "Batch: 32400,train loss is: 0.00044971672786965275\n",
      "test loss is 0.0014227395928412378\n",
      "Batch: 32500,train loss is: 0.0007412256801612247\n",
      "test loss is 0.0012913431734929651\n",
      "Batch: 32600,train loss is: 0.0014676453346955073\n",
      "test loss is 0.0011224680252301168\n",
      "Batch: 32700,train loss is: 0.0010512442741485593\n",
      "test loss is 0.002353574234135112\n",
      "Batch: 32800,train loss is: 0.0007072256951512025\n",
      "test loss is 0.0013668701999721613\n",
      "Batch: 32900,train loss is: 0.0014179136735989113\n",
      "test loss is 0.0013591583308178697\n",
      "Batch: 33000,train loss is: 0.0006551240571251792\n",
      "test loss is 0.0010800869398676742\n",
      "Batch: 33100,train loss is: 0.002015030696802061\n",
      "test loss is 0.00204333255606899\n",
      "Batch: 33200,train loss is: 0.0027681607075537872\n",
      "test loss is 0.002283115615731983\n",
      "Batch: 33300,train loss is: 0.0006943133582258651\n",
      "test loss is 0.001133429130370144\n",
      "Batch: 33400,train loss is: 0.000659488783164676\n",
      "test loss is 0.004369104734558255\n",
      "Batch: 33500,train loss is: 0.0006781222794318214\n",
      "test loss is 0.0025829646847683584\n",
      "Batch: 33600,train loss is: 0.0006734909221600595\n",
      "test loss is 0.0017463703347193763\n",
      "Batch: 33700,train loss is: 0.001127256362111985\n",
      "test loss is 0.0015675216271753638\n",
      "Batch: 33800,train loss is: 0.0020201000944547105\n",
      "test loss is 0.0019921931157457105\n",
      "Batch: 33900,train loss is: 0.005349176684188661\n",
      "test loss is 0.0016847409549478594\n",
      "-----------------------Epoch: 2----------------------------------\n",
      "Batch: 0,train loss is: 0.0013489441399777335\n",
      "test loss is 0.001246517778661617\n",
      "Batch: 100,train loss is: 0.00162878734337675\n",
      "test loss is 0.0010225160117817312\n",
      "Batch: 200,train loss is: 0.0020700038983033\n",
      "test loss is 0.0016730491028397268\n",
      "Batch: 300,train loss is: 0.002472353092363544\n",
      "test loss is 0.0021654636611300694\n",
      "Batch: 400,train loss is: 0.00041252657577091285\n",
      "test loss is 0.0017022800280244925\n",
      "Batch: 500,train loss is: 0.00281964790138033\n",
      "test loss is 0.005674579122986255\n",
      "Batch: 600,train loss is: 0.0016891185806065362\n",
      "test loss is 0.006022726816572021\n",
      "Batch: 700,train loss is: 0.001108697695213136\n",
      "test loss is 0.0011897068300025233\n",
      "Batch: 800,train loss is: 0.0017882783172391077\n",
      "test loss is 0.0015066528792681648\n",
      "Batch: 900,train loss is: 0.0013191572648320407\n",
      "test loss is 0.0012980879584798249\n",
      "Batch: 1000,train loss is: 0.00084439194745997\n",
      "test loss is 0.0011075322640462055\n",
      "Batch: 1100,train loss is: 0.0025591744807329105\n",
      "test loss is 0.0016569110868040128\n",
      "Batch: 1200,train loss is: 0.00044299840598784263\n",
      "test loss is 0.0023024323582664145\n",
      "Batch: 1300,train loss is: 0.0032074468304676114\n",
      "test loss is 0.0018472042691289007\n",
      "Batch: 1400,train loss is: 0.0007970037518730943\n",
      "test loss is 0.001361419658486758\n",
      "Batch: 1500,train loss is: 0.0021100565781312754\n",
      "test loss is 0.002799229634327848\n",
      "Batch: 1600,train loss is: 0.0019554710570684145\n",
      "test loss is 0.0015174797920224279\n",
      "Batch: 1700,train loss is: 0.0018202147715661973\n",
      "test loss is 0.001732054044926431\n",
      "Batch: 1800,train loss is: 0.0016793519617560604\n",
      "test loss is 0.0036828496403820484\n",
      "Batch: 1900,train loss is: 0.005531568047277959\n",
      "test loss is 0.004527658414520884\n",
      "Batch: 2000,train loss is: 0.004776350333369492\n",
      "test loss is 0.005014775582448399\n",
      "Batch: 2100,train loss is: 0.0029114051704369065\n",
      "test loss is 0.0014600235729925496\n",
      "Batch: 2200,train loss is: 0.0009487754796131723\n",
      "test loss is 0.0009491842966713494\n",
      "Batch: 2300,train loss is: 0.0006341318707007978\n",
      "test loss is 0.0009530995539544623\n",
      "Batch: 2400,train loss is: 0.0005971164611667886\n",
      "test loss is 0.000831844879294131\n",
      "Batch: 2500,train loss is: 0.0006691706772254824\n",
      "test loss is 0.0012640413935585382\n",
      "Batch: 2600,train loss is: 0.0021999342966039057\n",
      "test loss is 0.001131239262514191\n",
      "Batch: 2700,train loss is: 0.000613944520707527\n",
      "test loss is 0.0008328465562015121\n",
      "Batch: 2800,train loss is: 0.0008408778179697096\n",
      "test loss is 0.00104333346039078\n",
      "Batch: 2900,train loss is: 0.000673347839780997\n",
      "test loss is 0.0009623691884339451\n",
      "Batch: 3000,train loss is: 0.0006355142065708507\n",
      "test loss is 0.0010514118124786088\n",
      "Batch: 3100,train loss is: 0.0015634216310901742\n",
      "test loss is 0.001660926562516516\n",
      "Batch: 3200,train loss is: 0.0025152484565197464\n",
      "test loss is 0.0011905370336496523\n",
      "Batch: 3300,train loss is: 0.0006956657688252641\n",
      "test loss is 0.0012778481857022282\n",
      "Batch: 3400,train loss is: 0.0010335565318856696\n",
      "test loss is 0.0013415492077107198\n",
      "Batch: 3500,train loss is: 0.0008159026077841959\n",
      "test loss is 0.0019358584272831695\n",
      "Batch: 3600,train loss is: 0.000882471324097845\n",
      "test loss is 0.0017973648094947847\n",
      "Batch: 3700,train loss is: 0.002321399494824307\n",
      "test loss is 0.002934979452973676\n",
      "Batch: 3800,train loss is: 0.0012929871229467608\n",
      "test loss is 0.001382287920943386\n",
      "Batch: 3900,train loss is: 0.0024689406340867785\n",
      "test loss is 0.0020810432186796905\n",
      "Batch: 4000,train loss is: 0.00032650508310724666\n",
      "test loss is 0.0012824149708059662\n",
      "Batch: 4100,train loss is: 0.0017183086165973587\n",
      "test loss is 0.0008463265965544133\n",
      "Batch: 4200,train loss is: 0.0020759449943070357\n",
      "test loss is 0.0012659323292085634\n",
      "Batch: 4300,train loss is: 0.0006779209274979287\n",
      "test loss is 0.0013512551988613616\n",
      "Batch: 4400,train loss is: 0.0013341091072987224\n",
      "test loss is 0.0013022152634682188\n",
      "Batch: 4500,train loss is: 0.0004992270440020077\n",
      "test loss is 0.0013527592753938924\n",
      "Batch: 4600,train loss is: 0.009106340825055844\n",
      "test loss is 0.007601303793721503\n",
      "Batch: 4700,train loss is: 0.0013745121228701232\n",
      "test loss is 0.001488787890852722\n",
      "Batch: 4800,train loss is: 0.007004776220888578\n",
      "test loss is 0.006097709008170366\n",
      "Batch: 4900,train loss is: 0.0035431989487737427\n",
      "test loss is 0.0024403544900827336\n",
      "Batch: 5000,train loss is: 0.00048055846273207354\n",
      "test loss is 0.0010240406749481683\n",
      "Batch: 5100,train loss is: 0.0011028908054274253\n",
      "test loss is 0.003110978323892492\n",
      "Batch: 5200,train loss is: 0.0005049007299787452\n",
      "test loss is 0.0014578697741446313\n",
      "Batch: 5300,train loss is: 0.0060259526366116065\n",
      "test loss is 0.004149844869604396\n",
      "Batch: 5400,train loss is: 0.0018375257435178309\n",
      "test loss is 0.0020100634164531743\n",
      "Batch: 5500,train loss is: 0.0016647575031634367\n",
      "test loss is 0.0020361872366825287\n",
      "Batch: 5600,train loss is: 0.002355946619667171\n",
      "test loss is 0.0020427830595326295\n",
      "Batch: 5700,train loss is: 0.0005375071944388109\n",
      "test loss is 0.001242975979145372\n",
      "Batch: 5800,train loss is: 0.0005646955186246589\n",
      "test loss is 0.0013863579944750088\n",
      "Batch: 5900,train loss is: 0.0026951579076957537\n",
      "test loss is 0.0010910852893833165\n",
      "Batch: 6000,train loss is: 0.0018366729455513344\n",
      "test loss is 0.001430957395927978\n",
      "Batch: 6100,train loss is: 0.00040942810746553526\n",
      "test loss is 0.0012734946070224367\n",
      "Batch: 6200,train loss is: 0.0011872230127712487\n",
      "test loss is 0.0012829010087325701\n",
      "Batch: 6300,train loss is: 0.0013570491005053796\n",
      "test loss is 0.0013109658491532255\n",
      "Batch: 6400,train loss is: 0.0014588473532845632\n",
      "test loss is 0.0015328106959843222\n",
      "Batch: 6500,train loss is: 0.0005301777970339255\n",
      "test loss is 0.001101842161819783\n",
      "Batch: 6600,train loss is: 0.00883235401781514\n",
      "test loss is 0.0017243707925233334\n",
      "Batch: 6700,train loss is: 0.000348706665750769\n",
      "test loss is 0.001812434283905502\n",
      "Batch: 6800,train loss is: 0.0018710211952953392\n",
      "test loss is 0.0015406247464511496\n",
      "Batch: 6900,train loss is: 0.0028823875866536585\n",
      "test loss is 0.0009824434306885095\n",
      "Batch: 7000,train loss is: 0.0022397842946883435\n",
      "test loss is 0.00139469329478442\n",
      "Batch: 7100,train loss is: 0.0032882050150963584\n",
      "test loss is 0.0027927886477122706\n",
      "Batch: 7200,train loss is: 0.0004650910436072028\n",
      "test loss is 0.0010676610325647128\n",
      "Batch: 7300,train loss is: 0.0012491444686951552\n",
      "test loss is 0.0016654767817043808\n",
      "Batch: 7400,train loss is: 0.001505385955779699\n",
      "test loss is 0.002211792088700281\n",
      "Batch: 7500,train loss is: 0.0012415895752688052\n",
      "test loss is 0.0029484821473875033\n",
      "Batch: 7600,train loss is: 0.005696172037874994\n",
      "test loss is 0.004217657216867286\n",
      "Batch: 7700,train loss is: 0.0013863517892413217\n",
      "test loss is 0.0010200655641666703\n",
      "Batch: 7800,train loss is: 0.0008140848566520741\n",
      "test loss is 0.0010827513108109883\n",
      "Batch: 7900,train loss is: 0.0011412053430564797\n",
      "test loss is 0.0008016387315177683\n",
      "Batch: 8000,train loss is: 0.0018577704110737828\n",
      "test loss is 0.0034570472907861755\n",
      "Batch: 8100,train loss is: 0.0014667481742590912\n",
      "test loss is 0.0012921157245339573\n",
      "Batch: 8200,train loss is: 0.0029620063126240834\n",
      "test loss is 0.001324809463566284\n",
      "Batch: 8300,train loss is: 0.0007352508651873177\n",
      "test loss is 0.0014102315178398882\n",
      "Batch: 8400,train loss is: 0.002784375389574581\n",
      "test loss is 0.0015841722243213415\n",
      "Batch: 8500,train loss is: 0.001261825211323046\n",
      "test loss is 0.0012169107809245309\n",
      "Batch: 8600,train loss is: 0.0006898358187675287\n",
      "test loss is 0.001368479466465327\n",
      "Batch: 8700,train loss is: 0.0008520363589037374\n",
      "test loss is 0.00214785007510136\n",
      "Batch: 8800,train loss is: 0.0011873679308665745\n",
      "test loss is 0.0012698283936223432\n",
      "Batch: 8900,train loss is: 0.0014125204345992086\n",
      "test loss is 0.001028469284351801\n",
      "Batch: 9000,train loss is: 0.0005107965575502525\n",
      "test loss is 0.001474926319775274\n",
      "Batch: 9100,train loss is: 0.0035591059950670894\n",
      "test loss is 0.0038611541293150182\n",
      "Batch: 9200,train loss is: 0.0014169643310047012\n",
      "test loss is 0.005299019236820061\n",
      "Batch: 9300,train loss is: 0.0016684471440660971\n",
      "test loss is 0.0010120390904223451\n",
      "Batch: 9400,train loss is: 0.0004949195053615496\n",
      "test loss is 0.0010634311739040263\n",
      "Batch: 9500,train loss is: 0.001235709687568335\n",
      "test loss is 0.002207517151902714\n",
      "Batch: 9600,train loss is: 0.001123942334642685\n",
      "test loss is 0.0015632239706997054\n",
      "Batch: 9700,train loss is: 0.0003593794403451247\n",
      "test loss is 0.0011251078215097025\n",
      "Batch: 9800,train loss is: 0.0005716592361274807\n",
      "test loss is 0.0017232319188429405\n",
      "Batch: 9900,train loss is: 0.0005011301022297519\n",
      "test loss is 0.0018306651434745707\n",
      "Batch: 10000,train loss is: 0.0004945694267471829\n",
      "test loss is 0.001073682312270619\n",
      "Batch: 10100,train loss is: 0.0034029802542583864\n",
      "test loss is 0.0015599417155752714\n",
      "Batch: 10200,train loss is: 0.0019336603618161376\n",
      "test loss is 0.0012514814552603516\n",
      "Batch: 10300,train loss is: 0.021446857023454922\n",
      "test loss is 0.016639210691902315\n",
      "Batch: 10400,train loss is: 0.0005386371409290688\n",
      "test loss is 0.002128692784928073\n",
      "Batch: 10500,train loss is: 0.0031887767849794074\n",
      "test loss is 0.002587771531603184\n",
      "Batch: 10600,train loss is: 0.0007943257068039214\n",
      "test loss is 0.001448661840930244\n",
      "Batch: 10700,train loss is: 0.0016023781303024403\n",
      "test loss is 0.0012949165614712889\n",
      "Batch: 10800,train loss is: 0.0006233270850379591\n",
      "test loss is 0.0026011015187604905\n",
      "Batch: 10900,train loss is: 0.0009761678299791358\n",
      "test loss is 0.001925355429827605\n",
      "Batch: 11000,train loss is: 0.000902471833714576\n",
      "test loss is 0.0009671379138907426\n",
      "Batch: 11100,train loss is: 0.002418691442132721\n",
      "test loss is 0.0022467116639789913\n",
      "Batch: 11200,train loss is: 0.0010636519921775234\n",
      "test loss is 0.0008815925146034127\n",
      "Batch: 11300,train loss is: 0.0011641770789005907\n",
      "test loss is 0.0020686194037176715\n",
      "Batch: 11400,train loss is: 0.0017377791187773376\n",
      "test loss is 0.0013891671260505057\n",
      "Batch: 11500,train loss is: 0.0002586588664051155\n",
      "test loss is 0.001823345795099907\n",
      "Batch: 11600,train loss is: 0.007537517383787087\n",
      "test loss is 0.0016236964753991129\n",
      "Batch: 11700,train loss is: 0.00035541770924081285\n",
      "test loss is 0.0012459405819805706\n",
      "Batch: 11800,train loss is: 0.0006288636141599266\n",
      "test loss is 0.0008307410052031147\n",
      "Batch: 11900,train loss is: 0.0013563667523821797\n",
      "test loss is 0.0012424760399861907\n",
      "Batch: 12000,train loss is: 0.0018029298173083768\n",
      "test loss is 0.001679441906903534\n",
      "Batch: 12100,train loss is: 0.012480154030716487\n",
      "test loss is 0.0015391806105645963\n",
      "Batch: 12200,train loss is: 0.0009448442974242927\n",
      "test loss is 0.002350630515767913\n",
      "Batch: 12300,train loss is: 0.0038779748901236486\n",
      "test loss is 0.0026500678301793453\n",
      "Batch: 12400,train loss is: 0.001474903791173139\n",
      "test loss is 0.0014596951387827119\n",
      "Batch: 12500,train loss is: 0.004095740917811301\n",
      "test loss is 0.004388928122360667\n",
      "Batch: 12600,train loss is: 0.000669607471329936\n",
      "test loss is 0.0010125301203727565\n",
      "Batch: 12700,train loss is: 0.00040228508946974043\n",
      "test loss is 0.0007557919941769087\n",
      "Batch: 12800,train loss is: 0.00044874989534175817\n",
      "test loss is 0.0013856814244170782\n",
      "Batch: 12900,train loss is: 0.001793572774104113\n",
      "test loss is 0.0024786631995168773\n",
      "Batch: 13000,train loss is: 0.0013775933694509233\n",
      "test loss is 0.0013298282431669882\n",
      "Batch: 13100,train loss is: 0.0005752860405910528\n",
      "test loss is 0.0008163216444862794\n",
      "Batch: 13200,train loss is: 0.0009708666165595776\n",
      "test loss is 0.001977112286269373\n",
      "Batch: 13300,train loss is: 0.0015846925713539556\n",
      "test loss is 0.00493827752840492\n",
      "Batch: 13400,train loss is: 0.0010700683455481881\n",
      "test loss is 0.0012423720929937707\n",
      "Batch: 13500,train loss is: 0.0011623151827464312\n",
      "test loss is 0.0017100632641145425\n",
      "Batch: 13600,train loss is: 0.005325262393782733\n",
      "test loss is 0.00604399387273124\n",
      "Batch: 13700,train loss is: 0.0008744364568139667\n",
      "test loss is 0.0011975951725793881\n",
      "Batch: 13800,train loss is: 0.00030227457812660627\n",
      "test loss is 0.0010861355296852319\n",
      "Batch: 13900,train loss is: 0.0005081271472586086\n",
      "test loss is 0.0009616122503919338\n",
      "Batch: 14000,train loss is: 0.0005236473085273317\n",
      "test loss is 0.0011934109662033537\n",
      "Batch: 14100,train loss is: 0.0006729876160120506\n",
      "test loss is 0.0013483610400566671\n",
      "Batch: 14200,train loss is: 0.0006303618362231809\n",
      "test loss is 0.0007800471516047819\n",
      "Batch: 14300,train loss is: 0.0004124229537908519\n",
      "test loss is 0.0008367251467309247\n",
      "Batch: 14400,train loss is: 0.00112871249211186\n",
      "test loss is 0.0025236370422193452\n",
      "Batch: 14500,train loss is: 0.000556905174442255\n",
      "test loss is 0.0008647656231716537\n",
      "Batch: 14600,train loss is: 0.008017068914784443\n",
      "test loss is 0.004572708006404887\n",
      "Batch: 14700,train loss is: 0.00453243407883852\n",
      "test loss is 0.012946920156867107\n",
      "Batch: 14800,train loss is: 0.0013464911753820565\n",
      "test loss is 0.002185453940964169\n",
      "Batch: 14900,train loss is: 0.0007931974123122398\n",
      "test loss is 0.0015002611817803598\n",
      "Batch: 15000,train loss is: 0.0013452701119335462\n",
      "test loss is 0.001270148805753823\n",
      "Batch: 15100,train loss is: 0.001588103053048238\n",
      "test loss is 0.000950041038445546\n",
      "Batch: 15200,train loss is: 0.0006990655240246013\n",
      "test loss is 0.0007649696773160037\n",
      "Batch: 15300,train loss is: 0.0015387039158018677\n",
      "test loss is 0.0008726595704086106\n",
      "Batch: 15400,train loss is: 0.0005377084517916349\n",
      "test loss is 0.0008191380114253509\n",
      "Batch: 15500,train loss is: 0.0011908401929100875\n",
      "test loss is 0.0011976073699550688\n",
      "Batch: 15600,train loss is: 0.0017782190444334577\n",
      "test loss is 0.0010146701721690072\n",
      "Batch: 15700,train loss is: 0.0007886531638581995\n",
      "test loss is 0.002366826634156829\n",
      "Batch: 15800,train loss is: 0.001230699904773707\n",
      "test loss is 0.001134185506908294\n",
      "Batch: 15900,train loss is: 0.0008254033927349143\n",
      "test loss is 0.0008808027197057395\n",
      "Batch: 16000,train loss is: 0.0008765716787061329\n",
      "test loss is 0.000919968767261857\n",
      "Batch: 16100,train loss is: 0.003757298449858712\n",
      "test loss is 0.0015211771040218975\n",
      "Batch: 16200,train loss is: 0.0010072946769524459\n",
      "test loss is 0.002116081886052097\n",
      "Batch: 16300,train loss is: 0.0007075283255866309\n",
      "test loss is 0.0012221034569040684\n",
      "Batch: 16400,train loss is: 0.00036848912878300347\n",
      "test loss is 0.0007853426429749238\n",
      "Batch: 16500,train loss is: 0.0037943366656371887\n",
      "test loss is 0.0023010787393542695\n",
      "Batch: 16600,train loss is: 0.0005897776593799145\n",
      "test loss is 0.0011568367823611672\n",
      "Batch: 16700,train loss is: 0.006145429913665476\n",
      "test loss is 0.0042323482795227135\n",
      "Batch: 16800,train loss is: 0.0011358797200793538\n",
      "test loss is 0.0014185253318645364\n",
      "Batch: 16900,train loss is: 0.0024405537504189377\n",
      "test loss is 0.0032707345457560657\n",
      "Batch: 17000,train loss is: 0.0009086714409062863\n",
      "test loss is 0.001158988483517558\n",
      "Batch: 17100,train loss is: 0.0010983573215126315\n",
      "test loss is 0.001599458934065321\n",
      "Batch: 17200,train loss is: 0.0005944018121494875\n",
      "test loss is 0.0008806649068966698\n",
      "Batch: 17300,train loss is: 0.0021127825613666868\n",
      "test loss is 0.001303452779401681\n",
      "Batch: 17400,train loss is: 0.0007623187338246543\n",
      "test loss is 0.0011908533317465401\n",
      "Batch: 17500,train loss is: 0.0013729530689769807\n",
      "test loss is 0.001257709356883192\n",
      "Batch: 17600,train loss is: 0.0019934543012407506\n",
      "test loss is 0.002357463760540538\n",
      "Batch: 17700,train loss is: 0.0007659640021222014\n",
      "test loss is 0.0015563800684952678\n",
      "Batch: 17800,train loss is: 0.0018065942784670137\n",
      "test loss is 0.0018871895022977156\n",
      "Batch: 17900,train loss is: 0.003356116463403502\n",
      "test loss is 0.0018109756315423887\n",
      "Batch: 18000,train loss is: 0.0017846219526124335\n",
      "test loss is 0.0011897979915109931\n",
      "Batch: 18100,train loss is: 0.0006241933428964268\n",
      "test loss is 0.000784420458463261\n",
      "Batch: 18200,train loss is: 0.0006627907730058609\n",
      "test loss is 0.0011272031444154568\n",
      "Batch: 18300,train loss is: 0.0013696415067755495\n",
      "test loss is 0.0009768030952515576\n",
      "Batch: 18400,train loss is: 0.000471203830689073\n",
      "test loss is 0.000925964885486142\n",
      "Batch: 18500,train loss is: 0.005843758708075406\n",
      "test loss is 0.004879268852580092\n",
      "Batch: 18600,train loss is: 0.0021958444310890986\n",
      "test loss is 0.0028835777489254677\n",
      "Batch: 18700,train loss is: 0.0005906582617228706\n",
      "test loss is 0.001415690116086081\n",
      "Batch: 18800,train loss is: 0.0005693769866890115\n",
      "test loss is 0.000798403550188284\n",
      "Batch: 18900,train loss is: 0.001546760041696542\n",
      "test loss is 0.001084029211250637\n",
      "Batch: 19000,train loss is: 0.0007829902268824534\n",
      "test loss is 0.0008281900407198859\n",
      "Batch: 19100,train loss is: 0.0011616603178422652\n",
      "test loss is 0.001100727153533559\n",
      "Batch: 19200,train loss is: 0.0007799571052195875\n",
      "test loss is 0.0013650514871921507\n",
      "Batch: 19300,train loss is: 0.004482158874186414\n",
      "test loss is 0.007058144889050986\n",
      "Batch: 19400,train loss is: 0.0009675948569454933\n",
      "test loss is 0.0016985933889430932\n",
      "Batch: 19500,train loss is: 0.0004449990900811016\n",
      "test loss is 0.0014308655667168975\n",
      "Batch: 19600,train loss is: 0.0016173179905507684\n",
      "test loss is 0.001213093200512506\n",
      "Batch: 19700,train loss is: 0.0024835344389271576\n",
      "test loss is 0.0024017383805760616\n",
      "Batch: 19800,train loss is: 0.0005607263150344969\n",
      "test loss is 0.000751584228968856\n",
      "Batch: 19900,train loss is: 0.0014929723377880692\n",
      "test loss is 0.0015029031722816164\n",
      "Batch: 20000,train loss is: 0.0012684697554887507\n",
      "test loss is 0.0013862717676681703\n",
      "Batch: 20100,train loss is: 0.0005171065266604309\n",
      "test loss is 0.000984757327867733\n",
      "Batch: 20200,train loss is: 0.0015081526370909224\n",
      "test loss is 0.0013087208526527652\n",
      "Batch: 20300,train loss is: 0.001975894746188984\n",
      "test loss is 0.0027155620963854013\n",
      "Batch: 20400,train loss is: 0.0017402561017878944\n",
      "test loss is 0.004758847461701767\n",
      "Batch: 20500,train loss is: 0.0011587112128955955\n",
      "test loss is 0.0013692044505321911\n",
      "Batch: 20600,train loss is: 0.001341238921359549\n",
      "test loss is 0.0010618133081780833\n",
      "Batch: 20700,train loss is: 0.0020029199021893537\n",
      "test loss is 0.002146191021324218\n",
      "Batch: 20800,train loss is: 0.0017966924962787684\n",
      "test loss is 0.0010671192958337783\n",
      "Batch: 20900,train loss is: 0.000596289650649345\n",
      "test loss is 0.0015939400690433811\n",
      "Batch: 21000,train loss is: 0.0004926152114731085\n",
      "test loss is 0.0009577158865484345\n",
      "Batch: 21100,train loss is: 0.0006897391439476059\n",
      "test loss is 0.0006697315748265691\n",
      "Batch: 21200,train loss is: 0.00037646336484777835\n",
      "test loss is 0.0010021583857670732\n",
      "Batch: 21300,train loss is: 0.0007987342059038178\n",
      "test loss is 0.0009317105741760805\n",
      "Batch: 21400,train loss is: 0.0006488970130660217\n",
      "test loss is 0.0009252460824490709\n",
      "Batch: 21500,train loss is: 0.0005543606271934064\n",
      "test loss is 0.0009790946388802265\n",
      "Batch: 21600,train loss is: 0.0009232174140082639\n",
      "test loss is 0.0008621047042309345\n",
      "Batch: 21700,train loss is: 0.0003399110988970771\n",
      "test loss is 0.0009315956018762492\n",
      "Batch: 21800,train loss is: 0.0009675127077920636\n",
      "test loss is 0.0009628829963014382\n",
      "Batch: 21900,train loss is: 0.00020816952119835153\n",
      "test loss is 0.0007334770789548961\n",
      "Batch: 22000,train loss is: 0.0020846663055242255\n",
      "test loss is 0.00551654310336098\n",
      "Batch: 22100,train loss is: 0.0008014336738307443\n",
      "test loss is 0.0012267646850974638\n",
      "Batch: 22200,train loss is: 0.0016501312965016394\n",
      "test loss is 0.001896600281616847\n",
      "Batch: 22300,train loss is: 0.002225156182820939\n",
      "test loss is 0.0029209571364221245\n",
      "Batch: 22400,train loss is: 0.0007071340705720335\n",
      "test loss is 0.001901108665634957\n",
      "Batch: 22500,train loss is: 0.0008493525772626895\n",
      "test loss is 0.0013775841019020303\n",
      "Batch: 22600,train loss is: 0.0011846372469285914\n",
      "test loss is 0.0008701722835180417\n",
      "Batch: 22700,train loss is: 0.0006916907572665489\n",
      "test loss is 0.0009710856110086937\n",
      "Batch: 22800,train loss is: 0.0011983561683879204\n",
      "test loss is 0.0012420270481752608\n",
      "Batch: 22900,train loss is: 0.0007924021416457111\n",
      "test loss is 0.0013919617892030158\n",
      "Batch: 23000,train loss is: 0.001971159307691306\n",
      "test loss is 0.0009982393441601454\n",
      "Batch: 23100,train loss is: 0.0006492193705456904\n",
      "test loss is 0.0011701102423635004\n",
      "Batch: 23200,train loss is: 0.0004653113536805596\n",
      "test loss is 0.0008951342178176629\n",
      "Batch: 23300,train loss is: 0.0006013113297439114\n",
      "test loss is 0.000782322117311437\n",
      "Batch: 23400,train loss is: 0.00044599027987460743\n",
      "test loss is 0.0011313387334856801\n",
      "Batch: 23500,train loss is: 0.0063632907360980625\n",
      "test loss is 0.0011467898055931424\n",
      "Batch: 23600,train loss is: 0.0009329602292889492\n",
      "test loss is 0.0012743344741072537\n",
      "Batch: 23700,train loss is: 0.0006421080165657536\n",
      "test loss is 0.0010211321906605745\n",
      "Batch: 23800,train loss is: 0.001721312559137237\n",
      "test loss is 0.0009707506745066592\n",
      "Batch: 23900,train loss is: 0.00473493378669462\n",
      "test loss is 0.002924697125959093\n",
      "Batch: 24000,train loss is: 0.0014918886080003335\n",
      "test loss is 0.00243205082789163\n",
      "Batch: 24100,train loss is: 0.0003283709840438112\n",
      "test loss is 0.0011264242047557837\n",
      "Batch: 24200,train loss is: 0.004636147031539646\n",
      "test loss is 0.0008218750185672823\n",
      "Batch: 24300,train loss is: 0.0019771728306675704\n",
      "test loss is 0.0012673700703155107\n",
      "Batch: 24400,train loss is: 0.002576919060635382\n",
      "test loss is 0.0014442395777387125\n",
      "Batch: 24500,train loss is: 0.0013087571287534246\n",
      "test loss is 0.001497459471130364\n",
      "Batch: 24600,train loss is: 0.0013217726809609646\n",
      "test loss is 0.001082025037752312\n",
      "Batch: 24700,train loss is: 0.0005829288395191563\n",
      "test loss is 0.0012501099056986982\n",
      "Batch: 24800,train loss is: 0.002124730982951273\n",
      "test loss is 0.002066393247639029\n",
      "Batch: 24900,train loss is: 0.0008530689899932379\n",
      "test loss is 0.0055367861766447285\n",
      "Batch: 25000,train loss is: 0.0034266487809215076\n",
      "test loss is 0.004937256532433479\n",
      "Batch: 25100,train loss is: 0.0003381104880955422\n",
      "test loss is 0.002392490591753369\n",
      "Batch: 25200,train loss is: 0.001332835623207544\n",
      "test loss is 0.0011165533011580206\n",
      "Batch: 25300,train loss is: 0.0019284030244595982\n",
      "test loss is 0.001700704491427431\n",
      "Batch: 25400,train loss is: 0.001274490697294079\n",
      "test loss is 0.0009078212514775445\n",
      "Batch: 25500,train loss is: 0.0003778085540445451\n",
      "test loss is 0.0007069683731988373\n",
      "Batch: 25600,train loss is: 0.0014898677873814979\n",
      "test loss is 0.0009653687906016596\n",
      "Batch: 25700,train loss is: 0.0005910893547667195\n",
      "test loss is 0.0007068560550626514\n",
      "Batch: 25800,train loss is: 0.000764576704852411\n",
      "test loss is 0.0009331855703614515\n",
      "Batch: 25900,train loss is: 0.0006000799266064658\n",
      "test loss is 0.0011753722743298059\n",
      "Batch: 26000,train loss is: 0.0010232173952803502\n",
      "test loss is 0.005571830387889693\n",
      "Batch: 26100,train loss is: 0.000995550047796064\n",
      "test loss is 0.0015608065384798467\n",
      "Batch: 26200,train loss is: 0.001568158612013261\n",
      "test loss is 0.0010334942164048368\n",
      "Batch: 26300,train loss is: 0.0011302540022238162\n",
      "test loss is 0.0008390934171789319\n",
      "Batch: 26400,train loss is: 0.0007470612463466357\n",
      "test loss is 0.0010361223528647294\n",
      "Batch: 26500,train loss is: 0.0006636706563027605\n",
      "test loss is 0.0011920145787830765\n",
      "Batch: 26600,train loss is: 0.006464875596727737\n",
      "test loss is 0.0031006776074641884\n",
      "Batch: 26700,train loss is: 0.0005579017245149664\n",
      "test loss is 0.0008685139922766799\n",
      "Batch: 26800,train loss is: 0.0008108605988543301\n",
      "test loss is 0.0007898121081596718\n",
      "Batch: 26900,train loss is: 0.0005784172938190232\n",
      "test loss is 0.0011953024954212412\n",
      "Batch: 27000,train loss is: 0.0006686597000073779\n",
      "test loss is 0.0010406272149145034\n",
      "Batch: 27100,train loss is: 0.00044792987415899835\n",
      "test loss is 0.0009068557224482016\n",
      "Batch: 27200,train loss is: 0.001590370173876901\n",
      "test loss is 0.0014904228816834592\n",
      "Batch: 27300,train loss is: 0.0025384751708445287\n",
      "test loss is 0.003966318055740153\n",
      "Batch: 27400,train loss is: 0.002029255292491261\n",
      "test loss is 0.0027816652885728557\n",
      "Batch: 27500,train loss is: 0.0010392318475602755\n",
      "test loss is 0.0009614740554904884\n",
      "Batch: 27600,train loss is: 0.0011936302737065591\n",
      "test loss is 0.001350602813034762\n",
      "Batch: 27700,train loss is: 0.003119085621938064\n",
      "test loss is 0.0024686209743388535\n",
      "Batch: 27800,train loss is: 0.0012310851987644277\n",
      "test loss is 0.0033719954552110672\n",
      "Batch: 27900,train loss is: 0.0012796630741473665\n",
      "test loss is 0.0006637328967708759\n",
      "Batch: 28000,train loss is: 0.0008000771821236783\n",
      "test loss is 0.0018997053316548759\n",
      "Batch: 28100,train loss is: 0.0020013978027358962\n",
      "test loss is 0.0009764337785614901\n",
      "Batch: 28200,train loss is: 0.0009341043434848422\n",
      "test loss is 0.0018675799980316745\n",
      "Batch: 28300,train loss is: 0.0025945146910840637\n",
      "test loss is 0.0006878581437724311\n",
      "Batch: 28400,train loss is: 0.0016468832783989855\n",
      "test loss is 0.0009215841696564104\n",
      "Batch: 28500,train loss is: 0.0007342834582464366\n",
      "test loss is 0.0007138944467205764\n",
      "Batch: 28600,train loss is: 0.0006014398398287776\n",
      "test loss is 0.001006835519688724\n",
      "Batch: 28700,train loss is: 0.000639403388693801\n",
      "test loss is 0.0014194471327890342\n",
      "Batch: 28800,train loss is: 0.003911057057954845\n",
      "test loss is 0.0008795184548079599\n",
      "Batch: 28900,train loss is: 0.0009280439801382788\n",
      "test loss is 0.002631288620687335\n",
      "Batch: 29000,train loss is: 0.0010206243402707898\n",
      "test loss is 0.001410453960514062\n",
      "Batch: 29100,train loss is: 0.0029784840639743926\n",
      "test loss is 0.008628260323669364\n",
      "Batch: 29200,train loss is: 0.0004324982109330704\n",
      "test loss is 0.0010806092937609923\n",
      "Batch: 29300,train loss is: 0.0012905405055943587\n",
      "test loss is 0.0011740310437061583\n",
      "Batch: 29400,train loss is: 0.0005093733186926682\n",
      "test loss is 0.003475285107102791\n",
      "Batch: 29500,train loss is: 0.0007684474005990132\n",
      "test loss is 0.0017788053714365498\n",
      "Batch: 29600,train loss is: 0.0015715765858893755\n",
      "test loss is 0.0009054135703864885\n",
      "Batch: 29700,train loss is: 0.0002820559311810386\n",
      "test loss is 0.0009108035979582526\n",
      "Batch: 29800,train loss is: 0.0012487220335768094\n",
      "test loss is 0.0008910635664506078\n",
      "Batch: 29900,train loss is: 0.0012344149186974006\n",
      "test loss is 0.0009685117495434481\n",
      "Batch: 30000,train loss is: 0.0005001710588755607\n",
      "test loss is 0.0006164651030706433\n",
      "Batch: 30100,train loss is: 0.0014305656857600596\n",
      "test loss is 0.0021840373626654786\n",
      "Batch: 30200,train loss is: 0.0006735678772318503\n",
      "test loss is 0.0007947563800602716\n",
      "Batch: 30300,train loss is: 0.0004723110958504971\n",
      "test loss is 0.0011705544254227535\n",
      "Batch: 30400,train loss is: 0.0038317967558588136\n",
      "test loss is 0.0064382390364743575\n",
      "Batch: 30500,train loss is: 0.0005665780254066423\n",
      "test loss is 0.0009013419197101379\n",
      "Batch: 30600,train loss is: 0.000681088297125966\n",
      "test loss is 0.0020584877134952506\n",
      "Batch: 30700,train loss is: 0.0008740311512213858\n",
      "test loss is 0.0012529017907530102\n",
      "Batch: 30800,train loss is: 0.001162529522442149\n",
      "test loss is 0.0012217062623047868\n",
      "Batch: 30900,train loss is: 0.0007602061014241767\n",
      "test loss is 0.0013303554967792996\n",
      "Batch: 31000,train loss is: 0.00041298230128110244\n",
      "test loss is 0.0006357702282466509\n",
      "Batch: 31100,train loss is: 0.0008758376901968207\n",
      "test loss is 0.0015728014445528242\n",
      "Batch: 31200,train loss is: 0.000861573824905827\n",
      "test loss is 0.000915476092483951\n",
      "Batch: 31300,train loss is: 0.00045776636864794243\n",
      "test loss is 0.0010448932422494322\n",
      "Batch: 31400,train loss is: 0.0014940644295558011\n",
      "test loss is 0.002986521468866972\n",
      "Batch: 31500,train loss is: 0.0006799877066021165\n",
      "test loss is 0.0015813433464033466\n",
      "Batch: 31600,train loss is: 0.0011560962405638131\n",
      "test loss is 0.0008456611953797518\n",
      "Batch: 31700,train loss is: 0.00026868225086454925\n",
      "test loss is 0.0008246518338265182\n",
      "Batch: 31800,train loss is: 0.0012758352498214876\n",
      "test loss is 0.001591468481853816\n",
      "Batch: 31900,train loss is: 0.00025633060700311144\n",
      "test loss is 0.0007606804951263657\n",
      "Batch: 32000,train loss is: 0.00353206636511108\n",
      "test loss is 0.0006590307707531831\n",
      "Batch: 32100,train loss is: 0.0005657814101504801\n",
      "test loss is 0.0013504980189172336\n",
      "Batch: 32200,train loss is: 0.00024353094768342362\n",
      "test loss is 0.0008144416470967047\n",
      "Batch: 32300,train loss is: 0.0006311457263917104\n",
      "test loss is 0.0012040152462651627\n",
      "Batch: 32400,train loss is: 0.00042807949797602217\n",
      "test loss is 0.0012866421412146927\n",
      "Batch: 32500,train loss is: 0.0005831524729295989\n",
      "test loss is 0.0009456960220395403\n",
      "Batch: 32600,train loss is: 0.0007944350243634064\n",
      "test loss is 0.0007582057088151705\n",
      "Batch: 32700,train loss is: 0.0006134169178702436\n",
      "test loss is 0.0017263770508372518\n",
      "Batch: 32800,train loss is: 0.0004870451357764916\n",
      "test loss is 0.0009534546229800255\n",
      "Batch: 32900,train loss is: 0.001097026506165235\n",
      "test loss is 0.000902634943715246\n",
      "Batch: 33000,train loss is: 0.000632012744583529\n",
      "test loss is 0.0006910283483188522\n",
      "Batch: 33100,train loss is: 0.00088609049509553\n",
      "test loss is 0.001835165433074351\n",
      "Batch: 33200,train loss is: 0.002200439102748018\n",
      "test loss is 0.0018443649924355084\n",
      "Batch: 33300,train loss is: 0.0010462365048399724\n",
      "test loss is 0.0008928991874987142\n",
      "Batch: 33400,train loss is: 0.000741650138254792\n",
      "test loss is 0.003394263552710417\n",
      "Batch: 33500,train loss is: 0.00046700242328414244\n",
      "test loss is 0.001423630591327345\n",
      "Batch: 33600,train loss is: 0.00029536958156021353\n",
      "test loss is 0.0008877797858937814\n",
      "Batch: 33700,train loss is: 0.0006111603833816173\n",
      "test loss is 0.0008953364101436091\n",
      "Batch: 33800,train loss is: 0.0015509499253221774\n",
      "test loss is 0.0012946055411579738\n",
      "Batch: 33900,train loss is: 0.0023213459810964183\n",
      "test loss is 0.0008310688058712808\n",
      "-----------------------Epoch: 3----------------------------------\n",
      "Batch: 0,train loss is: 0.0009887950528434048\n",
      "test loss is 0.0007886121384056726\n",
      "Batch: 100,train loss is: 0.0008324882294039825\n",
      "test loss is 0.0006439119571274402\n",
      "Batch: 200,train loss is: 0.002408989548600572\n",
      "test loss is 0.00150121541855467\n",
      "Batch: 300,train loss is: 0.0016158965881793404\n",
      "test loss is 0.0017406466040517973\n",
      "Batch: 400,train loss is: 0.0003894523783397279\n",
      "test loss is 0.00109313002425123\n",
      "Batch: 500,train loss is: 0.0017139976164306132\n",
      "test loss is 0.004874641912730487\n",
      "Batch: 600,train loss is: 0.0006900699506138762\n",
      "test loss is 0.0031098391948292566\n",
      "Batch: 700,train loss is: 0.0006539109272905643\n",
      "test loss is 0.0007943896679779216\n",
      "Batch: 800,train loss is: 0.0011028556578619507\n",
      "test loss is 0.0011035202848378475\n",
      "Batch: 900,train loss is: 0.0007995789948372752\n",
      "test loss is 0.0009215927643456133\n",
      "Batch: 1000,train loss is: 0.00045190174417694467\n",
      "test loss is 0.0008077137700423368\n",
      "Batch: 1100,train loss is: 0.0017358344651066658\n",
      "test loss is 0.0010580309078942484\n",
      "Batch: 1200,train loss is: 0.00019923216346985664\n",
      "test loss is 0.0012278781508278992\n",
      "Batch: 1300,train loss is: 0.0028431939055758885\n",
      "test loss is 0.001565789568866989\n",
      "Batch: 1400,train loss is: 0.0006266412877159667\n",
      "test loss is 0.001018760886730326\n",
      "Batch: 1500,train loss is: 0.0028118078660971145\n",
      "test loss is 0.003468632444042022\n",
      "Batch: 1600,train loss is: 0.0009843906288930773\n",
      "test loss is 0.0008089058530279563\n",
      "Batch: 1700,train loss is: 0.0013755677308499692\n",
      "test loss is 0.0010240066889711835\n",
      "Batch: 1800,train loss is: 0.0003892513898150854\n",
      "test loss is 0.001278410347446739\n",
      "Batch: 1900,train loss is: 0.0015488507178051968\n",
      "test loss is 0.0027595139089642744\n",
      "Batch: 2000,train loss is: 0.00409468142024992\n",
      "test loss is 0.005530319541844569\n",
      "Batch: 2100,train loss is: 0.0008637174750629615\n",
      "test loss is 0.0007589837646269459\n",
      "Batch: 2200,train loss is: 0.0005314134507705816\n",
      "test loss is 0.0005873404677782979\n",
      "Batch: 2300,train loss is: 0.00043916874537408904\n",
      "test loss is 0.0006134792304448755\n",
      "Batch: 2400,train loss is: 0.0002586938779264653\n",
      "test loss is 0.000602337562780185\n",
      "Batch: 2500,train loss is: 0.0006114180313228838\n",
      "test loss is 0.0007115629012066889\n",
      "Batch: 2600,train loss is: 0.0017412692602600994\n",
      "test loss is 0.0007052526452135508\n",
      "Batch: 2700,train loss is: 0.000423113721547479\n",
      "test loss is 0.000692728985711018\n",
      "Batch: 2800,train loss is: 0.0004945384759773247\n",
      "test loss is 0.0007918493179703511\n",
      "Batch: 2900,train loss is: 0.00045497868347330793\n",
      "test loss is 0.0007905049527178045\n",
      "Batch: 3000,train loss is: 0.0006412140013042996\n",
      "test loss is 0.0015611884069026655\n",
      "Batch: 3100,train loss is: 0.0007445619916589762\n",
      "test loss is 0.001621713203196535\n",
      "Batch: 3200,train loss is: 0.0005270294287301437\n",
      "test loss is 0.0006376918504078516\n",
      "Batch: 3300,train loss is: 0.0004177904982397255\n",
      "test loss is 0.0007644261804651001\n",
      "Batch: 3400,train loss is: 0.0009619220459783694\n",
      "test loss is 0.0011333360683668647\n",
      "Batch: 3500,train loss is: 0.0006619804022983952\n",
      "test loss is 0.001235037694230604\n",
      "Batch: 3600,train loss is: 0.00044373487590881417\n",
      "test loss is 0.0009963355816028203\n",
      "Batch: 3700,train loss is: 0.0015102690790682548\n",
      "test loss is 0.0019732422607113345\n",
      "Batch: 3800,train loss is: 0.0011451205770766704\n",
      "test loss is 0.0015064526517005765\n",
      "Batch: 3900,train loss is: 0.0015303010072663021\n",
      "test loss is 0.0012616068972273347\n",
      "Batch: 4000,train loss is: 0.00038824362569961753\n",
      "test loss is 0.0007689185522013445\n",
      "Batch: 4100,train loss is: 0.0012004685684551882\n",
      "test loss is 0.0006977216179473844\n",
      "Batch: 4200,train loss is: 0.001604786526957494\n",
      "test loss is 0.0007343265273170044\n",
      "Batch: 4300,train loss is: 0.0004928875913811192\n",
      "test loss is 0.0011538644266529125\n",
      "Batch: 4400,train loss is: 0.0011312088542446475\n",
      "test loss is 0.0009687086693083854\n",
      "Batch: 4500,train loss is: 0.000278885435381724\n",
      "test loss is 0.0009155755707349704\n",
      "Batch: 4600,train loss is: 0.013564896365696594\n",
      "test loss is 0.013029388676023556\n",
      "Batch: 4700,train loss is: 0.0015437090837836175\n",
      "test loss is 0.0016288120425074998\n",
      "Batch: 4800,train loss is: 0.003524142600685617\n",
      "test loss is 0.0035062139264812058\n",
      "Batch: 4900,train loss is: 0.002948781712373952\n",
      "test loss is 0.0022218004457088417\n",
      "Batch: 5000,train loss is: 0.0003756673120481715\n",
      "test loss is 0.0008215335565445241\n",
      "Batch: 5100,train loss is: 0.00048464468422220543\n",
      "test loss is 0.0006368520309015329\n",
      "Batch: 5200,train loss is: 0.0006748363543951882\n",
      "test loss is 0.0009988704568520048\n",
      "Batch: 5300,train loss is: 0.002176173529266527\n",
      "test loss is 0.0016675307926171798\n",
      "Batch: 5400,train loss is: 0.0013007036260592225\n",
      "test loss is 0.0012716703445083345\n",
      "Batch: 5500,train loss is: 0.0009379744502275222\n",
      "test loss is 0.001037479913515278\n",
      "Batch: 5600,train loss is: 0.0013040881591092023\n",
      "test loss is 0.0007276688113323186\n",
      "Batch: 5700,train loss is: 0.0006629914923922727\n",
      "test loss is 0.0009067288486812058\n",
      "Batch: 5800,train loss is: 0.0004050908464176675\n",
      "test loss is 0.0014398412624468233\n",
      "Batch: 5900,train loss is: 0.0016732969016195357\n",
      "test loss is 0.0019852790273642805\n",
      "Batch: 6000,train loss is: 0.0008430417656824214\n",
      "test loss is 0.0011055509839122944\n",
      "Batch: 6100,train loss is: 0.0004232458353094975\n",
      "test loss is 0.0009394066252543615\n",
      "Batch: 6200,train loss is: 0.0011406732273510892\n",
      "test loss is 0.0011128765517042227\n",
      "Batch: 6300,train loss is: 0.0015090896570000623\n",
      "test loss is 0.001593643900995213\n",
      "Batch: 6400,train loss is: 0.0010425877611631481\n",
      "test loss is 0.0020347365313071763\n",
      "Batch: 6500,train loss is: 0.000711438949219932\n",
      "test loss is 0.0009767453614188733\n",
      "Batch: 6600,train loss is: 0.011982331650149104\n",
      "test loss is 0.0015493218277246475\n",
      "Batch: 6700,train loss is: 0.0003684743785138514\n",
      "test loss is 0.0014985443491680234\n",
      "Batch: 6800,train loss is: 0.0011468963937335973\n",
      "test loss is 0.0010348968662073313\n",
      "Batch: 6900,train loss is: 0.0012940497564027178\n",
      "test loss is 0.0006805619047036911\n",
      "Batch: 7000,train loss is: 0.001716035352394\n",
      "test loss is 0.0010037707583844427\n",
      "Batch: 7100,train loss is: 0.0010058202344467601\n",
      "test loss is 0.001119122498735288\n",
      "Batch: 7200,train loss is: 0.00044497339002099676\n",
      "test loss is 0.0005909059289616976\n",
      "Batch: 7300,train loss is: 0.0021903878260409997\n",
      "test loss is 0.000827935542238748\n",
      "Batch: 7400,train loss is: 0.0011485683954156835\n",
      "test loss is 0.001591642360940557\n",
      "Batch: 7500,train loss is: 0.0010709145009416504\n",
      "test loss is 0.001905547195862816\n",
      "Batch: 7600,train loss is: 0.0027764204581798356\n",
      "test loss is 0.0030571152803194586\n",
      "Batch: 7700,train loss is: 0.0012859644127218687\n",
      "test loss is 0.0008622625418579623\n",
      "Batch: 7800,train loss is: 0.000675243737989799\n",
      "test loss is 0.0007868731796306422\n",
      "Batch: 7900,train loss is: 0.0009177292124411001\n",
      "test loss is 0.0006333898653578347\n",
      "Batch: 8000,train loss is: 0.0008636139299512971\n",
      "test loss is 0.0018314813270291859\n",
      "Batch: 8100,train loss is: 0.0007636205785207048\n",
      "test loss is 0.0008994207473622851\n",
      "Batch: 8200,train loss is: 0.0016555165701605484\n",
      "test loss is 0.0008406583917418802\n",
      "Batch: 8300,train loss is: 0.0005782949671706456\n",
      "test loss is 0.0009141500271290582\n",
      "Batch: 8400,train loss is: 0.0009716504707932673\n",
      "test loss is 0.0009069749927026479\n",
      "Batch: 8500,train loss is: 0.0008906149532380855\n",
      "test loss is 0.0009311699997164077\n",
      "Batch: 8600,train loss is: 0.00034410712540053573\n",
      "test loss is 0.0008245882162838679\n",
      "Batch: 8700,train loss is: 0.0002817077311135708\n",
      "test loss is 0.0016086013613192732\n",
      "Batch: 8800,train loss is: 0.0007628807129915007\n",
      "test loss is 0.0007335126850545081\n",
      "Batch: 8900,train loss is: 0.0009742895373699888\n",
      "test loss is 0.000870828743554932\n",
      "Batch: 9000,train loss is: 0.00039389757664292074\n",
      "test loss is 0.0009012415438925893\n",
      "Batch: 9100,train loss is: 0.0018892756797662166\n",
      "test loss is 0.003151811563690254\n",
      "Batch: 9200,train loss is: 0.000998852865083103\n",
      "test loss is 0.004793870606834134\n",
      "Batch: 9300,train loss is: 0.0013430807832648436\n",
      "test loss is 0.000843567975936013\n",
      "Batch: 9400,train loss is: 0.0005999535530430003\n",
      "test loss is 0.0007609644710595337\n",
      "Batch: 9500,train loss is: 0.0008702735847599354\n",
      "test loss is 0.0017973038469453946\n",
      "Batch: 9600,train loss is: 0.0007003722733933736\n",
      "test loss is 0.001102484661126513\n",
      "Batch: 9700,train loss is: 0.00030579729169418596\n",
      "test loss is 0.0007347885483344642\n",
      "Batch: 9800,train loss is: 0.0002965281223044349\n",
      "test loss is 0.0010634087120867106\n",
      "Batch: 9900,train loss is: 0.0008928353061940878\n",
      "test loss is 0.0009200735260348303\n",
      "Batch: 10000,train loss is: 0.00026765294505516624\n",
      "test loss is 0.0008125675801564003\n",
      "Batch: 10100,train loss is: 0.0014915400500958747\n",
      "test loss is 0.00071633959120842\n",
      "Batch: 10200,train loss is: 0.001458412275963134\n",
      "test loss is 0.0012115593345823654\n",
      "Batch: 10300,train loss is: 0.0013538293524305094\n",
      "test loss is 0.0015264857278568544\n",
      "Batch: 10400,train loss is: 0.0021569889557516426\n",
      "test loss is 0.0032195218131302926\n",
      "Batch: 10500,train loss is: 0.0010721332212088126\n",
      "test loss is 0.0011858165020162922\n",
      "Batch: 10600,train loss is: 0.00032926316676854775\n",
      "test loss is 0.0015967141535159289\n",
      "Batch: 10700,train loss is: 0.0006052773588732785\n",
      "test loss is 0.0011141704393838768\n",
      "Batch: 10800,train loss is: 0.003787568116860927\n",
      "test loss is 0.007333946310970659\n",
      "Batch: 10900,train loss is: 0.0004965450082497084\n",
      "test loss is 0.0008892271417535704\n",
      "Batch: 11000,train loss is: 0.0005954951827774073\n",
      "test loss is 0.0008135983887168866\n",
      "Batch: 11100,train loss is: 0.0011593180937361257\n",
      "test loss is 0.0008673047917913253\n",
      "Batch: 11200,train loss is: 0.0005836092120011159\n",
      "test loss is 0.0006104098621876019\n",
      "Batch: 11300,train loss is: 0.0006509437253023608\n",
      "test loss is 0.000986535958760716\n",
      "Batch: 11400,train loss is: 0.0009702152608163376\n",
      "test loss is 0.0006439273314749435\n",
      "Batch: 11500,train loss is: 0.0003641532043647872\n",
      "test loss is 0.001279888274487051\n",
      "Batch: 11600,train loss is: 0.0032194726586277254\n",
      "test loss is 0.0013013095433266938\n",
      "Batch: 11700,train loss is: 0.0002673610777562208\n",
      "test loss is 0.0008622017041420194\n",
      "Batch: 11800,train loss is: 0.0005223966195158111\n",
      "test loss is 0.0007776530429737335\n",
      "Batch: 11900,train loss is: 0.0007896654327727737\n",
      "test loss is 0.0010498634521761095\n",
      "Batch: 12000,train loss is: 0.0010276407556253414\n",
      "test loss is 0.0013128384704032584\n",
      "Batch: 12100,train loss is: 0.0018383504709711\n",
      "test loss is 0.0007693204776031109\n",
      "Batch: 12200,train loss is: 0.001067069706425724\n",
      "test loss is 0.001793788262055545\n",
      "Batch: 12300,train loss is: 0.0030192529516076465\n",
      "test loss is 0.004815190265705546\n",
      "Batch: 12400,train loss is: 0.0039303100058963375\n",
      "test loss is 0.00536871826116567\n",
      "Batch: 12500,train loss is: 0.0014066381883048138\n",
      "test loss is 0.00950181676892009\n",
      "Batch: 12600,train loss is: 0.0010922054389008347\n",
      "test loss is 0.0012087569718359156\n",
      "Batch: 12700,train loss is: 0.0006071462006098576\n",
      "test loss is 0.0007775043835910582\n",
      "Batch: 12800,train loss is: 0.00031163071182472554\n",
      "test loss is 0.0011433781636008946\n",
      "Batch: 12900,train loss is: 0.0010090190603910102\n",
      "test loss is 0.0008605092036140177\n",
      "Batch: 13000,train loss is: 0.0003279505053244021\n",
      "test loss is 0.0006038114596329929\n",
      "Batch: 13100,train loss is: 0.0005627873316119378\n",
      "test loss is 0.0005341350027308922\n",
      "Batch: 13200,train loss is: 0.0004950137178453838\n",
      "test loss is 0.0007927748182321942\n",
      "Batch: 13300,train loss is: 0.00034583888679238654\n",
      "test loss is 0.0008511997294723439\n",
      "Batch: 13400,train loss is: 0.0005152781830295535\n",
      "test loss is 0.0007921066510972129\n",
      "Batch: 13500,train loss is: 0.0007579402149722189\n",
      "test loss is 0.0007565894510231532\n",
      "Batch: 13600,train loss is: 0.0019006307867521033\n",
      "test loss is 0.0014753834244279392\n",
      "Batch: 13700,train loss is: 0.0005554828622479457\n",
      "test loss is 0.0007556263223237046\n",
      "Batch: 13800,train loss is: 0.00015736018258001054\n",
      "test loss is 0.0006912757424443889\n",
      "Batch: 13900,train loss is: 0.00034000454900165244\n",
      "test loss is 0.0006088063478923054\n",
      "Batch: 14000,train loss is: 0.0002880441910340285\n",
      "test loss is 0.0009778137212625789\n",
      "Batch: 14100,train loss is: 0.00046836529178540914\n",
      "test loss is 0.0009703484470956916\n",
      "Batch: 14200,train loss is: 0.0004185156900239711\n",
      "test loss is 0.0005222498727027912\n",
      "Batch: 14300,train loss is: 0.00028499429543137576\n",
      "test loss is 0.0006705456606063582\n",
      "Batch: 14400,train loss is: 0.0012692444356687605\n",
      "test loss is 0.0011246752023812653\n",
      "Batch: 14500,train loss is: 0.0002621334609979575\n",
      "test loss is 0.0005884673338670557\n",
      "Batch: 14600,train loss is: 0.00544204354295558\n",
      "test loss is 0.002427880045168795\n",
      "Batch: 14700,train loss is: 0.0008526971803232248\n",
      "test loss is 0.006818955931700708\n",
      "Batch: 14800,train loss is: 0.0007026818237096386\n",
      "test loss is 0.001565584293008708\n",
      "Batch: 14900,train loss is: 0.000693189316247357\n",
      "test loss is 0.0014720800213100823\n",
      "Batch: 15000,train loss is: 0.0005843890985112855\n",
      "test loss is 0.0009668177559315179\n",
      "Batch: 15100,train loss is: 0.0011084221539395362\n",
      "test loss is 0.0007876085055063538\n",
      "Batch: 15200,train loss is: 0.0006975614794383293\n",
      "test loss is 0.0005876772804777518\n",
      "Batch: 15300,train loss is: 0.0006127659994536087\n",
      "test loss is 0.0005209769650788805\n",
      "Batch: 15400,train loss is: 0.0004561750115182068\n",
      "test loss is 0.0006528448248883078\n",
      "Batch: 15500,train loss is: 0.0008049537689348081\n",
      "test loss is 0.0007787404174336276\n",
      "Batch: 15600,train loss is: 0.0013533548233771776\n",
      "test loss is 0.0009101073669962434\n",
      "Batch: 15700,train loss is: 0.0009516726369853913\n",
      "test loss is 0.001266465382624986\n",
      "Batch: 15800,train loss is: 0.001340166132686263\n",
      "test loss is 0.0011538562496286732\n",
      "Batch: 15900,train loss is: 0.0010233509220960314\n",
      "test loss is 0.000814254696872796\n",
      "Batch: 16000,train loss is: 0.0004590028818050503\n",
      "test loss is 0.0005607739662891383\n",
      "Batch: 16100,train loss is: 0.001963924344949712\n",
      "test loss is 0.0008246768539209191\n",
      "Batch: 16200,train loss is: 0.0007428827865131288\n",
      "test loss is 0.0009870739419318953\n",
      "Batch: 16300,train loss is: 0.0003267330163923443\n",
      "test loss is 0.0005645701794398972\n",
      "Batch: 16400,train loss is: 0.00047056811181214476\n",
      "test loss is 0.0005201872646519345\n",
      "Batch: 16500,train loss is: 0.0028125983001472507\n",
      "test loss is 0.0016754565028141283\n",
      "Batch: 16600,train loss is: 0.00032994164174636844\n",
      "test loss is 0.0012982195171781614\n",
      "Batch: 16700,train loss is: 0.004040940102126101\n",
      "test loss is 0.0039919231860415984\n",
      "Batch: 16800,train loss is: 0.00045755740651749825\n",
      "test loss is 0.000704507927775677\n",
      "Batch: 16900,train loss is: 0.001825667560083941\n",
      "test loss is 0.002260018539810125\n",
      "Batch: 17000,train loss is: 0.0005807754705536137\n",
      "test loss is 0.0008822548253929557\n",
      "Batch: 17100,train loss is: 0.0008667629951066462\n",
      "test loss is 0.0012068187247613964\n",
      "Batch: 17200,train loss is: 0.00025196912416085206\n",
      "test loss is 0.0005217325818571279\n",
      "Batch: 17300,train loss is: 0.0009277343503102973\n",
      "test loss is 0.0006556189026365227\n",
      "Batch: 17400,train loss is: 0.0009310526223249549\n",
      "test loss is 0.0009352618667480224\n",
      "Batch: 17500,train loss is: 0.0010564594302909097\n",
      "test loss is 0.0010192086401484683\n",
      "Batch: 17600,train loss is: 0.0014262766780151666\n",
      "test loss is 0.0019000451299989783\n",
      "Batch: 17700,train loss is: 0.0008842820192110553\n",
      "test loss is 0.001125976905630138\n",
      "Batch: 17800,train loss is: 0.0011915654232474346\n",
      "test loss is 0.0019129095486966259\n",
      "Batch: 17900,train loss is: 0.0015129418977172\n",
      "test loss is 0.001381995714442888\n",
      "Batch: 18000,train loss is: 0.001130760394727797\n",
      "test loss is 0.0006959777661639667\n",
      "Batch: 18100,train loss is: 0.00034310608677167164\n",
      "test loss is 0.0006506224311986568\n",
      "Batch: 18200,train loss is: 0.000403917585764945\n",
      "test loss is 0.000964852619637783\n",
      "Batch: 18300,train loss is: 0.001098252408224526\n",
      "test loss is 0.0006380095200375738\n",
      "Batch: 18400,train loss is: 0.00033675443269999366\n",
      "test loss is 0.0007944193175008128\n",
      "Batch: 18500,train loss is: 0.005522855207174526\n",
      "test loss is 0.004712070516678117\n",
      "Batch: 18600,train loss is: 0.002367641493778091\n",
      "test loss is 0.003636064066220321\n",
      "Batch: 18700,train loss is: 0.000843552840486415\n",
      "test loss is 0.0009747605941213209\n",
      "Batch: 18800,train loss is: 0.0005812675027574058\n",
      "test loss is 0.0006034821383291051\n",
      "Batch: 18900,train loss is: 0.0012234664894052506\n",
      "test loss is 0.0008583628514876485\n",
      "Batch: 19000,train loss is: 0.0005234576677470332\n",
      "test loss is 0.0006093139541454928\n",
      "Batch: 19100,train loss is: 0.0008083930462107124\n",
      "test loss is 0.0008442847253305851\n",
      "Batch: 19200,train loss is: 0.0007706062728916956\n",
      "test loss is 0.0009316549362354747\n",
      "Batch: 19300,train loss is: 0.002906878601526393\n",
      "test loss is 0.004467749562797128\n",
      "Batch: 19400,train loss is: 0.0002706253514494913\n",
      "test loss is 0.0008733108953993836\n",
      "Batch: 19500,train loss is: 0.00044962470058502747\n",
      "test loss is 0.0010507795610066202\n",
      "Batch: 19600,train loss is: 0.0014283039890518108\n",
      "test loss is 0.0011797258411969825\n",
      "Batch: 19700,train loss is: 0.001845954505837116\n",
      "test loss is 0.0017427981604945409\n",
      "Batch: 19800,train loss is: 0.000615513521422838\n",
      "test loss is 0.000669190345117047\n",
      "Batch: 19900,train loss is: 0.0009440068974826007\n",
      "test loss is 0.0011304808789966648\n",
      "Batch: 20000,train loss is: 0.0007367074546416274\n",
      "test loss is 0.0008762507969082632\n",
      "Batch: 20100,train loss is: 0.0006258116625423081\n",
      "test loss is 0.0007009607902888814\n",
      "Batch: 20200,train loss is: 0.0004850913575358802\n",
      "test loss is 0.0007292286313861629\n",
      "Batch: 20300,train loss is: 0.0006452646239712446\n",
      "test loss is 0.0014298710290349091\n",
      "Batch: 20400,train loss is: 0.002293882996563413\n",
      "test loss is 0.00454892172699104\n",
      "Batch: 20500,train loss is: 0.0007776091531553075\n",
      "test loss is 0.0012037461672048495\n",
      "Batch: 20600,train loss is: 0.0016103402690841497\n",
      "test loss is 0.0008150378043086951\n",
      "Batch: 20700,train loss is: 0.000999137844482121\n",
      "test loss is 0.0016839078679219742\n",
      "Batch: 20800,train loss is: 0.0008607438031187797\n",
      "test loss is 0.000710207735600807\n",
      "Batch: 20900,train loss is: 0.00027151341407414544\n",
      "test loss is 0.0007741803611786661\n",
      "Batch: 21000,train loss is: 0.000420482460119949\n",
      "test loss is 0.000692648272254486\n",
      "Batch: 21100,train loss is: 0.00043152854902540183\n",
      "test loss is 0.0005267706286898586\n",
      "Batch: 21200,train loss is: 0.0003872436965985581\n",
      "test loss is 0.0007580118808152043\n",
      "Batch: 21300,train loss is: 0.0004756238668954287\n",
      "test loss is 0.000756384524599214\n",
      "Batch: 21400,train loss is: 0.0004606758181904171\n",
      "test loss is 0.0006222536439604802\n",
      "Batch: 21500,train loss is: 0.00046809389574979914\n",
      "test loss is 0.0007178842529840301\n",
      "Batch: 21600,train loss is: 0.0003614812778201007\n",
      "test loss is 0.0009647002966941863\n",
      "Batch: 21700,train loss is: 0.00037069821116407796\n",
      "test loss is 0.0009127301086709898\n",
      "Batch: 21800,train loss is: 0.0008428792047401803\n",
      "test loss is 0.0006801326199623066\n",
      "Batch: 21900,train loss is: 0.00035793653715896726\n",
      "test loss is 0.0006569566114879664\n",
      "Batch: 22000,train loss is: 0.0004785891896292096\n",
      "test loss is 0.00239777286702016\n",
      "Batch: 22100,train loss is: 0.0002885253522100372\n",
      "test loss is 0.0010369649649551583\n",
      "Batch: 22200,train loss is: 0.0015709729442596678\n",
      "test loss is 0.001186617460596682\n",
      "Batch: 22300,train loss is: 0.004580872441523694\n",
      "test loss is 0.00410693818409882\n",
      "Batch: 22400,train loss is: 0.0005515752058074269\n",
      "test loss is 0.001136168550545483\n",
      "Batch: 22500,train loss is: 0.000566204516922783\n",
      "test loss is 0.0008719277450394189\n",
      "Batch: 22600,train loss is: 0.00138247058849879\n",
      "test loss is 0.000893479306961423\n",
      "Batch: 22700,train loss is: 0.0009047267807773352\n",
      "test loss is 0.0009189638248904791\n",
      "Batch: 22800,train loss is: 0.0006144856856135132\n",
      "test loss is 0.0007839579708953153\n",
      "Batch: 22900,train loss is: 0.0009000167169546195\n",
      "test loss is 0.0015613416173863\n",
      "Batch: 23000,train loss is: 0.0013317697176518082\n",
      "test loss is 0.000566948280468607\n",
      "Batch: 23100,train loss is: 0.000544801487530569\n",
      "test loss is 0.0011242987943537483\n",
      "Batch: 23200,train loss is: 0.0003395833217718333\n",
      "test loss is 0.0007757983209702941\n",
      "Batch: 23300,train loss is: 0.0003650920343008132\n",
      "test loss is 0.0005192759180087303\n",
      "Batch: 23400,train loss is: 0.0005928759690132383\n",
      "test loss is 0.0007297155359252273\n",
      "Batch: 23500,train loss is: 0.0032859219360952163\n",
      "test loss is 0.0006480958945630979\n",
      "Batch: 23600,train loss is: 0.0005197146819908524\n",
      "test loss is 0.0007516751647320522\n",
      "Batch: 23700,train loss is: 0.000731360139340777\n",
      "test loss is 0.0010203074069952802\n",
      "Batch: 23800,train loss is: 0.0010981891722713914\n",
      "test loss is 0.0006901852622665745\n",
      "Batch: 23900,train loss is: 0.0026575480209516202\n",
      "test loss is 0.0016420138307092081\n",
      "Batch: 24000,train loss is: 0.0005561913505145037\n",
      "test loss is 0.001888674114573149\n",
      "Batch: 24100,train loss is: 0.000266959467663745\n",
      "test loss is 0.0008919772896552997\n",
      "Batch: 24200,train loss is: 0.006029020038619326\n",
      "test loss is 0.0007376621426291536\n",
      "Batch: 24300,train loss is: 0.0013987517583739294\n",
      "test loss is 0.001234958588524566\n",
      "Batch: 24400,train loss is: 0.0017734621839882983\n",
      "test loss is 0.0009081728182114854\n",
      "Batch: 24500,train loss is: 0.0007013870202992208\n",
      "test loss is 0.0009493117696392581\n",
      "Batch: 24600,train loss is: 0.0007597909910151684\n",
      "test loss is 0.0006854509959826091\n",
      "Batch: 24700,train loss is: 0.00040122389170218386\n",
      "test loss is 0.0007502600107048862\n",
      "Batch: 24800,train loss is: 0.0015571805654190209\n",
      "test loss is 0.0014410495220671664\n",
      "Batch: 24900,train loss is: 0.000706389397643654\n",
      "test loss is 0.0042403487477537215\n",
      "Batch: 25000,train loss is: 0.0013783120303283377\n",
      "test loss is 0.002169491929850414\n",
      "Batch: 25100,train loss is: 0.00035406156518351834\n",
      "test loss is 0.0013087301802710823\n",
      "Batch: 25200,train loss is: 0.0009324037900848075\n",
      "test loss is 0.0008709751095354191\n",
      "Batch: 25300,train loss is: 0.0013988237193252725\n",
      "test loss is 0.0010130215729211298\n",
      "Batch: 25400,train loss is: 0.00068491091888374\n",
      "test loss is 0.0005833114317837838\n",
      "Batch: 25500,train loss is: 0.00025488541575719814\n",
      "test loss is 0.0005586060497622098\n",
      "Batch: 25600,train loss is: 0.0009484491775029628\n",
      "test loss is 0.0005892608095047697\n",
      "Batch: 25700,train loss is: 0.0004787945030764857\n",
      "test loss is 0.0005360829502814485\n",
      "Batch: 25800,train loss is: 0.0005095039835177437\n",
      "test loss is 0.000648237019610298\n",
      "Batch: 25900,train loss is: 0.0004800712845170284\n",
      "test loss is 0.0010221336510998864\n",
      "Batch: 26000,train loss is: 0.0016537192722685618\n",
      "test loss is 0.004459598355307092\n",
      "Batch: 26100,train loss is: 0.0006113639026113873\n",
      "test loss is 0.001408270301981486\n",
      "Batch: 26200,train loss is: 0.0016011378514804858\n",
      "test loss is 0.0008004821549707986\n",
      "Batch: 26300,train loss is: 0.0007245865683273444\n",
      "test loss is 0.000713973623883781\n",
      "Batch: 26400,train loss is: 0.0005214543653862876\n",
      "test loss is 0.0006367135357915322\n",
      "Batch: 26500,train loss is: 0.0005251327512116355\n",
      "test loss is 0.0007111988910355895\n",
      "Batch: 26600,train loss is: 0.0017678314140427026\n",
      "test loss is 0.0010996102808514187\n",
      "Batch: 26700,train loss is: 0.0007570182802233815\n",
      "test loss is 0.0012306750374668275\n",
      "Batch: 26800,train loss is: 0.000925865029818864\n",
      "test loss is 0.0005991269785636153\n",
      "Batch: 26900,train loss is: 0.0005477049307422825\n",
      "test loss is 0.0007996817242857216\n",
      "Batch: 27000,train loss is: 0.00039100702916167526\n",
      "test loss is 0.0008637473759826815\n",
      "Batch: 27100,train loss is: 0.0002746550513269348\n",
      "test loss is 0.0006784192217358545\n",
      "Batch: 27200,train loss is: 0.0008234417854818556\n",
      "test loss is 0.0009575485820890233\n",
      "Batch: 27300,train loss is: 0.0026116943639799464\n",
      "test loss is 0.0039377361260374225\n",
      "Batch: 27400,train loss is: 0.0010369715926071329\n",
      "test loss is 0.0019373108885716137\n",
      "Batch: 27500,train loss is: 0.0005253082357540204\n",
      "test loss is 0.0006783644438111668\n",
      "Batch: 27600,train loss is: 0.0007709579352854539\n",
      "test loss is 0.0009660945676241234\n",
      "Batch: 27700,train loss is: 0.0017240912133295767\n",
      "test loss is 0.0016239362851853069\n",
      "Batch: 27800,train loss is: 0.000956763324569126\n",
      "test loss is 0.0020586514365202187\n",
      "Batch: 27900,train loss is: 0.0012399944691816789\n",
      "test loss is 0.0005801018058781517\n",
      "Batch: 28000,train loss is: 0.00045265463520682435\n",
      "test loss is 0.001361750232993605\n",
      "Batch: 28100,train loss is: 0.0014280534081300025\n",
      "test loss is 0.0008759121243357674\n",
      "Batch: 28200,train loss is: 0.0006355314111380596\n",
      "test loss is 0.0016795044181563837\n",
      "Batch: 28300,train loss is: 0.00119325243414398\n",
      "test loss is 0.0006140706182285248\n",
      "Batch: 28400,train loss is: 0.001041858936171487\n",
      "test loss is 0.0006563711308235283\n",
      "Batch: 28500,train loss is: 0.000481278072512574\n",
      "test loss is 0.0005386347709841602\n",
      "Batch: 28600,train loss is: 0.00066740469131543\n",
      "test loss is 0.0008011594139509953\n",
      "Batch: 28700,train loss is: 0.00033950213496411713\n",
      "test loss is 0.0007242678378479974\n",
      "Batch: 28800,train loss is: 0.0021455102403148607\n",
      "test loss is 0.0005766781885948191\n",
      "Batch: 28900,train loss is: 0.0008139010722809303\n",
      "test loss is 0.0013609938664458667\n",
      "Batch: 29000,train loss is: 0.0006813073706621894\n",
      "test loss is 0.0008778299670879124\n",
      "Batch: 29100,train loss is: 0.002333804776122368\n",
      "test loss is 0.00673570320835995\n",
      "Batch: 29200,train loss is: 0.00021393873499523814\n",
      "test loss is 0.0009275691881415642\n",
      "Batch: 29300,train loss is: 0.00153091603458997\n",
      "test loss is 0.0017678506435984834\n",
      "Batch: 29400,train loss is: 0.00042364600690231494\n",
      "test loss is 0.0022211877862474885\n",
      "Batch: 29500,train loss is: 0.00020253700430597146\n",
      "test loss is 0.0007157877239583192\n",
      "Batch: 29600,train loss is: 0.0008860988730988227\n",
      "test loss is 0.0005462129075982622\n",
      "Batch: 29700,train loss is: 0.0002593241996783028\n",
      "test loss is 0.0008325486392013844\n",
      "Batch: 29800,train loss is: 0.001175869218408045\n",
      "test loss is 0.0006023559243419472\n",
      "Batch: 29900,train loss is: 0.0011208145837800108\n",
      "test loss is 0.0008653521867360849\n",
      "Batch: 30000,train loss is: 0.00033666928609259097\n",
      "test loss is 0.0004589579505619294\n",
      "Batch: 30100,train loss is: 0.0012797475544743169\n",
      "test loss is 0.002509825087774216\n",
      "Batch: 30200,train loss is: 0.0006245852916911015\n",
      "test loss is 0.0007994522414914109\n",
      "Batch: 30300,train loss is: 0.0006265116424797943\n",
      "test loss is 0.0009059875080000315\n",
      "Batch: 30400,train loss is: 0.002091330056466106\n",
      "test loss is 0.003782776855489973\n",
      "Batch: 30500,train loss is: 0.0004269646053989426\n",
      "test loss is 0.0007227769753548897\n",
      "Batch: 30600,train loss is: 0.0005713722563491154\n",
      "test loss is 0.0015437825937260486\n",
      "Batch: 30700,train loss is: 0.0003868974917962236\n",
      "test loss is 0.00111227674212136\n",
      "Batch: 30800,train loss is: 0.00023799445923368802\n",
      "test loss is 0.0008590535375063037\n",
      "Batch: 30900,train loss is: 0.00039704172235630243\n",
      "test loss is 0.0005427672264202078\n",
      "Batch: 31000,train loss is: 0.0002469365125913316\n",
      "test loss is 0.0005425687396533239\n",
      "Batch: 31100,train loss is: 0.0007605819576001624\n",
      "test loss is 0.0012008450458093885\n",
      "Batch: 31200,train loss is: 0.000663087891703921\n",
      "test loss is 0.0006543731778940038\n",
      "Batch: 31300,train loss is: 0.0002540769161640489\n",
      "test loss is 0.0006927011679385579\n",
      "Batch: 31400,train loss is: 0.00038526360203467526\n",
      "test loss is 0.0012342868607949076\n",
      "Batch: 31500,train loss is: 0.0004199025839380376\n",
      "test loss is 0.001086294570051559\n",
      "Batch: 31600,train loss is: 0.0009734723174466671\n",
      "test loss is 0.0007017247629922016\n",
      "Batch: 31700,train loss is: 0.00032909105355113206\n",
      "test loss is 0.0006051367209002422\n",
      "Batch: 31800,train loss is: 0.0006549902815152578\n",
      "test loss is 0.0013105507937362223\n",
      "Batch: 31900,train loss is: 0.0002213192845869013\n",
      "test loss is 0.0005002677042515316\n",
      "Batch: 32000,train loss is: 0.002545522867841505\n",
      "test loss is 0.000651095310886522\n",
      "Batch: 32100,train loss is: 0.0007284907339528072\n",
      "test loss is 0.0012231297295507441\n",
      "Batch: 32200,train loss is: 0.00021468160590987087\n",
      "test loss is 0.0005324691257149465\n",
      "Batch: 32300,train loss is: 0.0006142851641369138\n",
      "test loss is 0.000834453103822652\n",
      "Batch: 32400,train loss is: 0.0004865061118425375\n",
      "test loss is 0.0010752686341813185\n",
      "Batch: 32500,train loss is: 0.0005113590318496973\n",
      "test loss is 0.0006680287955860178\n",
      "Batch: 32600,train loss is: 0.0006778163773380225\n",
      "test loss is 0.0008157949952435839\n",
      "Batch: 32700,train loss is: 0.0004807216524954229\n",
      "test loss is 0.0011721525264042191\n",
      "Batch: 32800,train loss is: 0.0004108059451337185\n",
      "test loss is 0.0006802918523477779\n",
      "Batch: 32900,train loss is: 0.0008830118270606329\n",
      "test loss is 0.0006502702800350512\n",
      "Batch: 33000,train loss is: 0.0005479302289611131\n",
      "test loss is 0.0006523559278078586\n",
      "Batch: 33100,train loss is: 0.000518853614788243\n",
      "test loss is 0.002002956908184862\n",
      "Batch: 33200,train loss is: 0.0012145039590743289\n",
      "test loss is 0.0009256074309604575\n",
      "Batch: 33300,train loss is: 0.0005652514626886928\n",
      "test loss is 0.0005826939268146428\n",
      "Batch: 33400,train loss is: 0.00039107384666466066\n",
      "test loss is 0.0026648676995487917\n",
      "Batch: 33500,train loss is: 0.0015126017611740681\n",
      "test loss is 0.008535670578515273\n",
      "Batch: 33600,train loss is: 0.0003859341848480948\n",
      "test loss is 0.0008255295575474955\n",
      "Batch: 33700,train loss is: 0.0018398807428111456\n",
      "test loss is 0.0009060194180741692\n",
      "Batch: 33800,train loss is: 0.0011370290224762645\n",
      "test loss is 0.0011751721232521146\n",
      "Batch: 33900,train loss is: 0.0006493719860728215\n",
      "test loss is 0.0004998458920063251\n",
      "-----------------------Epoch: 4----------------------------------\n",
      "Batch: 0,train loss is: 0.00038151308823141647\n",
      "test loss is 0.0005317295923930117\n",
      "Batch: 100,train loss is: 0.00041050221780169013\n",
      "test loss is 0.0004873684077559517\n",
      "Batch: 200,train loss is: 0.000692578877797948\n",
      "test loss is 0.0005816693937556314\n",
      "Batch: 300,train loss is: 0.0007054669479444021\n",
      "test loss is 0.000871640133923329\n",
      "Batch: 400,train loss is: 0.0003740469789184833\n",
      "test loss is 0.0006195722956466368\n",
      "Batch: 500,train loss is: 0.0003974313904776171\n",
      "test loss is 0.0020605578005952565\n",
      "Batch: 600,train loss is: 0.00037978089373127906\n",
      "test loss is 0.001935758488322321\n",
      "Batch: 700,train loss is: 0.00045575408513932896\n",
      "test loss is 0.0008358241626928186\n",
      "Batch: 800,train loss is: 0.0011377167938505268\n",
      "test loss is 0.001026121493312519\n",
      "Batch: 900,train loss is: 0.0010277169045165159\n",
      "test loss is 0.0009422616098117267\n",
      "Batch: 1000,train loss is: 0.0006337302629613743\n",
      "test loss is 0.000611461014736828\n",
      "Batch: 1100,train loss is: 0.0015084235445566034\n",
      "test loss is 0.0007095851917770493\n",
      "Batch: 1200,train loss is: 0.00031319133773618374\n",
      "test loss is 0.0007074334039169201\n",
      "Batch: 1300,train loss is: 0.0020922446852197417\n",
      "test loss is 0.000620654092245907\n",
      "Batch: 1400,train loss is: 0.00024303508489053783\n",
      "test loss is 0.0006306253890137283\n",
      "Batch: 1500,train loss is: 0.0015601432721705382\n",
      "test loss is 0.0023015506571355897\n",
      "Batch: 1600,train loss is: 0.0006223036373440903\n",
      "test loss is 0.0007637818146746429\n",
      "Batch: 1700,train loss is: 0.0016946834104253458\n",
      "test loss is 0.0010794179258658617\n",
      "Batch: 1800,train loss is: 0.0003299600171175427\n",
      "test loss is 0.0013903174852087524\n",
      "Batch: 1900,train loss is: 0.002451313301332992\n",
      "test loss is 0.0025776452140128664\n",
      "Batch: 2000,train loss is: 0.00290332373758839\n",
      "test loss is 0.0063876833468414235\n",
      "Batch: 2100,train loss is: 0.0016323085373073545\n",
      "test loss is 0.0006762247503021033\n",
      "Batch: 2200,train loss is: 0.0006028312488618236\n",
      "test loss is 0.0004939367482154184\n",
      "Batch: 2300,train loss is: 0.0003739847839892372\n",
      "test loss is 0.0005024330585254609\n",
      "Batch: 2400,train loss is: 0.00028979179364567486\n",
      "test loss is 0.0006481445460382638\n",
      "Batch: 2500,train loss is: 0.00030905780493806076\n",
      "test loss is 0.0004962586655466449\n",
      "Batch: 2600,train loss is: 0.0008959065595358862\n",
      "test loss is 0.0005458121346374161\n",
      "Batch: 2700,train loss is: 0.0002378298240657374\n",
      "test loss is 0.0005063959819436853\n",
      "Batch: 2800,train loss is: 0.00045262933843291604\n",
      "test loss is 0.000611536176710852\n",
      "Batch: 2900,train loss is: 0.00045060640384748376\n",
      "test loss is 0.0005089913769431322\n",
      "Batch: 3000,train loss is: 0.00035684055866910347\n",
      "test loss is 0.0008353354798623731\n",
      "Batch: 3100,train loss is: 0.0005012585817353951\n",
      "test loss is 0.0009632960805054414\n",
      "Batch: 3200,train loss is: 0.000642738534048326\n",
      "test loss is 0.0006257869800515083\n",
      "Batch: 3300,train loss is: 0.00031938217105098295\n",
      "test loss is 0.0007025304486296747\n",
      "Batch: 3400,train loss is: 0.0005783416762339179\n",
      "test loss is 0.0009261777331057675\n",
      "Batch: 3500,train loss is: 0.0004557018039484411\n",
      "test loss is 0.0010831372201124036\n",
      "Batch: 3600,train loss is: 0.0003986487182557554\n",
      "test loss is 0.0008382333499739611\n",
      "Batch: 3700,train loss is: 0.001113574099577124\n",
      "test loss is 0.0015580963037223898\n",
      "Batch: 3800,train loss is: 0.0009233469596034504\n",
      "test loss is 0.0010371626375176995\n",
      "Batch: 3900,train loss is: 0.0007061031356944804\n",
      "test loss is 0.0009627812368794651\n",
      "Batch: 4000,train loss is: 0.00043417130458991445\n",
      "test loss is 0.0008274451084165653\n",
      "Batch: 4100,train loss is: 0.0013379970238034137\n",
      "test loss is 0.0006363981390904736\n",
      "Batch: 4200,train loss is: 0.0007035394363910117\n",
      "test loss is 0.0005931843160960408\n",
      "Batch: 4300,train loss is: 0.0003651367754044097\n",
      "test loss is 0.0009635332528022601\n",
      "Batch: 4400,train loss is: 0.0005668774902874268\n",
      "test loss is 0.0006418207255653388\n",
      "Batch: 4500,train loss is: 0.000486533031265156\n",
      "test loss is 0.0009480377936136637\n",
      "Batch: 4600,train loss is: 0.0013746329701569394\n",
      "test loss is 0.001187018667871897\n",
      "Batch: 4700,train loss is: 0.0014751274605974314\n",
      "test loss is 0.0010720534448712055\n",
      "Batch: 4800,train loss is: 0.0027297328802981633\n",
      "test loss is 0.0022703175107846793\n",
      "Batch: 4900,train loss is: 0.002457744088907937\n",
      "test loss is 0.0012963723353444207\n",
      "Batch: 5000,train loss is: 0.0001948721502641272\n",
      "test loss is 0.0006611814085105654\n",
      "Batch: 5100,train loss is: 0.0003982684440412919\n",
      "test loss is 0.0008481518575731184\n",
      "Batch: 5200,train loss is: 0.0008482782104748215\n",
      "test loss is 0.0008913901754266157\n",
      "Batch: 5300,train loss is: 0.0032412268895407716\n",
      "test loss is 0.0021384000232014667\n",
      "Batch: 5400,train loss is: 0.0008811424751254493\n",
      "test loss is 0.0009018308893872598\n",
      "Batch: 5500,train loss is: 0.0008765564319922484\n",
      "test loss is 0.0008890953544536865\n",
      "Batch: 5600,train loss is: 0.0007160498408729697\n",
      "test loss is 0.0007926056573140468\n",
      "Batch: 5700,train loss is: 0.000508258104631387\n",
      "test loss is 0.0007337718433934808\n",
      "Batch: 5800,train loss is: 0.0009561365027671398\n",
      "test loss is 0.000723768121798366\n",
      "Batch: 5900,train loss is: 0.0014099487939703793\n",
      "test loss is 0.0010084362703233697\n",
      "Batch: 6000,train loss is: 0.0008421402553220088\n",
      "test loss is 0.0005486453027847048\n",
      "Batch: 6100,train loss is: 0.00034601436054332145\n",
      "test loss is 0.0007264940889054743\n",
      "Batch: 6200,train loss is: 0.000475024820707143\n",
      "test loss is 0.0007800684979400389\n",
      "Batch: 6300,train loss is: 0.00255922359619455\n",
      "test loss is 0.0024220540653204507\n",
      "Batch: 6400,train loss is: 0.000736925770262841\n",
      "test loss is 0.0008808224337229238\n",
      "Batch: 6500,train loss is: 0.004788040530554609\n",
      "test loss is 0.0014428648072834364\n",
      "Batch: 6600,train loss is: 0.02094870568858128\n",
      "test loss is 0.0014197479884808127\n",
      "Batch: 6700,train loss is: 0.00026228745449743333\n",
      "test loss is 0.0008535388308736494\n",
      "Batch: 6800,train loss is: 0.0007687009584460687\n",
      "test loss is 0.0006054174891208208\n",
      "Batch: 6900,train loss is: 0.0009710179908939151\n",
      "test loss is 0.000679787688942006\n",
      "Batch: 7000,train loss is: 0.002083577977866332\n",
      "test loss is 0.0007409022686667284\n",
      "Batch: 7100,train loss is: 0.0005172150371014945\n",
      "test loss is 0.0007407140277138554\n",
      "Batch: 7200,train loss is: 0.00032904543768252404\n",
      "test loss is 0.0005182333945274765\n",
      "Batch: 7300,train loss is: 0.0028744884048696254\n",
      "test loss is 0.0005504843557538054\n",
      "Batch: 7400,train loss is: 0.0013162166777202354\n",
      "test loss is 0.0008848688316500019\n",
      "Batch: 7500,train loss is: 0.0005337121042712291\n",
      "test loss is 0.0009695395130299553\n",
      "Batch: 7600,train loss is: 0.0019376111185735877\n",
      "test loss is 0.002372024446256337\n",
      "Batch: 7700,train loss is: 0.0021585893704802565\n",
      "test loss is 0.000920242533640807\n",
      "Batch: 7800,train loss is: 0.0004354652301208737\n",
      "test loss is 0.0006437340087521074\n",
      "Batch: 7900,train loss is: 0.0005374084940230905\n",
      "test loss is 0.0005776632345042983\n",
      "Batch: 8000,train loss is: 0.0013090106842656016\n",
      "test loss is 0.0023323564434982935\n",
      "Batch: 8100,train loss is: 0.0008063656787076184\n",
      "test loss is 0.0008398061772333772\n",
      "Batch: 8200,train loss is: 0.001421741634557942\n",
      "test loss is 0.0006484167490450951\n",
      "Batch: 8300,train loss is: 0.00041048587258844056\n",
      "test loss is 0.0008402964588196205\n",
      "Batch: 8400,train loss is: 0.0003512164175138264\n",
      "test loss is 0.0005715059680029646\n",
      "Batch: 8500,train loss is: 0.0005834595016311429\n",
      "test loss is 0.0007936647531924665\n",
      "Batch: 8600,train loss is: 0.00036172680178183264\n",
      "test loss is 0.0005455429702013005\n",
      "Batch: 8700,train loss is: 0.00041806867495109687\n",
      "test loss is 0.00128835388294597\n",
      "Batch: 8800,train loss is: 0.0006110074552175153\n",
      "test loss is 0.0006910799986811125\n",
      "Batch: 8900,train loss is: 0.0009464706304028309\n",
      "test loss is 0.0007090493401509355\n",
      "Batch: 9000,train loss is: 0.0002820856275146237\n",
      "test loss is 0.000845370969151332\n",
      "Batch: 9100,train loss is: 0.0023356073632374683\n",
      "test loss is 0.0015122654524628602\n",
      "Batch: 9200,train loss is: 0.0006084292003865222\n",
      "test loss is 0.0022574845717486104\n",
      "Batch: 9300,train loss is: 0.0008028149880870819\n",
      "test loss is 0.0006493208518178607\n",
      "Batch: 9400,train loss is: 0.0005514846620795724\n",
      "test loss is 0.0005964861265215764\n",
      "Batch: 9500,train loss is: 0.001201755719760498\n",
      "test loss is 0.0017143166146615862\n",
      "Batch: 9600,train loss is: 0.000438031542610494\n",
      "test loss is 0.0007670903483479811\n",
      "Batch: 9700,train loss is: 0.00020233392607094477\n",
      "test loss is 0.000503076923436598\n",
      "Batch: 9800,train loss is: 0.0002501202936094997\n",
      "test loss is 0.0008682364429967918\n",
      "Batch: 9900,train loss is: 0.0005078931125535572\n",
      "test loss is 0.0006042573504030924\n",
      "Batch: 10000,train loss is: 0.00021834931784256336\n",
      "test loss is 0.000693063662073549\n",
      "Batch: 10100,train loss is: 0.001404258007031272\n",
      "test loss is 0.0006096856786203578\n",
      "Batch: 10200,train loss is: 0.001928881122069774\n",
      "test loss is 0.0006051892971997828\n",
      "Batch: 10300,train loss is: 0.004713975167385599\n",
      "test loss is 0.003381040338579695\n",
      "Batch: 10400,train loss is: 0.00127694191817659\n",
      "test loss is 0.002162688258114708\n",
      "Batch: 10500,train loss is: 0.002136663426556634\n",
      "test loss is 0.0014305038990009022\n",
      "Batch: 10600,train loss is: 0.0002585715422779479\n",
      "test loss is 0.0006067225803271168\n",
      "Batch: 10700,train loss is: 0.0007904095170622691\n",
      "test loss is 0.0006039636794116604\n",
      "Batch: 10800,train loss is: 0.0004738592752611577\n",
      "test loss is 0.0006755599437318414\n",
      "Batch: 10900,train loss is: 0.0006464114453232627\n",
      "test loss is 0.0011116292873855084\n",
      "Batch: 11000,train loss is: 0.0006399880331629994\n",
      "test loss is 0.0004973502319093004\n",
      "Batch: 11100,train loss is: 0.0005533783823429699\n",
      "test loss is 0.000630197410821344\n",
      "Batch: 11200,train loss is: 0.0009283201329232486\n",
      "test loss is 0.0005200555727774715\n",
      "Batch: 11300,train loss is: 0.00046106905995789524\n",
      "test loss is 0.0008212385711204886\n",
      "Batch: 11400,train loss is: 0.000674739635184211\n",
      "test loss is 0.0007152493797745173\n",
      "Batch: 11500,train loss is: 0.0003008035190001424\n",
      "test loss is 0.001018942786430907\n",
      "Batch: 11600,train loss is: 0.0013025985051418254\n",
      "test loss is 0.001742224904291824\n",
      "Batch: 11700,train loss is: 0.00022344767054355432\n",
      "test loss is 0.0008351100941828081\n",
      "Batch: 11800,train loss is: 0.0007879583721302878\n",
      "test loss is 0.0010742762602844872\n",
      "Batch: 11900,train loss is: 0.0010647905454601965\n",
      "test loss is 0.0012340744283830707\n",
      "Batch: 12000,train loss is: 0.0004568796469526466\n",
      "test loss is 0.0007807030194641194\n",
      "Batch: 12100,train loss is: 0.0026630498740573996\n",
      "test loss is 0.0021492433552348887\n",
      "Batch: 12200,train loss is: 0.0006306606277313095\n",
      "test loss is 0.002219570624731323\n",
      "Batch: 12300,train loss is: 0.0027569816534376164\n",
      "test loss is 0.001369744186341929\n",
      "Batch: 12400,train loss is: 0.0005767018096268423\n",
      "test loss is 0.0009707828154103259\n",
      "Batch: 12500,train loss is: 0.002548926923267725\n",
      "test loss is 0.0022807730598399137\n",
      "Batch: 12600,train loss is: 0.00042373635011130705\n",
      "test loss is 0.0004972163864515628\n",
      "Batch: 12700,train loss is: 0.0004823500484090285\n",
      "test loss is 0.0004770254114715863\n",
      "Batch: 12800,train loss is: 0.00021847910780907535\n",
      "test loss is 0.0006787719572213332\n",
      "Batch: 12900,train loss is: 0.0004916759529133948\n",
      "test loss is 0.0014629296569865437\n",
      "Batch: 13000,train loss is: 0.0006021677054237524\n",
      "test loss is 0.0010086332145318277\n",
      "Batch: 13100,train loss is: 0.0008486651757921327\n",
      "test loss is 0.0006963909831929704\n",
      "Batch: 13200,train loss is: 0.000513516925780879\n",
      "test loss is 0.0005720019343666135\n",
      "Batch: 13300,train loss is: 0.0004314441497823427\n",
      "test loss is 0.0019507249688555247\n",
      "Batch: 13400,train loss is: 0.0006478858030031351\n",
      "test loss is 0.001192464820878717\n",
      "Batch: 13500,train loss is: 0.0006003932255080976\n",
      "test loss is 0.0008043457017914668\n",
      "Batch: 13600,train loss is: 0.004495311714244343\n",
      "test loss is 0.004237103051892206\n",
      "Batch: 13700,train loss is: 0.0007209866789923867\n",
      "test loss is 0.0010909204947767964\n",
      "Batch: 13800,train loss is: 0.000126386430396287\n",
      "test loss is 0.0006880352007435644\n",
      "Batch: 13900,train loss is: 0.0004904583989799103\n",
      "test loss is 0.0005249232923610967\n",
      "Batch: 14000,train loss is: 0.00024438447456906986\n",
      "test loss is 0.0007017033633586542\n",
      "Batch: 14100,train loss is: 0.0002726294660567681\n",
      "test loss is 0.0006739612634602455\n",
      "Batch: 14200,train loss is: 0.00032806405752147037\n",
      "test loss is 0.00040993221282157395\n",
      "Batch: 14300,train loss is: 0.0003954239107736828\n",
      "test loss is 0.0005954720622258561\n",
      "Batch: 14400,train loss is: 0.0005891872443652973\n",
      "test loss is 0.0007453358436333475\n",
      "Batch: 14500,train loss is: 0.00022981862513467792\n",
      "test loss is 0.0004221518481713268\n",
      "Batch: 14600,train loss is: 0.0021580219690735644\n",
      "test loss is 0.0008665798765292544\n",
      "Batch: 14700,train loss is: 0.00020597937189545284\n",
      "test loss is 0.001498515990103834\n",
      "Batch: 14800,train loss is: 0.0008893516303084111\n",
      "test loss is 0.0017869289073631181\n",
      "Batch: 14900,train loss is: 0.00042422133670064964\n",
      "test loss is 0.0011429613120201361\n",
      "Batch: 15000,train loss is: 0.00028392158905078425\n",
      "test loss is 0.0007076822699884197\n",
      "Batch: 15100,train loss is: 0.0005625062106267321\n",
      "test loss is 0.0004890209134707135\n",
      "Batch: 15200,train loss is: 0.0005174132778066053\n",
      "test loss is 0.000535819702106122\n",
      "Batch: 15300,train loss is: 0.0005197120341464616\n",
      "test loss is 0.0004519804277896734\n",
      "Batch: 15400,train loss is: 0.0007301485327923812\n",
      "test loss is 0.0008927106871181604\n",
      "Batch: 15500,train loss is: 0.0010392755907130437\n",
      "test loss is 0.0011109438985327038\n",
      "Batch: 15600,train loss is: 0.002869437557457026\n",
      "test loss is 0.001366077016249878\n",
      "Batch: 15700,train loss is: 0.0008146878788952535\n",
      "test loss is 0.0010546625111935008\n",
      "Batch: 15800,train loss is: 0.00042196689801052046\n",
      "test loss is 0.0007959409329366951\n",
      "Batch: 15900,train loss is: 0.0010033011001748533\n",
      "test loss is 0.001114997487816145\n",
      "Batch: 16000,train loss is: 0.0008842162109271968\n",
      "test loss is 0.000936934975371\n",
      "Batch: 16100,train loss is: 0.00180293565009815\n",
      "test loss is 0.0007295454967899496\n",
      "Batch: 16200,train loss is: 0.0004814256152449559\n",
      "test loss is 0.0009668799129234707\n",
      "Batch: 16300,train loss is: 0.0005065214308758433\n",
      "test loss is 0.0006659228002026915\n",
      "Batch: 16400,train loss is: 0.0006421545832482385\n",
      "test loss is 0.000514999367943748\n",
      "Batch: 16500,train loss is: 0.002170733579790324\n",
      "test loss is 0.001351517456067865\n",
      "Batch: 16600,train loss is: 0.00021420640234154614\n",
      "test loss is 0.0010524699216140703\n",
      "Batch: 16700,train loss is: 0.0011182022347707742\n",
      "test loss is 0.0014985839071151963\n",
      "Batch: 16800,train loss is: 0.00036658080527614457\n",
      "test loss is 0.0005183169356124662\n",
      "Batch: 16900,train loss is: 0.0015715513692044212\n",
      "test loss is 0.002092744989865198\n",
      "Batch: 17000,train loss is: 0.0002881600248538034\n",
      "test loss is 0.0006778392627555222\n",
      "Batch: 17100,train loss is: 0.0009850671650445738\n",
      "test loss is 0.0012858788949457647\n",
      "Batch: 17200,train loss is: 0.0002038235959093666\n",
      "test loss is 0.0004815035165230515\n",
      "Batch: 17300,train loss is: 0.000708912350654966\n",
      "test loss is 0.0004604407577431262\n",
      "Batch: 17400,train loss is: 0.0009294468755333242\n",
      "test loss is 0.0009134434184322655\n",
      "Batch: 17500,train loss is: 0.0008420799828540353\n",
      "test loss is 0.0008647407493543123\n",
      "Batch: 17600,train loss is: 0.0009618600780140592\n",
      "test loss is 0.0014267099003461554\n",
      "Batch: 17700,train loss is: 0.0008326305502645595\n",
      "test loss is 0.001253630541116822\n",
      "Batch: 17800,train loss is: 0.0006891358618804455\n",
      "test loss is 0.0013554695860297148\n",
      "Batch: 17900,train loss is: 0.001167973870419659\n",
      "test loss is 0.0012310181539586325\n",
      "Batch: 18000,train loss is: 0.0011965739876697132\n",
      "test loss is 0.0006098239349753057\n",
      "Batch: 18100,train loss is: 0.00031087951582014123\n",
      "test loss is 0.0006524078612395742\n",
      "Batch: 18200,train loss is: 0.00039606802143070076\n",
      "test loss is 0.001118385199363819\n",
      "Batch: 18300,train loss is: 0.0010338348290807736\n",
      "test loss is 0.0005991498541124676\n",
      "Batch: 18400,train loss is: 0.0003403587827318187\n",
      "test loss is 0.0007875260929577683\n",
      "Batch: 18500,train loss is: 0.002604580471325085\n",
      "test loss is 0.0023324305893778945\n",
      "Batch: 18600,train loss is: 0.0012877397534495395\n",
      "test loss is 0.0019667303086937167\n",
      "Batch: 18700,train loss is: 0.0007743983845964002\n",
      "test loss is 0.000769024275943862\n",
      "Batch: 18800,train loss is: 0.00045179746388480024\n",
      "test loss is 0.0005002429947668962\n",
      "Batch: 18900,train loss is: 0.000799543650906586\n",
      "test loss is 0.0006504358586127809\n",
      "Batch: 19000,train loss is: 0.0006098639153363684\n",
      "test loss is 0.0005543005583331257\n",
      "Batch: 19100,train loss is: 0.0009068785136931056\n",
      "test loss is 0.0008657328146853842\n",
      "Batch: 19200,train loss is: 0.0005958257179601298\n",
      "test loss is 0.0007317079632657873\n",
      "Batch: 19300,train loss is: 0.0021484000400451824\n",
      "test loss is 0.0038302570440205542\n",
      "Batch: 19400,train loss is: 0.00015879797546470377\n",
      "test loss is 0.0006181299976532891\n",
      "Batch: 19500,train loss is: 0.00047493821620607604\n",
      "test loss is 0.0008311902224375073\n",
      "Batch: 19600,train loss is: 0.0011884329731740436\n",
      "test loss is 0.0011678905532386502\n",
      "Batch: 19700,train loss is: 0.001966774548052558\n",
      "test loss is 0.0010864997775085477\n",
      "Batch: 19800,train loss is: 0.0005381160780220013\n",
      "test loss is 0.000607975043994156\n",
      "Batch: 19900,train loss is: 0.0009516355577334902\n",
      "test loss is 0.0009609685065818047\n",
      "Batch: 20000,train loss is: 0.0008325920775561562\n",
      "test loss is 0.0008288992479236574\n",
      "Batch: 20100,train loss is: 0.0008999735562171779\n",
      "test loss is 0.0005631992871953774\n",
      "Batch: 20200,train loss is: 0.00040317773561715953\n",
      "test loss is 0.000603437688206596\n",
      "Batch: 20300,train loss is: 0.00016483522591185102\n",
      "test loss is 0.0007168693639888729\n",
      "Batch: 20400,train loss is: 0.0036861409153951764\n",
      "test loss is 0.004644840525534331\n",
      "Batch: 20500,train loss is: 0.000555248183804904\n",
      "test loss is 0.0009291257901093676\n",
      "Batch: 20600,train loss is: 0.0019384944578216723\n",
      "test loss is 0.0007225458698578865\n",
      "Batch: 20700,train loss is: 0.0009724137539105116\n",
      "test loss is 0.0013266136062307806\n",
      "Batch: 20800,train loss is: 0.0007762517494187191\n",
      "test loss is 0.0005645475486781988\n",
      "Batch: 20900,train loss is: 0.00019974982044359448\n",
      "test loss is 0.000684192327314489\n",
      "Batch: 21000,train loss is: 0.0005297374767535141\n",
      "test loss is 0.000703735798775353\n",
      "Batch: 21100,train loss is: 0.0003548934315666554\n",
      "test loss is 0.0004356661995261073\n",
      "Batch: 21200,train loss is: 0.00023233698408635798\n",
      "test loss is 0.0005530535832073512\n",
      "Batch: 21300,train loss is: 0.0005594725230118998\n",
      "test loss is 0.0008801504773660938\n",
      "Batch: 21400,train loss is: 0.00044929871220366973\n",
      "test loss is 0.0005966302543597368\n",
      "Batch: 21500,train loss is: 0.0002898829454221981\n",
      "test loss is 0.0006010598409239992\n",
      "Batch: 21600,train loss is: 0.00029516662795436475\n",
      "test loss is 0.000889770422109945\n",
      "Batch: 21700,train loss is: 0.0003270422825432094\n",
      "test loss is 0.0007766832290399218\n",
      "Batch: 21800,train loss is: 0.0007610489500807836\n",
      "test loss is 0.0005065714299371572\n",
      "Batch: 21900,train loss is: 0.0006684221548196959\n",
      "test loss is 0.000717384915968761\n",
      "Batch: 22000,train loss is: 0.0008664772713666256\n",
      "test loss is 0.0014075326964805889\n",
      "Batch: 22100,train loss is: 0.00020475129833526378\n",
      "test loss is 0.001036978433867197\n",
      "Batch: 22200,train loss is: 0.0015869157723515798\n",
      "test loss is 0.0008739610785554693\n",
      "Batch: 22300,train loss is: 0.003191365309893863\n",
      "test loss is 0.0032093110994403646\n",
      "Batch: 22400,train loss is: 0.0004449121861865811\n",
      "test loss is 0.0008250398531025036\n",
      "Batch: 22500,train loss is: 0.00033434295337543947\n",
      "test loss is 0.0006335939957348453\n",
      "Batch: 22600,train loss is: 0.0007606683153508916\n",
      "test loss is 0.0006115642833491769\n",
      "Batch: 22700,train loss is: 0.0010864901326673696\n",
      "test loss is 0.0011048830232036112\n",
      "Batch: 22800,train loss is: 0.00036199472215120307\n",
      "test loss is 0.0004429765257681189\n",
      "Batch: 22900,train loss is: 0.0008557132499398201\n",
      "test loss is 0.0012569578133534477\n",
      "Batch: 23000,train loss is: 0.0006663068752865306\n",
      "test loss is 0.00041125215598453017\n",
      "Batch: 23100,train loss is: 0.0004909069011402894\n",
      "test loss is 0.0012115333015081692\n",
      "Batch: 23200,train loss is: 0.00033288019303633905\n",
      "test loss is 0.0006802534824106578\n",
      "Batch: 23300,train loss is: 0.00030830419205001773\n",
      "test loss is 0.0004272635141635131\n",
      "Batch: 23400,train loss is: 0.0004188155433732573\n",
      "test loss is 0.000602743208216035\n",
      "Batch: 23500,train loss is: 0.0025312970606157083\n",
      "test loss is 0.0005846959769589834\n",
      "Batch: 23600,train loss is: 0.0003586321895992514\n",
      "test loss is 0.0005327771901944709\n",
      "Batch: 23700,train loss is: 0.0005956964660499387\n",
      "test loss is 0.0006077664318763336\n",
      "Batch: 23800,train loss is: 0.0008616173729948373\n",
      "test loss is 0.00044132336576234525\n",
      "Batch: 23900,train loss is: 0.002624083783268679\n",
      "test loss is 0.0018172939715328414\n",
      "Batch: 24000,train loss is: 0.0006481441903958351\n",
      "test loss is 0.0023018159664598735\n",
      "Batch: 24100,train loss is: 0.0003061148316243995\n",
      "test loss is 0.0009913795860369396\n",
      "Batch: 24200,train loss is: 0.004830560839828824\n",
      "test loss is 0.0006153973740297866\n",
      "Batch: 24300,train loss is: 0.001243299767090573\n",
      "test loss is 0.0010925041453866081\n",
      "Batch: 24400,train loss is: 0.0013172865896637707\n",
      "test loss is 0.0006074011078018286\n",
      "Batch: 24500,train loss is: 0.0011526342143501522\n",
      "test loss is 0.001002941724274869\n",
      "Batch: 24600,train loss is: 0.0004591560873137699\n",
      "test loss is 0.0005499501284598404\n",
      "Batch: 24700,train loss is: 0.00041363756258605604\n",
      "test loss is 0.000620801525686188\n",
      "Batch: 24800,train loss is: 0.000972921599654788\n",
      "test loss is 0.0006263489134131928\n",
      "Batch: 24900,train loss is: 0.000528946751610826\n",
      "test loss is 0.002113141204186703\n",
      "Batch: 25000,train loss is: 0.0006718932599256067\n",
      "test loss is 0.001261471845192767\n",
      "Batch: 25100,train loss is: 0.0003871270765874516\n",
      "test loss is 0.0009248460405978381\n",
      "Batch: 25200,train loss is: 0.00044017104747727916\n",
      "test loss is 0.0006475983743479785\n",
      "Batch: 25300,train loss is: 0.0006175375628114327\n",
      "test loss is 0.0006293355668297925\n",
      "Batch: 25400,train loss is: 0.00026117436269459525\n",
      "test loss is 0.000419032849625833\n",
      "Batch: 25500,train loss is: 0.00025265525703533585\n",
      "test loss is 0.0004739119779983739\n",
      "Batch: 25600,train loss is: 0.0007528430408450181\n",
      "test loss is 0.0004956853949223829\n",
      "Batch: 25700,train loss is: 0.00035897593029747906\n",
      "test loss is 0.00044198921346924407\n",
      "Batch: 25800,train loss is: 0.0005612204975248204\n",
      "test loss is 0.0005601040301510333\n",
      "Batch: 25900,train loss is: 0.0006352443732780393\n",
      "test loss is 0.0008814205406484995\n",
      "Batch: 26000,train loss is: 0.0019550664182349095\n",
      "test loss is 0.0041542591998758255\n",
      "Batch: 26100,train loss is: 0.0005306385752156128\n",
      "test loss is 0.0013538608302986123\n",
      "Batch: 26200,train loss is: 0.0012244596570764714\n",
      "test loss is 0.0006470796905851294\n",
      "Batch: 26300,train loss is: 0.0005940161110139889\n",
      "test loss is 0.0006508574801863945\n",
      "Batch: 26400,train loss is: 0.00043668414665685144\n",
      "test loss is 0.000555057974366657\n",
      "Batch: 26500,train loss is: 0.0005524204084085026\n",
      "test loss is 0.000649207414825008\n",
      "Batch: 26600,train loss is: 0.003038383887877314\n",
      "test loss is 0.0013286747183130704\n",
      "Batch: 26700,train loss is: 0.00046626752550956867\n",
      "test loss is 0.0006628152983850886\n",
      "Batch: 26800,train loss is: 0.0008448320640839845\n",
      "test loss is 0.00047710761712535174\n",
      "Batch: 26900,train loss is: 0.00041493441067467455\n",
      "test loss is 0.0005164730226918076\n",
      "Batch: 27000,train loss is: 0.00048782732146832595\n",
      "test loss is 0.0006783968343991066\n",
      "Batch: 27100,train loss is: 0.0001743602412670596\n",
      "test loss is 0.0006043942061608929\n",
      "Batch: 27200,train loss is: 0.00043084193385976285\n",
      "test loss is 0.0011542152661670035\n",
      "Batch: 27300,train loss is: 0.000914154183244592\n",
      "test loss is 0.0014535490517197094\n",
      "Batch: 27400,train loss is: 0.000898286877011506\n",
      "test loss is 0.00126482253331353\n",
      "Batch: 27500,train loss is: 0.0006743359987244116\n",
      "test loss is 0.0005878416214045891\n",
      "Batch: 27600,train loss is: 0.000469793689780933\n",
      "test loss is 0.0008208554118952672\n",
      "Batch: 27700,train loss is: 0.0008822480549464079\n",
      "test loss is 0.0011009046172141366\n",
      "Batch: 27800,train loss is: 0.0014185775782894813\n",
      "test loss is 0.002122084336463993\n",
      "Batch: 27900,train loss is: 0.0011176305966267121\n",
      "test loss is 0.0005402383757015926\n",
      "Batch: 28000,train loss is: 0.0003178487819668497\n",
      "test loss is 0.0010661676413690972\n",
      "Batch: 28100,train loss is: 0.0008962367961230985\n",
      "test loss is 0.0007393756185509508\n",
      "Batch: 28200,train loss is: 0.0005811722699357702\n",
      "test loss is 0.0011876299658055067\n",
      "Batch: 28300,train loss is: 0.0008253927783035989\n",
      "test loss is 0.000549063566743456\n",
      "Batch: 28400,train loss is: 0.0009715684524920987\n",
      "test loss is 0.0005708085433966301\n",
      "Batch: 28500,train loss is: 0.0004642867932642138\n",
      "test loss is 0.000478817699128808\n",
      "Batch: 28600,train loss is: 0.000599751289681361\n",
      "test loss is 0.0008326281802399063\n",
      "Batch: 28700,train loss is: 0.0002927747139159746\n",
      "test loss is 0.0006997961267610834\n",
      "Batch: 28800,train loss is: 0.001665599203002103\n",
      "test loss is 0.00044024597339807387\n",
      "Batch: 28900,train loss is: 0.0006462929322859198\n",
      "test loss is 0.0012652514351067816\n",
      "Batch: 29000,train loss is: 0.000516541944037072\n",
      "test loss is 0.0007056595130412663\n",
      "Batch: 29100,train loss is: 0.001392891218433597\n",
      "test loss is 0.003993425722571543\n",
      "Batch: 29200,train loss is: 0.00018151285832934263\n",
      "test loss is 0.0007022781133038306\n",
      "Batch: 29300,train loss is: 0.0008328354168063407\n",
      "test loss is 0.0007882310207947678\n",
      "Batch: 29400,train loss is: 0.0002862660784678828\n",
      "test loss is 0.0019289932788159173\n",
      "Batch: 29500,train loss is: 0.000275202965542771\n",
      "test loss is 0.0007598016420078958\n",
      "Batch: 29600,train loss is: 0.0006777024993624905\n",
      "test loss is 0.00043634473710944463\n",
      "Batch: 29700,train loss is: 0.0002707505077647541\n",
      "test loss is 0.0008426216829398455\n",
      "Batch: 29800,train loss is: 0.0010055430506999627\n",
      "test loss is 0.0006608497846938847\n",
      "Batch: 29900,train loss is: 0.00123355661444715\n",
      "test loss is 0.0007186287819243242\n",
      "Batch: 30000,train loss is: 0.0003373188963498493\n",
      "test loss is 0.0004008398231251581\n",
      "Batch: 30100,train loss is: 0.0007659931757915842\n",
      "test loss is 0.0016410811966321276\n",
      "Batch: 30200,train loss is: 0.0005729856442282234\n",
      "test loss is 0.0006892235266120613\n",
      "Batch: 30300,train loss is: 0.00041730602721082424\n",
      "test loss is 0.0013213824314309793\n",
      "Batch: 30400,train loss is: 0.0024822992118695665\n",
      "test loss is 0.005263461675444584\n",
      "Batch: 30500,train loss is: 0.0005192511713886209\n",
      "test loss is 0.000794369995255113\n",
      "Batch: 30600,train loss is: 0.0003725825791721348\n",
      "test loss is 0.0009583213609966925\n",
      "Batch: 30700,train loss is: 0.0010352858045146345\n",
      "test loss is 0.001049021102777141\n",
      "Batch: 30800,train loss is: 0.00018731256027516778\n",
      "test loss is 0.0006271448501693768\n",
      "Batch: 30900,train loss is: 0.0003630807059780349\n",
      "test loss is 0.00047273609402323285\n",
      "Batch: 31000,train loss is: 0.00016275019545375415\n",
      "test loss is 0.00044611422857347054\n",
      "Batch: 31100,train loss is: 0.0006545671906336156\n",
      "test loss is 0.0010059340664375163\n",
      "Batch: 31200,train loss is: 0.0006993490474337694\n",
      "test loss is 0.0005983706840773481\n",
      "Batch: 31300,train loss is: 0.00020456309589708782\n",
      "test loss is 0.00044866083487182464\n",
      "Batch: 31400,train loss is: 0.000292852122026279\n",
      "test loss is 0.0009765986854986457\n",
      "Batch: 31500,train loss is: 0.00046342378180781534\n",
      "test loss is 0.0008492108186569357\n",
      "Batch: 31600,train loss is: 0.001319465966351526\n",
      "test loss is 0.0005796068464646442\n",
      "Batch: 31700,train loss is: 0.0004006004960281469\n",
      "test loss is 0.00043964625748552777\n",
      "Batch: 31800,train loss is: 0.00045651577115649804\n",
      "test loss is 0.001050509123323736\n",
      "Batch: 31900,train loss is: 0.00024050426194082965\n",
      "test loss is 0.0004347190384405382\n",
      "Batch: 32000,train loss is: 0.002166005511232789\n",
      "test loss is 0.0005957574285998339\n",
      "Batch: 32100,train loss is: 0.0007147451880283557\n",
      "test loss is 0.0012012871918731236\n",
      "Batch: 32200,train loss is: 0.0001996523518056326\n",
      "test loss is 0.0004407181275974377\n",
      "Batch: 32300,train loss is: 0.0007632354946283647\n",
      "test loss is 0.0008984145565060849\n",
      "Batch: 32400,train loss is: 0.000534954951951165\n",
      "test loss is 0.0008936457753866818\n",
      "Batch: 32500,train loss is: 0.0004919376935284402\n",
      "test loss is 0.0005931399484213664\n",
      "Batch: 32600,train loss is: 0.0005753433940029663\n",
      "test loss is 0.0007425374972176438\n",
      "Batch: 32700,train loss is: 0.00031350740892340017\n",
      "test loss is 0.001386722180863487\n",
      "Batch: 32800,train loss is: 0.00047053504082304526\n",
      "test loss is 0.0005340160293930593\n",
      "Batch: 32900,train loss is: 0.0006851627361450557\n",
      "test loss is 0.0005714466313037971\n",
      "Batch: 33000,train loss is: 0.00038818512337756244\n",
      "test loss is 0.0005012394445243372\n",
      "Batch: 33100,train loss is: 0.000456238795254214\n",
      "test loss is 0.001677482577122873\n",
      "Batch: 33200,train loss is: 0.0007029231904343421\n",
      "test loss is 0.0006093366938769583\n",
      "Batch: 33300,train loss is: 0.0005477638767429582\n",
      "test loss is 0.0004457048887652351\n",
      "Batch: 33400,train loss is: 0.0003617055248434368\n",
      "test loss is 0.0019663885380576947\n",
      "Batch: 33500,train loss is: 0.0007205127320038993\n",
      "test loss is 0.0030805556552062816\n",
      "Batch: 33600,train loss is: 0.0002367733045598977\n",
      "test loss is 0.0007109111159538234\n",
      "Batch: 33700,train loss is: 0.000871731939509884\n",
      "test loss is 0.0004878387570273199\n",
      "Batch: 33800,train loss is: 0.0009009615362659491\n",
      "test loss is 0.0008292657024731895\n",
      "Batch: 33900,train loss is: 0.0006946117146008717\n",
      "test loss is 0.00044437282749149314\n",
      "-----------------------Epoch: 5----------------------------------\n",
      "Batch: 0,train loss is: 0.0002675129207303122\n",
      "test loss is 0.0004667740003827596\n",
      "Batch: 100,train loss is: 0.000523399676620253\n",
      "test loss is 0.0003983995344910389\n",
      "Batch: 200,train loss is: 0.0013028152572001853\n",
      "test loss is 0.0007275900418854152\n",
      "Batch: 300,train loss is: 0.0007314360094125341\n",
      "test loss is 0.0010236200700515802\n",
      "Batch: 400,train loss is: 0.0005303294186657943\n",
      "test loss is 0.0006853766693663909\n",
      "Batch: 500,train loss is: 0.00031746334438928785\n",
      "test loss is 0.002692140513002779\n",
      "Batch: 600,train loss is: 0.00042204865250532235\n",
      "test loss is 0.0020863997024711633\n",
      "Batch: 700,train loss is: 0.00040037832461790597\n",
      "test loss is 0.00067039249839389\n",
      "Batch: 800,train loss is: 0.0008252894135350924\n",
      "test loss is 0.0006174894453607755\n",
      "Batch: 900,train loss is: 0.0006011666681462143\n",
      "test loss is 0.0006682797038456139\n",
      "Batch: 1000,train loss is: 0.0004445506662566578\n",
      "test loss is 0.0005221224113640487\n",
      "Batch: 1100,train loss is: 0.0014181101345741066\n",
      "test loss is 0.0007104187464126304\n",
      "Batch: 1200,train loss is: 0.0005356651562621884\n",
      "test loss is 0.0008904485567047405\n",
      "Batch: 1300,train loss is: 0.0003627282049502\n",
      "test loss is 0.0015106690923133359\n",
      "Batch: 1400,train loss is: 0.00025586219721191687\n",
      "test loss is 0.0005262604911988581\n",
      "Batch: 1500,train loss is: 0.0016614719097250708\n",
      "test loss is 0.0010583344942053196\n",
      "Batch: 1600,train loss is: 0.0017349147908175253\n",
      "test loss is 0.0009034784372964279\n",
      "Batch: 1700,train loss is: 0.0014564643518467167\n",
      "test loss is 0.0007156314587024931\n",
      "Batch: 1800,train loss is: 0.0003663315750657798\n",
      "test loss is 0.0005921635388153725\n",
      "Batch: 1900,train loss is: 0.003835727505703487\n",
      "test loss is 0.001994565171960425\n",
      "Batch: 2000,train loss is: 0.0007763201974746054\n",
      "test loss is 0.0016065896993640826\n",
      "Batch: 2100,train loss is: 0.0022425432256813283\n",
      "test loss is 0.000683892767910977\n",
      "Batch: 2200,train loss is: 0.0004537202694517159\n",
      "test loss is 0.0004058025434035819\n",
      "Batch: 2300,train loss is: 0.0003592414948304059\n",
      "test loss is 0.0004261941065597359\n",
      "Batch: 2400,train loss is: 0.00023762432687931688\n",
      "test loss is 0.0004974304329920435\n",
      "Batch: 2500,train loss is: 0.0003924636518823574\n",
      "test loss is 0.0006677385370828819\n",
      "Batch: 2600,train loss is: 0.0011666698301573957\n",
      "test loss is 0.0005682018438844888\n",
      "Batch: 2700,train loss is: 0.0003191166425735231\n",
      "test loss is 0.00046778582907530745\n",
      "Batch: 2800,train loss is: 0.000410514760948129\n",
      "test loss is 0.0006008960249501492\n",
      "Batch: 2900,train loss is: 0.0005139283768563762\n",
      "test loss is 0.0005836368112172406\n",
      "Batch: 3000,train loss is: 0.0003739346951868786\n",
      "test loss is 0.0011718980209637138\n",
      "Batch: 3100,train loss is: 0.00029664571155125965\n",
      "test loss is 0.0006606825791720157\n",
      "Batch: 3200,train loss is: 0.0005446990051129555\n",
      "test loss is 0.0007192600527789053\n",
      "Batch: 3300,train loss is: 0.00041506041518483706\n",
      "test loss is 0.0005608988255967158\n",
      "Batch: 3400,train loss is: 0.000736014184629481\n",
      "test loss is 0.0006039126156080399\n",
      "Batch: 3500,train loss is: 0.0003708806188443311\n",
      "test loss is 0.0008341848025338651\n",
      "Batch: 3600,train loss is: 0.00018100545238855428\n",
      "test loss is 0.001078295306475376\n",
      "Batch: 3700,train loss is: 0.000400313585338426\n",
      "test loss is 0.0009340352896541768\n",
      "Batch: 3800,train loss is: 0.0007670060833332847\n",
      "test loss is 0.0006082807306102243\n",
      "Batch: 3900,train loss is: 0.000678741341227591\n",
      "test loss is 0.0010757131201833599\n",
      "Batch: 4000,train loss is: 0.0003229241173996521\n",
      "test loss is 0.0008365736695282827\n",
      "Batch: 4100,train loss is: 0.0008717640174064972\n",
      "test loss is 0.0004602729096352667\n",
      "Batch: 4200,train loss is: 0.000751993976074997\n",
      "test loss is 0.00041317460323084287\n",
      "Batch: 4300,train loss is: 0.0003102814129500576\n",
      "test loss is 0.0007479244744155977\n",
      "Batch: 4400,train loss is: 0.0007142454449433024\n",
      "test loss is 0.0006334189612468347\n",
      "Batch: 4500,train loss is: 0.00031054867158929346\n",
      "test loss is 0.000600081601927983\n",
      "Batch: 4600,train loss is: 0.0009135589221080223\n",
      "test loss is 0.0011237223779354164\n",
      "Batch: 4700,train loss is: 0.0015019170091733096\n",
      "test loss is 0.0011496496740918873\n",
      "Batch: 4800,train loss is: 0.001998839731289248\n",
      "test loss is 0.002265197130681107\n",
      "Batch: 4900,train loss is: 0.002419123214756643\n",
      "test loss is 0.0012399233836437355\n",
      "Batch: 5000,train loss is: 0.00015470800241050169\n",
      "test loss is 0.00047762135217100174\n",
      "Batch: 5100,train loss is: 0.0002545786937291798\n",
      "test loss is 0.0004396163135722335\n",
      "Batch: 5200,train loss is: 0.0007625408610152803\n",
      "test loss is 0.0008914068791082211\n",
      "Batch: 5300,train loss is: 0.002261089994448588\n",
      "test loss is 0.0017485719931203125\n",
      "Batch: 5400,train loss is: 0.0005101994176868768\n",
      "test loss is 0.0005293681297067614\n",
      "Batch: 5500,train loss is: 0.0009224431683982097\n",
      "test loss is 0.0014061804438908405\n",
      "Batch: 5600,train loss is: 0.0006832129221490036\n",
      "test loss is 0.001190104191042898\n",
      "Batch: 5700,train loss is: 0.00039493986987717194\n",
      "test loss is 0.0006196270714081233\n",
      "Batch: 5800,train loss is: 0.0003386756609935324\n",
      "test loss is 0.0005424419100172738\n",
      "Batch: 5900,train loss is: 0.0012757390888106718\n",
      "test loss is 0.0012305255008331305\n",
      "Batch: 6000,train loss is: 0.000863175944349356\n",
      "test loss is 0.0006165886901581107\n",
      "Batch: 6100,train loss is: 0.0005523662140560646\n",
      "test loss is 0.0005616779358415358\n",
      "Batch: 6200,train loss is: 0.0003620693484076524\n",
      "test loss is 0.0005043498560165397\n",
      "Batch: 6300,train loss is: 0.0012532731328337315\n",
      "test loss is 0.0011309822948713584\n",
      "Batch: 6400,train loss is: 0.0007139477974222444\n",
      "test loss is 0.0010051163317520524\n",
      "Batch: 6500,train loss is: 0.0021099820043700224\n",
      "test loss is 0.0009583446681232829\n",
      "Batch: 6600,train loss is: 0.02354729751361492\n",
      "test loss is 0.0013654399753408057\n",
      "Batch: 6700,train loss is: 0.00026798573932653087\n",
      "test loss is 0.000798513894530351\n",
      "Batch: 6800,train loss is: 0.0006374727022389488\n",
      "test loss is 0.0005097956832340809\n",
      "Batch: 6900,train loss is: 0.0021524518004886455\n",
      "test loss is 0.0008635848878711207\n",
      "Batch: 7000,train loss is: 0.0010622623766836795\n",
      "test loss is 0.0005421021753654289\n",
      "Batch: 7100,train loss is: 0.0004425936104066849\n",
      "test loss is 0.0004764534587687869\n",
      "Batch: 7200,train loss is: 0.00018250767824847417\n",
      "test loss is 0.0004135114407565016\n",
      "Batch: 7300,train loss is: 0.0022226587718270885\n",
      "test loss is 0.000396230259235563\n",
      "Batch: 7400,train loss is: 0.0011903390074749168\n",
      "test loss is 0.0006141278944497707\n",
      "Batch: 7500,train loss is: 0.0006009597021381927\n",
      "test loss is 0.0004152764211438728\n",
      "Batch: 7600,train loss is: 0.0010339032917702677\n",
      "test loss is 0.0006216002781757182\n",
      "Batch: 7700,train loss is: 0.0007494482459569444\n",
      "test loss is 0.0018147014058131888\n",
      "Batch: 7800,train loss is: 0.00032164961034658737\n",
      "test loss is 0.0007975188865491257\n",
      "Batch: 7900,train loss is: 0.0008781896997167592\n",
      "test loss is 0.0005296191336124037\n",
      "Batch: 8000,train loss is: 0.0008813185769065449\n",
      "test loss is 0.0012350898944395578\n",
      "Batch: 8100,train loss is: 0.0009583945680034644\n",
      "test loss is 0.0006451692316779222\n",
      "Batch: 8200,train loss is: 0.0007084407152932126\n",
      "test loss is 0.00047674970612694045\n",
      "Batch: 8300,train loss is: 0.00034128263899548726\n",
      "test loss is 0.000980655441256499\n",
      "Batch: 8400,train loss is: 0.00048763557179974724\n",
      "test loss is 0.0007905968593117269\n",
      "Batch: 8500,train loss is: 0.0004087293114624799\n",
      "test loss is 0.0006164039204178044\n",
      "Batch: 8600,train loss is: 0.0004761229212065072\n",
      "test loss is 0.0006328882390815613\n",
      "Batch: 8700,train loss is: 0.0002943417365790495\n",
      "test loss is 0.0008200817862692976\n",
      "Batch: 8800,train loss is: 0.00043248950584980707\n",
      "test loss is 0.0006115391756765309\n",
      "Batch: 8900,train loss is: 0.0008219456068533117\n",
      "test loss is 0.00065487895674288\n",
      "Batch: 9000,train loss is: 0.00024361700311922272\n",
      "test loss is 0.000554449285647297\n",
      "Batch: 9100,train loss is: 0.0013013085108229392\n",
      "test loss is 0.0019479074445654956\n",
      "Batch: 9200,train loss is: 0.000693352675603541\n",
      "test loss is 0.002499258004506584\n",
      "Batch: 9300,train loss is: 0.0011769929598514153\n",
      "test loss is 0.0006607253302732618\n",
      "Batch: 9400,train loss is: 0.0007065181145352154\n",
      "test loss is 0.0006158045275111904\n",
      "Batch: 9500,train loss is: 0.000696113674241576\n",
      "test loss is 0.001516549413401062\n",
      "Batch: 9600,train loss is: 0.0004542586185910116\n",
      "test loss is 0.0007558742789920976\n",
      "Batch: 9700,train loss is: 0.00019130650453790893\n",
      "test loss is 0.000485492836613037\n",
      "Batch: 9800,train loss is: 0.00024722927486908877\n",
      "test loss is 0.0008457764365408227\n",
      "Batch: 9900,train loss is: 0.0006491195179649938\n",
      "test loss is 0.0007001672620946988\n",
      "Batch: 10000,train loss is: 0.0003683331745279436\n",
      "test loss is 0.000596107579248268\n",
      "Batch: 10100,train loss is: 0.0021297687581746643\n",
      "test loss is 0.00045543739768693373\n",
      "Batch: 10200,train loss is: 0.0016781594501950005\n",
      "test loss is 0.00044251343555955984\n",
      "Batch: 10300,train loss is: 0.0016336228213409798\n",
      "test loss is 0.001284328330027262\n",
      "Batch: 10400,train loss is: 0.0015444061373842374\n",
      "test loss is 0.0019054701554336328\n",
      "Batch: 10500,train loss is: 0.0018274953552918337\n",
      "test loss is 0.0010699223596625901\n",
      "Batch: 10600,train loss is: 0.00014719548457971525\n",
      "test loss is 0.0007218772191302547\n",
      "Batch: 10700,train loss is: 0.0007331562102249625\n",
      "test loss is 0.0006318337881572137\n",
      "Batch: 10800,train loss is: 0.00039551924486724464\n",
      "test loss is 0.0005585078870472364\n",
      "Batch: 10900,train loss is: 0.0005043129223537193\n",
      "test loss is 0.0008668703200357446\n",
      "Batch: 11000,train loss is: 0.000602207741358434\n",
      "test loss is 0.000400764079152281\n",
      "Batch: 11100,train loss is: 0.0003447671790468819\n",
      "test loss is 0.00038583795236007474\n",
      "Batch: 11200,train loss is: 0.0006379277353447744\n",
      "test loss is 0.0004578245540903505\n",
      "Batch: 11300,train loss is: 0.0001928642111461313\n",
      "test loss is 0.00036309345033027154\n",
      "Batch: 11400,train loss is: 0.0006960213550064497\n",
      "test loss is 0.0010449552377327538\n",
      "Batch: 11500,train loss is: 0.00021987394795402622\n",
      "test loss is 0.000894467670753672\n",
      "Batch: 11600,train loss is: 0.0015468236821545667\n",
      "test loss is 0.001611434076574151\n",
      "Batch: 11700,train loss is: 0.00026114093801023067\n",
      "test loss is 0.0005867150613779668\n",
      "Batch: 11800,train loss is: 0.001039824174418336\n",
      "test loss is 0.0010689263439728981\n",
      "Batch: 11900,train loss is: 0.0007666418874282952\n",
      "test loss is 0.0009709909639460822\n",
      "Batch: 12000,train loss is: 0.0004177834089086301\n",
      "test loss is 0.0005623313391703606\n",
      "Batch: 12100,train loss is: 0.004231470434262328\n",
      "test loss is 0.0015110253071388116\n",
      "Batch: 12200,train loss is: 0.0006172034184611099\n",
      "test loss is 0.002237158216059538\n",
      "Batch: 12300,train loss is: 0.0015757557774909382\n",
      "test loss is 0.0006676149844356339\n",
      "Batch: 12400,train loss is: 0.0002794217415560971\n",
      "test loss is 0.00046688745348408934\n",
      "Batch: 12500,train loss is: 0.0019934241839835807\n",
      "test loss is 0.001260571154874933\n",
      "Batch: 12600,train loss is: 0.0005221285617424142\n",
      "test loss is 0.0004544228741620966\n",
      "Batch: 12700,train loss is: 0.00034539433807855805\n",
      "test loss is 0.0003986268536704759\n",
      "Batch: 12800,train loss is: 0.0001662539639277893\n",
      "test loss is 0.0004964605184456286\n",
      "Batch: 12900,train loss is: 0.00038236475263852453\n",
      "test loss is 0.0010622332302436715\n",
      "Batch: 13000,train loss is: 0.0007222009383150742\n",
      "test loss is 0.0008342540711154914\n",
      "Batch: 13100,train loss is: 0.0005015241200274255\n",
      "test loss is 0.0006009834121608538\n",
      "Batch: 13200,train loss is: 0.0003395774261138544\n",
      "test loss is 0.00047151378264973605\n",
      "Batch: 13300,train loss is: 0.00030257065633613106\n",
      "test loss is 0.002239682721820183\n",
      "Batch: 13400,train loss is: 0.0005601730835829717\n",
      "test loss is 0.000971966968929118\n",
      "Batch: 13500,train loss is: 0.0004726729577067254\n",
      "test loss is 0.0006634663876316742\n",
      "Batch: 13600,train loss is: 0.003946489042152678\n",
      "test loss is 0.0038374829916988863\n",
      "Batch: 13700,train loss is: 0.00047720468803198827\n",
      "test loss is 0.0009892844265834994\n",
      "Batch: 13800,train loss is: 0.00017288833107461765\n",
      "test loss is 0.0006093506207494261\n",
      "Batch: 13900,train loss is: 0.0006499849083999089\n",
      "test loss is 0.0005535969615633991\n",
      "Batch: 14000,train loss is: 0.00030396949118680696\n",
      "test loss is 0.0005393661437038287\n",
      "Batch: 14100,train loss is: 0.00022566343255105937\n",
      "test loss is 0.0007212111166369232\n",
      "Batch: 14200,train loss is: 0.0002789402598584928\n",
      "test loss is 0.00035583793070435334\n",
      "Batch: 14300,train loss is: 0.00045701046485815196\n",
      "test loss is 0.0005509262496258765\n",
      "Batch: 14400,train loss is: 0.0004031292672441339\n",
      "test loss is 0.00046779189052659524\n",
      "Batch: 14500,train loss is: 0.0002664237902936514\n",
      "test loss is 0.0003593117362843669\n",
      "Batch: 14600,train loss is: 0.0011299112981187434\n",
      "test loss is 0.0005724252381912894\n",
      "Batch: 14700,train loss is: 0.00022606128672313414\n",
      "test loss is 0.00041454250539868585\n",
      "Batch: 14800,train loss is: 0.00039359702562457075\n",
      "test loss is 0.0005667435648030157\n",
      "Batch: 14900,train loss is: 0.00018125352330106978\n",
      "test loss is 0.0006207260276722021\n",
      "Batch: 15000,train loss is: 0.00023296802170191195\n",
      "test loss is 0.0006454205607076489\n",
      "Batch: 15100,train loss is: 0.00075926582974794\n",
      "test loss is 0.0004588235564124121\n",
      "Batch: 15200,train loss is: 0.003241314498154856\n",
      "test loss is 0.0007035412701448682\n",
      "Batch: 15300,train loss is: 0.0011095521399645343\n",
      "test loss is 0.0010518032921911053\n",
      "Batch: 15400,train loss is: 0.0007714456839579535\n",
      "test loss is 0.0013868782028626325\n",
      "Batch: 15500,train loss is: 0.0009332205910298287\n",
      "test loss is 0.0009309058524511341\n",
      "Batch: 15600,train loss is: 0.0006302260629191797\n",
      "test loss is 0.0006663132515195889\n",
      "Batch: 15700,train loss is: 0.0006736027132444828\n",
      "test loss is 0.0009687676064681966\n",
      "Batch: 15800,train loss is: 0.0006851616639231068\n",
      "test loss is 0.0007808225083459535\n",
      "Batch: 15900,train loss is: 0.0006776933134029616\n",
      "test loss is 0.0005277797635928228\n",
      "Batch: 16000,train loss is: 0.0006868503690406137\n",
      "test loss is 0.0004441495033587928\n",
      "Batch: 16100,train loss is: 0.0010550216678569715\n",
      "test loss is 0.0007089754983670307\n",
      "Batch: 16200,train loss is: 0.0005665625070723956\n",
      "test loss is 0.0009064619318542539\n",
      "Batch: 16300,train loss is: 0.0004241133131909561\n",
      "test loss is 0.000553821092562274\n",
      "Batch: 16400,train loss is: 0.0004574608096156247\n",
      "test loss is 0.00038415478974435227\n",
      "Batch: 16500,train loss is: 0.0016129755588890446\n",
      "test loss is 0.0011783742864146702\n",
      "Batch: 16600,train loss is: 0.00022376785664389857\n",
      "test loss is 0.0013561684240530486\n",
      "Batch: 16700,train loss is: 0.0008349482934311665\n",
      "test loss is 0.0015973908713463624\n",
      "Batch: 16800,train loss is: 0.0007745119785377295\n",
      "test loss is 0.0005928454196477054\n",
      "Batch: 16900,train loss is: 0.0011667167614652493\n",
      "test loss is 0.0015754961290543757\n",
      "Batch: 17000,train loss is: 0.0002586104065992064\n",
      "test loss is 0.0005499599089421787\n",
      "Batch: 17100,train loss is: 0.0007335914438740341\n",
      "test loss is 0.0007993131042219337\n",
      "Batch: 17200,train loss is: 0.0001583812285528998\n",
      "test loss is 0.0003357415808109438\n",
      "Batch: 17300,train loss is: 0.0004055807132883953\n",
      "test loss is 0.00040623475344375145\n",
      "Batch: 17400,train loss is: 0.0009219527731456567\n",
      "test loss is 0.0008977304873324098\n",
      "Batch: 17500,train loss is: 0.0008152268786699195\n",
      "test loss is 0.0005922196045211436\n",
      "Batch: 17600,train loss is: 0.00030402342980498446\n",
      "test loss is 0.0006360856671056832\n",
      "Batch: 17700,train loss is: 0.0005894647628681166\n",
      "test loss is 0.0009100424614040177\n",
      "Batch: 17800,train loss is: 0.0006297654178725876\n",
      "test loss is 0.0006641475046129949\n",
      "Batch: 17900,train loss is: 0.0015415125995556828\n",
      "test loss is 0.0013854528764001845\n",
      "Batch: 18000,train loss is: 0.001061847437941926\n",
      "test loss is 0.0006025724636183233\n",
      "Batch: 18100,train loss is: 0.00021991780092226167\n",
      "test loss is 0.0005355110443697083\n",
      "Batch: 18200,train loss is: 0.0002518588405375497\n",
      "test loss is 0.0009334415735511262\n",
      "Batch: 18300,train loss is: 0.002143895164443823\n",
      "test loss is 0.0005344081037152408\n",
      "Batch: 18400,train loss is: 0.0002528208574828808\n",
      "test loss is 0.0006153552778237343\n",
      "Batch: 18500,train loss is: 0.0027134128189438543\n",
      "test loss is 0.0013867600957545318\n",
      "Batch: 18600,train loss is: 0.0007725184489299107\n",
      "test loss is 0.0013187484101597358\n",
      "Batch: 18700,train loss is: 0.0005940679034664876\n",
      "test loss is 0.0005460827670738877\n",
      "Batch: 18800,train loss is: 0.0003980572585522175\n",
      "test loss is 0.0005413654165747277\n",
      "Batch: 18900,train loss is: 0.0006203817526034797\n",
      "test loss is 0.00045590354061844835\n",
      "Batch: 19000,train loss is: 0.0006904823376619881\n",
      "test loss is 0.0006105910001290975\n",
      "Batch: 19100,train loss is: 0.0008150403008750615\n",
      "test loss is 0.0007899265039202326\n",
      "Batch: 19200,train loss is: 0.0005314062837505431\n",
      "test loss is 0.0006500909089106737\n",
      "Batch: 19300,train loss is: 0.0013352788399018886\n",
      "test loss is 0.002677394735023578\n",
      "Batch: 19400,train loss is: 0.00015028084556260795\n",
      "test loss is 0.0005032518711758\n",
      "Batch: 19500,train loss is: 0.0004115417379311486\n",
      "test loss is 0.0007385280955453798\n",
      "Batch: 19600,train loss is: 0.0009115159856188925\n",
      "test loss is 0.0010220386779646502\n",
      "Batch: 19700,train loss is: 0.0016062008154322076\n",
      "test loss is 0.0007593282970875238\n",
      "Batch: 19800,train loss is: 0.0004289988741372137\n",
      "test loss is 0.0005522627271853369\n",
      "Batch: 19900,train loss is: 0.0009043344725510222\n",
      "test loss is 0.0008374362634751548\n",
      "Batch: 20000,train loss is: 0.000719640867883535\n",
      "test loss is 0.000619039804197318\n",
      "Batch: 20100,train loss is: 0.0008844198833888289\n",
      "test loss is 0.0005289321658896374\n",
      "Batch: 20200,train loss is: 0.00026433133480852506\n",
      "test loss is 0.00046933272380633153\n",
      "Batch: 20300,train loss is: 0.000125640736231908\n",
      "test loss is 0.0005750690406886855\n",
      "Batch: 20400,train loss is: 0.00207958560217149\n",
      "test loss is 0.0026696691530121775\n",
      "Batch: 20500,train loss is: 0.0003472052970741203\n",
      "test loss is 0.0006332012011010369\n",
      "Batch: 20600,train loss is: 0.0024572749933894893\n",
      "test loss is 0.0007276437276451495\n",
      "Batch: 20700,train loss is: 0.0005234313527578446\n",
      "test loss is 0.0008840287723474472\n",
      "Batch: 20800,train loss is: 0.000843023223488717\n",
      "test loss is 0.0005248049631311203\n",
      "Batch: 20900,train loss is: 0.00016758665574513043\n",
      "test loss is 0.0005959310022265619\n",
      "Batch: 21000,train loss is: 0.0003274953072644702\n",
      "test loss is 0.0006953820436726185\n",
      "Batch: 21100,train loss is: 0.0003134574306882541\n",
      "test loss is 0.00040650148340390895\n",
      "Batch: 21200,train loss is: 0.00016741401362182736\n",
      "test loss is 0.000566276008017235\n",
      "Batch: 21300,train loss is: 0.0005944515654110777\n",
      "test loss is 0.0007964993961003988\n",
      "Batch: 21400,train loss is: 0.0004978500574538923\n",
      "test loss is 0.0006660688772199165\n",
      "Batch: 21500,train loss is: 0.00018163068159044612\n",
      "test loss is 0.00047824785745034386\n",
      "Batch: 21600,train loss is: 0.0004877750849404413\n",
      "test loss is 0.0005808403419890705\n",
      "Batch: 21700,train loss is: 0.0002771645923594334\n",
      "test loss is 0.0007887004695426577\n",
      "Batch: 21800,train loss is: 0.00047763524508229324\n",
      "test loss is 0.0004558602725264669\n",
      "Batch: 21900,train loss is: 0.0009854355035658852\n",
      "test loss is 0.0009020614951002873\n",
      "Batch: 22000,train loss is: 0.0017941515574038639\n",
      "test loss is 0.001234574565317567\n",
      "Batch: 22100,train loss is: 0.00019519509846385757\n",
      "test loss is 0.0011190549684199113\n",
      "Batch: 22200,train loss is: 0.0011521855867476678\n",
      "test loss is 0.0011502730582859755\n",
      "Batch: 22300,train loss is: 0.00129221304965161\n",
      "test loss is 0.001773661144568503\n",
      "Batch: 22400,train loss is: 0.0004788441322329941\n",
      "test loss is 0.0006634440312096787\n",
      "Batch: 22500,train loss is: 0.00022190870671928527\n",
      "test loss is 0.00046686485160142165\n",
      "Batch: 22600,train loss is: 0.0005992192532560544\n",
      "test loss is 0.0005484216959673793\n",
      "Batch: 22700,train loss is: 0.0007448460689646732\n",
      "test loss is 0.0010592290608092535\n",
      "Batch: 22800,train loss is: 0.0003837685158070316\n",
      "test loss is 0.0006269229208791938\n",
      "Batch: 22900,train loss is: 0.0005935837248350641\n",
      "test loss is 0.0009652120872478454\n",
      "Batch: 23000,train loss is: 0.00027984092578699985\n",
      "test loss is 0.00034747042591578344\n",
      "Batch: 23100,train loss is: 0.00030981456673587475\n",
      "test loss is 0.0008973520350539533\n",
      "Batch: 23200,train loss is: 0.0002323383941722372\n",
      "test loss is 0.00044883008878560154\n",
      "Batch: 23300,train loss is: 0.00018451504012016278\n",
      "test loss is 0.00035538504533701973\n",
      "Batch: 23400,train loss is: 0.00034294746158480924\n",
      "test loss is 0.0005109128857648707\n",
      "Batch: 23500,train loss is: 0.0020107750328368203\n",
      "test loss is 0.0005957401134654093\n",
      "Batch: 23600,train loss is: 0.0003851211505931941\n",
      "test loss is 0.0005609609777762161\n",
      "Batch: 23700,train loss is: 0.0003899111041252465\n",
      "test loss is 0.0003960057550981193\n",
      "Batch: 23800,train loss is: 0.0008379756913741212\n",
      "test loss is 0.0006045878721589824\n",
      "Batch: 23900,train loss is: 0.0022743216820329645\n",
      "test loss is 0.001514739309733577\n",
      "Batch: 24000,train loss is: 0.0003295930042250312\n",
      "test loss is 0.0016363962073052706\n",
      "Batch: 24100,train loss is: 0.0006074136339177833\n",
      "test loss is 0.00096257227003906\n",
      "Batch: 24200,train loss is: 0.0036508886939591715\n",
      "test loss is 0.000518836390024759\n",
      "Batch: 24300,train loss is: 0.0007533457084165418\n",
      "test loss is 0.0008502242576226076\n",
      "Batch: 24400,train loss is: 0.000694846090802456\n",
      "test loss is 0.0004310278097896705\n",
      "Batch: 24500,train loss is: 0.0007989239882160769\n",
      "test loss is 0.0007109744412934248\n",
      "Batch: 24600,train loss is: 0.0004382329826606065\n",
      "test loss is 0.0004723063381588081\n",
      "Batch: 24700,train loss is: 0.00036369509141542675\n",
      "test loss is 0.00039642197397174694\n",
      "Batch: 24800,train loss is: 0.0007709464189028087\n",
      "test loss is 0.0004763653496976243\n",
      "Batch: 24900,train loss is: 0.0003371742999381119\n",
      "test loss is 0.0010528515141660433\n",
      "Batch: 25000,train loss is: 0.00043609063417606483\n",
      "test loss is 0.0007444517833570736\n",
      "Batch: 25100,train loss is: 0.00031115287544262224\n",
      "test loss is 0.0007990629450391936\n",
      "Batch: 25200,train loss is: 0.00040016297460841813\n",
      "test loss is 0.0005660812493024958\n",
      "Batch: 25300,train loss is: 0.0005266854220785021\n",
      "test loss is 0.00046209268781222075\n",
      "Batch: 25400,train loss is: 0.000236405635129496\n",
      "test loss is 0.0003396404117860421\n",
      "Batch: 25500,train loss is: 0.000257116383567416\n",
      "test loss is 0.0004517464883020395\n",
      "Batch: 25600,train loss is: 0.0005535350238057414\n",
      "test loss is 0.00037160218975089974\n",
      "Batch: 25700,train loss is: 0.0002889927321182098\n",
      "test loss is 0.00039082456988534953\n",
      "Batch: 25800,train loss is: 0.0006516173833489426\n",
      "test loss is 0.0006238400396575983\n",
      "Batch: 25900,train loss is: 0.0008647684636212415\n",
      "test loss is 0.001022299206003723\n",
      "Batch: 26000,train loss is: 0.0009645579953735056\n",
      "test loss is 0.0034266483635765127\n",
      "Batch: 26100,train loss is: 0.0004728435321937637\n",
      "test loss is 0.0010817356152425768\n",
      "Batch: 26200,train loss is: 0.0010867846795931209\n",
      "test loss is 0.0005281343422161715\n",
      "Batch: 26300,train loss is: 0.00047720290649404945\n",
      "test loss is 0.0005633029194440396\n",
      "Batch: 26400,train loss is: 0.00045604779806639574\n",
      "test loss is 0.0006160615529037554\n",
      "Batch: 26500,train loss is: 0.0006427648899935613\n",
      "test loss is 0.000737861543453168\n",
      "Batch: 26600,train loss is: 0.0034949393381461216\n",
      "test loss is 0.0013907254287290414\n",
      "Batch: 26700,train loss is: 0.00032768483614025937\n",
      "test loss is 0.0005990236351705357\n",
      "Batch: 26800,train loss is: 0.0008620828414774737\n",
      "test loss is 0.0003993747764349715\n",
      "Batch: 26900,train loss is: 0.00033256448864319403\n",
      "test loss is 0.0003817613394162898\n",
      "Batch: 27000,train loss is: 0.00048189307038062304\n",
      "test loss is 0.0005029480199877395\n",
      "Batch: 27100,train loss is: 0.0002486745207308977\n",
      "test loss is 0.00045311729359553526\n",
      "Batch: 27200,train loss is: 0.00036177891758861885\n",
      "test loss is 0.0008527260215651553\n",
      "Batch: 27300,train loss is: 0.0012286457342921273\n",
      "test loss is 0.002082129442969849\n",
      "Batch: 27400,train loss is: 0.000675913815840497\n",
      "test loss is 0.001059146752400626\n",
      "Batch: 27500,train loss is: 0.0012741077846964235\n",
      "test loss is 0.0007381407393904326\n",
      "Batch: 27600,train loss is: 0.000290529466600953\n",
      "test loss is 0.0005106406532988358\n",
      "Batch: 27700,train loss is: 0.0005106802765064364\n",
      "test loss is 0.0006548333250024479\n",
      "Batch: 27800,train loss is: 0.000500824813841946\n",
      "test loss is 0.0008777193897653713\n",
      "Batch: 27900,train loss is: 0.000854948804110138\n",
      "test loss is 0.00047453990102699575\n",
      "Batch: 28000,train loss is: 0.00021424797338509742\n",
      "test loss is 0.0007229515487814481\n",
      "Batch: 28100,train loss is: 0.0005479580177377692\n",
      "test loss is 0.0006070849779834715\n",
      "Batch: 28200,train loss is: 0.0004168111529209912\n",
      "test loss is 0.0008054854566755197\n",
      "Batch: 28300,train loss is: 0.0007055103980396091\n",
      "test loss is 0.00047837529259089315\n",
      "Batch: 28400,train loss is: 0.000852556614366919\n",
      "test loss is 0.0005296354483822814\n",
      "Batch: 28500,train loss is: 0.0005057586503673186\n",
      "test loss is 0.00040860982415242896\n",
      "Batch: 28600,train loss is: 0.0004589269771624493\n",
      "test loss is 0.0007048409901079096\n",
      "Batch: 28700,train loss is: 0.0002659725749621521\n",
      "test loss is 0.0006295979302371498\n",
      "Batch: 28800,train loss is: 0.0010932787871395526\n",
      "test loss is 0.0003492147892098689\n",
      "Batch: 28900,train loss is: 0.00039790224725803333\n",
      "test loss is 0.001134350446188037\n",
      "Batch: 29000,train loss is: 0.0004822260627026025\n",
      "test loss is 0.0006308464310663543\n",
      "Batch: 29100,train loss is: 0.000778374399016043\n",
      "test loss is 0.0025120252088474486\n",
      "Batch: 29200,train loss is: 0.00023287925744511597\n",
      "test loss is 0.0006476430995815969\n",
      "Batch: 29300,train loss is: 0.0003727137526703852\n",
      "test loss is 0.000589988052549043\n",
      "Batch: 29400,train loss is: 0.00026760394556212637\n",
      "test loss is 0.0011347970140471258\n",
      "Batch: 29500,train loss is: 0.0003747846347170496\n",
      "test loss is 0.0011208458012563897\n",
      "Batch: 29600,train loss is: 0.00042303947772312104\n",
      "test loss is 0.0003709070873469207\n",
      "Batch: 29700,train loss is: 0.0004651018179642708\n",
      "test loss is 0.0008404303887812857\n",
      "Batch: 29800,train loss is: 0.0009414615778281576\n",
      "test loss is 0.0006609710409727499\n",
      "Batch: 29900,train loss is: 0.001063788362169174\n",
      "test loss is 0.0006485988299613586\n",
      "Batch: 30000,train loss is: 0.00023258409924741425\n",
      "test loss is 0.0003649714804695989\n",
      "Batch: 30100,train loss is: 0.0006958075265436584\n",
      "test loss is 0.0008976352120515039\n",
      "Batch: 30200,train loss is: 0.0004680141223961976\n",
      "test loss is 0.0006408869006445026\n",
      "Batch: 30300,train loss is: 0.0005103599865718758\n",
      "test loss is 0.0014691851225528641\n",
      "Batch: 30400,train loss is: 0.0025115219029021025\n",
      "test loss is 0.006662534398805743\n",
      "Batch: 30500,train loss is: 0.0007258769936337468\n",
      "test loss is 0.001002817497466749\n",
      "Batch: 30600,train loss is: 0.00014858508708554373\n",
      "test loss is 0.0005496502480452487\n",
      "Batch: 30700,train loss is: 0.001449065059596045\n",
      "test loss is 0.001117930606041963\n",
      "Batch: 30800,train loss is: 0.00016398157971108288\n",
      "test loss is 0.00043598697645035235\n",
      "Batch: 30900,train loss is: 0.00027043290596725936\n",
      "test loss is 0.0004068678614910336\n",
      "Batch: 31000,train loss is: 0.00019357174102539496\n",
      "test loss is 0.0003808222623572968\n",
      "Batch: 31100,train loss is: 0.0004458908701213451\n",
      "test loss is 0.0007014830702481156\n",
      "Batch: 31200,train loss is: 0.0009575462510235078\n",
      "test loss is 0.0006421024786734589\n",
      "Batch: 31300,train loss is: 0.0003544619844026695\n",
      "test loss is 0.0005589194586779884\n",
      "Batch: 31400,train loss is: 0.00038659767893431694\n",
      "test loss is 0.0008183710249158153\n",
      "Batch: 31500,train loss is: 0.0005165147301172631\n",
      "test loss is 0.000905554700965387\n",
      "Batch: 31600,train loss is: 0.0013172197914325125\n",
      "test loss is 0.0005783594816771747\n",
      "Batch: 31700,train loss is: 0.000293162310070156\n",
      "test loss is 0.0003392906309448278\n",
      "Batch: 31800,train loss is: 0.0004418740730988038\n",
      "test loss is 0.0008256501512106843\n",
      "Batch: 31900,train loss is: 0.00033972492799265924\n",
      "test loss is 0.00041098053412853277\n",
      "Batch: 32000,train loss is: 0.0020190874898668313\n",
      "test loss is 0.000493658314266033\n",
      "Batch: 32100,train loss is: 0.0007161925032671868\n",
      "test loss is 0.0013537195692444027\n",
      "Batch: 32200,train loss is: 0.00018995607367158806\n",
      "test loss is 0.00044365749642720666\n",
      "Batch: 32300,train loss is: 0.000792757259241886\n",
      "test loss is 0.0009331491455977095\n",
      "Batch: 32400,train loss is: 0.00040373158647516444\n",
      "test loss is 0.0006500929868132791\n",
      "Batch: 32500,train loss is: 0.0004842441206816018\n",
      "test loss is 0.0005780574107323924\n",
      "Batch: 32600,train loss is: 0.00040522953075973386\n",
      "test loss is 0.0005647978048824349\n",
      "Batch: 32700,train loss is: 0.0002082508880698966\n",
      "test loss is 0.0012614545796063295\n",
      "Batch: 32800,train loss is: 0.0005934420098000478\n",
      "test loss is 0.0005215151556315963\n",
      "Batch: 32900,train loss is: 0.00040005848485563916\n",
      "test loss is 0.0004045985013876183\n",
      "Batch: 33000,train loss is: 0.0001816265182370569\n",
      "test loss is 0.0004086334736161799\n",
      "Batch: 33100,train loss is: 0.0005534415378116418\n",
      "test loss is 0.001308575946379464\n",
      "Batch: 33200,train loss is: 0.0004996596978998465\n",
      "test loss is 0.0004944221365619598\n",
      "Batch: 33300,train loss is: 0.00038174042937286846\n",
      "test loss is 0.00035083941858272705\n",
      "Batch: 33400,train loss is: 0.0010983346707657902\n",
      "test loss is 0.0015329756975559358\n",
      "Batch: 33500,train loss is: 0.0014577007906613154\n",
      "test loss is 0.010245879557057495\n",
      "Batch: 33600,train loss is: 0.0010751546683729397\n",
      "test loss is 0.001030400063208006\n",
      "Batch: 33700,train loss is: 0.0007828969194135909\n",
      "test loss is 0.00048271068374493043\n",
      "Batch: 33800,train loss is: 0.0007375213669116556\n",
      "test loss is 0.000454628338053491\n",
      "Batch: 33900,train loss is: 0.0007599720659804829\n",
      "test loss is 0.0004408823065796385\n",
      "-----------------------Epoch: 6----------------------------------\n",
      "Batch: 0,train loss is: 0.00026796900404858054\n",
      "test loss is 0.00039321150019746415\n",
      "Batch: 100,train loss is: 0.0004580070793965008\n",
      "test loss is 0.0003745527310571073\n",
      "Batch: 200,train loss is: 0.00031279936502021934\n",
      "test loss is 0.00030283939991704474\n",
      "Batch: 300,train loss is: 0.0001798530870322233\n",
      "test loss is 0.00034411043660165274\n",
      "Batch: 400,train loss is: 0.00030666670045055995\n",
      "test loss is 0.00046999275276228477\n",
      "Batch: 500,train loss is: 0.00018527277500134353\n",
      "test loss is 0.000676145815780089\n",
      "Batch: 600,train loss is: 0.00011837731762945024\n",
      "test loss is 0.00038294081399955934\n",
      "Batch: 700,train loss is: 0.00020926312710814758\n",
      "test loss is 0.0006134531343035138\n",
      "Batch: 800,train loss is: 0.0005902395312651302\n",
      "test loss is 0.00039415210093856083\n",
      "Batch: 900,train loss is: 0.0002024950774966884\n",
      "test loss is 0.0005552814426515552\n",
      "Batch: 1000,train loss is: 0.0005747118399916674\n",
      "test loss is 0.000491238647350815\n",
      "Batch: 1100,train loss is: 0.001969149178014763\n",
      "test loss is 0.0005062449616413631\n",
      "Batch: 1200,train loss is: 0.00046812030626040455\n",
      "test loss is 0.0007045135984279797\n",
      "Batch: 1300,train loss is: 0.00044645190694072505\n",
      "test loss is 0.0024947973455412115\n",
      "Batch: 1400,train loss is: 0.00024306491509799622\n",
      "test loss is 0.0006002414272211498\n",
      "Batch: 1500,train loss is: 0.0013747445835230081\n",
      "test loss is 0.0012157693378837412\n",
      "Batch: 1600,train loss is: 0.003958462006534021\n",
      "test loss is 0.00249242589939146\n",
      "Batch: 1700,train loss is: 0.0014618094238445561\n",
      "test loss is 0.0007664166451380711\n",
      "Batch: 1800,train loss is: 0.000454088262620741\n",
      "test loss is 0.0007521104485538446\n",
      "Batch: 1900,train loss is: 0.0008590642744317931\n",
      "test loss is 0.0008660928612959915\n",
      "Batch: 2000,train loss is: 0.00029728638055838793\n",
      "test loss is 0.0006945870834277487\n",
      "Batch: 2100,train loss is: 0.0005792034576498179\n",
      "test loss is 0.00040092352629400227\n",
      "Batch: 2200,train loss is: 0.00045948886390573707\n",
      "test loss is 0.0003689973498839156\n",
      "Batch: 2300,train loss is: 0.0006334001069549156\n",
      "test loss is 0.0005097630874529108\n",
      "Batch: 2400,train loss is: 0.0003561823274162275\n",
      "test loss is 0.000749359618546346\n",
      "Batch: 2500,train loss is: 0.000670379988047303\n",
      "test loss is 0.0006247026617717085\n",
      "Batch: 2600,train loss is: 0.0016979060592104708\n",
      "test loss is 0.0006914053729002557\n",
      "Batch: 2700,train loss is: 0.00035316688818920087\n",
      "test loss is 0.00045978899571495773\n",
      "Batch: 2800,train loss is: 0.0003695093374421322\n",
      "test loss is 0.0005932996271994759\n",
      "Batch: 2900,train loss is: 0.0004583709065634088\n",
      "test loss is 0.0006173330739148451\n",
      "Batch: 3000,train loss is: 0.0003569825919674551\n",
      "test loss is 0.0011825965676091523\n",
      "Batch: 3100,train loss is: 0.0002226094316915743\n",
      "test loss is 0.0004250151293898549\n",
      "Batch: 3200,train loss is: 0.0005334745660318576\n",
      "test loss is 0.0009322886543207743\n",
      "Batch: 3300,train loss is: 0.0003421680968658702\n",
      "test loss is 0.00053296477955706\n",
      "Batch: 3400,train loss is: 0.001088040990277563\n",
      "test loss is 0.0009565292945541035\n",
      "Batch: 3500,train loss is: 0.00035457643249251224\n",
      "test loss is 0.0009584560398763533\n",
      "Batch: 3600,train loss is: 0.00026278755250881225\n",
      "test loss is 0.001265704297336675\n",
      "Batch: 3700,train loss is: 0.0003849338930421237\n",
      "test loss is 0.0006799442016907972\n",
      "Batch: 3800,train loss is: 0.0009031555141875535\n",
      "test loss is 0.00044552117859350147\n",
      "Batch: 3900,train loss is: 0.0005412289771072071\n",
      "test loss is 0.0012796647808326256\n",
      "Batch: 4000,train loss is: 0.0002675647204559779\n",
      "test loss is 0.000622768118891551\n",
      "Batch: 4100,train loss is: 0.000308950294430726\n",
      "test loss is 0.0003274946547727397\n",
      "Batch: 4200,train loss is: 0.0004946812501368857\n",
      "test loss is 0.0003446686494105248\n",
      "Batch: 4300,train loss is: 0.00017952821583620157\n",
      "test loss is 0.0005057431434414639\n",
      "Batch: 4400,train loss is: 0.0005313420257843711\n",
      "test loss is 0.0006051279565785453\n",
      "Batch: 4500,train loss is: 0.00023643475698162047\n",
      "test loss is 0.0003292748622893561\n",
      "Batch: 4600,train loss is: 0.0007894630308185765\n",
      "test loss is 0.0008918017509593177\n",
      "Batch: 4700,train loss is: 0.0007946842359188461\n",
      "test loss is 0.0007112427734268599\n",
      "Batch: 4800,train loss is: 0.0018683827445298879\n",
      "test loss is 0.0020703709797608924\n",
      "Batch: 4900,train loss is: 0.0019380956366057062\n",
      "test loss is 0.0010449621310681671\n",
      "Batch: 5000,train loss is: 0.0001765144176059151\n",
      "test loss is 0.0004400621514320429\n",
      "Batch: 5100,train loss is: 0.00024808516982466935\n",
      "test loss is 0.0003877013673991738\n",
      "Batch: 5200,train loss is: 0.0005407592369426974\n",
      "test loss is 0.0007805017108737809\n",
      "Batch: 5300,train loss is: 0.00172217435483923\n",
      "test loss is 0.002497612393059558\n",
      "Batch: 5400,train loss is: 0.0005131315042004131\n",
      "test loss is 0.0007375204461818703\n",
      "Batch: 5500,train loss is: 0.0010121336471343991\n",
      "test loss is 0.0010474034675105209\n",
      "Batch: 5600,train loss is: 0.0006857575654845707\n",
      "test loss is 0.00044282043828905166\n",
      "Batch: 5700,train loss is: 0.0003769560257688159\n",
      "test loss is 0.0005465939320449329\n",
      "Batch: 5800,train loss is: 0.00026371694320289957\n",
      "test loss is 0.0004236786284413873\n",
      "Batch: 5900,train loss is: 0.0007741339560907635\n",
      "test loss is 0.000985512331122867\n",
      "Batch: 6000,train loss is: 0.0013672071856392423\n",
      "test loss is 0.0005297162309946979\n",
      "Batch: 6100,train loss is: 0.0005186919241775733\n",
      "test loss is 0.0006575354267073506\n",
      "Batch: 6200,train loss is: 0.0003399214726600156\n",
      "test loss is 0.0004219861312363642\n",
      "Batch: 6300,train loss is: 0.0007033887211525042\n",
      "test loss is 0.0006123103421894665\n",
      "Batch: 6400,train loss is: 0.0010994613316191748\n",
      "test loss is 0.0013559318529863958\n",
      "Batch: 6500,train loss is: 0.0006551251746318662\n",
      "test loss is 0.0005746570404441939\n",
      "Batch: 6600,train loss is: 0.00917755588586892\n",
      "test loss is 0.0005059865366290667\n",
      "Batch: 6700,train loss is: 0.0002525379660809488\n",
      "test loss is 0.0006734739882635679\n",
      "Batch: 6800,train loss is: 0.0009056607596720066\n",
      "test loss is 0.00045288889920744443\n",
      "Batch: 6900,train loss is: 0.0007657501548821336\n",
      "test loss is 0.0005542121726627631\n",
      "Batch: 7000,train loss is: 0.0006882751327898809\n",
      "test loss is 0.0004307055174915706\n",
      "Batch: 7100,train loss is: 0.00038538747753576804\n",
      "test loss is 0.00044877037131182984\n",
      "Batch: 7200,train loss is: 0.00019292799700313076\n",
      "test loss is 0.00035102111286269545\n",
      "Batch: 7300,train loss is: 0.0032838243860225822\n",
      "test loss is 0.0005350782574601617\n",
      "Batch: 7400,train loss is: 0.0008325846901041545\n",
      "test loss is 0.000681939996665465\n",
      "Batch: 7500,train loss is: 0.00040660326026364636\n",
      "test loss is 0.0004222130643440761\n",
      "Batch: 7600,train loss is: 0.0013027096744389337\n",
      "test loss is 0.0005841682035684649\n",
      "Batch: 7700,train loss is: 0.0009100618225721622\n",
      "test loss is 0.0017900258424518043\n",
      "Batch: 7800,train loss is: 0.0003934079099635638\n",
      "test loss is 0.0009061702273224554\n",
      "Batch: 7900,train loss is: 0.0008087762326545758\n",
      "test loss is 0.00048287156438194624\n",
      "Batch: 8000,train loss is: 0.0005023710810586192\n",
      "test loss is 0.0007431375404950929\n",
      "Batch: 8100,train loss is: 0.0006348936714286042\n",
      "test loss is 0.00047476501042840915\n",
      "Batch: 8200,train loss is: 0.0004574346759226188\n",
      "test loss is 0.0003750944513278531\n",
      "Batch: 8300,train loss is: 0.00031576509254939173\n",
      "test loss is 0.0008067938440639639\n",
      "Batch: 8400,train loss is: 0.0005461757338537942\n",
      "test loss is 0.0006983746721251034\n",
      "Batch: 8500,train loss is: 0.00034804396944456193\n",
      "test loss is 0.00044504366128899926\n",
      "Batch: 8600,train loss is: 0.0006278738407775654\n",
      "test loss is 0.0007504344843952063\n",
      "Batch: 8700,train loss is: 0.00017551284317604484\n",
      "test loss is 0.00048182014858850987\n",
      "Batch: 8800,train loss is: 0.0003884501329938217\n",
      "test loss is 0.0005770347950935619\n",
      "Batch: 8900,train loss is: 0.0010480131083268405\n",
      "test loss is 0.00047170397818295686\n",
      "Batch: 9000,train loss is: 0.0002428692022111236\n",
      "test loss is 0.0005554962076513519\n",
      "Batch: 9100,train loss is: 0.0010736944839844023\n",
      "test loss is 0.0020097555078248644\n",
      "Batch: 9200,train loss is: 0.000721408191223637\n",
      "test loss is 0.0013080790341574988\n",
      "Batch: 9300,train loss is: 0.0005898394632380287\n",
      "test loss is 0.0005437072151734874\n",
      "Batch: 9400,train loss is: 0.00042409177566321386\n",
      "test loss is 0.0004768616210194494\n",
      "Batch: 9500,train loss is: 0.0007239569603227398\n",
      "test loss is 0.0013177119793430967\n",
      "Batch: 9600,train loss is: 0.00034640484131290495\n",
      "test loss is 0.0005903264179069148\n",
      "Batch: 9700,train loss is: 0.00026011995175149853\n",
      "test loss is 0.00041994406592807014\n",
      "Batch: 9800,train loss is: 0.0002747390368018618\n",
      "test loss is 0.0008667426342465452\n",
      "Batch: 9900,train loss is: 0.00047378526344891286\n",
      "test loss is 0.0007010817087156495\n",
      "Batch: 10000,train loss is: 0.0007994097463442263\n",
      "test loss is 0.0005212938726467647\n",
      "Batch: 10100,train loss is: 0.0013263601754865942\n",
      "test loss is 0.0003629062097671309\n",
      "Batch: 10200,train loss is: 0.000784924386182349\n",
      "test loss is 0.00030534026394424133\n",
      "Batch: 10300,train loss is: 0.0007778269794266062\n",
      "test loss is 0.0007787706616924248\n",
      "Batch: 10400,train loss is: 0.0015158356144622318\n",
      "test loss is 0.0016952641520202778\n",
      "Batch: 10500,train loss is: 0.0014568369369754141\n",
      "test loss is 0.0008982071346184981\n",
      "Batch: 10600,train loss is: 0.0001276179661691518\n",
      "test loss is 0.0006203143301041924\n",
      "Batch: 10700,train loss is: 0.0004911119228175113\n",
      "test loss is 0.0005451057611946791\n",
      "Batch: 10800,train loss is: 0.0003742269872230721\n",
      "test loss is 0.0006509247966893556\n",
      "Batch: 10900,train loss is: 0.0005237277150724351\n",
      "test loss is 0.0008416734346719067\n",
      "Batch: 11000,train loss is: 0.0006467139710777576\n",
      "test loss is 0.00039531065425582173\n",
      "Batch: 11100,train loss is: 0.0004235805897329577\n",
      "test loss is 0.0003600233572942181\n",
      "Batch: 11200,train loss is: 0.0005485412236130416\n",
      "test loss is 0.0004087626301070395\n",
      "Batch: 11300,train loss is: 0.0002502212356295828\n",
      "test loss is 0.0003971920545332056\n",
      "Batch: 11400,train loss is: 0.0007207806367178308\n",
      "test loss is 0.0011702719348707167\n",
      "Batch: 11500,train loss is: 0.0001956897984002689\n",
      "test loss is 0.0005919967084023141\n",
      "Batch: 11600,train loss is: 0.001679805800637709\n",
      "test loss is 0.0010279913273418346\n",
      "Batch: 11700,train loss is: 0.0002548975806374827\n",
      "test loss is 0.0004694026694673459\n",
      "Batch: 11800,train loss is: 0.0012103857838216229\n",
      "test loss is 0.0011569970882992695\n",
      "Batch: 11900,train loss is: 0.0008722956989994391\n",
      "test loss is 0.000821215858241952\n",
      "Batch: 12000,train loss is: 0.0003328693530161021\n",
      "test loss is 0.0004446197903210112\n",
      "Batch: 12100,train loss is: 0.004532349491036002\n",
      "test loss is 0.0011923265412770783\n",
      "Batch: 12200,train loss is: 0.0004660881030464067\n",
      "test loss is 0.0013855366043797495\n",
      "Batch: 12300,train loss is: 0.0008472203407858008\n",
      "test loss is 0.0004940299065174895\n",
      "Batch: 12400,train loss is: 0.00021883572324352403\n",
      "test loss is 0.000325492931707679\n",
      "Batch: 12500,train loss is: 0.001817556598577657\n",
      "test loss is 0.0005970528210847879\n",
      "Batch: 12600,train loss is: 0.00047668955661683534\n",
      "test loss is 0.00045267166001589993\n",
      "Batch: 12700,train loss is: 0.0002755578535789732\n",
      "test loss is 0.0003687799065542134\n",
      "Batch: 12800,train loss is: 0.00018028897311152568\n",
      "test loss is 0.0004245166126208958\n",
      "Batch: 12900,train loss is: 0.000276488179883856\n",
      "test loss is 0.0007960376061757478\n",
      "Batch: 13000,train loss is: 0.00040577736556439095\n",
      "test loss is 0.0005181645999766984\n",
      "Batch: 13100,train loss is: 0.0003484060625631307\n",
      "test loss is 0.0005310615923000912\n",
      "Batch: 13200,train loss is: 0.0002516201265889571\n",
      "test loss is 0.00037054687421260083\n",
      "Batch: 13300,train loss is: 0.00035933482321242867\n",
      "test loss is 0.0024830580356300968\n",
      "Batch: 13400,train loss is: 0.0006334860194850574\n",
      "test loss is 0.0006750193431049471\n",
      "Batch: 13500,train loss is: 0.00034729572162283296\n",
      "test loss is 0.0005892689205443983\n",
      "Batch: 13600,train loss is: 0.003245300098987626\n",
      "test loss is 0.0035329709415438206\n",
      "Batch: 13700,train loss is: 0.0002902996351708819\n",
      "test loss is 0.0006860878171500014\n",
      "Batch: 13800,train loss is: 0.00016450140791524127\n",
      "test loss is 0.0005532685575958315\n",
      "Batch: 13900,train loss is: 0.0005099576262966466\n",
      "test loss is 0.0004882792378093186\n",
      "Batch: 14000,train loss is: 0.0003159187791449341\n",
      "test loss is 0.0004939282019296974\n",
      "Batch: 14100,train loss is: 0.0001911795617355315\n",
      "test loss is 0.000687487511654113\n",
      "Batch: 14200,train loss is: 0.00023850304239323401\n",
      "test loss is 0.0003227869630645888\n",
      "Batch: 14300,train loss is: 0.00040533392659513267\n",
      "test loss is 0.000503058687452685\n",
      "Batch: 14400,train loss is: 0.0004341293755831591\n",
      "test loss is 0.00036739566643448747\n",
      "Batch: 14500,train loss is: 0.00021480900046078183\n",
      "test loss is 0.0003196998580401506\n",
      "Batch: 14600,train loss is: 0.0004289992911403841\n",
      "test loss is 0.00039387613924278335\n",
      "Batch: 14700,train loss is: 0.0002137387253037595\n",
      "test loss is 0.0006895870004881633\n",
      "Batch: 14800,train loss is: 0.00044571208537641185\n",
      "test loss is 0.0007444020661596626\n",
      "Batch: 14900,train loss is: 0.00026038027766781864\n",
      "test loss is 0.00038510364904985144\n",
      "Batch: 15000,train loss is: 0.00017686288087763913\n",
      "test loss is 0.0005142044733838331\n",
      "Batch: 15100,train loss is: 0.0005195546924250439\n",
      "test loss is 0.0005356950852065608\n",
      "Batch: 15200,train loss is: 0.0014734041738695238\n",
      "test loss is 0.00048303697527806604\n",
      "Batch: 15300,train loss is: 0.0004085963863544959\n",
      "test loss is 0.00038087024864524497\n",
      "Batch: 15400,train loss is: 0.0005926875877002533\n",
      "test loss is 0.000757844667426181\n",
      "Batch: 15500,train loss is: 0.0004973444143027479\n",
      "test loss is 0.0004325673788612533\n",
      "Batch: 15600,train loss is: 0.0009465031786107302\n",
      "test loss is 0.0006255614097848899\n",
      "Batch: 15700,train loss is: 0.0012516111137087843\n",
      "test loss is 0.0008513965948549823\n",
      "Batch: 15800,train loss is: 0.0002862054681125927\n",
      "test loss is 0.0004574127466127173\n",
      "Batch: 15900,train loss is: 0.0010222609259174177\n",
      "test loss is 0.0005107633615573693\n",
      "Batch: 16000,train loss is: 0.0006924459907043367\n",
      "test loss is 0.0006322756909434358\n",
      "Batch: 16100,train loss is: 0.0007544280626324253\n",
      "test loss is 0.0005486219058176875\n",
      "Batch: 16200,train loss is: 0.0003894124589463881\n",
      "test loss is 0.0007415356621033305\n",
      "Batch: 16300,train loss is: 0.00030618510269505086\n",
      "test loss is 0.0003921229119504189\n",
      "Batch: 16400,train loss is: 0.0006120318265790351\n",
      "test loss is 0.00042654075156550633\n",
      "Batch: 16500,train loss is: 0.0016018378770315336\n",
      "test loss is 0.0009575783757913686\n",
      "Batch: 16600,train loss is: 0.00020212097996361597\n",
      "test loss is 0.0009118308842646212\n",
      "Batch: 16700,train loss is: 0.0007531894051312448\n",
      "test loss is 0.0010785451291417906\n",
      "Batch: 16800,train loss is: 0.0006244462375627552\n",
      "test loss is 0.0004859081631807458\n",
      "Batch: 16900,train loss is: 0.0012434140345027624\n",
      "test loss is 0.0014895876448280908\n",
      "Batch: 17000,train loss is: 0.00019949742157578267\n",
      "test loss is 0.00044828291199846675\n",
      "Batch: 17100,train loss is: 0.0007129155648509515\n",
      "test loss is 0.0008278512439863784\n",
      "Batch: 17200,train loss is: 0.0001410829130111125\n",
      "test loss is 0.00031546643324638834\n",
      "Batch: 17300,train loss is: 0.0003050315372090242\n",
      "test loss is 0.0003037975792736313\n",
      "Batch: 17400,train loss is: 0.0008146250128196983\n",
      "test loss is 0.0008052380508664579\n",
      "Batch: 17500,train loss is: 0.0007278406765666368\n",
      "test loss is 0.0005524341994804072\n",
      "Batch: 17600,train loss is: 0.0002118854647377418\n",
      "test loss is 0.0005199635417016844\n",
      "Batch: 17700,train loss is: 0.0004552136865442226\n",
      "test loss is 0.0008714410990815492\n",
      "Batch: 17800,train loss is: 0.0005001320737218342\n",
      "test loss is 0.0004918877696669743\n",
      "Batch: 17900,train loss is: 0.0013934510635320687\n",
      "test loss is 0.0010311205927034243\n",
      "Batch: 18000,train loss is: 0.0010820691215274434\n",
      "test loss is 0.000647346148728517\n",
      "Batch: 18100,train loss is: 0.00024311331228844328\n",
      "test loss is 0.0005526251156701611\n",
      "Batch: 18200,train loss is: 0.00025759859757525723\n",
      "test loss is 0.0008834394331450129\n",
      "Batch: 18300,train loss is: 0.0015282722006119901\n",
      "test loss is 0.000469171337700601\n",
      "Batch: 18400,train loss is: 0.00019287011906676514\n",
      "test loss is 0.0006645150021544237\n",
      "Batch: 18500,train loss is: 0.0011098698758275523\n",
      "test loss is 0.0005768013446765036\n",
      "Batch: 18600,train loss is: 0.0001869260989118362\n",
      "test loss is 0.00045798280633598074\n",
      "Batch: 18700,train loss is: 0.0004920131049148178\n",
      "test loss is 0.0011687071653465964\n",
      "Batch: 18800,train loss is: 0.0005489932133733676\n",
      "test loss is 0.0007170193094383023\n",
      "Batch: 18900,train loss is: 0.00034476656186581205\n",
      "test loss is 0.0004220350244464733\n",
      "Batch: 19000,train loss is: 0.0005690824088615459\n",
      "test loss is 0.0006749070457257735\n",
      "Batch: 19100,train loss is: 0.00040429881425141004\n",
      "test loss is 0.0004709778725885091\n",
      "Batch: 19200,train loss is: 0.000684626121691823\n",
      "test loss is 0.0006216391792220802\n",
      "Batch: 19300,train loss is: 0.001354477360159703\n",
      "test loss is 0.0026010436407935193\n",
      "Batch: 19400,train loss is: 0.00014994949235121372\n",
      "test loss is 0.0005025336335659706\n",
      "Batch: 19500,train loss is: 0.0004091553821682198\n",
      "test loss is 0.0007817272577663808\n",
      "Batch: 19600,train loss is: 0.0007250014709925721\n",
      "test loss is 0.0009711471747844709\n",
      "Batch: 19700,train loss is: 0.0010269955314364907\n",
      "test loss is 0.0005972971715129761\n",
      "Batch: 19800,train loss is: 0.00039357017366739057\n",
      "test loss is 0.00043134348887158215\n",
      "Batch: 19900,train loss is: 0.00044121691590096615\n",
      "test loss is 0.0005259965407637058\n",
      "Batch: 20000,train loss is: 0.0007715232353348328\n",
      "test loss is 0.0006240893648838396\n",
      "Batch: 20100,train loss is: 0.0010842603313428664\n",
      "test loss is 0.0005499681009414363\n",
      "Batch: 20200,train loss is: 0.00012992189174289656\n",
      "test loss is 0.00046334539296389476\n",
      "Batch: 20300,train loss is: 0.00012023716559222858\n",
      "test loss is 0.0005923691461947076\n",
      "Batch: 20400,train loss is: 0.00075146589224165\n",
      "test loss is 0.0012493415553746809\n",
      "Batch: 20500,train loss is: 0.0002006075348847442\n",
      "test loss is 0.00042148147494712056\n",
      "Batch: 20600,train loss is: 0.0029209311411897585\n",
      "test loss is 0.0008991148366046482\n",
      "Batch: 20700,train loss is: 0.000618060809644458\n",
      "test loss is 0.000701658649585939\n",
      "Batch: 20800,train loss is: 0.0005259412080990997\n",
      "test loss is 0.0003961664689783783\n",
      "Batch: 20900,train loss is: 0.00027610824906853935\n",
      "test loss is 0.0007356905992279261\n",
      "Batch: 21000,train loss is: 0.00019659164241859196\n",
      "test loss is 0.0005853128860219976\n",
      "Batch: 21100,train loss is: 0.0002658007314739441\n",
      "test loss is 0.0003979899470844271\n",
      "Batch: 21200,train loss is: 0.0004095053157055566\n",
      "test loss is 0.0006295184087813383\n",
      "Batch: 21300,train loss is: 0.0005073569189566519\n",
      "test loss is 0.001400983935776746\n",
      "Batch: 21400,train loss is: 0.0004882080203896282\n",
      "test loss is 0.0006221952106203779\n",
      "Batch: 21500,train loss is: 0.0001691817881829751\n",
      "test loss is 0.0004427215504878427\n",
      "Batch: 21600,train loss is: 0.00017361301524765278\n",
      "test loss is 0.00043344440130916613\n",
      "Batch: 21700,train loss is: 0.0003013689448456072\n",
      "test loss is 0.0006917268225008024\n",
      "Batch: 21800,train loss is: 0.0004752092145038059\n",
      "test loss is 0.0005691576636935471\n",
      "Batch: 21900,train loss is: 0.0009339060526381771\n",
      "test loss is 0.0010393668486885584\n",
      "Batch: 22000,train loss is: 0.0018244093103048289\n",
      "test loss is 0.0008391186794506067\n",
      "Batch: 22100,train loss is: 0.0001925991469048051\n",
      "test loss is 0.00089980114951902\n",
      "Batch: 22200,train loss is: 0.0010409459299373749\n",
      "test loss is 0.0010400310410916048\n",
      "Batch: 22300,train loss is: 0.0007917839831826254\n",
      "test loss is 0.0011387338547737644\n",
      "Batch: 22400,train loss is: 0.0005437124603058681\n",
      "test loss is 0.0006722230178408037\n",
      "Batch: 22500,train loss is: 0.0002519688666235909\n",
      "test loss is 0.0003912269724517365\n",
      "Batch: 22600,train loss is: 0.0005835867842956854\n",
      "test loss is 0.0005862893516068677\n",
      "Batch: 22700,train loss is: 0.0005788996492743521\n",
      "test loss is 0.0009172066009073276\n",
      "Batch: 22800,train loss is: 0.0003311912270108297\n",
      "test loss is 0.000573241573871643\n",
      "Batch: 22900,train loss is: 0.000619007172463459\n",
      "test loss is 0.0010025189569739133\n",
      "Batch: 23000,train loss is: 0.00020540649343730967\n",
      "test loss is 0.00033596202036389837\n",
      "Batch: 23100,train loss is: 0.00023427627539742164\n",
      "test loss is 0.0007496232922714025\n",
      "Batch: 23200,train loss is: 0.00020869585813243553\n",
      "test loss is 0.0003395631353861746\n",
      "Batch: 23300,train loss is: 0.00021140366092113383\n",
      "test loss is 0.0003210840269127424\n",
      "Batch: 23400,train loss is: 0.0002459648625750558\n",
      "test loss is 0.00047042465704850127\n",
      "Batch: 23500,train loss is: 0.0010302135664104685\n",
      "test loss is 0.0004097046914914597\n",
      "Batch: 23600,train loss is: 0.00026278923855412397\n",
      "test loss is 0.0003836249735523256\n",
      "Batch: 23700,train loss is: 0.000302395208621626\n",
      "test loss is 0.00036060438565756886\n",
      "Batch: 23800,train loss is: 0.0006382088741336973\n",
      "test loss is 0.00047931849015946657\n",
      "Batch: 23900,train loss is: 0.0016857873395736632\n",
      "test loss is 0.0009792272369055918\n",
      "Batch: 24000,train loss is: 0.0003287672945415354\n",
      "test loss is 0.0010637719281268456\n",
      "Batch: 24100,train loss is: 0.0002833630145128019\n",
      "test loss is 0.0005002771233371563\n",
      "Batch: 24200,train loss is: 0.0031243224495980724\n",
      "test loss is 0.0005332539471455431\n",
      "Batch: 24300,train loss is: 0.0005999432908712245\n",
      "test loss is 0.0006456403633383376\n",
      "Batch: 24400,train loss is: 0.00048739782272662563\n",
      "test loss is 0.00035348036111253986\n",
      "Batch: 24500,train loss is: 0.0003322346443547371\n",
      "test loss is 0.0005530220673612689\n",
      "Batch: 24600,train loss is: 0.00015335339701010522\n",
      "test loss is 0.0003837747560709352\n",
      "Batch: 24700,train loss is: 0.000294227384059216\n",
      "test loss is 0.0003399528685768719\n",
      "Batch: 24800,train loss is: 0.0006262777134622218\n",
      "test loss is 0.0004115291621639032\n",
      "Batch: 24900,train loss is: 0.00026862616625203265\n",
      "test loss is 0.0006240685756579178\n",
      "Batch: 25000,train loss is: 0.00028320596833590974\n",
      "test loss is 0.0005549475193856335\n",
      "Batch: 25100,train loss is: 0.00021813573234730972\n",
      "test loss is 0.000699619350124359\n",
      "Batch: 25200,train loss is: 0.00037696172008104713\n",
      "test loss is 0.0004179133346321784\n",
      "Batch: 25300,train loss is: 0.00036520611135586775\n",
      "test loss is 0.00038077038469491736\n",
      "Batch: 25400,train loss is: 0.00025716956392466816\n",
      "test loss is 0.0003854566155458583\n",
      "Batch: 25500,train loss is: 0.00020918611456441726\n",
      "test loss is 0.00039395259152191385\n",
      "Batch: 25600,train loss is: 0.00038806992627464715\n",
      "test loss is 0.00035233237864604086\n",
      "Batch: 25700,train loss is: 0.0002590521122066294\n",
      "test loss is 0.00033885236253622446\n",
      "Batch: 25800,train loss is: 0.00039950096018852814\n",
      "test loss is 0.00048509662125499533\n",
      "Batch: 25900,train loss is: 0.0007901136479442895\n",
      "test loss is 0.000864539631987697\n",
      "Batch: 26000,train loss is: 0.0007057558885099569\n",
      "test loss is 0.0026071929976426325\n",
      "Batch: 26100,train loss is: 0.0004597123154183492\n",
      "test loss is 0.0009221018151059314\n",
      "Batch: 26200,train loss is: 0.0009375186470791455\n",
      "test loss is 0.0004472262664605984\n",
      "Batch: 26300,train loss is: 0.00038086601066715395\n",
      "test loss is 0.0005016337078250545\n",
      "Batch: 26400,train loss is: 0.0003193235798725897\n",
      "test loss is 0.0004715092881960718\n",
      "Batch: 26500,train loss is: 0.0005685870555345407\n",
      "test loss is 0.0006742101690584382\n",
      "Batch: 26600,train loss is: 0.0029804029499111693\n",
      "test loss is 0.001266542175247718\n",
      "Batch: 26700,train loss is: 0.0002690095380659985\n",
      "test loss is 0.0005491164847262277\n",
      "Batch: 26800,train loss is: 0.0008401403266836794\n",
      "test loss is 0.0003245770762770406\n",
      "Batch: 26900,train loss is: 0.0003070796909725081\n",
      "test loss is 0.0003523332288735155\n",
      "Batch: 27000,train loss is: 0.0005607429274250195\n",
      "test loss is 0.0004671716715081006\n",
      "Batch: 27100,train loss is: 0.0002927349960553898\n",
      "test loss is 0.00038803442812263803\n",
      "Batch: 27200,train loss is: 0.0005903349182700048\n",
      "test loss is 0.0011370295739772458\n",
      "Batch: 27300,train loss is: 0.000964348454913107\n",
      "test loss is 0.0017484948065324192\n",
      "Batch: 27400,train loss is: 0.0006242231567294476\n",
      "test loss is 0.0007967417356684866\n",
      "Batch: 27500,train loss is: 0.0015568837914612184\n",
      "test loss is 0.0007812249322644569\n",
      "Batch: 27600,train loss is: 0.0002652685082362773\n",
      "test loss is 0.00047284745752284983\n",
      "Batch: 27700,train loss is: 0.0003309860240624313\n",
      "test loss is 0.0004592640441477623\n",
      "Batch: 27800,train loss is: 0.0002596184017066614\n",
      "test loss is 0.0004615635257887541\n",
      "Batch: 27900,train loss is: 0.0007998046340869483\n",
      "test loss is 0.0003910912786895822\n",
      "Batch: 28000,train loss is: 0.00016704148913243731\n",
      "test loss is 0.0005321896312387546\n",
      "Batch: 28100,train loss is: 0.00039283126201894496\n",
      "test loss is 0.0005769034859113715\n",
      "Batch: 28200,train loss is: 0.0003443017019054701\n",
      "test loss is 0.0005417962702755721\n",
      "Batch: 28300,train loss is: 0.0008006905494664433\n",
      "test loss is 0.00047478230828286\n",
      "Batch: 28400,train loss is: 0.0007662043560194844\n",
      "test loss is 0.0005631914789329438\n",
      "Batch: 28500,train loss is: 0.0004610153317203732\n",
      "test loss is 0.0003797038750609467\n",
      "Batch: 28600,train loss is: 0.00038421799405421883\n",
      "test loss is 0.0006647599254645027\n",
      "Batch: 28700,train loss is: 0.00025357416696074764\n",
      "test loss is 0.0005180294335054265\n",
      "Batch: 28800,train loss is: 0.0008482795906072498\n",
      "test loss is 0.0002922842688699666\n",
      "Batch: 28900,train loss is: 0.00033112776875510955\n",
      "test loss is 0.0009268032659090815\n",
      "Batch: 29000,train loss is: 0.0004148585625233752\n",
      "test loss is 0.0005170206379112774\n",
      "Batch: 29100,train loss is: 0.0006257769658935977\n",
      "test loss is 0.0015154279991889663\n",
      "Batch: 29200,train loss is: 0.00018118750594849664\n",
      "test loss is 0.0004817364934390565\n",
      "Batch: 29300,train loss is: 0.00035252531082673964\n",
      "test loss is 0.0004468220220291926\n",
      "Batch: 29400,train loss is: 0.000326385185220156\n",
      "test loss is 0.0011459605338210105\n",
      "Batch: 29500,train loss is: 0.000582816742418408\n",
      "test loss is 0.0017155267936209827\n",
      "Batch: 29600,train loss is: 0.00037961628133257176\n",
      "test loss is 0.0003414579738090285\n",
      "Batch: 29700,train loss is: 0.0003415677622860614\n",
      "test loss is 0.0007049948698410791\n",
      "Batch: 29800,train loss is: 0.0008036236887429336\n",
      "test loss is 0.0006530086356110042\n",
      "Batch: 29900,train loss is: 0.0009281660209069853\n",
      "test loss is 0.000600863099277033\n",
      "Batch: 30000,train loss is: 0.00027829377165825546\n",
      "test loss is 0.00034145095536675487\n",
      "Batch: 30100,train loss is: 0.0007899744541323448\n",
      "test loss is 0.0007360428767715333\n",
      "Batch: 30200,train loss is: 0.000476325679692093\n",
      "test loss is 0.0006347485366092652\n",
      "Batch: 30300,train loss is: 0.00035939488368844026\n",
      "test loss is 0.0009923646277436567\n",
      "Batch: 30400,train loss is: 0.0010827350734983823\n",
      "test loss is 0.004760846740186852\n",
      "Batch: 30500,train loss is: 0.00045968294123704056\n",
      "test loss is 0.0007756637615276369\n",
      "Batch: 30600,train loss is: 0.0002430423658168693\n",
      "test loss is 0.0005530249406218827\n",
      "Batch: 30700,train loss is: 0.0013359729935673932\n",
      "test loss is 0.0007935561287905663\n",
      "Batch: 30800,train loss is: 0.00023801523402516955\n",
      "test loss is 0.0004825128836280098\n",
      "Batch: 30900,train loss is: 0.0002820629541516083\n",
      "test loss is 0.00036374924295306843\n",
      "Batch: 31000,train loss is: 0.00019512861227901943\n",
      "test loss is 0.00036174289040065226\n",
      "Batch: 31100,train loss is: 0.00025913172679822177\n",
      "test loss is 0.00048547658660007113\n",
      "Batch: 31200,train loss is: 0.000702183348262523\n",
      "test loss is 0.00046419958861770855\n",
      "Batch: 31300,train loss is: 0.0004062737992592668\n",
      "test loss is 0.0007220595572964241\n",
      "Batch: 31400,train loss is: 0.00032065869527022635\n",
      "test loss is 0.0006070084965790546\n",
      "Batch: 31500,train loss is: 0.00044733308186259044\n",
      "test loss is 0.0006823414415202461\n",
      "Batch: 31600,train loss is: 0.0008311342787628166\n",
      "test loss is 0.000400269631687696\n",
      "Batch: 31700,train loss is: 0.00017321283714840228\n",
      "test loss is 0.0003007354880391976\n",
      "Batch: 31800,train loss is: 0.0005935146233957317\n",
      "test loss is 0.00046695874775824466\n",
      "Batch: 31900,train loss is: 0.00031087797154852185\n",
      "test loss is 0.0003505780736201818\n",
      "Batch: 32000,train loss is: 0.0018488806217762613\n",
      "test loss is 0.0005558944889643355\n",
      "Batch: 32100,train loss is: 0.0009730759077658721\n",
      "test loss is 0.001666286823056057\n",
      "Batch: 32200,train loss is: 0.0001817574162922092\n",
      "test loss is 0.00047780650219159815\n",
      "Batch: 32300,train loss is: 0.0010535481290243555\n",
      "test loss is 0.0012020537281237605\n",
      "Batch: 32400,train loss is: 0.0003480795452933223\n",
      "test loss is 0.0005381667927958956\n",
      "Batch: 32500,train loss is: 0.0004309159885849426\n",
      "test loss is 0.0005777299120571718\n",
      "Batch: 32600,train loss is: 0.00033057035582071407\n",
      "test loss is 0.0004615777284383396\n",
      "Batch: 32700,train loss is: 0.00022593276867701357\n",
      "test loss is 0.0013110060613045033\n",
      "Batch: 32800,train loss is: 0.0007802563758555405\n",
      "test loss is 0.0005241533768144709\n",
      "Batch: 32900,train loss is: 0.0002775328139394405\n",
      "test loss is 0.00035352837134705974\n",
      "Batch: 33000,train loss is: 0.00014788016898810403\n",
      "test loss is 0.00034117155927720894\n",
      "Batch: 33100,train loss is: 0.0006221938266201234\n",
      "test loss is 0.0013087535708626215\n",
      "Batch: 33200,train loss is: 0.0002662952658261953\n",
      "test loss is 0.00048243934878224563\n",
      "Batch: 33300,train loss is: 0.0004481258032566911\n",
      "test loss is 0.0003321160778249635\n",
      "Batch: 33400,train loss is: 0.0004013704727164896\n",
      "test loss is 0.0014472994338914687\n",
      "Batch: 33500,train loss is: 0.0007811492916897808\n",
      "test loss is 0.0038481997729236735\n",
      "Batch: 33600,train loss is: 0.0005284783602105543\n",
      "test loss is 0.0005983038395129214\n",
      "Batch: 33700,train loss is: 0.0008568194015040243\n",
      "test loss is 0.00041218786784981594\n",
      "Batch: 33800,train loss is: 0.0003881787607217885\n",
      "test loss is 0.0005011127127863301\n",
      "Batch: 33900,train loss is: 0.0003744647793395317\n",
      "test loss is 0.0003010838172790844\n",
      "-----------------------Epoch: 7----------------------------------\n",
      "Batch: 0,train loss is: 0.00023843955256585333\n",
      "test loss is 0.00035382471610806735\n",
      "Batch: 100,train loss is: 0.00037077607964033005\n",
      "test loss is 0.00029873868364566883\n",
      "Batch: 200,train loss is: 0.00027423959419735334\n",
      "test loss is 0.0002879887965030528\n",
      "Batch: 300,train loss is: 0.00022981131172845586\n",
      "test loss is 0.0003750098288573807\n",
      "Batch: 400,train loss is: 0.000316646766309867\n",
      "test loss is 0.00043549753333205026\n",
      "Batch: 500,train loss is: 0.00016874279209483332\n",
      "test loss is 0.001181073476327188\n",
      "Batch: 600,train loss is: 9.01574518082539e-05\n",
      "test loss is 0.0004148604899491782\n",
      "Batch: 700,train loss is: 0.00018154688377830256\n",
      "test loss is 0.0005117446674152476\n",
      "Batch: 800,train loss is: 0.0008250729949774337\n",
      "test loss is 0.0006754077502084568\n",
      "Batch: 900,train loss is: 0.00034222928247015517\n",
      "test loss is 0.0006152144612675517\n",
      "Batch: 1000,train loss is: 0.0006816169483880901\n",
      "test loss is 0.0005077968189958689\n",
      "Batch: 1100,train loss is: 0.0016273919100781588\n",
      "test loss is 0.00048800863530569326\n",
      "Batch: 1200,train loss is: 0.0005055189221197593\n",
      "test loss is 0.00067906896847642\n",
      "Batch: 1300,train loss is: 0.00033199107599499704\n",
      "test loss is 0.0023680353089435705\n",
      "Batch: 1400,train loss is: 0.000242146023433506\n",
      "test loss is 0.0005579714659737354\n",
      "Batch: 1500,train loss is: 0.0008441410385405777\n",
      "test loss is 0.0006610095131956646\n",
      "Batch: 1600,train loss is: 0.0017546014538489083\n",
      "test loss is 0.0009107692814251014\n",
      "Batch: 1700,train loss is: 0.001326110514489582\n",
      "test loss is 0.0005849213409286499\n",
      "Batch: 1800,train loss is: 0.00040769553870859896\n",
      "test loss is 0.0005323334923296346\n",
      "Batch: 1900,train loss is: 0.00406168058124424\n",
      "test loss is 0.0019098452185920846\n",
      "Batch: 2000,train loss is: 0.00026664576354882007\n",
      "test loss is 0.0005396062297516874\n",
      "Batch: 2100,train loss is: 0.0011668739236063241\n",
      "test loss is 0.00036386623096835633\n",
      "Batch: 2200,train loss is: 0.00033124119087306304\n",
      "test loss is 0.00028692773975984504\n",
      "Batch: 2300,train loss is: 0.00027155552196608666\n",
      "test loss is 0.0003901418587967827\n",
      "Batch: 2400,train loss is: 0.0002402652418022839\n",
      "test loss is 0.00047691350455033357\n",
      "Batch: 2500,train loss is: 0.0003285804765529019\n",
      "test loss is 0.0005550808211195388\n",
      "Batch: 2600,train loss is: 0.00043322033470803766\n",
      "test loss is 0.00033734036053328383\n",
      "Batch: 2700,train loss is: 0.00029265471298202416\n",
      "test loss is 0.0003395279072872289\n",
      "Batch: 2800,train loss is: 0.00033727439934009266\n",
      "test loss is 0.0004090083059537307\n",
      "Batch: 2900,train loss is: 0.0003996913467149197\n",
      "test loss is 0.0003944252021080117\n",
      "Batch: 3000,train loss is: 0.0002711311814365633\n",
      "test loss is 0.0007046968280718514\n",
      "Batch: 3100,train loss is: 0.00020346520935722148\n",
      "test loss is 0.00037138197940576336\n",
      "Batch: 3200,train loss is: 0.0005379915264377072\n",
      "test loss is 0.000739928310923719\n",
      "Batch: 3300,train loss is: 0.0004192877226670978\n",
      "test loss is 0.00045759001274103383\n",
      "Batch: 3400,train loss is: 0.0014217246260985303\n",
      "test loss is 0.0009651071969570163\n",
      "Batch: 3500,train loss is: 0.00030602696878111955\n",
      "test loss is 0.0009814547489103054\n",
      "Batch: 3600,train loss is: 0.00021564375646678218\n",
      "test loss is 0.001248616829363569\n",
      "Batch: 3700,train loss is: 0.0002595380733153673\n",
      "test loss is 0.0005710555921487132\n",
      "Batch: 3800,train loss is: 0.000907125435408933\n",
      "test loss is 0.00037127389676029025\n",
      "Batch: 3900,train loss is: 0.0002484795069558152\n",
      "test loss is 0.0008026723856671582\n",
      "Batch: 4000,train loss is: 0.00020701945908765119\n",
      "test loss is 0.0005038234050291074\n",
      "Batch: 4100,train loss is: 0.00023630546891755636\n",
      "test loss is 0.00029116009110880615\n",
      "Batch: 4200,train loss is: 0.0006278181611212982\n",
      "test loss is 0.0003434417285811874\n",
      "Batch: 4300,train loss is: 0.00015642737095712182\n",
      "test loss is 0.0003715343272935042\n",
      "Batch: 4400,train loss is: 0.0007922871293538912\n",
      "test loss is 0.0005645851470325196\n",
      "Batch: 4500,train loss is: 0.0001280008863820638\n",
      "test loss is 0.00035422301251338836\n",
      "Batch: 4600,train loss is: 0.0007214704575623286\n",
      "test loss is 0.0008329153839365588\n",
      "Batch: 4700,train loss is: 0.0005202006873123967\n",
      "test loss is 0.0004973747184556112\n",
      "Batch: 4800,train loss is: 0.0014905513762269\n",
      "test loss is 0.0014604544906241172\n",
      "Batch: 4900,train loss is: 0.0015395054737609413\n",
      "test loss is 0.0008947038556315598\n",
      "Batch: 5000,train loss is: 0.00033221227171530696\n",
      "test loss is 0.00046216467415891314\n",
      "Batch: 5100,train loss is: 0.0003622637804734756\n",
      "test loss is 0.0003974095368123318\n",
      "Batch: 5200,train loss is: 0.0003999327084969632\n",
      "test loss is 0.0006459002204514445\n",
      "Batch: 5300,train loss is: 0.00175393339674789\n",
      "test loss is 0.0025597131179860085\n",
      "Batch: 5400,train loss is: 0.00044891225529570806\n",
      "test loss is 0.0006582423031133766\n",
      "Batch: 5500,train loss is: 0.001221974648384306\n",
      "test loss is 0.0011067285942181885\n",
      "Batch: 5600,train loss is: 0.000566072838646469\n",
      "test loss is 0.0003663291523863226\n",
      "Batch: 5700,train loss is: 0.00025287371019617837\n",
      "test loss is 0.0004006256238461831\n",
      "Batch: 5800,train loss is: 0.00033695410890866934\n",
      "test loss is 0.00037268428035550853\n",
      "Batch: 5900,train loss is: 0.0005084550905806338\n",
      "test loss is 0.0006482150486883639\n",
      "Batch: 6000,train loss is: 0.0010632600709077828\n",
      "test loss is 0.00041044102561403265\n",
      "Batch: 6100,train loss is: 0.000478486599975827\n",
      "test loss is 0.000571767948664175\n",
      "Batch: 6200,train loss is: 0.00038793428835938286\n",
      "test loss is 0.0005188722768280122\n",
      "Batch: 6300,train loss is: 0.0005721905731954363\n",
      "test loss is 0.0004986748260248745\n",
      "Batch: 6400,train loss is: 0.0012326906956607179\n",
      "test loss is 0.0013016453142796446\n",
      "Batch: 6500,train loss is: 0.00010890167005212863\n",
      "test loss is 0.0003827899656780429\n",
      "Batch: 6600,train loss is: 0.004642662856239426\n",
      "test loss is 0.00032729736667125596\n",
      "Batch: 6700,train loss is: 0.00015579951143749197\n",
      "test loss is 0.0005211740984829702\n",
      "Batch: 6800,train loss is: 0.0008315172719661322\n",
      "test loss is 0.00041054812735684523\n",
      "Batch: 6900,train loss is: 0.000355826365695167\n",
      "test loss is 0.00040811557171913755\n",
      "Batch: 7000,train loss is: 0.0004665660815068346\n",
      "test loss is 0.0003469995371740313\n",
      "Batch: 7100,train loss is: 0.00042763046486502275\n",
      "test loss is 0.0003708467049697337\n",
      "Batch: 7200,train loss is: 0.0001232721793453604\n",
      "test loss is 0.00029374988401075386\n",
      "Batch: 7300,train loss is: 0.0030779615431998446\n",
      "test loss is 0.0005295133892744974\n",
      "Batch: 7400,train loss is: 0.000689718775459113\n",
      "test loss is 0.0008427666704606169\n",
      "Batch: 7500,train loss is: 0.00033906943838915414\n",
      "test loss is 0.0005270505340020247\n",
      "Batch: 7600,train loss is: 0.0010509904314030363\n",
      "test loss is 0.0005173324195426116\n",
      "Batch: 7700,train loss is: 0.0007147934097989426\n",
      "test loss is 0.0014567177310463191\n",
      "Batch: 7800,train loss is: 0.0002747486945168573\n",
      "test loss is 0.0006576258791822922\n",
      "Batch: 7900,train loss is: 0.0008707542001002909\n",
      "test loss is 0.0004183358320289173\n",
      "Batch: 8000,train loss is: 0.00034855782161235036\n",
      "test loss is 0.0006899479021784721\n",
      "Batch: 8100,train loss is: 0.0005521976905898711\n",
      "test loss is 0.0003911806354824367\n",
      "Batch: 8200,train loss is: 0.00022896165702295115\n",
      "test loss is 0.00031572577143647486\n",
      "Batch: 8300,train loss is: 0.00032312949875028463\n",
      "test loss is 0.000649770534705275\n",
      "Batch: 8400,train loss is: 0.0004204763938786481\n",
      "test loss is 0.0005156663376194171\n",
      "Batch: 8500,train loss is: 0.0003195323438364503\n",
      "test loss is 0.0003877741697825747\n",
      "Batch: 8600,train loss is: 0.00035033285235871477\n",
      "test loss is 0.0004908457513088118\n",
      "Batch: 8700,train loss is: 0.00018438166751240357\n",
      "test loss is 0.0003493003613645751\n",
      "Batch: 8800,train loss is: 0.0004420239790946476\n",
      "test loss is 0.0005885441601423844\n",
      "Batch: 8900,train loss is: 0.0010143293287580913\n",
      "test loss is 0.0004326502098458538\n",
      "Batch: 9000,train loss is: 0.00021119374220786614\n",
      "test loss is 0.0005288920417521316\n",
      "Batch: 9100,train loss is: 0.0009121947076552755\n",
      "test loss is 0.0014924381843090125\n",
      "Batch: 9200,train loss is: 0.000609177776069533\n",
      "test loss is 0.0010280611850425356\n",
      "Batch: 9300,train loss is: 0.0005950011337534735\n",
      "test loss is 0.0005176421833318067\n",
      "Batch: 9400,train loss is: 0.00028113552273113687\n",
      "test loss is 0.00040541587202999136\n",
      "Batch: 9500,train loss is: 0.0006185678333638665\n",
      "test loss is 0.0010990287420378885\n",
      "Batch: 9600,train loss is: 0.00027083750175704976\n",
      "test loss is 0.00044503555912187335\n",
      "Batch: 9700,train loss is: 0.00034186144231446094\n",
      "test loss is 0.0003819306290687782\n",
      "Batch: 9800,train loss is: 0.00024499255688922075\n",
      "test loss is 0.000875415968075235\n",
      "Batch: 9900,train loss is: 0.00041236496139821925\n",
      "test loss is 0.0006290382049540372\n",
      "Batch: 10000,train loss is: 0.0009412644844441701\n",
      "test loss is 0.0004948703146555433\n",
      "Batch: 10100,train loss is: 0.0009279593802446459\n",
      "test loss is 0.0003596573605249342\n",
      "Batch: 10200,train loss is: 0.0005464455456902123\n",
      "test loss is 0.0002615755984561889\n",
      "Batch: 10300,train loss is: 0.00025997630520587883\n",
      "test loss is 0.0003578322241625648\n",
      "Batch: 10400,train loss is: 0.0005074189668223122\n",
      "test loss is 0.0007595281348972237\n",
      "Batch: 10500,train loss is: 0.0003351853128136581\n",
      "test loss is 0.0006389091247172602\n",
      "Batch: 10600,train loss is: 0.00024551491810509906\n",
      "test loss is 0.0011159638026205108\n",
      "Batch: 10700,train loss is: 0.00044499875331433983\n",
      "test loss is 0.0004461623644396785\n",
      "Batch: 10800,train loss is: 0.00035832513023993596\n",
      "test loss is 0.000857420974598836\n",
      "Batch: 10900,train loss is: 0.0010071797623637223\n",
      "test loss is 0.000567665654367156\n",
      "Batch: 11000,train loss is: 0.00046928191663485655\n",
      "test loss is 0.0003726438387409837\n",
      "Batch: 11100,train loss is: 0.00029292316110963795\n",
      "test loss is 0.00030181476358231117\n",
      "Batch: 11200,train loss is: 0.0006097153674295178\n",
      "test loss is 0.0003463291913554559\n",
      "Batch: 11300,train loss is: 0.0003110958022874026\n",
      "test loss is 0.0004881168927342069\n",
      "Batch: 11400,train loss is: 0.0004585823971376907\n",
      "test loss is 0.0006225951363419463\n",
      "Batch: 11500,train loss is: 0.0004522326056755309\n",
      "test loss is 0.0007045740503203326\n",
      "Batch: 11600,train loss is: 0.0036533258251514582\n",
      "test loss is 0.0011928411263234773\n",
      "Batch: 11700,train loss is: 0.00025221005239623096\n",
      "test loss is 0.0005309098097954135\n",
      "Batch: 11800,train loss is: 0.0009842667659897852\n",
      "test loss is 0.0010721265028372116\n",
      "Batch: 11900,train loss is: 0.0012271505419432953\n",
      "test loss is 0.0009602761601504851\n",
      "Batch: 12000,train loss is: 0.0003577875034432146\n",
      "test loss is 0.0003600594686065435\n",
      "Batch: 12100,train loss is: 0.0038706965584583302\n",
      "test loss is 0.0007021487682605483\n",
      "Batch: 12200,train loss is: 0.0005339431796617747\n",
      "test loss is 0.001441011813857017\n",
      "Batch: 12300,train loss is: 0.0004209807329718871\n",
      "test loss is 0.0004344527953393594\n",
      "Batch: 12400,train loss is: 0.0005978172666476995\n",
      "test loss is 0.00034898874011604755\n",
      "Batch: 12500,train loss is: 0.0012073487791512728\n",
      "test loss is 0.0004752371434175092\n",
      "Batch: 12600,train loss is: 0.0003911070552464661\n",
      "test loss is 0.0003582549000868693\n",
      "Batch: 12700,train loss is: 0.000302211757002534\n",
      "test loss is 0.0003430544207600043\n",
      "Batch: 12800,train loss is: 0.00018844329446139377\n",
      "test loss is 0.0003212705902340316\n",
      "Batch: 12900,train loss is: 0.0005645750628214153\n",
      "test loss is 0.00040584479882526574\n",
      "Batch: 13000,train loss is: 0.00015075813095160342\n",
      "test loss is 0.00030357433530838303\n",
      "Batch: 13100,train loss is: 0.0002962381728503208\n",
      "test loss is 0.0005730229969403417\n",
      "Batch: 13200,train loss is: 0.00026631734877106646\n",
      "test loss is 0.000276519768375617\n",
      "Batch: 13300,train loss is: 0.00044950312444249507\n",
      "test loss is 0.0021306927219812747\n",
      "Batch: 13400,train loss is: 0.0005791628187946526\n",
      "test loss is 0.0007727536649928759\n",
      "Batch: 13500,train loss is: 0.0003464128250300791\n",
      "test loss is 0.0006345480024171099\n",
      "Batch: 13600,train loss is: 0.0031466998783193467\n",
      "test loss is 0.003258753099764127\n",
      "Batch: 13700,train loss is: 0.00021235237358437057\n",
      "test loss is 0.0005523793704965207\n",
      "Batch: 13800,train loss is: 0.00024018420141780773\n",
      "test loss is 0.0005375742536267457\n",
      "Batch: 13900,train loss is: 0.0007028270911868326\n",
      "test loss is 0.0005489516842857283\n",
      "Batch: 14000,train loss is: 0.00025690056573708203\n",
      "test loss is 0.00035978719321222056\n",
      "Batch: 14100,train loss is: 0.0001914234615362576\n",
      "test loss is 0.0004866225774018057\n",
      "Batch: 14200,train loss is: 0.00016811518993464924\n",
      "test loss is 0.00028937335740616624\n",
      "Batch: 14300,train loss is: 0.0002220827655525591\n",
      "test loss is 0.0003181199952509335\n",
      "Batch: 14400,train loss is: 0.0004830081391717264\n",
      "test loss is 0.00032746175660013094\n",
      "Batch: 14500,train loss is: 0.00022000114536032496\n",
      "test loss is 0.0002947282406393889\n",
      "Batch: 14600,train loss is: 0.0004403955347636455\n",
      "test loss is 0.0004339194537394217\n",
      "Batch: 14700,train loss is: 0.00022045292586259647\n",
      "test loss is 0.0009023634440660095\n",
      "Batch: 14800,train loss is: 0.0003925096360138875\n",
      "test loss is 0.0005046571118963682\n",
      "Batch: 14900,train loss is: 0.00027231679737229947\n",
      "test loss is 0.00037191651148865424\n",
      "Batch: 15000,train loss is: 0.00024599438959786035\n",
      "test loss is 0.0004295201482513392\n",
      "Batch: 15100,train loss is: 0.00035434104432533424\n",
      "test loss is 0.00047662420426920917\n",
      "Batch: 15200,train loss is: 0.0006594568159876528\n",
      "test loss is 0.0003124609985494709\n",
      "Batch: 15300,train loss is: 0.00043109511803744553\n",
      "test loss is 0.00040505355744689615\n",
      "Batch: 15400,train loss is: 0.0007285280474803693\n",
      "test loss is 0.0008457520331647018\n",
      "Batch: 15500,train loss is: 0.0003117227726868238\n",
      "test loss is 0.00040890154133336483\n",
      "Batch: 15600,train loss is: 0.0009252350903457293\n",
      "test loss is 0.0007656885849950495\n",
      "Batch: 15700,train loss is: 0.0010889842515486741\n",
      "test loss is 0.0007620007323371147\n",
      "Batch: 15800,train loss is: 0.0002533824021741318\n",
      "test loss is 0.0004021835885617062\n",
      "Batch: 15900,train loss is: 0.0009548819524685465\n",
      "test loss is 0.0004413286447240237\n",
      "Batch: 16000,train loss is: 0.0008076910877416344\n",
      "test loss is 0.0006130532265370293\n",
      "Batch: 16100,train loss is: 0.0006687330758153086\n",
      "test loss is 0.0005676166783863486\n",
      "Batch: 16200,train loss is: 0.00044143502266622887\n",
      "test loss is 0.0006733125210651728\n",
      "Batch: 16300,train loss is: 0.0003122659169461556\n",
      "test loss is 0.00035563002610361293\n",
      "Batch: 16400,train loss is: 0.00042846634582730545\n",
      "test loss is 0.00036460893801056987\n",
      "Batch: 16500,train loss is: 0.0014640559060522542\n",
      "test loss is 0.0008545612609782684\n",
      "Batch: 16600,train loss is: 0.00019284750099089786\n",
      "test loss is 0.0008723286926593997\n",
      "Batch: 16700,train loss is: 0.0006127635627843876\n",
      "test loss is 0.0008852694130536831\n",
      "Batch: 16800,train loss is: 0.0004732083536918285\n",
      "test loss is 0.00043658701456457193\n",
      "Batch: 16900,train loss is: 0.001174327332115467\n",
      "test loss is 0.0011720703287345163\n",
      "Batch: 17000,train loss is: 0.00019601158054670414\n",
      "test loss is 0.0003851517794558037\n",
      "Batch: 17100,train loss is: 0.000730008594565477\n",
      "test loss is 0.000923044888248508\n",
      "Batch: 17200,train loss is: 9.965787543434918e-05\n",
      "test loss is 0.00030404672850498413\n",
      "Batch: 17300,train loss is: 0.0002537649244032056\n",
      "test loss is 0.00027992784843751357\n",
      "Batch: 17400,train loss is: 0.0005465690116571417\n",
      "test loss is 0.0006576769688973407\n",
      "Batch: 17500,train loss is: 0.0007325455890137829\n",
      "test loss is 0.0004381479906919747\n",
      "Batch: 17600,train loss is: 0.0001826315815393125\n",
      "test loss is 0.000470980314928972\n",
      "Batch: 17700,train loss is: 0.00038401954963346926\n",
      "test loss is 0.0007614981973860757\n",
      "Batch: 17800,train loss is: 0.00045554543800808067\n",
      "test loss is 0.0005049845712056355\n",
      "Batch: 17900,train loss is: 0.0014634682431820469\n",
      "test loss is 0.0010280810649753085\n",
      "Batch: 18000,train loss is: 0.0010500667860394624\n",
      "test loss is 0.0006049943247405367\n",
      "Batch: 18100,train loss is: 0.0002582160743119069\n",
      "test loss is 0.000562040954551259\n",
      "Batch: 18200,train loss is: 0.00020863904995298304\n",
      "test loss is 0.0007121311335342791\n",
      "Batch: 18300,train loss is: 0.001366460166607187\n",
      "test loss is 0.0004219018533127914\n",
      "Batch: 18400,train loss is: 0.00016542806116262468\n",
      "test loss is 0.0005996864313506312\n",
      "Batch: 18500,train loss is: 0.0010398907135639969\n",
      "test loss is 0.00045872181055733603\n",
      "Batch: 18600,train loss is: 0.00023236835428568832\n",
      "test loss is 0.0004118686181901036\n",
      "Batch: 18700,train loss is: 0.0004878573717815495\n",
      "test loss is 0.0010576046457714693\n",
      "Batch: 18800,train loss is: 0.00044608629816775334\n",
      "test loss is 0.0005339484289885202\n",
      "Batch: 18900,train loss is: 0.00026694353438910595\n",
      "test loss is 0.0003626623388270255\n",
      "Batch: 19000,train loss is: 0.00037963683019951784\n",
      "test loss is 0.0005749550673613101\n",
      "Batch: 19100,train loss is: 0.00047826246903561544\n",
      "test loss is 0.0004903857640850202\n",
      "Batch: 19200,train loss is: 0.0006457723999658511\n",
      "test loss is 0.0005504213872080565\n",
      "Batch: 19300,train loss is: 0.000894648496540032\n",
      "test loss is 0.0019270533639223725\n",
      "Batch: 19400,train loss is: 0.00017684371988896443\n",
      "test loss is 0.0004377187057076583\n",
      "Batch: 19500,train loss is: 0.00039420085625129257\n",
      "test loss is 0.0006144956638432876\n",
      "Batch: 19600,train loss is: 0.0005222600843965074\n",
      "test loss is 0.000726319625066133\n",
      "Batch: 19700,train loss is: 0.0010054173459747512\n",
      "test loss is 0.0005320759619865674\n",
      "Batch: 19800,train loss is: 0.00036950180971536735\n",
      "test loss is 0.0004426185392423033\n",
      "Batch: 19900,train loss is: 0.00030449330051513364\n",
      "test loss is 0.0004015512705604806\n",
      "Batch: 20000,train loss is: 0.000749992249437974\n",
      "test loss is 0.000685271892193838\n",
      "Batch: 20100,train loss is: 0.0009133673356883784\n",
      "test loss is 0.0005670035718930879\n",
      "Batch: 20200,train loss is: 9.389854707019763e-05\n",
      "test loss is 0.0003814465298536692\n",
      "Batch: 20300,train loss is: 0.00015680472572318673\n",
      "test loss is 0.00046755628545195043\n",
      "Batch: 20400,train loss is: 0.0006285582678866435\n",
      "test loss is 0.0010033554374534685\n",
      "Batch: 20500,train loss is: 0.00019870159612912783\n",
      "test loss is 0.0003651463947730139\n",
      "Batch: 20600,train loss is: 0.0034292716420678683\n",
      "test loss is 0.0008324118450176971\n",
      "Batch: 20700,train loss is: 0.0006404132002419553\n",
      "test loss is 0.0006061840495130223\n",
      "Batch: 20800,train loss is: 0.00038937120663470485\n",
      "test loss is 0.0003906329251729268\n",
      "Batch: 20900,train loss is: 0.00036342558302951796\n",
      "test loss is 0.0008496927791087916\n",
      "Batch: 21000,train loss is: 0.0002870774867550554\n",
      "test loss is 0.0006379238034177185\n",
      "Batch: 21100,train loss is: 0.0002601592969123766\n",
      "test loss is 0.000433584991840802\n",
      "Batch: 21200,train loss is: 0.0009618668816269919\n",
      "test loss is 0.0007232890917437512\n",
      "Batch: 21300,train loss is: 0.00024048772845454292\n",
      "test loss is 0.001340258961920208\n",
      "Batch: 21400,train loss is: 0.000434865888042267\n",
      "test loss is 0.0005511351786987258\n",
      "Batch: 21500,train loss is: 0.00016210470773832285\n",
      "test loss is 0.00042531441613495943\n",
      "Batch: 21600,train loss is: 0.00017617483751984297\n",
      "test loss is 0.00035952082054774455\n",
      "Batch: 21700,train loss is: 0.0004286485491249346\n",
      "test loss is 0.0006069197281888254\n",
      "Batch: 21800,train loss is: 0.00037750158447551755\n",
      "test loss is 0.0004886704231727507\n",
      "Batch: 21900,train loss is: 0.0006555728758199068\n",
      "test loss is 0.0008926253036839873\n",
      "Batch: 22000,train loss is: 0.0011992443318686273\n",
      "test loss is 0.0005899862349757299\n",
      "Batch: 22100,train loss is: 0.0001190222169990589\n",
      "test loss is 0.0007023458479202409\n",
      "Batch: 22200,train loss is: 0.0012081081963260984\n",
      "test loss is 0.0010063710247619523\n",
      "Batch: 22300,train loss is: 0.0006263904327737202\n",
      "test loss is 0.0008353602502306887\n",
      "Batch: 22400,train loss is: 0.0006205158403296499\n",
      "test loss is 0.0006751259326853739\n",
      "Batch: 22500,train loss is: 0.00019546428422052392\n",
      "test loss is 0.00040504568752230366\n",
      "Batch: 22600,train loss is: 0.0004670790925220186\n",
      "test loss is 0.0005102246089406713\n",
      "Batch: 22700,train loss is: 0.00037167558228505304\n",
      "test loss is 0.0006350049704394825\n",
      "Batch: 22800,train loss is: 0.00037964280513421117\n",
      "test loss is 0.0006025281531430573\n",
      "Batch: 22900,train loss is: 0.0006253331919126283\n",
      "test loss is 0.0010017353201079852\n",
      "Batch: 23000,train loss is: 0.00022513154237313165\n",
      "test loss is 0.0003456291525303341\n",
      "Batch: 23100,train loss is: 0.00016131642424193534\n",
      "test loss is 0.0004229125326183534\n",
      "Batch: 23200,train loss is: 0.00021426642735963885\n",
      "test loss is 0.00031632971463771214\n",
      "Batch: 23300,train loss is: 0.00027544985548848776\n",
      "test loss is 0.000327392650713106\n",
      "Batch: 23400,train loss is: 0.00023211191228905397\n",
      "test loss is 0.0005277413481418131\n",
      "Batch: 23500,train loss is: 0.0004368966856367952\n",
      "test loss is 0.00033235723570853215\n",
      "Batch: 23600,train loss is: 0.00018357613246993237\n",
      "test loss is 0.0002718447503083378\n",
      "Batch: 23700,train loss is: 0.0003052196604298306\n",
      "test loss is 0.0003904668566267079\n",
      "Batch: 23800,train loss is: 0.00047064043265765664\n",
      "test loss is 0.0002984938849552115\n",
      "Batch: 23900,train loss is: 0.0013525348135544665\n",
      "test loss is 0.0007395113183361981\n",
      "Batch: 24000,train loss is: 0.00025201936812307643\n",
      "test loss is 0.0008434659364277602\n",
      "Batch: 24100,train loss is: 0.00015102821646932957\n",
      "test loss is 0.00038554771405893424\n",
      "Batch: 24200,train loss is: 0.0019669614448698555\n",
      "test loss is 0.0005901177982626268\n",
      "Batch: 24300,train loss is: 0.0004420853419589567\n",
      "test loss is 0.000468450323264723\n",
      "Batch: 24400,train loss is: 0.0003773371589141612\n",
      "test loss is 0.00032796276351311347\n",
      "Batch: 24500,train loss is: 0.00018289968046862804\n",
      "test loss is 0.00036769956008483705\n",
      "Batch: 24600,train loss is: 0.00013548559930653302\n",
      "test loss is 0.0003716885588627227\n",
      "Batch: 24700,train loss is: 0.0002189746939285572\n",
      "test loss is 0.00029821391726816434\n",
      "Batch: 24800,train loss is: 0.0004992058409202034\n",
      "test loss is 0.00042231191872431063\n",
      "Batch: 24900,train loss is: 0.00020284964704360013\n",
      "test loss is 0.00040010760662731346\n",
      "Batch: 25000,train loss is: 0.00021234395368484438\n",
      "test loss is 0.000532340119273627\n",
      "Batch: 25100,train loss is: 0.00016510525883753308\n",
      "test loss is 0.0005265967170867529\n",
      "Batch: 25200,train loss is: 0.0003303520304015485\n",
      "test loss is 0.00036866070463711784\n",
      "Batch: 25300,train loss is: 0.00033604573760417694\n",
      "test loss is 0.0003889080365344284\n",
      "Batch: 25400,train loss is: 0.00023200050770500285\n",
      "test loss is 0.00039123066618934675\n",
      "Batch: 25500,train loss is: 0.00017973444364398477\n",
      "test loss is 0.000381896374474195\n",
      "Batch: 25600,train loss is: 0.0003718069781030671\n",
      "test loss is 0.0003342305406384246\n",
      "Batch: 25700,train loss is: 0.0002623830928083622\n",
      "test loss is 0.00031923393670759006\n",
      "Batch: 25800,train loss is: 0.00015642310481466228\n",
      "test loss is 0.0003329684073026698\n",
      "Batch: 25900,train loss is: 0.000539500217506508\n",
      "test loss is 0.0007162607233207755\n",
      "Batch: 26000,train loss is: 0.0006785416717666418\n",
      "test loss is 0.0014063974677879512\n",
      "Batch: 26100,train loss is: 0.00043011216589707165\n",
      "test loss is 0.000605991801030205\n",
      "Batch: 26200,train loss is: 0.0006803988046354255\n",
      "test loss is 0.0004171863015133972\n",
      "Batch: 26300,train loss is: 0.0003499740728370113\n",
      "test loss is 0.0005528688329081072\n",
      "Batch: 26400,train loss is: 0.00042347758265578566\n",
      "test loss is 0.0004141276673688506\n",
      "Batch: 26500,train loss is: 0.00047375622639940983\n",
      "test loss is 0.0006230443797555601\n",
      "Batch: 26600,train loss is: 0.0027767577400052367\n",
      "test loss is 0.0010216872368352607\n",
      "Batch: 26700,train loss is: 0.0002694338488302978\n",
      "test loss is 0.00043549463844493855\n",
      "Batch: 26800,train loss is: 0.0006262251660122539\n",
      "test loss is 0.00028549618920325516\n",
      "Batch: 26900,train loss is: 0.00034649916811399804\n",
      "test loss is 0.00038129707601927\n",
      "Batch: 27000,train loss is: 0.0005149104924758376\n",
      "test loss is 0.000463506527285277\n",
      "Batch: 27100,train loss is: 0.00015726958218624716\n",
      "test loss is 0.00036144610418711783\n",
      "Batch: 27200,train loss is: 0.0010311996997249453\n",
      "test loss is 0.0017414637974792107\n",
      "Batch: 27300,train loss is: 0.0004731359305577259\n",
      "test loss is 0.0006815820924159624\n",
      "Batch: 27400,train loss is: 0.0004198275570232035\n",
      "test loss is 0.0005639093852866191\n",
      "Batch: 27500,train loss is: 0.0014764427281765227\n",
      "test loss is 0.0007336183806061152\n",
      "Batch: 27600,train loss is: 0.00026686848941959466\n",
      "test loss is 0.0004044866887242893\n",
      "Batch: 27700,train loss is: 0.0003421521866434548\n",
      "test loss is 0.0003994362483268701\n",
      "Batch: 27800,train loss is: 0.00017820347357851827\n",
      "test loss is 0.0005697856449701229\n",
      "Batch: 27900,train loss is: 0.0007615636036391333\n",
      "test loss is 0.00039475395755051806\n",
      "Batch: 28000,train loss is: 0.00015598989379347574\n",
      "test loss is 0.0005477698628070522\n",
      "Batch: 28100,train loss is: 0.0003351705849879294\n",
      "test loss is 0.0005992823981168218\n",
      "Batch: 28200,train loss is: 0.000431186256272004\n",
      "test loss is 0.0004136149132128197\n",
      "Batch: 28300,train loss is: 0.000662973647367056\n",
      "test loss is 0.0003810850309759454\n",
      "Batch: 28400,train loss is: 0.0008143702830444733\n",
      "test loss is 0.000565921679955922\n",
      "Batch: 28500,train loss is: 0.00037471718619574523\n",
      "test loss is 0.00033036253784791503\n",
      "Batch: 28600,train loss is: 0.00027477130248845067\n",
      "test loss is 0.0005497851164223832\n",
      "Batch: 28700,train loss is: 0.00024001584202492193\n",
      "test loss is 0.0004715375478171433\n",
      "Batch: 28800,train loss is: 0.0006460191797024061\n",
      "test loss is 0.00026251083709350696\n",
      "Batch: 28900,train loss is: 0.00028086702933036726\n",
      "test loss is 0.0007671896843749324\n",
      "Batch: 29000,train loss is: 0.00031346027953550244\n",
      "test loss is 0.0005179894569416889\n",
      "Batch: 29100,train loss is: 0.000700693638361663\n",
      "test loss is 0.0015115965291947842\n",
      "Batch: 29200,train loss is: 0.00024994026090436396\n",
      "test loss is 0.0005535109801434798\n",
      "Batch: 29300,train loss is: 0.0003204607362837598\n",
      "test loss is 0.0005837991992245928\n",
      "Batch: 29400,train loss is: 0.0002726055633781984\n",
      "test loss is 0.0007779084989097248\n",
      "Batch: 29500,train loss is: 0.00020716566471218516\n",
      "test loss is 0.0006354531375599495\n",
      "Batch: 29600,train loss is: 0.0004030181970204565\n",
      "test loss is 0.0002929814079089633\n",
      "Batch: 29700,train loss is: 0.0002441518300493857\n",
      "test loss is 0.0006847577749251297\n",
      "Batch: 29800,train loss is: 0.0006380793424499467\n",
      "test loss is 0.0005184069406990626\n",
      "Batch: 29900,train loss is: 0.0009037495842639637\n",
      "test loss is 0.00039579610276855724\n",
      "Batch: 30000,train loss is: 0.00017823503154685514\n",
      "test loss is 0.0002921287774008172\n",
      "Batch: 30100,train loss is: 0.0006179101627063355\n",
      "test loss is 0.0006825542754677898\n",
      "Batch: 30200,train loss is: 0.0004315976954479437\n",
      "test loss is 0.0005528294082449935\n",
      "Batch: 30300,train loss is: 0.0003702976462437992\n",
      "test loss is 0.0009511301558614403\n",
      "Batch: 30400,train loss is: 0.0008228876605769697\n",
      "test loss is 0.00380967426776489\n",
      "Batch: 30500,train loss is: 0.00032577568297624077\n",
      "test loss is 0.0006154927914359635\n",
      "Batch: 30600,train loss is: 0.00023090455568174377\n",
      "test loss is 0.00043731010134276805\n",
      "Batch: 30700,train loss is: 0.0010918178159449712\n",
      "test loss is 0.0006603869714868401\n",
      "Batch: 30800,train loss is: 0.00021394680666860153\n",
      "test loss is 0.00040080788266961604\n",
      "Batch: 30900,train loss is: 0.00024395965458445822\n",
      "test loss is 0.0003033449914804059\n",
      "Batch: 31000,train loss is: 0.00016836736931447779\n",
      "test loss is 0.00032296033308416123\n",
      "Batch: 31100,train loss is: 0.00028187074392148034\n",
      "test loss is 0.0004989408529905136\n",
      "Batch: 31200,train loss is: 0.0005184817691505762\n",
      "test loss is 0.00036426068961439514\n",
      "Batch: 31300,train loss is: 0.0003059500157073593\n",
      "test loss is 0.0007075482891390089\n",
      "Batch: 31400,train loss is: 0.0003124318450181455\n",
      "test loss is 0.00046466572422593916\n",
      "Batch: 31500,train loss is: 0.000417078869531632\n",
      "test loss is 0.0005214070945900817\n",
      "Batch: 31600,train loss is: 0.0007016063875039865\n",
      "test loss is 0.00036916284323544433\n",
      "Batch: 31700,train loss is: 0.00016817841401968597\n",
      "test loss is 0.00026634717033537363\n",
      "Batch: 31800,train loss is: 0.0007382715480716072\n",
      "test loss is 0.000488515443362018\n",
      "Batch: 31900,train loss is: 0.00032410740089495907\n",
      "test loss is 0.00032486599529049425\n",
      "Batch: 32000,train loss is: 0.0019920388060362983\n",
      "test loss is 0.00039871021694074747\n",
      "Batch: 32100,train loss is: 0.0006056645569543318\n",
      "test loss is 0.0013752280597933448\n",
      "Batch: 32200,train loss is: 0.00014221023971467854\n",
      "test loss is 0.0004340793116666371\n",
      "Batch: 32300,train loss is: 0.0012788647525789688\n",
      "test loss is 0.0013188802348905908\n",
      "Batch: 32400,train loss is: 0.0002644529419899845\n",
      "test loss is 0.00046314110423694127\n",
      "Batch: 32500,train loss is: 0.0002928608525561589\n",
      "test loss is 0.00042055772114400205\n",
      "Batch: 32600,train loss is: 0.00029796760760305506\n",
      "test loss is 0.00040649746737869626\n",
      "Batch: 32700,train loss is: 0.0001808793147071632\n",
      "test loss is 0.0009053827984923987\n",
      "Batch: 32800,train loss is: 0.0006363372573254454\n",
      "test loss is 0.0004467687034524183\n",
      "Batch: 32900,train loss is: 0.00024014962566669732\n",
      "test loss is 0.00032657196777764893\n",
      "Batch: 33000,train loss is: 0.00012222676303887304\n",
      "test loss is 0.00030740219330305794\n",
      "Batch: 33100,train loss is: 0.0008792398763239866\n",
      "test loss is 0.0014068947229963309\n",
      "Batch: 33200,train loss is: 0.0001652204203022205\n",
      "test loss is 0.0004457070102787711\n",
      "Batch: 33300,train loss is: 0.0005049456032230377\n",
      "test loss is 0.00033837423737710865\n",
      "Batch: 33400,train loss is: 0.0002691136382211432\n",
      "test loss is 0.0012459044577927943\n",
      "Batch: 33500,train loss is: 0.0004830967675711631\n",
      "test loss is 0.002109494651854643\n",
      "Batch: 33600,train loss is: 0.00029458039179618264\n",
      "test loss is 0.0005112312681084449\n",
      "Batch: 33700,train loss is: 0.0004981841010419499\n",
      "test loss is 0.00032621321941433235\n",
      "Batch: 33800,train loss is: 0.0003556006688354568\n",
      "test loss is 0.00045969453802536354\n",
      "Batch: 33900,train loss is: 0.00028443063715590574\n",
      "test loss is 0.00024178177988827738\n",
      "-----------------------Epoch: 8----------------------------------\n",
      "Batch: 0,train loss is: 0.00019207432170153025\n",
      "test loss is 0.00032380436395591366\n",
      "Batch: 100,train loss is: 0.00023537912779702112\n",
      "test loss is 0.00026226490213195503\n",
      "Batch: 200,train loss is: 0.00018985892933387396\n",
      "test loss is 0.0002621431411493857\n",
      "Batch: 300,train loss is: 0.0002712505197636516\n",
      "test loss is 0.0004014750430476382\n",
      "Batch: 400,train loss is: 0.0003135519397792196\n",
      "test loss is 0.00033581245925242976\n",
      "Batch: 500,train loss is: 0.00012744766882399373\n",
      "test loss is 0.0011973804349554853\n",
      "Batch: 600,train loss is: 0.000107873418296835\n",
      "test loss is 0.00036058786029398616\n",
      "Batch: 700,train loss is: 0.00018603038856994508\n",
      "test loss is 0.00042781035210424234\n",
      "Batch: 800,train loss is: 0.0006595032634131311\n",
      "test loss is 0.0006808420159540726\n",
      "Batch: 900,train loss is: 0.0002608411832349448\n",
      "test loss is 0.0005880406898144472\n",
      "Batch: 1000,train loss is: 0.0004987188309498534\n",
      "test loss is 0.000416148411891723\n",
      "Batch: 1100,train loss is: 0.001814000237794381\n",
      "test loss is 0.0004463458915848206\n",
      "Batch: 1200,train loss is: 0.0006031409677919322\n",
      "test loss is 0.000758884133269023\n",
      "Batch: 1300,train loss is: 0.0004448675439406591\n",
      "test loss is 0.002721734866667658\n",
      "Batch: 1400,train loss is: 0.00026501994381725245\n",
      "test loss is 0.0005872261474112689\n",
      "Batch: 1500,train loss is: 0.0007852861081779741\n",
      "test loss is 0.0006971096624032711\n",
      "Batch: 1600,train loss is: 0.0020353989348106134\n",
      "test loss is 0.0013441285667265556\n",
      "Batch: 1700,train loss is: 0.0007123506991961158\n",
      "test loss is 0.0003817634897510164\n",
      "Batch: 1800,train loss is: 0.0003697770881554272\n",
      "test loss is 0.00046693177838180006\n",
      "Batch: 1900,train loss is: 0.001053216389490636\n",
      "test loss is 0.0008234489673222421\n",
      "Batch: 2000,train loss is: 0.00016234636196316225\n",
      "test loss is 0.00040008224905475475\n",
      "Batch: 2100,train loss is: 0.00036527533927496324\n",
      "test loss is 0.0003507191563553264\n",
      "Batch: 2200,train loss is: 0.00041029286722660073\n",
      "test loss is 0.0003444444468185632\n",
      "Batch: 2300,train loss is: 0.0005924177623616336\n",
      "test loss is 0.000574575933726065\n",
      "Batch: 2400,train loss is: 0.000322257344790887\n",
      "test loss is 0.0006499929907931578\n",
      "Batch: 2500,train loss is: 0.00031831316055666125\n",
      "test loss is 0.0003781462205650828\n",
      "Batch: 2600,train loss is: 0.00035397378843825565\n",
      "test loss is 0.0002803023813314353\n",
      "Batch: 2700,train loss is: 0.0002960775280301012\n",
      "test loss is 0.00034568441603369916\n",
      "Batch: 2800,train loss is: 0.0002713904017768153\n",
      "test loss is 0.00041021500162971965\n",
      "Batch: 2900,train loss is: 0.0003009412850022184\n",
      "test loss is 0.00034742650419094417\n",
      "Batch: 3000,train loss is: 0.0002829747751370674\n",
      "test loss is 0.0007304981175483854\n",
      "Batch: 3100,train loss is: 0.00017524687139300517\n",
      "test loss is 0.0003359998359721137\n",
      "Batch: 3200,train loss is: 0.0007842783191613119\n",
      "test loss is 0.0009150074276373919\n",
      "Batch: 3300,train loss is: 0.0006334669298336985\n",
      "test loss is 0.0005346033405238729\n",
      "Batch: 3400,train loss is: 0.002121500844732039\n",
      "test loss is 0.0010941632613616348\n",
      "Batch: 3500,train loss is: 0.00018743022835351563\n",
      "test loss is 0.0006969561455274992\n",
      "Batch: 3600,train loss is: 0.00013703564573818214\n",
      "test loss is 0.000806689469460744\n",
      "Batch: 3700,train loss is: 0.0001382842046335492\n",
      "test loss is 0.000537362687983121\n",
      "Batch: 3800,train loss is: 0.0005101011153237559\n",
      "test loss is 0.00035828916234558784\n",
      "Batch: 3900,train loss is: 0.0001733946857503756\n",
      "test loss is 0.00044832510060280464\n",
      "Batch: 4000,train loss is: 0.00020828917698663743\n",
      "test loss is 0.00045394230139473196\n",
      "Batch: 4100,train loss is: 0.00022893783725905798\n",
      "test loss is 0.00028056111599957835\n",
      "Batch: 4200,train loss is: 0.000740442913491716\n",
      "test loss is 0.00039504521575857177\n",
      "Batch: 4300,train loss is: 0.00015411349446714057\n",
      "test loss is 0.00033726234912254767\n",
      "Batch: 4400,train loss is: 0.0011473595414725121\n",
      "test loss is 0.000733304934151237\n",
      "Batch: 4500,train loss is: 8.769907842257306e-05\n",
      "test loss is 0.0003464437662805016\n",
      "Batch: 4600,train loss is: 0.00040307795053324695\n",
      "test loss is 0.0005281190179874695\n",
      "Batch: 4700,train loss is: 0.00046548797629790905\n",
      "test loss is 0.00038384125369361933\n",
      "Batch: 4800,train loss is: 0.0012241234359372884\n",
      "test loss is 0.00098449663984686\n",
      "Batch: 4900,train loss is: 0.0017927159689744056\n",
      "test loss is 0.0009820748959427409\n",
      "Batch: 5000,train loss is: 0.00045704245878435405\n",
      "test loss is 0.0004781672912065946\n",
      "Batch: 5100,train loss is: 0.0004873912900242545\n",
      "test loss is 0.00043263358325118783\n",
      "Batch: 5200,train loss is: 0.00032423700352159806\n",
      "test loss is 0.0004609197658006566\n",
      "Batch: 5300,train loss is: 0.0015677800972659325\n",
      "test loss is 0.002200812843655843\n",
      "Batch: 5400,train loss is: 0.00043741467696159145\n",
      "test loss is 0.0006701131733630262\n",
      "Batch: 5500,train loss is: 0.0006498897834179899\n",
      "test loss is 0.0007924425882907817\n",
      "Batch: 5600,train loss is: 0.00053506709929776\n",
      "test loss is 0.00033232116975407837\n",
      "Batch: 5700,train loss is: 0.00026375874717073474\n",
      "test loss is 0.0003274686945412108\n",
      "Batch: 5800,train loss is: 0.00036134993658032806\n",
      "test loss is 0.00032231332870937647\n",
      "Batch: 5900,train loss is: 0.00048699394024331443\n",
      "test loss is 0.0005516089940024795\n",
      "Batch: 6000,train loss is: 0.0012716097745942016\n",
      "test loss is 0.00035968896110442493\n",
      "Batch: 6100,train loss is: 0.0003425118889012909\n",
      "test loss is 0.0005253288279207932\n",
      "Batch: 6200,train loss is: 0.0003567572021960317\n",
      "test loss is 0.00048780244400564206\n",
      "Batch: 6300,train loss is: 0.0004719748598830778\n",
      "test loss is 0.00048319306663830716\n",
      "Batch: 6400,train loss is: 0.0011759205161270142\n",
      "test loss is 0.0014180778541563262\n",
      "Batch: 6500,train loss is: 0.0001367119650369534\n",
      "test loss is 0.0003327794777446869\n",
      "Batch: 6600,train loss is: 0.00223684589394985\n",
      "test loss is 0.00029392433052299253\n",
      "Batch: 6700,train loss is: 0.00010699984177837069\n",
      "test loss is 0.00043817325353388527\n",
      "Batch: 6800,train loss is: 0.0007149305398275456\n",
      "test loss is 0.000419486035310734\n",
      "Batch: 6900,train loss is: 0.00020026963520767435\n",
      "test loss is 0.0003692908941099789\n",
      "Batch: 7000,train loss is: 0.00048755449078608567\n",
      "test loss is 0.00039139451209879033\n",
      "Batch: 7100,train loss is: 0.0005230565864341282\n",
      "test loss is 0.00040407453019984327\n",
      "Batch: 7200,train loss is: 0.00010154052059240785\n",
      "test loss is 0.000292048930821677\n",
      "Batch: 7300,train loss is: 0.0031885761221508526\n",
      "test loss is 0.000582640765061846\n",
      "Batch: 7400,train loss is: 0.0005055758977082019\n",
      "test loss is 0.0006777730671245792\n",
      "Batch: 7500,train loss is: 0.0002697272331522277\n",
      "test loss is 0.0004922493182788555\n",
      "Batch: 7600,train loss is: 0.0007843657874574322\n",
      "test loss is 0.00042275748121170374\n",
      "Batch: 7700,train loss is: 0.000636068087679524\n",
      "test loss is 0.0013601528577894596\n",
      "Batch: 7800,train loss is: 0.00028595471031567227\n",
      "test loss is 0.000629850220778599\n",
      "Batch: 7900,train loss is: 0.000803057233192728\n",
      "test loss is 0.00039101460516956124\n",
      "Batch: 8000,train loss is: 0.0003110323773772735\n",
      "test loss is 0.00055868254851274\n",
      "Batch: 8100,train loss is: 0.0005157235707908716\n",
      "test loss is 0.0003522135074073719\n",
      "Batch: 8200,train loss is: 0.00020230367868772115\n",
      "test loss is 0.00028996399066043196\n",
      "Batch: 8300,train loss is: 0.00023952096666360255\n",
      "test loss is 0.0005232293827179768\n",
      "Batch: 8400,train loss is: 0.00042693457411895275\n",
      "test loss is 0.0005560326355380856\n",
      "Batch: 8500,train loss is: 0.0003437306478958298\n",
      "test loss is 0.0003124014513836053\n",
      "Batch: 8600,train loss is: 0.00045553182837776586\n",
      "test loss is 0.00070217308424086\n",
      "Batch: 8700,train loss is: 0.00022547532968266568\n",
      "test loss is 0.0005159663590303974\n",
      "Batch: 8800,train loss is: 0.0002718610753584984\n",
      "test loss is 0.0005512776221914531\n",
      "Batch: 8900,train loss is: 0.0010128488851681957\n",
      "test loss is 0.0004707665105329831\n",
      "Batch: 9000,train loss is: 0.000127008442908297\n",
      "test loss is 0.0003780097638418484\n",
      "Batch: 9100,train loss is: 0.0005502989521814275\n",
      "test loss is 0.0004946086021336979\n",
      "Batch: 9200,train loss is: 0.0005942228043197365\n",
      "test loss is 0.00060739974048235\n",
      "Batch: 9300,train loss is: 0.00044241357977096936\n",
      "test loss is 0.00042372650310510524\n",
      "Batch: 9400,train loss is: 0.00024604058492599403\n",
      "test loss is 0.0003326753525401425\n",
      "Batch: 9500,train loss is: 0.0007193712514582724\n",
      "test loss is 0.0009983977849009776\n",
      "Batch: 9600,train loss is: 0.0001973280652282318\n",
      "test loss is 0.00038781383816773783\n",
      "Batch: 9700,train loss is: 0.0001825019767502268\n",
      "test loss is 0.00039560556640338404\n",
      "Batch: 9800,train loss is: 0.00018617101824780586\n",
      "test loss is 0.000770348295065246\n",
      "Batch: 9900,train loss is: 0.0005123264256811487\n",
      "test loss is 0.0007947858655232677\n",
      "Batch: 10000,train loss is: 0.0010312561645193083\n",
      "test loss is 0.00040442540750306843\n",
      "Batch: 10100,train loss is: 0.0005110383098111348\n",
      "test loss is 0.00040176194278317254\n",
      "Batch: 10200,train loss is: 0.0005730095560578368\n",
      "test loss is 0.0003551593321850533\n",
      "Batch: 10300,train loss is: 0.00157415570530635\n",
      "test loss is 0.0011387373986997864\n",
      "Batch: 10400,train loss is: 0.000137122443610536\n",
      "test loss is 0.0006476462320131238\n",
      "Batch: 10500,train loss is: 0.00028674130927166234\n",
      "test loss is 0.000548424599214458\n",
      "Batch: 10600,train loss is: 0.00018608472037815686\n",
      "test loss is 0.0008375871255857984\n",
      "Batch: 10700,train loss is: 0.00022829538531018764\n",
      "test loss is 0.00029732349688488783\n",
      "Batch: 10800,train loss is: 0.0003120992987688522\n",
      "test loss is 0.00039412578884814804\n",
      "Batch: 10900,train loss is: 0.0005997606509184078\n",
      "test loss is 0.0004787387152362766\n",
      "Batch: 11000,train loss is: 0.00039647822550937015\n",
      "test loss is 0.00030477129154193926\n",
      "Batch: 11100,train loss is: 0.000299118227103607\n",
      "test loss is 0.00030382468247615465\n",
      "Batch: 11200,train loss is: 0.000447296949638393\n",
      "test loss is 0.0002947796636836667\n",
      "Batch: 11300,train loss is: 0.0001367327018309174\n",
      "test loss is 0.0002986181251711454\n",
      "Batch: 11400,train loss is: 0.0006277278329860451\n",
      "test loss is 0.0008288412100280524\n",
      "Batch: 11500,train loss is: 0.0003662755368210476\n",
      "test loss is 0.0009421312513368363\n",
      "Batch: 11600,train loss is: 0.0009134885333691915\n",
      "test loss is 0.0010699962371766808\n",
      "Batch: 11700,train loss is: 0.0002909535991203271\n",
      "test loss is 0.0005502658808817198\n",
      "Batch: 11800,train loss is: 0.0006550879078523472\n",
      "test loss is 0.0006127722692870314\n",
      "Batch: 11900,train loss is: 0.0005740960416739056\n",
      "test loss is 0.0005143845955195809\n",
      "Batch: 12000,train loss is: 0.0002650586289856636\n",
      "test loss is 0.00038181251028318445\n",
      "Batch: 12100,train loss is: 0.006169603079549726\n",
      "test loss is 0.0007688486871688354\n",
      "Batch: 12200,train loss is: 0.0004178155909195126\n",
      "test loss is 0.0007882869625802623\n",
      "Batch: 12300,train loss is: 0.0008587312147461167\n",
      "test loss is 0.0004528338264538022\n",
      "Batch: 12400,train loss is: 0.0003936698745608487\n",
      "test loss is 0.00024627505152496903\n",
      "Batch: 12500,train loss is: 0.0010741663487203418\n",
      "test loss is 0.00044218566029331996\n",
      "Batch: 12600,train loss is: 0.00038209346758646793\n",
      "test loss is 0.00033810857523537517\n",
      "Batch: 12700,train loss is: 0.0001973166391041453\n",
      "test loss is 0.000286439662803664\n",
      "Batch: 12800,train loss is: 0.00013669848227942157\n",
      "test loss is 0.00028019750994268865\n",
      "Batch: 12900,train loss is: 0.00034889275972869885\n",
      "test loss is 0.0003320624811220436\n",
      "Batch: 13000,train loss is: 0.0001434231078016043\n",
      "test loss is 0.00026154849363316295\n",
      "Batch: 13100,train loss is: 0.0004258442959769637\n",
      "test loss is 0.0005232978780568164\n",
      "Batch: 13200,train loss is: 0.00028177818291750834\n",
      "test loss is 0.00022952536176324507\n",
      "Batch: 13300,train loss is: 0.0003048496536471219\n",
      "test loss is 0.0012651002416494844\n",
      "Batch: 13400,train loss is: 0.0006597125495507388\n",
      "test loss is 0.0006197376831319177\n",
      "Batch: 13500,train loss is: 0.0002701109605725103\n",
      "test loss is 0.0005618269940716138\n",
      "Batch: 13600,train loss is: 0.00212077113390637\n",
      "test loss is 0.0030500322102026863\n",
      "Batch: 13700,train loss is: 0.0001817906988987073\n",
      "test loss is 0.0004981957121414612\n",
      "Batch: 13800,train loss is: 0.00021796933394086532\n",
      "test loss is 0.0004813020693933482\n",
      "Batch: 13900,train loss is: 0.0006485366669736035\n",
      "test loss is 0.000489631046248763\n",
      "Batch: 14000,train loss is: 0.0001973944263912738\n",
      "test loss is 0.00031653116852655127\n",
      "Batch: 14100,train loss is: 0.00021074594447718234\n",
      "test loss is 0.00041296815352706914\n",
      "Batch: 14200,train loss is: 0.00011918444527795167\n",
      "test loss is 0.0002612296217280063\n",
      "Batch: 14300,train loss is: 0.0002120196582384874\n",
      "test loss is 0.00032342433092324047\n",
      "Batch: 14400,train loss is: 0.0005437161842693838\n",
      "test loss is 0.0003028455300908272\n",
      "Batch: 14500,train loss is: 0.00018493250763698118\n",
      "test loss is 0.0002792292042633621\n",
      "Batch: 14600,train loss is: 0.0004895511715937467\n",
      "test loss is 0.0003087250722407078\n",
      "Batch: 14700,train loss is: 0.0001595549806795656\n",
      "test loss is 0.0006818829332031538\n",
      "Batch: 14800,train loss is: 0.0004614230841338768\n",
      "test loss is 0.0005410431042468162\n",
      "Batch: 14900,train loss is: 0.0002307454925012833\n",
      "test loss is 0.0002845348941624057\n",
      "Batch: 15000,train loss is: 0.00014775929419703259\n",
      "test loss is 0.0004089944537347992\n",
      "Batch: 15100,train loss is: 0.00031754182515561193\n",
      "test loss is 0.0004262686310492524\n",
      "Batch: 15200,train loss is: 0.0006617491205668777\n",
      "test loss is 0.0003161074703876997\n",
      "Batch: 15300,train loss is: 0.00026398665615373314\n",
      "test loss is 0.000286111856433993\n",
      "Batch: 15400,train loss is: 0.0006258875223234775\n",
      "test loss is 0.0009444754233155809\n",
      "Batch: 15500,train loss is: 0.0003648498431427888\n",
      "test loss is 0.0004440470295791302\n",
      "Batch: 15600,train loss is: 0.001029665853209992\n",
      "test loss is 0.0008795580771056473\n",
      "Batch: 15700,train loss is: 0.000280541400048976\n",
      "test loss is 0.0007403069000640428\n",
      "Batch: 15800,train loss is: 0.00024967744526107087\n",
      "test loss is 0.0005235525948993782\n",
      "Batch: 15900,train loss is: 0.0005728189034080327\n",
      "test loss is 0.0005263745738490176\n",
      "Batch: 16000,train loss is: 0.0007014946547403349\n",
      "test loss is 0.0006982048764398177\n",
      "Batch: 16100,train loss is: 0.0011409884064051787\n",
      "test loss is 0.0006049807075079217\n",
      "Batch: 16200,train loss is: 0.00020149134930201362\n",
      "test loss is 0.0006300950762637626\n",
      "Batch: 16300,train loss is: 0.00029070263018829895\n",
      "test loss is 0.0003268119935702411\n",
      "Batch: 16400,train loss is: 0.00047109162483137184\n",
      "test loss is 0.0003323971874589774\n",
      "Batch: 16500,train loss is: 0.0007225598831033987\n",
      "test loss is 0.0005546692238161769\n",
      "Batch: 16600,train loss is: 0.0001723625034773276\n",
      "test loss is 0.0007073948315346882\n",
      "Batch: 16700,train loss is: 0.00039144193878580373\n",
      "test loss is 0.0007409845197309808\n",
      "Batch: 16800,train loss is: 0.0004195308131790702\n",
      "test loss is 0.00042566142976087926\n",
      "Batch: 16900,train loss is: 0.0012154885737386699\n",
      "test loss is 0.0008701868439915022\n",
      "Batch: 17000,train loss is: 0.00018460072472903507\n",
      "test loss is 0.0003459655517186744\n",
      "Batch: 17100,train loss is: 0.0005137409070566772\n",
      "test loss is 0.0008328828683902955\n",
      "Batch: 17200,train loss is: 8.691026413564713e-05\n",
      "test loss is 0.00031717963614591037\n",
      "Batch: 17300,train loss is: 0.0001844649339507963\n",
      "test loss is 0.00032095032519872203\n",
      "Batch: 17400,train loss is: 0.00045846224130187277\n",
      "test loss is 0.000539554008434182\n",
      "Batch: 17500,train loss is: 0.0006003576603301\n",
      "test loss is 0.0003306156991102936\n",
      "Batch: 17600,train loss is: 0.0001578009320986487\n",
      "test loss is 0.00042503414306385874\n",
      "Batch: 17700,train loss is: 0.00024846170709399214\n",
      "test loss is 0.0005646160554182781\n",
      "Batch: 17800,train loss is: 0.0003998024609133606\n",
      "test loss is 0.00035549394719905673\n",
      "Batch: 17900,train loss is: 0.0013336051763645272\n",
      "test loss is 0.0007912576725713084\n",
      "Batch: 18000,train loss is: 0.0006721911808470498\n",
      "test loss is 0.0005058166711674777\n",
      "Batch: 18100,train loss is: 0.0002201910674086237\n",
      "test loss is 0.0005054581390867387\n",
      "Batch: 18200,train loss is: 0.00022235850541289163\n",
      "test loss is 0.0006936545379584186\n",
      "Batch: 18300,train loss is: 0.0010166400886310355\n",
      "test loss is 0.00038720761086605173\n",
      "Batch: 18400,train loss is: 0.0001501654744265989\n",
      "test loss is 0.0005794292831298887\n",
      "Batch: 18500,train loss is: 0.0008744245436441568\n",
      "test loss is 0.00039578264620122834\n",
      "Batch: 18600,train loss is: 0.00040801077791561484\n",
      "test loss is 0.0005239956562423182\n",
      "Batch: 18700,train loss is: 0.0007047171689888972\n",
      "test loss is 0.001043189386541718\n",
      "Batch: 18800,train loss is: 0.0004677161543997827\n",
      "test loss is 0.0004563300232136002\n",
      "Batch: 18900,train loss is: 0.0001854394790083899\n",
      "test loss is 0.0003197586189448419\n",
      "Batch: 19000,train loss is: 0.00034134263674804254\n",
      "test loss is 0.0004972211147428776\n",
      "Batch: 19100,train loss is: 0.0007923095100668211\n",
      "test loss is 0.00052890598927699\n",
      "Batch: 19200,train loss is: 0.0004751539364440971\n",
      "test loss is 0.00044529475910450386\n",
      "Batch: 19300,train loss is: 0.0006827647760129377\n",
      "test loss is 0.0013258900011995961\n",
      "Batch: 19400,train loss is: 0.00024426796360581563\n",
      "test loss is 0.000410835350138028\n",
      "Batch: 19500,train loss is: 0.00030283984050385435\n",
      "test loss is 0.0004935775129787144\n",
      "Batch: 19600,train loss is: 0.0003958766467670658\n",
      "test loss is 0.000595869643768702\n",
      "Batch: 19700,train loss is: 0.001016292228597685\n",
      "test loss is 0.0004889193200896182\n",
      "Batch: 19800,train loss is: 0.00024279026505921218\n",
      "test loss is 0.00038479283889467477\n",
      "Batch: 19900,train loss is: 0.0002258494528168503\n",
      "test loss is 0.00035826730560619644\n",
      "Batch: 20000,train loss is: 0.0008256417079519659\n",
      "test loss is 0.0008985915691607397\n",
      "Batch: 20100,train loss is: 0.0012604125125186283\n",
      "test loss is 0.0007622479372251745\n",
      "Batch: 20200,train loss is: 0.0001387361765273924\n",
      "test loss is 0.00035673188864867905\n",
      "Batch: 20300,train loss is: 0.0001543501573741286\n",
      "test loss is 0.0003654565523059364\n",
      "Batch: 20400,train loss is: 0.0004573357577584906\n",
      "test loss is 0.0007128338943617683\n",
      "Batch: 20500,train loss is: 0.00019582835986105837\n",
      "test loss is 0.00034945002893716837\n",
      "Batch: 20600,train loss is: 0.0034764196437884034\n",
      "test loss is 0.0006465846075819857\n",
      "Batch: 20700,train loss is: 0.0007249737402344413\n",
      "test loss is 0.0005230113378063198\n",
      "Batch: 20800,train loss is: 0.0003377061593960005\n",
      "test loss is 0.00036289775000016564\n",
      "Batch: 20900,train loss is: 0.0003988652024025704\n",
      "test loss is 0.0008452918849462205\n",
      "Batch: 21000,train loss is: 0.00042248680979846756\n",
      "test loss is 0.000692431593022423\n",
      "Batch: 21100,train loss is: 0.0002425533481441919\n",
      "test loss is 0.00045920864316139823\n",
      "Batch: 21200,train loss is: 0.0012459313650713829\n",
      "test loss is 0.0009359992027965968\n",
      "Batch: 21300,train loss is: 0.00013676124071082424\n",
      "test loss is 0.00100956739187388\n",
      "Batch: 21400,train loss is: 0.00044053007590187654\n",
      "test loss is 0.0005207461422327709\n",
      "Batch: 21500,train loss is: 0.00015615785561586337\n",
      "test loss is 0.0004136776492172211\n",
      "Batch: 21600,train loss is: 0.00017787027025208187\n",
      "test loss is 0.00029822051328380026\n",
      "Batch: 21700,train loss is: 0.0003750640680412202\n",
      "test loss is 0.0005406521578530476\n",
      "Batch: 21800,train loss is: 0.00035559203608613273\n",
      "test loss is 0.00038320293769981175\n",
      "Batch: 21900,train loss is: 0.0003444655420572026\n",
      "test loss is 0.0006244099776754473\n",
      "Batch: 22000,train loss is: 0.0006237424093004453\n",
      "test loss is 0.000413305076892394\n",
      "Batch: 22100,train loss is: 9.95889180536866e-05\n",
      "test loss is 0.00048648156417464074\n",
      "Batch: 22200,train loss is: 0.0011970305914081269\n",
      "test loss is 0.0006262234009113717\n",
      "Batch: 22300,train loss is: 0.0012609257072892452\n",
      "test loss is 0.0013393338048895943\n",
      "Batch: 22400,train loss is: 0.00047994494811710094\n",
      "test loss is 0.0006034333023589663\n",
      "Batch: 22500,train loss is: 0.00014372599969952272\n",
      "test loss is 0.00037451076861016596\n",
      "Batch: 22600,train loss is: 0.0003554566673482456\n",
      "test loss is 0.0004116405712077353\n",
      "Batch: 22700,train loss is: 0.00022516364641228187\n",
      "test loss is 0.0003970877435261468\n",
      "Batch: 22800,train loss is: 0.00023565319878227263\n",
      "test loss is 0.0003677168237790163\n",
      "Batch: 22900,train loss is: 0.00029512211194268915\n",
      "test loss is 0.0007004411587081348\n",
      "Batch: 23000,train loss is: 0.00022796196994620343\n",
      "test loss is 0.0003448430334519139\n",
      "Batch: 23100,train loss is: 0.00018123884516827986\n",
      "test loss is 0.0005046378104978602\n",
      "Batch: 23200,train loss is: 0.00017439730436705664\n",
      "test loss is 0.00027537503592258496\n",
      "Batch: 23300,train loss is: 0.0002463840197683004\n",
      "test loss is 0.0003041688105962088\n",
      "Batch: 23400,train loss is: 0.00021558085044840513\n",
      "test loss is 0.0004449883640642692\n",
      "Batch: 23500,train loss is: 0.00034905329494369067\n",
      "test loss is 0.0003100335611127455\n",
      "Batch: 23600,train loss is: 0.00016690558507124003\n",
      "test loss is 0.0002690744702694315\n",
      "Batch: 23700,train loss is: 0.00030329569302348227\n",
      "test loss is 0.000307640879191648\n",
      "Batch: 23800,train loss is: 0.0004998519306202902\n",
      "test loss is 0.00041856835529111426\n",
      "Batch: 23900,train loss is: 0.0015588412832856988\n",
      "test loss is 0.0007471623151610621\n",
      "Batch: 24000,train loss is: 0.0002789010663700819\n",
      "test loss is 0.0006373915939837169\n",
      "Batch: 24100,train loss is: 0.00015768345923698493\n",
      "test loss is 0.00037545564261961357\n",
      "Batch: 24200,train loss is: 0.0011388745091854883\n",
      "test loss is 0.00034589396021230605\n",
      "Batch: 24300,train loss is: 0.00041188256732958416\n",
      "test loss is 0.00036881252770359745\n",
      "Batch: 24400,train loss is: 0.0003089911617667789\n",
      "test loss is 0.00027415175937912706\n",
      "Batch: 24500,train loss is: 0.00016775239490296652\n",
      "test loss is 0.0002886462582502554\n",
      "Batch: 24600,train loss is: 0.00030099707468855016\n",
      "test loss is 0.0003412668192402101\n",
      "Batch: 24700,train loss is: 0.00018316324892517767\n",
      "test loss is 0.0002752980608800498\n",
      "Batch: 24800,train loss is: 0.00038083418039827567\n",
      "test loss is 0.0003436763621761999\n",
      "Batch: 24900,train loss is: 0.00012591631401243303\n",
      "test loss is 0.00030534473191856323\n",
      "Batch: 25000,train loss is: 0.00020065456980451505\n",
      "test loss is 0.00037951662512747045\n",
      "Batch: 25100,train loss is: 0.00014800705425851605\n",
      "test loss is 0.0004346293820053069\n",
      "Batch: 25200,train loss is: 0.00030625018037615633\n",
      "test loss is 0.00030362901758010364\n",
      "Batch: 25300,train loss is: 0.0003016504495961809\n",
      "test loss is 0.00032897878336255365\n",
      "Batch: 25400,train loss is: 0.0003600914533693043\n",
      "test loss is 0.0005070312165408963\n",
      "Batch: 25500,train loss is: 0.00016531479667832738\n",
      "test loss is 0.000374705532681479\n",
      "Batch: 25600,train loss is: 0.0005308667034482114\n",
      "test loss is 0.0004038769357768264\n",
      "Batch: 25700,train loss is: 0.0003289497950389727\n",
      "test loss is 0.00035582737501686524\n",
      "Batch: 25800,train loss is: 0.0003411752880250262\n",
      "test loss is 0.0005664274136049083\n",
      "Batch: 25900,train loss is: 0.0004962938420720227\n",
      "test loss is 0.0011156793744632777\n",
      "Batch: 26000,train loss is: 0.0018042194900593249\n",
      "test loss is 0.0010007119531387921\n",
      "Batch: 26100,train loss is: 0.00086725436251344\n",
      "test loss is 0.0007749857351161634\n",
      "Batch: 26200,train loss is: 0.0009373198305094098\n",
      "test loss is 0.0004675719904345179\n",
      "Batch: 26300,train loss is: 0.00033093410767810847\n",
      "test loss is 0.0004169399279321118\n",
      "Batch: 26400,train loss is: 0.00016468927556221704\n",
      "test loss is 0.000299613962496082\n",
      "Batch: 26500,train loss is: 0.00018367384563260246\n",
      "test loss is 0.0003244260200983795\n",
      "Batch: 26600,train loss is: 0.00152457107349171\n",
      "test loss is 0.0006242524386861984\n",
      "Batch: 26700,train loss is: 0.00012233857679661326\n",
      "test loss is 0.00035578039760484785\n",
      "Batch: 26800,train loss is: 0.0004866811325893729\n",
      "test loss is 0.00027287962232267507\n",
      "Batch: 26900,train loss is: 0.00032151743515280214\n",
      "test loss is 0.0002939586166347312\n",
      "Batch: 27000,train loss is: 0.00039670324994385585\n",
      "test loss is 0.0005550774535123909\n",
      "Batch: 27100,train loss is: 0.00019322724506288584\n",
      "test loss is 0.00033357534058125614\n",
      "Batch: 27200,train loss is: 0.0009184599235874631\n",
      "test loss is 0.001747987312475502\n",
      "Batch: 27300,train loss is: 0.0006236067600189354\n",
      "test loss is 0.0009469513537362811\n",
      "Batch: 27400,train loss is: 0.00044000134543136567\n",
      "test loss is 0.00042539372445329546\n",
      "Batch: 27500,train loss is: 0.001433089192688691\n",
      "test loss is 0.0006325713713985667\n",
      "Batch: 27600,train loss is: 0.00026417184488224457\n",
      "test loss is 0.00040790789255141136\n",
      "Batch: 27700,train loss is: 0.00027626524108935\n",
      "test loss is 0.0003748212979086595\n",
      "Batch: 27800,train loss is: 0.00013312995337441116\n",
      "test loss is 0.0003694220245745127\n",
      "Batch: 27900,train loss is: 0.0007949179562965896\n",
      "test loss is 0.0003423613644469431\n",
      "Batch: 28000,train loss is: 0.00012877256306433342\n",
      "test loss is 0.00041056443611999877\n",
      "Batch: 28100,train loss is: 0.000290790577492398\n",
      "test loss is 0.000498985804050668\n",
      "Batch: 28200,train loss is: 0.0004350661703953994\n",
      "test loss is 0.0003460951315713563\n",
      "Batch: 28300,train loss is: 0.0007182572523076233\n",
      "test loss is 0.0003801230067453493\n",
      "Batch: 28400,train loss is: 0.0007041711381069196\n",
      "test loss is 0.0006190395796213691\n",
      "Batch: 28500,train loss is: 0.0005347177527543958\n",
      "test loss is 0.000346297435305204\n",
      "Batch: 28600,train loss is: 0.00019960218589372896\n",
      "test loss is 0.000455970249746197\n",
      "Batch: 28700,train loss is: 0.00024350594519320046\n",
      "test loss is 0.0003877234483191845\n",
      "Batch: 28800,train loss is: 0.0006381024821285717\n",
      "test loss is 0.00025131611810782015\n",
      "Batch: 28900,train loss is: 0.000212603935751717\n",
      "test loss is 0.0004241186817333313\n",
      "Batch: 29000,train loss is: 0.00028333618984909464\n",
      "test loss is 0.0003814710916669007\n",
      "Batch: 29100,train loss is: 0.0005155655809100938\n",
      "test loss is 0.0011078078639815926\n",
      "Batch: 29200,train loss is: 0.00025662248538896976\n",
      "test loss is 0.0006035007928792257\n",
      "Batch: 29300,train loss is: 0.000336241374356803\n",
      "test loss is 0.0012156222986467338\n",
      "Batch: 29400,train loss is: 0.00020398934980224028\n",
      "test loss is 0.0006375740854244407\n",
      "Batch: 29500,train loss is: 0.00018534449371962778\n",
      "test loss is 0.0003146068407205439\n",
      "Batch: 29600,train loss is: 0.0005662724378312688\n",
      "test loss is 0.0003396092706437365\n",
      "Batch: 29700,train loss is: 9.20917064968368e-05\n",
      "test loss is 0.00036027035473955445\n",
      "Batch: 29800,train loss is: 0.00047978449742252144\n",
      "test loss is 0.00036529570541306327\n",
      "Batch: 29900,train loss is: 0.0007081074512329916\n",
      "test loss is 0.000337252120544183\n",
      "Batch: 30000,train loss is: 0.0001602543671081569\n",
      "test loss is 0.0002522984616002238\n",
      "Batch: 30100,train loss is: 0.0004463305159426861\n",
      "test loss is 0.0007348762855057622\n",
      "Batch: 30200,train loss is: 0.0004663442174806406\n",
      "test loss is 0.0005004213980184313\n",
      "Batch: 30300,train loss is: 0.000478022105312624\n",
      "test loss is 0.0007244630971392692\n",
      "Batch: 30400,train loss is: 0.0006174249043477137\n",
      "test loss is 0.002438454140042324\n",
      "Batch: 30500,train loss is: 0.00028819037737550906\n",
      "test loss is 0.0005464753887134407\n",
      "Batch: 30600,train loss is: 0.0001821358726112668\n",
      "test loss is 0.00033960858609508135\n",
      "Batch: 30700,train loss is: 0.0008754381564676283\n",
      "test loss is 0.0005791590137053898\n",
      "Batch: 30800,train loss is: 0.0001194486242160407\n",
      "test loss is 0.0003190180930001568\n",
      "Batch: 30900,train loss is: 0.0002731200966105589\n",
      "test loss is 0.0002933293559291679\n",
      "Batch: 31000,train loss is: 0.00015116093360492125\n",
      "test loss is 0.0002992907673813224\n",
      "Batch: 31100,train loss is: 0.0002588496978799581\n",
      "test loss is 0.00043630285841084746\n",
      "Batch: 31200,train loss is: 0.00041156320108864806\n",
      "test loss is 0.0003166077163271082\n",
      "Batch: 31300,train loss is: 0.00023142192226550067\n",
      "test loss is 0.0005102749859029804\n",
      "Batch: 31400,train loss is: 0.00021990529952940408\n",
      "test loss is 0.00031798525852321885\n",
      "Batch: 31500,train loss is: 0.00040487624030096004\n",
      "test loss is 0.0004588137936015252\n",
      "Batch: 31600,train loss is: 0.0010506670440299825\n",
      "test loss is 0.0005019554664748835\n",
      "Batch: 31700,train loss is: 0.0001689581335785082\n",
      "test loss is 0.0002609035345200908\n",
      "Batch: 31800,train loss is: 0.0006529582808397496\n",
      "test loss is 0.000567260352738108\n",
      "Batch: 31900,train loss is: 0.00024152710990562516\n",
      "test loss is 0.00034878704847536156\n",
      "Batch: 32000,train loss is: 0.0021128714416176588\n",
      "test loss is 0.0003827941040598405\n",
      "Batch: 32100,train loss is: 0.00048087634051026536\n",
      "test loss is 0.0010856648013259015\n",
      "Batch: 32200,train loss is: 0.0001335789840677409\n",
      "test loss is 0.0004612568830154879\n",
      "Batch: 32300,train loss is: 0.0012448100410165682\n",
      "test loss is 0.001130619565276061\n",
      "Batch: 32400,train loss is: 0.0002221620498365266\n",
      "test loss is 0.0003587656221924335\n",
      "Batch: 32500,train loss is: 0.00024861354759619285\n",
      "test loss is 0.0003145457065579984\n",
      "Batch: 32600,train loss is: 0.00027902737341290996\n",
      "test loss is 0.0003825450110878624\n",
      "Batch: 32700,train loss is: 0.00019032058268735245\n",
      "test loss is 0.0006740664513220278\n",
      "Batch: 32800,train loss is: 0.0005731975785399338\n",
      "test loss is 0.00036923342480185743\n",
      "Batch: 32900,train loss is: 0.0002577199589410708\n",
      "test loss is 0.0003123677557883681\n",
      "Batch: 33000,train loss is: 9.048932347376056e-05\n",
      "test loss is 0.0002850354939756316\n",
      "Batch: 33100,train loss is: 0.0010260615327554316\n",
      "test loss is 0.001677603154305226\n",
      "Batch: 33200,train loss is: 0.0001295595971990267\n",
      "test loss is 0.00041862325273204175\n",
      "Batch: 33300,train loss is: 0.00060312341527012\n",
      "test loss is 0.0003171005124471793\n",
      "Batch: 33400,train loss is: 0.0003536555544811408\n",
      "test loss is 0.0011148113276506413\n",
      "Batch: 33500,train loss is: 0.00031400527193464333\n",
      "test loss is 0.0018916170753094038\n",
      "Batch: 33600,train loss is: 8.284204585555773e-05\n",
      "test loss is 0.0004784099650743989\n",
      "Batch: 33700,train loss is: 0.0002906575984125493\n",
      "test loss is 0.00031328371905086933\n",
      "Batch: 33800,train loss is: 0.0005160934276872758\n",
      "test loss is 0.0003263984779339135\n",
      "Batch: 33900,train loss is: 0.00019611660126730006\n",
      "test loss is 0.00022663914978188532\n",
      "-----------------------Epoch: 9----------------------------------\n",
      "Batch: 0,train loss is: 0.00017657979054684837\n",
      "test loss is 0.0002780888295823684\n",
      "Batch: 100,train loss is: 0.00022431961892443303\n",
      "test loss is 0.00027936079816637954\n",
      "Batch: 200,train loss is: 0.00023275990746046937\n",
      "test loss is 0.0002513304991203686\n",
      "Batch: 300,train loss is: 0.0002574878744775088\n",
      "test loss is 0.0003633026359855649\n",
      "Batch: 400,train loss is: 0.0002513664819071507\n",
      "test loss is 0.00029427306760707023\n",
      "Batch: 500,train loss is: 0.00010777405160498098\n",
      "test loss is 0.0008098275508105667\n",
      "Batch: 600,train loss is: 0.00014248934159577818\n",
      "test loss is 0.0003172296043224749\n",
      "Batch: 700,train loss is: 0.00013453025343203008\n",
      "test loss is 0.0003881915427610706\n",
      "Batch: 800,train loss is: 0.0002507189083016785\n",
      "test loss is 0.00039303067173678447\n",
      "Batch: 900,train loss is: 0.00013851632850680538\n",
      "test loss is 0.0004591570481413986\n",
      "Batch: 1000,train loss is: 0.000393391051320857\n",
      "test loss is 0.00036033186667590154\n",
      "Batch: 1100,train loss is: 0.0021064152428298248\n",
      "test loss is 0.0004780560490840183\n",
      "Batch: 1200,train loss is: 0.0003819269721400018\n",
      "test loss is 0.0005541353213289502\n",
      "Batch: 1300,train loss is: 0.0003473213399567231\n",
      "test loss is 0.0013886313930771732\n",
      "Batch: 1400,train loss is: 0.00026653107537955234\n",
      "test loss is 0.0005048433680032901\n",
      "Batch: 1500,train loss is: 0.000640543802137742\n",
      "test loss is 0.0006480427071395253\n",
      "Batch: 1600,train loss is: 0.0013045074565761093\n",
      "test loss is 0.0007944858211839563\n",
      "Batch: 1700,train loss is: 0.0006903566093860137\n",
      "test loss is 0.00036190046019623763\n",
      "Batch: 1800,train loss is: 0.0003662462290672106\n",
      "test loss is 0.0004020908874250311\n",
      "Batch: 1900,train loss is: 0.002730804115132076\n",
      "test loss is 0.001497786518424888\n",
      "Batch: 2000,train loss is: 0.0002660807704802325\n",
      "test loss is 0.00029416523353904513\n",
      "Batch: 2100,train loss is: 0.0003475087582356781\n",
      "test loss is 0.00028052059145065785\n",
      "Batch: 2200,train loss is: 0.00022852445657291806\n",
      "test loss is 0.00026541564968699004\n",
      "Batch: 2300,train loss is: 0.0002483772530403424\n",
      "test loss is 0.0004371191480546578\n",
      "Batch: 2400,train loss is: 0.0002040415127879601\n",
      "test loss is 0.00045949711326617276\n",
      "Batch: 2500,train loss is: 0.0003062149851889061\n",
      "test loss is 0.0002964237356170864\n",
      "Batch: 2600,train loss is: 0.0002815584447099286\n",
      "test loss is 0.0002471946284867246\n",
      "Batch: 2700,train loss is: 0.0003299939577924795\n",
      "test loss is 0.0002697205801925896\n",
      "Batch: 2800,train loss is: 0.00031237924617898974\n",
      "test loss is 0.0003806365170928069\n",
      "Batch: 2900,train loss is: 0.0002462523636139792\n",
      "test loss is 0.00035446655651041864\n",
      "Batch: 3000,train loss is: 0.0002431442994690463\n",
      "test loss is 0.0005286921150406014\n",
      "Batch: 3100,train loss is: 0.00028880378192880135\n",
      "test loss is 0.00024526556580380435\n",
      "Batch: 3200,train loss is: 0.0009720264007641293\n",
      "test loss is 0.0012203609250148694\n",
      "Batch: 3300,train loss is: 0.00040236183842615295\n",
      "test loss is 0.0004609590285668578\n",
      "Batch: 3400,train loss is: 0.0018239712943198551\n",
      "test loss is 0.001046041098182187\n",
      "Batch: 3500,train loss is: 0.00014977742380992715\n",
      "test loss is 0.000610027439418157\n",
      "Batch: 3600,train loss is: 0.00011532604959042457\n",
      "test loss is 0.0006390309578186449\n",
      "Batch: 3700,train loss is: 0.00013821569672916058\n",
      "test loss is 0.0005166447800532143\n",
      "Batch: 3800,train loss is: 0.00044947064362040913\n",
      "test loss is 0.0003362324625520595\n",
      "Batch: 3900,train loss is: 0.00019538666617755946\n",
      "test loss is 0.00039218301687212717\n",
      "Batch: 4000,train loss is: 0.00017563928474817684\n",
      "test loss is 0.0003930961798742473\n",
      "Batch: 4100,train loss is: 0.00021208999016659823\n",
      "test loss is 0.00025528408167132947\n",
      "Batch: 4200,train loss is: 0.0006187414759487558\n",
      "test loss is 0.00039521134523043253\n",
      "Batch: 4300,train loss is: 0.00014233901809312022\n",
      "test loss is 0.0002736174263590356\n",
      "Batch: 4400,train loss is: 0.0010111752888674317\n",
      "test loss is 0.0006361271377718778\n",
      "Batch: 4500,train loss is: 0.0001457184329205343\n",
      "test loss is 0.00032063346126207033\n",
      "Batch: 4600,train loss is: 0.00041465699232242785\n",
      "test loss is 0.000541500135337975\n",
      "Batch: 4700,train loss is: 0.0005113579725625248\n",
      "test loss is 0.00036434291558903707\n",
      "Batch: 4800,train loss is: 0.001212582403978674\n",
      "test loss is 0.0008449159003487023\n",
      "Batch: 4900,train loss is: 0.0014764644655583122\n",
      "test loss is 0.0009818659391306584\n",
      "Batch: 5000,train loss is: 0.0004314386919716608\n",
      "test loss is 0.0004470587566910575\n",
      "Batch: 5100,train loss is: 0.0006708742039741099\n",
      "test loss is 0.0004742504297147295\n",
      "Batch: 5200,train loss is: 0.0003473967947067308\n",
      "test loss is 0.00044081398481263625\n",
      "Batch: 5300,train loss is: 0.0017658270801401315\n",
      "test loss is 0.0018565707799044352\n",
      "Batch: 5400,train loss is: 0.00036781475862471367\n",
      "test loss is 0.0005727899944300407\n",
      "Batch: 5500,train loss is: 0.0005496547113891583\n",
      "test loss is 0.0006762277651539855\n",
      "Batch: 5600,train loss is: 0.0004169630354336871\n",
      "test loss is 0.000306332037344617\n",
      "Batch: 5700,train loss is: 0.00029247464689908164\n",
      "test loss is 0.00028621900588255604\n",
      "Batch: 5800,train loss is: 0.0002972647355037408\n",
      "test loss is 0.00031177803377216917\n",
      "Batch: 5900,train loss is: 0.0007246698280384373\n",
      "test loss is 0.0005112517491804149\n",
      "Batch: 6000,train loss is: 0.0013605438260192095\n",
      "test loss is 0.0003293965617505181\n",
      "Batch: 6100,train loss is: 0.00030331950654375147\n",
      "test loss is 0.00054124352647073\n",
      "Batch: 6200,train loss is: 0.0002272137520181477\n",
      "test loss is 0.00040785593879928857\n",
      "Batch: 6300,train loss is: 0.00045105489853826186\n",
      "test loss is 0.0006019922139682798\n",
      "Batch: 6400,train loss is: 0.0009369489822914713\n",
      "test loss is 0.001303400874349153\n",
      "Batch: 6500,train loss is: 0.0007935373493074129\n",
      "test loss is 0.0003562856421314515\n",
      "Batch: 6600,train loss is: 0.0012296372407190645\n",
      "test loss is 0.00033971319595496716\n",
      "Batch: 6700,train loss is: 0.00011552187593065079\n",
      "test loss is 0.00035440620368340595\n",
      "Batch: 6800,train loss is: 0.0002773068099103645\n",
      "test loss is 0.0003503276737290534\n",
      "Batch: 6900,train loss is: 0.00021309604759783195\n",
      "test loss is 0.0003114915742393339\n",
      "Batch: 7000,train loss is: 0.0004188584742231286\n",
      "test loss is 0.00030138611715829034\n",
      "Batch: 7100,train loss is: 0.0003345416346159273\n",
      "test loss is 0.00027445435044829705\n",
      "Batch: 7200,train loss is: 9.706615231916863e-05\n",
      "test loss is 0.00025202191938199805\n",
      "Batch: 7300,train loss is: 0.0019351176285353471\n",
      "test loss is 0.0004525662048360768\n",
      "Batch: 7400,train loss is: 0.0001509129211688365\n",
      "test loss is 0.00036979372851705976\n",
      "Batch: 7500,train loss is: 0.0002095259624189841\n",
      "test loss is 0.00046157413578077586\n",
      "Batch: 7600,train loss is: 0.0006841995974101059\n",
      "test loss is 0.0005275871011405039\n",
      "Batch: 7700,train loss is: 0.0008380994920576649\n",
      "test loss is 0.0009472841335628426\n",
      "Batch: 7800,train loss is: 0.0003325827761549384\n",
      "test loss is 0.0006052360135146816\n",
      "Batch: 7900,train loss is: 0.0005057969839029605\n",
      "test loss is 0.0003388391441349968\n",
      "Batch: 8000,train loss is: 0.0003606878326844873\n",
      "test loss is 0.000519361498681796\n",
      "Batch: 8100,train loss is: 0.00044210635537159\n",
      "test loss is 0.0003086937112551137\n",
      "Batch: 8200,train loss is: 0.00021933333263145067\n",
      "test loss is 0.0002728184569929707\n",
      "Batch: 8300,train loss is: 0.00023321604986598307\n",
      "test loss is 0.0004931060822253424\n",
      "Batch: 8400,train loss is: 0.00038239918514984506\n",
      "test loss is 0.0005416759859372406\n",
      "Batch: 8500,train loss is: 0.0002932687843251825\n",
      "test loss is 0.00028397089649150986\n",
      "Batch: 8600,train loss is: 0.0002761183366204294\n",
      "test loss is 0.00042361577320951005\n",
      "Batch: 8700,train loss is: 0.0001810811595541652\n",
      "test loss is 0.0005951799612114737\n",
      "Batch: 8800,train loss is: 0.00022556970092789524\n",
      "test loss is 0.0005463812385662151\n",
      "Batch: 8900,train loss is: 0.0008063569989061676\n",
      "test loss is 0.000432733358296331\n",
      "Batch: 9000,train loss is: 0.00011041115457546039\n",
      "test loss is 0.00030503662479664455\n",
      "Batch: 9100,train loss is: 0.0003721667089041636\n",
      "test loss is 0.00035883664305214096\n",
      "Batch: 9200,train loss is: 0.0004797447215613872\n",
      "test loss is 0.00041220509466473364\n",
      "Batch: 9300,train loss is: 0.0003726580213214663\n",
      "test loss is 0.00034256514399653325\n",
      "Batch: 9400,train loss is: 0.00022589709062717102\n",
      "test loss is 0.00033884889588482915\n",
      "Batch: 9500,train loss is: 0.0009995472302021828\n",
      "test loss is 0.0010569133355573693\n",
      "Batch: 9600,train loss is: 0.00023689113425277722\n",
      "test loss is 0.0003604620142370823\n",
      "Batch: 9700,train loss is: 0.00019043184941039423\n",
      "test loss is 0.0004414978966279038\n",
      "Batch: 9800,train loss is: 0.00024908269712447655\n",
      "test loss is 0.0006795998083237371\n",
      "Batch: 9900,train loss is: 0.0003215443944388862\n",
      "test loss is 0.0005102359725817562\n",
      "Batch: 10000,train loss is: 0.0004786408076128599\n",
      "test loss is 0.00031402863855555744\n",
      "Batch: 10100,train loss is: 0.00046404672813607576\n",
      "test loss is 0.00025301862228411034\n",
      "Batch: 10200,train loss is: 0.0004004630461888628\n",
      "test loss is 0.0003450262526225945\n",
      "Batch: 10300,train loss is: 0.0007862514503345412\n",
      "test loss is 0.0006353295447085371\n",
      "Batch: 10400,train loss is: 0.0001379468276454686\n",
      "test loss is 0.0004885798462542936\n",
      "Batch: 10500,train loss is: 0.00021134661525499862\n",
      "test loss is 0.0004716076978925911\n",
      "Batch: 10600,train loss is: 0.0001511786874778021\n",
      "test loss is 0.0009461166909310907\n",
      "Batch: 10700,train loss is: 0.0004061921580112267\n",
      "test loss is 0.0005175737651715587\n",
      "Batch: 10800,train loss is: 0.0006016881872845716\n",
      "test loss is 0.0011503987356621798\n",
      "Batch: 10900,train loss is: 0.0008307359666939507\n",
      "test loss is 0.0004576971222607345\n",
      "Batch: 11000,train loss is: 0.000410425046361549\n",
      "test loss is 0.0002808226315461267\n",
      "Batch: 11100,train loss is: 0.0003248027085031611\n",
      "test loss is 0.00028267774408998294\n",
      "Batch: 11200,train loss is: 0.0005576517385346345\n",
      "test loss is 0.00029803149441039325\n",
      "Batch: 11300,train loss is: 0.00010920256798683187\n",
      "test loss is 0.00024346320233645464\n",
      "Batch: 11400,train loss is: 0.0005106812800194278\n",
      "test loss is 0.0005273911344501104\n",
      "Batch: 11500,train loss is: 0.00032766745413369237\n",
      "test loss is 0.0009082090334928134\n",
      "Batch: 11600,train loss is: 0.0015976710229961284\n",
      "test loss is 0.0009276720527499209\n",
      "Batch: 11700,train loss is: 0.00040519027967150063\n",
      "test loss is 0.00048523851354346695\n",
      "Batch: 11800,train loss is: 0.0004953052814222444\n",
      "test loss is 0.0005169865060631032\n",
      "Batch: 11900,train loss is: 0.0004950473195304371\n",
      "test loss is 0.00044982599395226576\n",
      "Batch: 12000,train loss is: 0.00022786749600171434\n",
      "test loss is 0.00030437936236348516\n",
      "Batch: 12100,train loss is: 0.006227123982990135\n",
      "test loss is 0.0006203507184574459\n",
      "Batch: 12200,train loss is: 0.00033555340571096205\n",
      "test loss is 0.0004236093097614332\n",
      "Batch: 12300,train loss is: 0.0006289192048938672\n",
      "test loss is 0.0003657826778103313\n",
      "Batch: 12400,train loss is: 0.00040823995116929797\n",
      "test loss is 0.00024051681421361092\n",
      "Batch: 12500,train loss is: 0.0007523851401664615\n",
      "test loss is 0.00028646152305467055\n",
      "Batch: 12600,train loss is: 0.00038862784615267815\n",
      "test loss is 0.0003255877698429237\n",
      "Batch: 12700,train loss is: 0.0001715095692482353\n",
      "test loss is 0.00028759650384436596\n",
      "Batch: 12800,train loss is: 0.0002484932140940764\n",
      "test loss is 0.0002588508358010977\n",
      "Batch: 12900,train loss is: 0.0002927697849157334\n",
      "test loss is 0.00029821125533528687\n",
      "Batch: 13000,train loss is: 0.00016702205824132194\n",
      "test loss is 0.0002779841853786524\n",
      "Batch: 13100,train loss is: 0.0005059506177338094\n",
      "test loss is 0.0006100592417472144\n",
      "Batch: 13200,train loss is: 0.00038177986933600016\n",
      "test loss is 0.0002527855432359561\n",
      "Batch: 13300,train loss is: 0.00023005680817963207\n",
      "test loss is 0.0013128481380396395\n",
      "Batch: 13400,train loss is: 0.0006732206204615076\n",
      "test loss is 0.0007280311148156739\n",
      "Batch: 13500,train loss is: 0.0002446792015222769\n",
      "test loss is 0.00047515142940640913\n",
      "Batch: 13600,train loss is: 0.0018733772958375514\n",
      "test loss is 0.0027825782307597\n",
      "Batch: 13700,train loss is: 0.00019749504549528655\n",
      "test loss is 0.0004552045192009617\n",
      "Batch: 13800,train loss is: 0.0002260350130695759\n",
      "test loss is 0.00044557461659423526\n",
      "Batch: 13900,train loss is: 0.0005283043791438991\n",
      "test loss is 0.00043183892802802284\n",
      "Batch: 14000,train loss is: 0.00018561850216001466\n",
      "test loss is 0.00032378893286346436\n",
      "Batch: 14100,train loss is: 0.00017458265619753427\n",
      "test loss is 0.0003849618314230785\n",
      "Batch: 14200,train loss is: 0.00010382893868111135\n",
      "test loss is 0.00025775248264830433\n",
      "Batch: 14300,train loss is: 0.00018678386037592268\n",
      "test loss is 0.0002944186607747212\n",
      "Batch: 14400,train loss is: 0.0004540482317474742\n",
      "test loss is 0.0002626048670722834\n",
      "Batch: 14500,train loss is: 0.00019719324232500858\n",
      "test loss is 0.000277421039024145\n",
      "Batch: 14600,train loss is: 0.000326227035281897\n",
      "test loss is 0.0002736328608479402\n",
      "Batch: 14700,train loss is: 0.00014581221153433575\n",
      "test loss is 0.0006073625478583205\n",
      "Batch: 14800,train loss is: 0.0003911897126718864\n",
      "test loss is 0.0004452772577264743\n",
      "Batch: 14900,train loss is: 0.00019287863632301696\n",
      "test loss is 0.00023384615001188124\n",
      "Batch: 15000,train loss is: 0.0001750424342448947\n",
      "test loss is 0.0004046586579699386\n",
      "Batch: 15100,train loss is: 0.00023453276239898484\n",
      "test loss is 0.00033683600955881644\n",
      "Batch: 15200,train loss is: 0.0005723414127196587\n",
      "test loss is 0.0003112257607155221\n",
      "Batch: 15300,train loss is: 0.000250841885670825\n",
      "test loss is 0.0002296509351561638\n",
      "Batch: 15400,train loss is: 0.0004678951263536766\n",
      "test loss is 0.0007774728022234759\n",
      "Batch: 15500,train loss is: 0.00022502414512759117\n",
      "test loss is 0.0003713823969890587\n",
      "Batch: 15600,train loss is: 0.0010399307116936837\n",
      "test loss is 0.0008775941443908712\n",
      "Batch: 15700,train loss is: 0.0002551306949147966\n",
      "test loss is 0.0008055955981381576\n",
      "Batch: 15800,train loss is: 0.0002384454550666347\n",
      "test loss is 0.0005810886598702484\n",
      "Batch: 15900,train loss is: 0.00043072192549023443\n",
      "test loss is 0.0004944890995922585\n",
      "Batch: 16000,train loss is: 0.0007282919657626576\n",
      "test loss is 0.0005902330821889419\n",
      "Batch: 16100,train loss is: 0.0011450423458257528\n",
      "test loss is 0.0006114612494464512\n",
      "Batch: 16200,train loss is: 0.00015650292822647923\n",
      "test loss is 0.0005748270893994261\n",
      "Batch: 16300,train loss is: 0.00017616768689334512\n",
      "test loss is 0.0002714070139351901\n",
      "Batch: 16400,train loss is: 0.0003569722445125014\n",
      "test loss is 0.00028398713979131397\n",
      "Batch: 16500,train loss is: 0.0005082501378341194\n",
      "test loss is 0.0004303698786185092\n",
      "Batch: 16600,train loss is: 0.00018479635770635966\n",
      "test loss is 0.0005283355762139603\n",
      "Batch: 16700,train loss is: 0.000275358738650977\n",
      "test loss is 0.0004139240723489423\n",
      "Batch: 16800,train loss is: 0.0002759586274024367\n",
      "test loss is 0.0003596763177429787\n",
      "Batch: 16900,train loss is: 0.0010435540094326711\n",
      "test loss is 0.0009289134271787267\n",
      "Batch: 17000,train loss is: 0.0001870768022369055\n",
      "test loss is 0.00034781987157273767\n",
      "Batch: 17100,train loss is: 0.00047875100919560416\n",
      "test loss is 0.0008126461786262541\n",
      "Batch: 17200,train loss is: 9.288129858621646e-05\n",
      "test loss is 0.00034236395680304195\n",
      "Batch: 17300,train loss is: 0.0002871839471059888\n",
      "test loss is 0.00034221517283418364\n",
      "Batch: 17400,train loss is: 0.00028362931465941374\n",
      "test loss is 0.000341922398135856\n",
      "Batch: 17500,train loss is: 0.0005150813670448137\n",
      "test loss is 0.00027674638091020276\n",
      "Batch: 17600,train loss is: 0.00012174836402826139\n",
      "test loss is 0.0003744474223807758\n",
      "Batch: 17700,train loss is: 0.0001900048207765583\n",
      "test loss is 0.0003757938286290841\n",
      "Batch: 17800,train loss is: 0.0003433886430556592\n",
      "test loss is 0.00036621141235100156\n",
      "Batch: 17900,train loss is: 0.0009839782824743201\n",
      "test loss is 0.0006099896322698964\n",
      "Batch: 18000,train loss is: 0.0004735600045862297\n",
      "test loss is 0.00044911336762892763\n",
      "Batch: 18100,train loss is: 0.00017714038470246536\n",
      "test loss is 0.0004207236782620791\n",
      "Batch: 18200,train loss is: 0.0002454104335990503\n",
      "test loss is 0.000745768850108043\n",
      "Batch: 18300,train loss is: 0.001481671276569188\n",
      "test loss is 0.00037866848917556835\n",
      "Batch: 18400,train loss is: 0.00015819656518742934\n",
      "test loss is 0.0005163342878202849\n",
      "Batch: 18500,train loss is: 0.0007694893013789376\n",
      "test loss is 0.00040633882145219133\n",
      "Batch: 18600,train loss is: 0.0006503151664058731\n",
      "test loss is 0.0009361448798244112\n",
      "Batch: 18700,train loss is: 0.0014156857633875978\n",
      "test loss is 0.0007054733849550443\n",
      "Batch: 18800,train loss is: 0.00038727570597171\n",
      "test loss is 0.0004419052880278581\n",
      "Batch: 18900,train loss is: 0.0001677178977780523\n",
      "test loss is 0.00041603618987839794\n",
      "Batch: 19000,train loss is: 0.00018960979175000205\n",
      "test loss is 0.00037813875141360184\n",
      "Batch: 19100,train loss is: 0.0007323793396400677\n",
      "test loss is 0.0004992391875384138\n",
      "Batch: 19200,train loss is: 0.00033613378832755326\n",
      "test loss is 0.00034683113361779394\n",
      "Batch: 19300,train loss is: 0.0005524998092617166\n",
      "test loss is 0.000866060723065228\n",
      "Batch: 19400,train loss is: 0.00021550501525241752\n",
      "test loss is 0.00033445840414648607\n",
      "Batch: 19500,train loss is: 0.00022344321950074775\n",
      "test loss is 0.00033864465315653426\n",
      "Batch: 19600,train loss is: 0.0002567504811047187\n",
      "test loss is 0.00044932161315744987\n",
      "Batch: 19700,train loss is: 0.000648114676002917\n",
      "test loss is 0.00043955116146277283\n",
      "Batch: 19800,train loss is: 0.00022220228067959473\n",
      "test loss is 0.00036101641381873137\n",
      "Batch: 19900,train loss is: 0.0002608782185288316\n",
      "test loss is 0.00038811150056704166\n",
      "Batch: 20000,train loss is: 0.000891037520153022\n",
      "test loss is 0.0008715001354417045\n",
      "Batch: 20100,train loss is: 0.0009197643948669334\n",
      "test loss is 0.0006516390284721598\n",
      "Batch: 20200,train loss is: 0.0001251967816301482\n",
      "test loss is 0.00031846802595676086\n",
      "Batch: 20300,train loss is: 0.00014872198574369377\n",
      "test loss is 0.0003217151313126126\n",
      "Batch: 20400,train loss is: 0.0004532455176058717\n",
      "test loss is 0.000738483454010953\n",
      "Batch: 20500,train loss is: 0.00019411480971547108\n",
      "test loss is 0.00033862315703098194\n",
      "Batch: 20600,train loss is: 0.002717752604075595\n",
      "test loss is 0.000617156824965504\n",
      "Batch: 20700,train loss is: 0.000838534087337615\n",
      "test loss is 0.0005158426730033688\n",
      "Batch: 20800,train loss is: 0.0004522630750756212\n",
      "test loss is 0.0003450035119341496\n",
      "Batch: 20900,train loss is: 0.0003425271168703628\n",
      "test loss is 0.0007768737073333152\n",
      "Batch: 21000,train loss is: 0.0005331596137054019\n",
      "test loss is 0.0005587869881000924\n",
      "Batch: 21100,train loss is: 0.00017353891682632595\n",
      "test loss is 0.00039179445596013076\n",
      "Batch: 21200,train loss is: 0.001171087221854408\n",
      "test loss is 0.0009852796378559238\n",
      "Batch: 21300,train loss is: 0.00012997936120921562\n",
      "test loss is 0.0007896674302858179\n",
      "Batch: 21400,train loss is: 0.0003966623483158734\n",
      "test loss is 0.0004723976827077604\n",
      "Batch: 21500,train loss is: 0.00015914095855257827\n",
      "test loss is 0.00041427354678173225\n",
      "Batch: 21600,train loss is: 0.00022159878354474153\n",
      "test loss is 0.00027321624474017063\n",
      "Batch: 21700,train loss is: 0.00033109676704406343\n",
      "test loss is 0.0004973406246435017\n",
      "Batch: 21800,train loss is: 0.00030672178198457137\n",
      "test loss is 0.0003396537915512472\n",
      "Batch: 21900,train loss is: 0.00026211309632437243\n",
      "test loss is 0.0005049029368830557\n",
      "Batch: 22000,train loss is: 0.0003031300214307498\n",
      "test loss is 0.0003249444707076371\n",
      "Batch: 22100,train loss is: 8.316392145427333e-05\n",
      "test loss is 0.00040229149185199857\n",
      "Batch: 22200,train loss is: 0.0011240656200357723\n",
      "test loss is 0.00047226447492087055\n",
      "Batch: 22300,train loss is: 0.0014927937633943094\n",
      "test loss is 0.0017593417617847997\n",
      "Batch: 22400,train loss is: 0.00042938128837687793\n",
      "test loss is 0.0005594783468493142\n",
      "Batch: 22500,train loss is: 0.00016893285279994663\n",
      "test loss is 0.00038448764197012423\n",
      "Batch: 22600,train loss is: 0.0003385176799692805\n",
      "test loss is 0.00039481176255080187\n",
      "Batch: 22700,train loss is: 0.0002501132803255725\n",
      "test loss is 0.0004512023859348216\n",
      "Batch: 22800,train loss is: 0.0002797561210601366\n",
      "test loss is 0.0003691860967434717\n",
      "Batch: 22900,train loss is: 0.00015883830560543306\n",
      "test loss is 0.0004744696919767847\n",
      "Batch: 23000,train loss is: 0.000297533830356114\n",
      "test loss is 0.000370426941195054\n",
      "Batch: 23100,train loss is: 0.00012213144735280995\n",
      "test loss is 0.0003676979912242071\n",
      "Batch: 23200,train loss is: 0.00013219963299566703\n",
      "test loss is 0.000252827460624762\n",
      "Batch: 23300,train loss is: 0.00021856761305742763\n",
      "test loss is 0.00027349724348019\n",
      "Batch: 23400,train loss is: 0.0002635107657488888\n",
      "test loss is 0.0005420207462939277\n",
      "Batch: 23500,train loss is: 0.00013117045505189125\n",
      "test loss is 0.0002419406041371178\n",
      "Batch: 23600,train loss is: 0.00016720368030466863\n",
      "test loss is 0.00023352176301648432\n",
      "Batch: 23700,train loss is: 0.00024123192047440398\n",
      "test loss is 0.000304071763540104\n",
      "Batch: 23800,train loss is: 0.00046265958375073317\n",
      "test loss is 0.0005327719902787551\n",
      "Batch: 23900,train loss is: 0.0017453523622860978\n",
      "test loss is 0.0005975278916518232\n",
      "Batch: 24000,train loss is: 0.00016217588135864947\n",
      "test loss is 0.0005068774534993051\n",
      "Batch: 24100,train loss is: 0.00014661824530754546\n",
      "test loss is 0.0002881534332519265\n",
      "Batch: 24200,train loss is: 0.0009425272397758556\n",
      "test loss is 0.0003060682973035197\n",
      "Batch: 24300,train loss is: 0.00045305699042987035\n",
      "test loss is 0.0003784755842502358\n",
      "Batch: 24400,train loss is: 0.0005302633722347196\n",
      "test loss is 0.00024496136337686205\n",
      "Batch: 24500,train loss is: 0.00017588565907016056\n",
      "test loss is 0.0002390373319461482\n",
      "Batch: 24600,train loss is: 0.00035954248569471044\n",
      "test loss is 0.000312465276390333\n",
      "Batch: 24700,train loss is: 0.00013788775353625193\n",
      "test loss is 0.00026507291550746876\n",
      "Batch: 24800,train loss is: 0.0003564432950053471\n",
      "test loss is 0.00026684124444813516\n",
      "Batch: 24900,train loss is: 0.00012878864187631465\n",
      "test loss is 0.0002940969557739464\n",
      "Batch: 25000,train loss is: 0.0001328535563952226\n",
      "test loss is 0.0003645951337339992\n",
      "Batch: 25100,train loss is: 0.00014114419756753357\n",
      "test loss is 0.0003229895785900128\n",
      "Batch: 25200,train loss is: 0.00031729991665042765\n",
      "test loss is 0.0003120848229148387\n",
      "Batch: 25300,train loss is: 0.00032413917296481584\n",
      "test loss is 0.00033574604683713887\n",
      "Batch: 25400,train loss is: 0.0002690172271308649\n",
      "test loss is 0.00048398283041854034\n",
      "Batch: 25500,train loss is: 0.00013510548750738822\n",
      "test loss is 0.0003250154881043051\n",
      "Batch: 25600,train loss is: 0.00045838828426913373\n",
      "test loss is 0.0003216612055108801\n",
      "Batch: 25700,train loss is: 0.0002590307493907124\n",
      "test loss is 0.00023934928678406853\n",
      "Batch: 25800,train loss is: 0.00017829756166146924\n",
      "test loss is 0.0003656366136149084\n",
      "Batch: 25900,train loss is: 0.000501058135684708\n",
      "test loss is 0.0007613804946082278\n",
      "Batch: 26000,train loss is: 0.0006833929722418109\n",
      "test loss is 0.0009205449885471157\n",
      "Batch: 26100,train loss is: 0.0012383502987084296\n",
      "test loss is 0.000982258408926632\n",
      "Batch: 26200,train loss is: 0.0010113805487964027\n",
      "test loss is 0.0004720446436274993\n",
      "Batch: 26300,train loss is: 0.00032019217677544363\n",
      "test loss is 0.0004407810765915623\n",
      "Batch: 26400,train loss is: 0.00019417389838165576\n",
      "test loss is 0.0003724348451889452\n",
      "Batch: 26500,train loss is: 0.00021472046929510641\n",
      "test loss is 0.00030055498507263655\n",
      "Batch: 26600,train loss is: 0.001428655856644203\n",
      "test loss is 0.000845787073382682\n",
      "Batch: 26700,train loss is: 0.00016279577189834733\n",
      "test loss is 0.0004361252677706762\n",
      "Batch: 26800,train loss is: 0.00040312350437062596\n",
      "test loss is 0.00026572660233346624\n",
      "Batch: 26900,train loss is: 0.00025143960449686457\n",
      "test loss is 0.0003030977747531099\n",
      "Batch: 27000,train loss is: 0.0004161440017500831\n",
      "test loss is 0.00031891952149285994\n",
      "Batch: 27100,train loss is: 0.0001655212090661354\n",
      "test loss is 0.0002756600247847586\n",
      "Batch: 27200,train loss is: 0.0008644402858020428\n",
      "test loss is 0.0010128724344633658\n",
      "Batch: 27300,train loss is: 0.0008218046403969916\n",
      "test loss is 0.0016124596035068492\n",
      "Batch: 27400,train loss is: 0.0003947590723040919\n",
      "test loss is 0.00044976069725026675\n",
      "Batch: 27500,train loss is: 0.0016464355149860797\n",
      "test loss is 0.0006368141858891945\n",
      "Batch: 27600,train loss is: 0.00039438002420542873\n",
      "test loss is 0.0006137102743076501\n",
      "Batch: 27700,train loss is: 0.00025697564487567207\n",
      "test loss is 0.00040933674758841446\n",
      "Batch: 27800,train loss is: 0.00024404420530304028\n",
      "test loss is 0.0004205294104411185\n",
      "Batch: 27900,train loss is: 0.0005685746740334774\n",
      "test loss is 0.000333790777385925\n",
      "Batch: 28000,train loss is: 0.00010710333180458017\n",
      "test loss is 0.0003481187600817865\n",
      "Batch: 28100,train loss is: 0.00025286873489587886\n",
      "test loss is 0.0004935675236706579\n",
      "Batch: 28200,train loss is: 0.00033663435584492453\n",
      "test loss is 0.0002645864228563563\n",
      "Batch: 28300,train loss is: 0.0007609962034962864\n",
      "test loss is 0.00043058145449486815\n",
      "Batch: 28400,train loss is: 0.00042054387557507577\n",
      "test loss is 0.0005320275216276476\n",
      "Batch: 28500,train loss is: 0.0005621447758135149\n",
      "test loss is 0.00030949542315457406\n",
      "Batch: 28600,train loss is: 0.00026687360495056945\n",
      "test loss is 0.00039661629370367496\n",
      "Batch: 28700,train loss is: 0.0001995671420128804\n",
      "test loss is 0.0002625778598535841\n",
      "Batch: 28800,train loss is: 0.0004844557062243791\n",
      "test loss is 0.0002327349960491279\n",
      "Batch: 28900,train loss is: 0.00022802517684722144\n",
      "test loss is 0.0002751596158722653\n",
      "Batch: 29000,train loss is: 0.000258764797993478\n",
      "test loss is 0.0003246669985521798\n",
      "Batch: 29100,train loss is: 0.0005411209339183698\n",
      "test loss is 0.001189741260826765\n",
      "Batch: 29200,train loss is: 0.00022506938561026895\n",
      "test loss is 0.0005709709817916498\n",
      "Batch: 29300,train loss is: 0.00026893272273897684\n",
      "test loss is 0.0006698649653275576\n",
      "Batch: 29400,train loss is: 0.0001918584227654602\n",
      "test loss is 0.00048179892092371394\n",
      "Batch: 29500,train loss is: 0.00015049096236308644\n",
      "test loss is 0.0002729929580825024\n",
      "Batch: 29600,train loss is: 0.0005095625489315987\n",
      "test loss is 0.00031493103502783004\n",
      "Batch: 29700,train loss is: 0.00012399819187100194\n",
      "test loss is 0.0004358082035672779\n",
      "Batch: 29800,train loss is: 0.0003752294342622484\n",
      "test loss is 0.0003514455636662122\n",
      "Batch: 29900,train loss is: 0.0005367067342138505\n",
      "test loss is 0.00027920996091368016\n",
      "Batch: 30000,train loss is: 0.0001434134275497135\n",
      "test loss is 0.00023905156754268578\n",
      "Batch: 30100,train loss is: 0.0003613123720417907\n",
      "test loss is 0.0005128051700489673\n",
      "Batch: 30200,train loss is: 0.00044753697517085304\n",
      "test loss is 0.0004891872692977046\n",
      "Batch: 30300,train loss is: 0.0005593016584359076\n",
      "test loss is 0.0008213528949234775\n",
      "Batch: 30400,train loss is: 0.0004141868166898422\n",
      "test loss is 0.0022449089880202566\n",
      "Batch: 30500,train loss is: 0.00025118721996893515\n",
      "test loss is 0.0004387389789648163\n",
      "Batch: 30600,train loss is: 0.00018272450189410112\n",
      "test loss is 0.00033113889734838204\n",
      "Batch: 30700,train loss is: 0.0006612496453803819\n",
      "test loss is 0.0004914700678522548\n",
      "Batch: 30800,train loss is: 0.0001109347502759575\n",
      "test loss is 0.00032577026996680696\n",
      "Batch: 30900,train loss is: 0.0002264907089252797\n",
      "test loss is 0.0002447649803228505\n",
      "Batch: 31000,train loss is: 0.00015865770411416382\n",
      "test loss is 0.000250924520464402\n",
      "Batch: 31100,train loss is: 0.00021348326922151096\n",
      "test loss is 0.0003645104677532013\n",
      "Batch: 31200,train loss is: 0.0002544860246159207\n",
      "test loss is 0.0002439863902927436\n",
      "Batch: 31300,train loss is: 0.00017704644737649217\n",
      "test loss is 0.0005110935455323718\n",
      "Batch: 31400,train loss is: 0.00018043284640169709\n",
      "test loss is 0.00030546689999958967\n",
      "Batch: 31500,train loss is: 0.0003320954277105721\n",
      "test loss is 0.0004187939152845148\n",
      "Batch: 31600,train loss is: 0.0006992946426926331\n",
      "test loss is 0.00039165873750118157\n",
      "Batch: 31700,train loss is: 0.0001576056109113778\n",
      "test loss is 0.0002446713320859977\n",
      "Batch: 31800,train loss is: 0.0006162970768963755\n",
      "test loss is 0.0005240149007401762\n",
      "Batch: 31900,train loss is: 0.00023847836248815264\n",
      "test loss is 0.0003735857224773962\n",
      "Batch: 32000,train loss is: 0.0024983632607594813\n",
      "test loss is 0.0003196039274922772\n",
      "Batch: 32100,train loss is: 0.0003034365231450153\n",
      "test loss is 0.0008159920844619602\n",
      "Batch: 32200,train loss is: 0.00011255081075051381\n",
      "test loss is 0.0003901867875238447\n",
      "Batch: 32300,train loss is: 0.0007828375297845987\n",
      "test loss is 0.0007734420338176666\n",
      "Batch: 32400,train loss is: 0.00019346233980685942\n",
      "test loss is 0.00032678019448616235\n",
      "Batch: 32500,train loss is: 0.00021969062743360364\n",
      "test loss is 0.000312707039207294\n",
      "Batch: 32600,train loss is: 0.0002532617251859323\n",
      "test loss is 0.00033036216227010155\n",
      "Batch: 32700,train loss is: 0.00022061867054353883\n",
      "test loss is 0.0007590180147510118\n",
      "Batch: 32800,train loss is: 0.0007149890777960324\n",
      "test loss is 0.0004143924540532787\n",
      "Batch: 32900,train loss is: 0.00032800135468599473\n",
      "test loss is 0.0002668230422060785\n",
      "Batch: 33000,train loss is: 7.798970425346487e-05\n",
      "test loss is 0.00025982805145808663\n",
      "Batch: 33100,train loss is: 0.0005966448992167524\n",
      "test loss is 0.0011047412694827918\n",
      "Batch: 33200,train loss is: 0.00014018895715003338\n",
      "test loss is 0.0002787228754946758\n",
      "Batch: 33300,train loss is: 0.000449321003925052\n",
      "test loss is 0.00028057937843583577\n",
      "Batch: 33400,train loss is: 0.0006128671103230495\n",
      "test loss is 0.0010295030946937398\n",
      "Batch: 33500,train loss is: 0.00024352982370636834\n",
      "test loss is 0.001640152829599033\n",
      "Batch: 33600,train loss is: 7.177516871956586e-05\n",
      "test loss is 0.0005078207510854313\n",
      "Batch: 33700,train loss is: 0.0002064972437605969\n",
      "test loss is 0.00025370485718290213\n",
      "Batch: 33800,train loss is: 0.0004189914428997278\n",
      "test loss is 0.00029779376258752907\n",
      "Batch: 33900,train loss is: 0.00017706202254133785\n",
      "test loss is 0.0002046975696010222\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch_NN.train import train_model\n",
    "\n",
    "train_model(loss_MSE,optim_Adam,model,data_loader,train_data,test_data,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Epoch: 0----------------------------------\n",
      "Batch: 0,train loss is: 0.00015938306118040346\n",
      "test loss is 0.00026589435373596394\n",
      "Batch: 100,train loss is: 0.00018291812182440233\n",
      "test loss is 0.00019714359289503279\n",
      "Batch: 200,train loss is: 0.00014783317508543868\n",
      "test loss is 0.00017317530026282716\n",
      "Batch: 300,train loss is: 8.729125595704304e-05\n",
      "test loss is 0.00017470230136676086\n",
      "Batch: 400,train loss is: 0.00012360176831352634\n",
      "test loss is 0.00016690938990650823\n",
      "Batch: 500,train loss is: 0.00013904636590840488\n",
      "test loss is 0.00017917966054503942\n",
      "Batch: 600,train loss is: 0.00013363104417950478\n",
      "test loss is 0.00018607758511154942\n",
      "Batch: 700,train loss is: 8.954739138974364e-05\n",
      "test loss is 0.0001707791805521262\n",
      "Batch: 800,train loss is: 0.00020195557220911424\n",
      "test loss is 0.00016495444312846158\n",
      "Batch: 900,train loss is: 7.113716709802596e-05\n",
      "test loss is 0.000163829578199741\n",
      "Batch: 1000,train loss is: 0.00010137130871827813\n",
      "test loss is 0.0001632520594489481\n",
      "Batch: 1100,train loss is: 0.0003094155238327839\n",
      "test loss is 0.0001659414412193334\n",
      "Batch: 1200,train loss is: 5.830425397588206e-05\n",
      "test loss is 0.00016499015394285968\n",
      "Batch: 1300,train loss is: 0.0001664180876353879\n",
      "test loss is 0.0001596384533700754\n",
      "Batch: 1400,train loss is: 0.00014669665159651403\n",
      "test loss is 0.00016241941218318707\n",
      "Batch: 1500,train loss is: 0.00011815099603364873\n",
      "test loss is 0.00016626666487926014\n",
      "Batch: 1600,train loss is: 8.08295696173718e-05\n",
      "test loss is 0.000159607179844427\n",
      "Batch: 1700,train loss is: 0.0002272398494041537\n",
      "test loss is 0.0001630717560353694\n",
      "Batch: 1800,train loss is: 0.00010480687795745581\n",
      "test loss is 0.0001582198701504645\n",
      "Batch: 1900,train loss is: 7.550530534471263e-05\n",
      "test loss is 0.00017134889490631375\n",
      "Batch: 2000,train loss is: 0.00014995474635127483\n",
      "test loss is 0.00016321143085684794\n",
      "Batch: 2100,train loss is: 9.994220237639862e-05\n",
      "test loss is 0.00016308985470795504\n",
      "Batch: 2200,train loss is: 0.000158780381523188\n",
      "test loss is 0.00016220739851897196\n",
      "Batch: 2300,train loss is: 0.00011225167059357646\n",
      "test loss is 0.0001566394688481729\n",
      "Batch: 2400,train loss is: 0.0001279671129790773\n",
      "test loss is 0.00016268714404665238\n",
      "Batch: 2500,train loss is: 0.00017559565232841122\n",
      "test loss is 0.00015984485046876702\n",
      "Batch: 2600,train loss is: 8.822860791612256e-05\n",
      "test loss is 0.00016017773622188902\n",
      "Batch: 2700,train loss is: 0.00018488771140088946\n",
      "test loss is 0.00016209038497009656\n",
      "Batch: 2800,train loss is: 0.00011734813534440937\n",
      "test loss is 0.00016104639584417854\n",
      "Batch: 2900,train loss is: 0.00013221355928657018\n",
      "test loss is 0.00015447905356776758\n",
      "Batch: 3000,train loss is: 0.00012140756757432654\n",
      "test loss is 0.00016171509097854625\n",
      "Batch: 3100,train loss is: 0.00010743457748565238\n",
      "test loss is 0.0001559838013037753\n",
      "Batch: 3200,train loss is: 0.0002685234914434349\n",
      "test loss is 0.00015619180387169872\n",
      "Batch: 3300,train loss is: 0.0001321719940977487\n",
      "test loss is 0.0001559331956374057\n",
      "Batch: 3400,train loss is: 0.0001545482747239094\n",
      "test loss is 0.00015894114973656175\n",
      "Batch: 3500,train loss is: 0.00011947033731429678\n",
      "test loss is 0.00016346502340619995\n",
      "Batch: 3600,train loss is: 9.199311777944169e-05\n",
      "test loss is 0.00016051506589647116\n",
      "Batch: 3700,train loss is: 6.512777942391754e-05\n",
      "test loss is 0.00015411302187131405\n",
      "Batch: 3800,train loss is: 9.440782004349796e-05\n",
      "test loss is 0.0001599556308977617\n",
      "Batch: 3900,train loss is: 0.00014571089005768322\n",
      "test loss is 0.00016355332199874224\n",
      "Batch: 4000,train loss is: 6.99067421502711e-05\n",
      "test loss is 0.0001552368061978586\n",
      "Batch: 4100,train loss is: 0.00014156241660309875\n",
      "test loss is 0.0001654209017686669\n",
      "Batch: 4200,train loss is: 0.0001591591978368205\n",
      "test loss is 0.00015924075459584214\n",
      "Batch: 4300,train loss is: 0.00013306691114975158\n",
      "test loss is 0.00015389811402406005\n",
      "Batch: 4400,train loss is: 9.364273421678792e-05\n",
      "test loss is 0.00015489719695422398\n",
      "Batch: 4500,train loss is: 0.00014314085808903794\n",
      "test loss is 0.0001575946034507171\n",
      "Batch: 4600,train loss is: 0.00010609803421338371\n",
      "test loss is 0.00015404281081394998\n",
      "Batch: 4700,train loss is: 0.00016530580102993875\n",
      "test loss is 0.00016166294243018684\n",
      "Batch: 4800,train loss is: 0.00036811656627628165\n",
      "test loss is 0.00018194436554590183\n",
      "Batch: 4900,train loss is: 0.00012872473018559207\n",
      "test loss is 0.00015602225786188128\n",
      "Batch: 5000,train loss is: 7.384702428790751e-05\n",
      "test loss is 0.00015359822344299896\n",
      "Batch: 5100,train loss is: 0.00011041767064770014\n",
      "test loss is 0.00015893994890074778\n",
      "Batch: 5200,train loss is: 0.0001020887396780141\n",
      "test loss is 0.00015540095891613336\n",
      "Batch: 5300,train loss is: 0.00013585233314429703\n",
      "test loss is 0.0001582634125786895\n",
      "Batch: 5400,train loss is: 0.00023969074573852446\n",
      "test loss is 0.00015990037777715506\n",
      "Batch: 5500,train loss is: 0.00010920790386373403\n",
      "test loss is 0.00016807718731725387\n",
      "Batch: 5600,train loss is: 0.00018086210806031222\n",
      "test loss is 0.00015402574182135016\n",
      "Batch: 5700,train loss is: 0.00014097973973952051\n",
      "test loss is 0.00016699841536298686\n",
      "Batch: 5800,train loss is: 0.000215787658170378\n",
      "test loss is 0.00015996992751997764\n",
      "Batch: 5900,train loss is: 0.0005855409930027411\n",
      "test loss is 0.00015682525205730597\n",
      "Batch: 6000,train loss is: 0.0001863974439629744\n",
      "test loss is 0.00015717201020204615\n",
      "Batch: 6100,train loss is: 8.911361051016618e-05\n",
      "test loss is 0.0001554130528070078\n",
      "Batch: 6200,train loss is: 0.00010265495283454309\n",
      "test loss is 0.00015373469312257527\n",
      "Batch: 6300,train loss is: 0.00018901165999101109\n",
      "test loss is 0.00018346627669715997\n",
      "Batch: 6400,train loss is: 0.00011650884528808862\n",
      "test loss is 0.00015388697937209912\n",
      "Batch: 6500,train loss is: 7.00238511478483e-05\n",
      "test loss is 0.0001573283904988538\n",
      "Batch: 6600,train loss is: 0.0005230622563131073\n",
      "test loss is 0.00015042366394805464\n",
      "Batch: 6700,train loss is: 0.00011175079317676897\n",
      "test loss is 0.00015116690186605687\n",
      "Batch: 6800,train loss is: 0.00018131242172265903\n",
      "test loss is 0.0001612431010494848\n",
      "Batch: 6900,train loss is: 0.00012336050778825305\n",
      "test loss is 0.00015408380517625895\n",
      "Batch: 7000,train loss is: 0.0002063056743683152\n",
      "test loss is 0.00016643536651270882\n",
      "Batch: 7100,train loss is: 0.000165558785071545\n",
      "test loss is 0.00018360103983148008\n",
      "Batch: 7200,train loss is: 9.196073944498752e-05\n",
      "test loss is 0.0001512751388555238\n",
      "Batch: 7300,train loss is: 0.0002519222238279346\n",
      "test loss is 0.00015479424904526286\n",
      "Batch: 7400,train loss is: 0.00010992008456572572\n",
      "test loss is 0.0001962078846999718\n",
      "Batch: 7500,train loss is: 0.00016966954738841113\n",
      "test loss is 0.00016422737620834505\n",
      "Batch: 7600,train loss is: 0.00019913958766650154\n",
      "test loss is 0.0001696162869107761\n",
      "Batch: 7700,train loss is: 0.0001797282104286351\n",
      "test loss is 0.0001514739507809223\n",
      "Batch: 7800,train loss is: 7.878786888261351e-05\n",
      "test loss is 0.00014657326109347293\n",
      "Batch: 7900,train loss is: 0.00010849117230964426\n",
      "test loss is 0.00015000534129301675\n",
      "Batch: 8000,train loss is: 6.442253458399242e-05\n",
      "test loss is 0.00015507027881627548\n",
      "Batch: 8100,train loss is: 0.00028849654835928513\n",
      "test loss is 0.00016417048354070183\n",
      "Batch: 8200,train loss is: 0.00012750396474707447\n",
      "test loss is 0.0001529370616407384\n",
      "Batch: 8300,train loss is: 8.12186325956412e-05\n",
      "test loss is 0.00015518223699165492\n",
      "Batch: 8400,train loss is: 0.00013606303795127262\n",
      "test loss is 0.00015414916579210538\n",
      "Batch: 8500,train loss is: 0.0002671217680652759\n",
      "test loss is 0.00015732945363461994\n",
      "Batch: 8600,train loss is: 9.64727869653327e-05\n",
      "test loss is 0.00016233409191603677\n",
      "Batch: 8700,train loss is: 0.00012945943972254267\n",
      "test loss is 0.0001582900606962259\n",
      "Batch: 8800,train loss is: 9.984974033984818e-05\n",
      "test loss is 0.00015028368034596045\n",
      "Batch: 8900,train loss is: 0.00015196335498956302\n",
      "test loss is 0.00014776592619124792\n",
      "Batch: 9000,train loss is: 0.00010126348217459357\n",
      "test loss is 0.0001527273035238903\n",
      "Batch: 9100,train loss is: 0.00011595125137768866\n",
      "test loss is 0.00015486733792982002\n",
      "Batch: 9200,train loss is: 0.00015892872392841295\n",
      "test loss is 0.0001539336980636653\n",
      "Batch: 9300,train loss is: 0.0001821302805373198\n",
      "test loss is 0.00014933208518351024\n",
      "Batch: 9400,train loss is: 0.00016654168704725205\n",
      "test loss is 0.0001809986362785408\n",
      "Batch: 9500,train loss is: 0.00010122751791991224\n",
      "test loss is 0.0001605640992955375\n",
      "Batch: 9600,train loss is: 0.000100727750473566\n",
      "test loss is 0.00015027932432763264\n",
      "Batch: 9700,train loss is: 7.844397490212029e-05\n",
      "test loss is 0.00015227476935836232\n",
      "Batch: 9800,train loss is: 8.103409048259776e-05\n",
      "test loss is 0.00015002795375264796\n",
      "Batch: 9900,train loss is: 9.635211320378746e-05\n",
      "test loss is 0.0001536731180565514\n",
      "Batch: 10000,train loss is: 7.062643048346527e-05\n",
      "test loss is 0.00015866903762702818\n",
      "Batch: 10100,train loss is: 0.00033437056665602707\n",
      "test loss is 0.0001512839249604322\n",
      "Batch: 10200,train loss is: 0.00037929904644030867\n",
      "test loss is 0.00014955608310237377\n",
      "Batch: 10300,train loss is: 0.0001640375125740127\n",
      "test loss is 0.00015817891934228264\n",
      "Batch: 10400,train loss is: 6.343289968562981e-05\n",
      "test loss is 0.00015373480026386512\n",
      "Batch: 10500,train loss is: 0.00014431355991971807\n",
      "test loss is 0.00015194631692960737\n",
      "Batch: 10600,train loss is: 9.174986400239284e-05\n",
      "test loss is 0.00015175743165048416\n",
      "Batch: 10700,train loss is: 0.00011025957779305207\n",
      "test loss is 0.00014692421218912035\n",
      "Batch: 10800,train loss is: 0.00012893949172070038\n",
      "test loss is 0.00015645121218461238\n",
      "Batch: 10900,train loss is: 0.00013674468343035485\n",
      "test loss is 0.0001486526093226354\n",
      "Batch: 11000,train loss is: 0.00018342476400903996\n",
      "test loss is 0.00014942185960228595\n",
      "Batch: 11100,train loss is: 0.00018108467680831998\n",
      "test loss is 0.0001515612355793309\n",
      "Batch: 11200,train loss is: 0.00020560489148816605\n",
      "test loss is 0.00014993491809033583\n",
      "Batch: 11300,train loss is: 9.798442432075474e-05\n",
      "test loss is 0.00016080714420995865\n",
      "Batch: 11400,train loss is: 0.00030620080341787906\n",
      "test loss is 0.00015013392127538727\n",
      "Batch: 11500,train loss is: 7.686337464891146e-05\n",
      "test loss is 0.00014983603249424195\n",
      "Batch: 11600,train loss is: 0.00020898507836673624\n",
      "test loss is 0.00016404720426209791\n",
      "Batch: 11700,train loss is: 0.00010940945915492217\n",
      "test loss is 0.00015004686499902754\n",
      "Batch: 11800,train loss is: 0.00012975828031276202\n",
      "test loss is 0.00014903125630129314\n",
      "Batch: 11900,train loss is: 8.837665388499289e-05\n",
      "test loss is 0.00016107766698009776\n",
      "Batch: 12000,train loss is: 0.00013617373205774752\n",
      "test loss is 0.0001569489537076943\n",
      "Batch: 12100,train loss is: 0.00021226205255638815\n",
      "test loss is 0.0001485230905942615\n",
      "Batch: 12200,train loss is: 0.0001067333902343179\n",
      "test loss is 0.00015420207391331843\n",
      "Batch: 12300,train loss is: 0.00011058417129420805\n",
      "test loss is 0.0001534815029276167\n",
      "Batch: 12400,train loss is: 0.0001276083034531296\n",
      "test loss is 0.00014694983855204102\n",
      "Batch: 12500,train loss is: 0.0003393010997152648\n",
      "test loss is 0.00015259883384176697\n",
      "Batch: 12600,train loss is: 9.330590723917875e-05\n",
      "test loss is 0.00014782797031004448\n",
      "Batch: 12700,train loss is: 0.0001110182933969501\n",
      "test loss is 0.00014735525429125838\n",
      "Batch: 12800,train loss is: 6.507705542712819e-05\n",
      "test loss is 0.0001498362922894634\n",
      "Batch: 12900,train loss is: 0.00020118238799734963\n",
      "test loss is 0.0001575750914777039\n",
      "Batch: 13000,train loss is: 0.00010276457302002625\n",
      "test loss is 0.0001468568977039255\n",
      "Batch: 13100,train loss is: 0.00021697340019070444\n",
      "test loss is 0.0001469916816361147\n",
      "Batch: 13200,train loss is: 0.00010459474034014665\n",
      "test loss is 0.00016394159619587736\n",
      "Batch: 13300,train loss is: 0.00014648900466001117\n",
      "test loss is 0.0001840920897681509\n",
      "Batch: 13400,train loss is: 0.00010424639395618991\n",
      "test loss is 0.0001498150640471955\n",
      "Batch: 13500,train loss is: 8.484982085504081e-05\n",
      "test loss is 0.00015448798637151474\n",
      "Batch: 13600,train loss is: 8.768150442331998e-05\n",
      "test loss is 0.00016052833114156955\n",
      "Batch: 13700,train loss is: 0.00013171635656610955\n",
      "test loss is 0.00014679552009803085\n",
      "Batch: 13800,train loss is: 8.433425282736605e-05\n",
      "test loss is 0.00015797791557722697\n",
      "Batch: 13900,train loss is: 7.19871047380836e-05\n",
      "test loss is 0.00014905451017942285\n",
      "Batch: 14000,train loss is: 0.00015537930667480667\n",
      "test loss is 0.00016103491216134126\n",
      "Batch: 14100,train loss is: 9.371454742660238e-05\n",
      "test loss is 0.00014644816558398498\n",
      "Batch: 14200,train loss is: 8.341289018881887e-05\n",
      "test loss is 0.00014833921503714875\n",
      "Batch: 14300,train loss is: 9.840997198476755e-05\n",
      "test loss is 0.00014788168634118383\n",
      "Batch: 14400,train loss is: 0.00019304832760935831\n",
      "test loss is 0.00014556051690148167\n",
      "Batch: 14500,train loss is: 0.00010456724652445178\n",
      "test loss is 0.00014604987979549715\n",
      "Batch: 14600,train loss is: 0.0002004861615301498\n",
      "test loss is 0.00015715443426496007\n",
      "Batch: 14700,train loss is: 0.00010386188704009612\n",
      "test loss is 0.00014538231694099428\n",
      "Batch: 14800,train loss is: 0.0001976485775434266\n",
      "test loss is 0.0001492745615520647\n",
      "Batch: 14900,train loss is: 0.00010371832265264309\n",
      "test loss is 0.0001436877548020783\n",
      "Batch: 15000,train loss is: 0.00010220756097319565\n",
      "test loss is 0.0001521992549240241\n",
      "Batch: 15100,train loss is: 0.00017300022079515527\n",
      "test loss is 0.00014751326851270875\n",
      "Batch: 15200,train loss is: 0.0001833797013218133\n",
      "test loss is 0.00014654650386219402\n",
      "Batch: 15300,train loss is: 0.00018628183610340496\n",
      "test loss is 0.00014910798899532297\n",
      "Batch: 15400,train loss is: 8.672774060927086e-05\n",
      "test loss is 0.00014624177092397044\n",
      "Batch: 15500,train loss is: 0.0001320012468878855\n",
      "test loss is 0.0001500003454041999\n",
      "Batch: 15600,train loss is: 0.00025366877675347504\n",
      "test loss is 0.0001465598974811255\n",
      "Batch: 15700,train loss is: 9.192213126697481e-05\n",
      "test loss is 0.00014971472929477297\n",
      "Batch: 15800,train loss is: 8.249849702495481e-05\n",
      "test loss is 0.00014866463916215334\n",
      "Batch: 15900,train loss is: 0.0001838444866728448\n",
      "test loss is 0.00015124131820517414\n",
      "Batch: 16000,train loss is: 0.0001604861160630986\n",
      "test loss is 0.00014654283424174507\n",
      "Batch: 16100,train loss is: 0.0002479370221677333\n",
      "test loss is 0.00014488367458334584\n",
      "Batch: 16200,train loss is: 0.00012732902806899074\n",
      "test loss is 0.00014830312705740132\n",
      "Batch: 16300,train loss is: 0.00011337201423143564\n",
      "test loss is 0.00015222681753239716\n",
      "Batch: 16400,train loss is: 9.232872847737215e-05\n",
      "test loss is 0.00015155409641487858\n",
      "Batch: 16500,train loss is: 0.00010524959409051728\n",
      "test loss is 0.00014730548485470706\n",
      "Batch: 16600,train loss is: 0.00011401910492626099\n",
      "test loss is 0.0001471106509990211\n",
      "Batch: 16700,train loss is: 0.00023060798525335454\n",
      "test loss is 0.00015475779269414158\n",
      "Batch: 16800,train loss is: 0.00014916707146689053\n",
      "test loss is 0.0001468582735907223\n",
      "Batch: 16900,train loss is: 0.0001227438085731349\n",
      "test loss is 0.00015945957763397655\n",
      "Batch: 17000,train loss is: 0.00012570331324814366\n",
      "test loss is 0.00015180310708535225\n",
      "Batch: 17100,train loss is: 0.00011524835874293644\n",
      "test loss is 0.00014605548448797846\n",
      "Batch: 17200,train loss is: 8.033233065550575e-05\n",
      "test loss is 0.0001463096503298809\n",
      "Batch: 17300,train loss is: 8.067519933234698e-05\n",
      "test loss is 0.000158219663466846\n",
      "Batch: 17400,train loss is: 6.669229340659282e-05\n",
      "test loss is 0.00015464691034254298\n",
      "Batch: 17500,train loss is: 0.0001385223818269008\n",
      "test loss is 0.0001487425994897854\n",
      "Batch: 17600,train loss is: 0.00010948025141552775\n",
      "test loss is 0.00014999211417146332\n",
      "Batch: 17700,train loss is: 7.60427893659809e-05\n",
      "test loss is 0.00014316154915236474\n",
      "Batch: 17800,train loss is: 0.0002822090781490564\n",
      "test loss is 0.00014673533853895988\n",
      "Batch: 17900,train loss is: 8.361729757769752e-05\n",
      "test loss is 0.00014627458849396846\n",
      "Batch: 18000,train loss is: 0.00010870033556522507\n",
      "test loss is 0.00015046988621860767\n",
      "Batch: 18100,train loss is: 0.00026796990372556905\n",
      "test loss is 0.00014667968684211027\n",
      "Batch: 18200,train loss is: 0.00010774177876004465\n",
      "test loss is 0.00014503956719816535\n",
      "Batch: 18300,train loss is: 0.00014299097227429117\n",
      "test loss is 0.00014137120858912725\n",
      "Batch: 18400,train loss is: 0.00012963836278736117\n",
      "test loss is 0.0001423560907358108\n",
      "Batch: 18500,train loss is: 0.0002162829811538521\n",
      "test loss is 0.0001790579302304996\n",
      "Batch: 18600,train loss is: 0.00010032385885088492\n",
      "test loss is 0.00014908227148576378\n",
      "Batch: 18700,train loss is: 0.00018444209838313535\n",
      "test loss is 0.00014946478684637495\n",
      "Batch: 18800,train loss is: 0.0001019669054439356\n",
      "test loss is 0.00014574908707978073\n",
      "Batch: 18900,train loss is: 0.00015673069238286404\n",
      "test loss is 0.0001548177938847808\n",
      "Batch: 19000,train loss is: 7.835249575654576e-05\n",
      "test loss is 0.0001469975446231576\n",
      "Batch: 19100,train loss is: 0.00017532991348835194\n",
      "test loss is 0.00015250408300331115\n",
      "Batch: 19200,train loss is: 0.00013839147616990046\n",
      "test loss is 0.00014360178915769582\n",
      "Batch: 19300,train loss is: 0.00015574807868089935\n",
      "test loss is 0.0001424727767310093\n",
      "Batch: 19400,train loss is: 0.00011347122995778259\n",
      "test loss is 0.00014499390731645238\n",
      "Batch: 19500,train loss is: 0.00010452931733631716\n",
      "test loss is 0.00017368379659372373\n",
      "Batch: 19600,train loss is: 0.0001170414957384595\n",
      "test loss is 0.00014603494353803835\n",
      "Batch: 19700,train loss is: 0.0001680311595807288\n",
      "test loss is 0.0001465829343572783\n",
      "Batch: 19800,train loss is: 0.00010081907540955247\n",
      "test loss is 0.00014909825598044402\n",
      "Batch: 19900,train loss is: 8.561518465116053e-05\n",
      "test loss is 0.00016463521586222152\n",
      "Batch: 20000,train loss is: 0.00023199084530395935\n",
      "test loss is 0.00016329399619053274\n",
      "Batch: 20100,train loss is: 0.00019032886938649408\n",
      "test loss is 0.00015772674769016346\n",
      "Batch: 20200,train loss is: 9.059782519988533e-05\n",
      "test loss is 0.00014598670696810664\n",
      "Batch: 20300,train loss is: 6.896212747479586e-05\n",
      "test loss is 0.00016177043670636396\n",
      "Batch: 20400,train loss is: 0.00016667753550328354\n",
      "test loss is 0.00016975063173303904\n",
      "Batch: 20500,train loss is: 0.00013589310826828021\n",
      "test loss is 0.0001427667870395058\n",
      "Batch: 20600,train loss is: 0.00023464865055872615\n",
      "test loss is 0.0001487606240311515\n",
      "Batch: 20700,train loss is: 0.00013223741026238052\n",
      "test loss is 0.0001500810524131044\n",
      "Batch: 20800,train loss is: 8.932066879112507e-05\n",
      "test loss is 0.00015128607054714927\n",
      "Batch: 20900,train loss is: 0.00010721169165485924\n",
      "test loss is 0.00014884926266451045\n",
      "Batch: 21000,train loss is: 5.489588306020368e-05\n",
      "test loss is 0.00014092804080085119\n",
      "Batch: 21100,train loss is: 0.0001006522835746863\n",
      "test loss is 0.00014137908427135697\n",
      "Batch: 21200,train loss is: 9.232572417489558e-05\n",
      "test loss is 0.0001508157595288312\n",
      "Batch: 21300,train loss is: 0.0001223005897341137\n",
      "test loss is 0.00014891371562062006\n",
      "Batch: 21400,train loss is: 0.00013901916666995783\n",
      "test loss is 0.00014429622017187065\n",
      "Batch: 21500,train loss is: 0.00010897410911778605\n",
      "test loss is 0.0001556171066811063\n",
      "Batch: 21600,train loss is: 0.00012121837637191621\n",
      "test loss is 0.00015023534352797665\n",
      "Batch: 21700,train loss is: 9.67741070927918e-05\n",
      "test loss is 0.00014874773407417446\n",
      "Batch: 21800,train loss is: 0.00017062884933892846\n",
      "test loss is 0.0001451729701898452\n",
      "Batch: 21900,train loss is: 0.00011963204268206888\n",
      "test loss is 0.0001568632106224637\n",
      "Batch: 22000,train loss is: 0.00016630474004985024\n",
      "test loss is 0.00016828734507138828\n",
      "Batch: 22100,train loss is: 7.906122510047807e-05\n",
      "test loss is 0.0001442065130199921\n",
      "Batch: 22200,train loss is: 7.215361671092962e-05\n",
      "test loss is 0.0001455215951130009\n",
      "Batch: 22300,train loss is: 0.00012384526199298417\n",
      "test loss is 0.00016472941373949748\n",
      "Batch: 22400,train loss is: 0.00013797204403882813\n",
      "test loss is 0.00014729959850739538\n",
      "Batch: 22500,train loss is: 8.409297497093017e-05\n",
      "test loss is 0.00014540041354442864\n",
      "Batch: 22600,train loss is: 0.00015367302507977404\n",
      "test loss is 0.00015197911213328294\n",
      "Batch: 22700,train loss is: 7.696267266969672e-05\n",
      "test loss is 0.00014112172683161442\n",
      "Batch: 22800,train loss is: 0.00011822953643965083\n",
      "test loss is 0.000147998109650272\n",
      "Batch: 22900,train loss is: 0.00010984484179861945\n",
      "test loss is 0.00014229477888146898\n",
      "Batch: 23000,train loss is: 0.000263401915702166\n",
      "test loss is 0.00016025681483571751\n",
      "Batch: 23100,train loss is: 0.00010441463872421114\n",
      "test loss is 0.00014359686339079276\n",
      "Batch: 23200,train loss is: 9.953203046298403e-05\n",
      "test loss is 0.00014208338727679585\n",
      "Batch: 23300,train loss is: 7.836511821378376e-05\n",
      "test loss is 0.00014316541996069234\n",
      "Batch: 23400,train loss is: 0.00010421020131320622\n",
      "test loss is 0.00014468414306199493\n",
      "Batch: 23500,train loss is: 0.00021186614045299587\n",
      "test loss is 0.0001440576446597494\n",
      "Batch: 23600,train loss is: 8.109230075627515e-05\n",
      "test loss is 0.00014754212863129487\n",
      "Batch: 23700,train loss is: 0.00015289931809547605\n",
      "test loss is 0.00014133679097373116\n",
      "Batch: 23800,train loss is: 0.00022504220982553225\n",
      "test loss is 0.00014527554485867727\n",
      "Batch: 23900,train loss is: 0.00047881949170471455\n",
      "test loss is 0.00014586178438820367\n",
      "Batch: 24000,train loss is: 9.640390993409676e-05\n",
      "test loss is 0.00017248444299815157\n",
      "Batch: 24100,train loss is: 0.00013593952896690867\n",
      "test loss is 0.0001424753285159654\n",
      "Batch: 24200,train loss is: 0.000229089431039716\n",
      "test loss is 0.00015474993397786562\n",
      "Batch: 24300,train loss is: 0.0001445250754980759\n",
      "test loss is 0.00019542543258516218\n",
      "Batch: 24400,train loss is: 0.00012943969598099806\n",
      "test loss is 0.00014917719434501327\n",
      "Batch: 24500,train loss is: 0.0001850534143360525\n",
      "test loss is 0.0001472497082000891\n",
      "Batch: 24600,train loss is: 0.00013604307484899132\n",
      "test loss is 0.0001672994973526018\n",
      "Batch: 24700,train loss is: 9.613024381572789e-05\n",
      "test loss is 0.00014444951284773785\n",
      "Batch: 24800,train loss is: 0.0001683640927768849\n",
      "test loss is 0.00014673430620233097\n",
      "Batch: 24900,train loss is: 6.284802292757076e-05\n",
      "test loss is 0.0001426385259311467\n",
      "Batch: 25000,train loss is: 0.00010669138966838886\n",
      "test loss is 0.00014294973831910913\n",
      "Batch: 25100,train loss is: 0.00010664503035018044\n",
      "test loss is 0.00014485874448400712\n",
      "Batch: 25200,train loss is: 0.0001514653095803768\n",
      "test loss is 0.0001498491697762355\n",
      "Batch: 25300,train loss is: 0.00012136380144610409\n",
      "test loss is 0.00014026059958367968\n",
      "Batch: 25400,train loss is: 0.00011961167812637992\n",
      "test loss is 0.00015319446666453007\n",
      "Batch: 25500,train loss is: 9.639354012139461e-05\n",
      "test loss is 0.00014038851632177442\n",
      "Batch: 25600,train loss is: 0.00012242690582740234\n",
      "test loss is 0.00014454095065518293\n",
      "Batch: 25700,train loss is: 0.00015907842562417157\n",
      "test loss is 0.00014001755210356147\n",
      "Batch: 25800,train loss is: 9.696482141195545e-05\n",
      "test loss is 0.00014680712596365538\n",
      "Batch: 25900,train loss is: 7.24131986283262e-05\n",
      "test loss is 0.00014443903342356858\n",
      "Batch: 26000,train loss is: 0.00014529531634858072\n",
      "test loss is 0.0001451520254624235\n",
      "Batch: 26100,train loss is: 0.0002138634149979551\n",
      "test loss is 0.00014322766083379277\n",
      "Batch: 26200,train loss is: 0.0002761448182138565\n",
      "test loss is 0.00014432982481037425\n",
      "Batch: 26300,train loss is: 0.0001693392580219945\n",
      "test loss is 0.00014152801586347707\n",
      "Batch: 26400,train loss is: 0.00010051822071777521\n",
      "test loss is 0.0001399949229370023\n",
      "Batch: 26500,train loss is: 0.00014313735587973407\n",
      "test loss is 0.00014895616759517632\n",
      "Batch: 26600,train loss is: 0.00017494053181285387\n",
      "test loss is 0.00014674713000525788\n",
      "Batch: 26700,train loss is: 7.215599364710663e-05\n",
      "test loss is 0.00014589943457305878\n",
      "Batch: 26800,train loss is: 0.00021875360490009868\n",
      "test loss is 0.00014605143923431327\n",
      "Batch: 26900,train loss is: 8.138587854796484e-05\n",
      "test loss is 0.00015077898418798682\n",
      "Batch: 27000,train loss is: 0.00012335683172270547\n",
      "test loss is 0.00015470856773628993\n",
      "Batch: 27100,train loss is: 8.563993009742101e-05\n",
      "test loss is 0.0001540395374541032\n",
      "Batch: 27200,train loss is: 0.00010778713426870659\n",
      "test loss is 0.00014281832263718337\n",
      "Batch: 27300,train loss is: 0.00015145119167207284\n",
      "test loss is 0.00014665837530050845\n",
      "Batch: 27400,train loss is: 0.0001079250463096737\n",
      "test loss is 0.00014552921385987876\n",
      "Batch: 27500,train loss is: 0.00010849113288968834\n",
      "test loss is 0.00015259247438627663\n",
      "Batch: 27600,train loss is: 0.00011537999090692522\n",
      "test loss is 0.00014954785317889256\n",
      "Batch: 27700,train loss is: 0.00040504311861789107\n",
      "test loss is 0.00015632856639768693\n",
      "Batch: 27800,train loss is: 0.0001077170823390504\n",
      "test loss is 0.00014416214316041853\n",
      "Batch: 27900,train loss is: 0.00018301198367277058\n",
      "test loss is 0.00014347715700809956\n",
      "Batch: 28000,train loss is: 9.692022462052326e-05\n",
      "test loss is 0.0001526954637657169\n",
      "Batch: 28100,train loss is: 0.0001934643001386128\n",
      "test loss is 0.0001582714070583435\n",
      "Batch: 28200,train loss is: 0.00015931138435963131\n",
      "test loss is 0.00014692337064684688\n",
      "Batch: 28300,train loss is: 0.0004352545638490759\n",
      "test loss is 0.0001432322732937164\n",
      "Batch: 28400,train loss is: 0.00012349533658231592\n",
      "test loss is 0.00014364958051577364\n",
      "Batch: 28500,train loss is: 0.0001702621278968538\n",
      "test loss is 0.00015324019994919033\n",
      "Batch: 28600,train loss is: 9.31822489056172e-05\n",
      "test loss is 0.00014355697880978751\n",
      "Batch: 28700,train loss is: 0.00013069630176339457\n",
      "test loss is 0.00014437695253032639\n",
      "Batch: 28800,train loss is: 0.0001799043281011803\n",
      "test loss is 0.00014047898494444033\n",
      "Batch: 28900,train loss is: 0.00015442283515768065\n",
      "test loss is 0.00017255946649896438\n",
      "Batch: 29000,train loss is: 0.00012775501292288166\n",
      "test loss is 0.00014339296085232912\n",
      "Batch: 29100,train loss is: 0.00010935280044421066\n",
      "test loss is 0.00015617635453822075\n",
      "Batch: 29200,train loss is: 0.00010646056102498265\n",
      "test loss is 0.00016475530315094986\n",
      "Batch: 29300,train loss is: 0.00011639295912996572\n",
      "test loss is 0.0001419961202804971\n",
      "Batch: 29400,train loss is: 0.00017416772646235845\n",
      "test loss is 0.00014848536852443898\n",
      "Batch: 29500,train loss is: 8.404249055981702e-05\n",
      "test loss is 0.00014370591902234127\n",
      "Batch: 29600,train loss is: 0.00024119825487521894\n",
      "test loss is 0.00016238574368574298\n",
      "Batch: 29700,train loss is: 8.55528220957866e-05\n",
      "test loss is 0.00014125803395916212\n",
      "Batch: 29800,train loss is: 0.00026098603144205534\n",
      "test loss is 0.0001401772650721885\n",
      "Batch: 29900,train loss is: 0.00035444073872525504\n",
      "test loss is 0.0001440924293266001\n",
      "Batch: 30000,train loss is: 8.21599453235049e-05\n",
      "test loss is 0.0001462288044596485\n",
      "Batch: 30100,train loss is: 9.798631764663173e-05\n",
      "test loss is 0.000151805797792136\n",
      "Batch: 30200,train loss is: 0.00012500212261664251\n",
      "test loss is 0.00013857673932610043\n",
      "Batch: 30300,train loss is: 9.653943532113139e-05\n",
      "test loss is 0.00014349655004641655\n",
      "Batch: 30400,train loss is: 9.703564584203484e-05\n",
      "test loss is 0.000148573710690828\n",
      "Batch: 30500,train loss is: 0.00011414003365790123\n",
      "test loss is 0.00014373813723285694\n",
      "Batch: 30600,train loss is: 8.829201622479607e-05\n",
      "test loss is 0.00014048369888636987\n",
      "Batch: 30700,train loss is: 0.00010313956871154708\n",
      "test loss is 0.00014112506392512222\n",
      "Batch: 30800,train loss is: 9.823536824420403e-05\n",
      "test loss is 0.00014521032163064066\n",
      "Batch: 30900,train loss is: 0.00012002264526980357\n",
      "test loss is 0.0001448641401784976\n",
      "Batch: 31000,train loss is: 7.303109125156545e-05\n",
      "test loss is 0.0001450286526291679\n",
      "Batch: 31100,train loss is: 0.00011305532914189135\n",
      "test loss is 0.0001474254548483178\n",
      "Batch: 31200,train loss is: 8.807530189151137e-05\n",
      "test loss is 0.00014095768252775484\n",
      "Batch: 31300,train loss is: 0.00014054385985167226\n",
      "test loss is 0.00013926894557702478\n",
      "Batch: 31400,train loss is: 0.00010734225348493129\n",
      "test loss is 0.00016113267026290245\n",
      "Batch: 31500,train loss is: 9.187967930990308e-05\n",
      "test loss is 0.00015401418476083912\n",
      "Batch: 31600,train loss is: 0.00011065239231580937\n",
      "test loss is 0.00013878587575704075\n",
      "Batch: 31700,train loss is: 9.20853384736373e-05\n",
      "test loss is 0.00014130325136276436\n",
      "Batch: 31800,train loss is: 0.00014003845332519226\n",
      "test loss is 0.00014088265037836875\n",
      "Batch: 31900,train loss is: 0.00016160417605429187\n",
      "test loss is 0.00015484968038760174\n",
      "Batch: 32000,train loss is: 0.0005560027296344143\n",
      "test loss is 0.00014164219411699923\n",
      "Batch: 32100,train loss is: 5.909928020906659e-05\n",
      "test loss is 0.00013966273891414957\n",
      "Batch: 32200,train loss is: 0.00010529683671174645\n",
      "test loss is 0.00013832965853098042\n",
      "Batch: 32300,train loss is: 9.530843032811432e-05\n",
      "test loss is 0.00013964241090173152\n",
      "Batch: 32400,train loss is: 0.00011468656960586858\n",
      "test loss is 0.00014800513704645756\n",
      "Batch: 32500,train loss is: 0.00015730926773527986\n",
      "test loss is 0.00014197385781093615\n",
      "Batch: 32600,train loss is: 8.383836969965592e-05\n",
      "test loss is 0.000144272232881054\n",
      "Batch: 32700,train loss is: 8.549943771190023e-05\n",
      "test loss is 0.00014131783305748995\n",
      "Batch: 32800,train loss is: 9.131966634381163e-05\n",
      "test loss is 0.00015755743139465723\n",
      "Batch: 32900,train loss is: 0.00014932724214825115\n",
      "test loss is 0.00014313871205529392\n",
      "Batch: 33000,train loss is: 6.461999348897923e-05\n",
      "test loss is 0.00014118027259344253\n",
      "Batch: 33100,train loss is: 9.613926289780161e-05\n",
      "test loss is 0.00015136708578031494\n",
      "Batch: 33200,train loss is: 0.00013051398680339574\n",
      "test loss is 0.00016183507113320028\n",
      "Batch: 33300,train loss is: 0.00010923759638845147\n",
      "test loss is 0.00016588261591261427\n",
      "Batch: 33400,train loss is: 9.767811190546636e-05\n",
      "test loss is 0.0001407106156245446\n",
      "Batch: 33500,train loss is: 0.00011405263002159702\n",
      "test loss is 0.00014213285304490435\n",
      "Batch: 33600,train loss is: 9.392861007169848e-05\n",
      "test loss is 0.00014371928811917546\n",
      "Batch: 33700,train loss is: 0.00010076256070239193\n",
      "test loss is 0.00014615000314913837\n",
      "Batch: 33800,train loss is: 0.00012607408151941155\n",
      "test loss is 0.00014409302161512303\n",
      "Batch: 33900,train loss is: 0.00012178953536529669\n",
      "test loss is 0.00014024723900727664\n",
      "-----------------------Epoch: 1----------------------------------\n",
      "Batch: 0,train loss is: 0.00013147698780932107\n",
      "test loss is 0.00014699206702127493\n",
      "Batch: 100,train loss is: 0.00016049931354773894\n",
      "test loss is 0.0001430364753109112\n",
      "Batch: 200,train loss is: 0.0001271512521689254\n",
      "test loss is 0.00013980005606872942\n",
      "Batch: 300,train loss is: 9.04339772667613e-05\n",
      "test loss is 0.000140777486510115\n",
      "Batch: 400,train loss is: 9.726591158807666e-05\n",
      "test loss is 0.0001391040622727223\n",
      "Batch: 500,train loss is: 0.00013852147127298625\n",
      "test loss is 0.0001671184608990728\n",
      "Batch: 600,train loss is: 0.0001196026993589502\n",
      "test loss is 0.00013992022559531435\n",
      "Batch: 700,train loss is: 8.184557480729199e-05\n",
      "test loss is 0.0001417624301878091\n",
      "Batch: 800,train loss is: 0.00018986909942339655\n",
      "test loss is 0.0001429947153154411\n",
      "Batch: 900,train loss is: 6.280901201814284e-05\n",
      "test loss is 0.00014635053710756682\n",
      "Batch: 1000,train loss is: 0.00010725978573526063\n",
      "test loss is 0.00014044104242396586\n",
      "Batch: 1100,train loss is: 0.00027179610144556726\n",
      "test loss is 0.00014404643354250205\n",
      "Batch: 1200,train loss is: 5.380560896832288e-05\n",
      "test loss is 0.00014432133787677638\n",
      "Batch: 1300,train loss is: 0.00012463928592544677\n",
      "test loss is 0.0001412033196459594\n",
      "Batch: 1400,train loss is: 0.00011484654321106149\n",
      "test loss is 0.00013836749941259813\n",
      "Batch: 1500,train loss is: 0.0001085974925220311\n",
      "test loss is 0.0001452624423450539\n",
      "Batch: 1600,train loss is: 6.97174165966936e-05\n",
      "test loss is 0.00014372094971193657\n",
      "Batch: 1700,train loss is: 0.00016466136332511387\n",
      "test loss is 0.0001552929464977533\n",
      "Batch: 1800,train loss is: 8.736327150137234e-05\n",
      "test loss is 0.00013706796465770026\n",
      "Batch: 1900,train loss is: 7.061103479216762e-05\n",
      "test loss is 0.00014551699101308794\n",
      "Batch: 2000,train loss is: 0.00014013844628092147\n",
      "test loss is 0.0001432333650670267\n",
      "Batch: 2100,train loss is: 7.989132874790247e-05\n",
      "test loss is 0.0001420192506975447\n",
      "Batch: 2200,train loss is: 0.00014021536791654983\n",
      "test loss is 0.00014270856312444382\n",
      "Batch: 2300,train loss is: 0.00010851518321929533\n",
      "test loss is 0.00013805156544084723\n",
      "Batch: 2400,train loss is: 0.00012269878431268484\n",
      "test loss is 0.0001408249522840614\n",
      "Batch: 2500,train loss is: 0.00014798536929683058\n",
      "test loss is 0.00013905864519529778\n",
      "Batch: 2600,train loss is: 9.404480218797118e-05\n",
      "test loss is 0.00014103507089303436\n",
      "Batch: 2700,train loss is: 0.0001580848156380281\n",
      "test loss is 0.00014978615776864177\n",
      "Batch: 2800,train loss is: 0.0001137077493858336\n",
      "test loss is 0.00014353977211390353\n",
      "Batch: 2900,train loss is: 0.00012877247682467282\n",
      "test loss is 0.00013965548301638438\n",
      "Batch: 3000,train loss is: 0.0001085350734194131\n",
      "test loss is 0.00014198409579923324\n",
      "Batch: 3100,train loss is: 9.186337316267097e-05\n",
      "test loss is 0.00013755903063180058\n",
      "Batch: 3200,train loss is: 0.00021691812122074928\n",
      "test loss is 0.0001437962069500613\n",
      "Batch: 3300,train loss is: 0.00011292140612970038\n",
      "test loss is 0.00013916491055541356\n",
      "Batch: 3400,train loss is: 0.00011344942612098399\n",
      "test loss is 0.00015117678349441506\n",
      "Batch: 3500,train loss is: 0.0001069206890937614\n",
      "test loss is 0.00014557914364734707\n",
      "Batch: 3600,train loss is: 9.131159633534239e-05\n",
      "test loss is 0.00014397043465744127\n",
      "Batch: 3700,train loss is: 6.319795117876565e-05\n",
      "test loss is 0.0001371091394643895\n",
      "Batch: 3800,train loss is: 8.595338453008342e-05\n",
      "test loss is 0.00014017235165237447\n",
      "Batch: 3900,train loss is: 0.00012900711881005546\n",
      "test loss is 0.00014347122993760514\n",
      "Batch: 4000,train loss is: 6.382329538421075e-05\n",
      "test loss is 0.0001416612867038574\n",
      "Batch: 4100,train loss is: 0.00014098496664347368\n",
      "test loss is 0.00015005621281780556\n",
      "Batch: 4200,train loss is: 0.00013636412100524067\n",
      "test loss is 0.00014889028972729074\n",
      "Batch: 4300,train loss is: 0.00011612791675241585\n",
      "test loss is 0.00013864035240854604\n",
      "Batch: 4400,train loss is: 8.729740475493263e-05\n",
      "test loss is 0.00014091307535918837\n",
      "Batch: 4500,train loss is: 0.00011077214126515625\n",
      "test loss is 0.0001391166953097562\n",
      "Batch: 4600,train loss is: 9.925426224842965e-05\n",
      "test loss is 0.00013985574226352528\n",
      "Batch: 4700,train loss is: 0.0001713786128036243\n",
      "test loss is 0.00014927460676716106\n",
      "Batch: 4800,train loss is: 0.00031978670320264094\n",
      "test loss is 0.00016206727894747968\n",
      "Batch: 4900,train loss is: 0.00011142331643205971\n",
      "test loss is 0.0001408280604637388\n",
      "Batch: 5000,train loss is: 7.348546825282332e-05\n",
      "test loss is 0.00014075753720598194\n",
      "Batch: 5100,train loss is: 8.475141795748187e-05\n",
      "test loss is 0.0001402427110842103\n",
      "Batch: 5200,train loss is: 9.564283989988128e-05\n",
      "test loss is 0.0001448374310592714\n",
      "Batch: 5300,train loss is: 0.00011332323457521496\n",
      "test loss is 0.00014412875136671056\n",
      "Batch: 5400,train loss is: 0.00023114373685272847\n",
      "test loss is 0.00014842911092951687\n",
      "Batch: 5500,train loss is: 9.151812591947812e-05\n",
      "test loss is 0.00015952451026590912\n",
      "Batch: 5600,train loss is: 0.00017546511213709055\n",
      "test loss is 0.0001402736346170874\n",
      "Batch: 5700,train loss is: 0.00014128370063818115\n",
      "test loss is 0.0001476218763601092\n",
      "Batch: 5800,train loss is: 0.00018874927547955065\n",
      "test loss is 0.00014414258543858332\n",
      "Batch: 5900,train loss is: 0.00044088511809696213\n",
      "test loss is 0.0001424890279002385\n",
      "Batch: 6000,train loss is: 0.0001765823587714348\n",
      "test loss is 0.00014378874175958526\n",
      "Batch: 6100,train loss is: 7.763687745368894e-05\n",
      "test loss is 0.00014111853692760858\n",
      "Batch: 6200,train loss is: 9.920772560777024e-05\n",
      "test loss is 0.00013948108198108563\n",
      "Batch: 6300,train loss is: 0.00016968021261357056\n",
      "test loss is 0.00016538877652634375\n",
      "Batch: 6400,train loss is: 0.00010498328857023441\n",
      "test loss is 0.00014125421031998089\n",
      "Batch: 6500,train loss is: 7.439061997213453e-05\n",
      "test loss is 0.00014417302274764108\n",
      "Batch: 6600,train loss is: 0.00035013047203573557\n",
      "test loss is 0.00013797504902114818\n",
      "Batch: 6700,train loss is: 0.00010586677101253303\n",
      "test loss is 0.00013854903475969641\n",
      "Batch: 6800,train loss is: 0.0001630657733056827\n",
      "test loss is 0.00014382647247075813\n",
      "Batch: 6900,train loss is: 0.00010807490246627614\n",
      "test loss is 0.0001398852210092985\n",
      "Batch: 7000,train loss is: 0.00015928761798947845\n",
      "test loss is 0.00014651479764262164\n",
      "Batch: 7100,train loss is: 0.00015281280982705475\n",
      "test loss is 0.00016526847967048767\n",
      "Batch: 7200,train loss is: 8.695321479040422e-05\n",
      "test loss is 0.00013746409186026802\n",
      "Batch: 7300,train loss is: 0.0002501985646407212\n",
      "test loss is 0.00014301675394483125\n",
      "Batch: 7400,train loss is: 0.00010356070972311282\n",
      "test loss is 0.0001756046519703122\n",
      "Batch: 7500,train loss is: 0.0001435813314370924\n",
      "test loss is 0.00014681824963100044\n",
      "Batch: 7600,train loss is: 0.00020247478038510112\n",
      "test loss is 0.00015941109121125508\n",
      "Batch: 7700,train loss is: 0.00019148239363432772\n",
      "test loss is 0.00013978739674745644\n",
      "Batch: 7800,train loss is: 7.215788948087793e-05\n",
      "test loss is 0.00013510207050821836\n",
      "Batch: 7900,train loss is: 9.394345592857448e-05\n",
      "test loss is 0.00013744880293852372\n",
      "Batch: 8000,train loss is: 5.832451556547328e-05\n",
      "test loss is 0.0001411826337318856\n",
      "Batch: 8100,train loss is: 0.00023817668573007018\n",
      "test loss is 0.00014666071654294982\n",
      "Batch: 8200,train loss is: 0.00011052639121832559\n",
      "test loss is 0.00013762389847073433\n",
      "Batch: 8300,train loss is: 7.496025702712119e-05\n",
      "test loss is 0.00014267497328246533\n",
      "Batch: 8400,train loss is: 0.00013157602546800138\n",
      "test loss is 0.00014457559914462853\n",
      "Batch: 8500,train loss is: 0.0002514065749497196\n",
      "test loss is 0.00014544573210633263\n",
      "Batch: 8600,train loss is: 9.793368514112851e-05\n",
      "test loss is 0.00015199465323986983\n",
      "Batch: 8700,train loss is: 0.00012680612053361345\n",
      "test loss is 0.0001496099064148609\n",
      "Batch: 8800,train loss is: 9.456240763433097e-05\n",
      "test loss is 0.0001380499110716291\n",
      "Batch: 8900,train loss is: 0.00013170393974441948\n",
      "test loss is 0.00013477318085863883\n",
      "Batch: 9000,train loss is: 8.679303733150983e-05\n",
      "test loss is 0.00013919843933982523\n",
      "Batch: 9100,train loss is: 0.00011055910509898768\n",
      "test loss is 0.00013945585506936755\n",
      "Batch: 9200,train loss is: 0.00014061600729699467\n",
      "test loss is 0.00014104350843066355\n",
      "Batch: 9300,train loss is: 0.00017561389509922385\n",
      "test loss is 0.00013762123732194943\n",
      "Batch: 9400,train loss is: 0.00015425381296950048\n",
      "test loss is 0.00015322959888275465\n",
      "Batch: 9500,train loss is: 8.471811196322691e-05\n",
      "test loss is 0.0001464986901947733\n",
      "Batch: 9600,train loss is: 9.488645690282769e-05\n",
      "test loss is 0.00013769598719256057\n",
      "Batch: 9700,train loss is: 7.377610837407673e-05\n",
      "test loss is 0.0001406454556996672\n",
      "Batch: 9800,train loss is: 7.834609799098669e-05\n",
      "test loss is 0.00013838495504518917\n",
      "Batch: 9900,train loss is: 8.498092889744304e-05\n",
      "test loss is 0.00013925624229229423\n",
      "Batch: 10000,train loss is: 7.077725146923988e-05\n",
      "test loss is 0.00014503245549690012\n",
      "Batch: 10100,train loss is: 0.00024233539914148838\n",
      "test loss is 0.0001390232810984191\n",
      "Batch: 10200,train loss is: 0.00037152544625321417\n",
      "test loss is 0.00013820372308617955\n",
      "Batch: 10300,train loss is: 0.00014100489691983684\n",
      "test loss is 0.00014343996041068114\n",
      "Batch: 10400,train loss is: 6.338677202321065e-05\n",
      "test loss is 0.0001431817660498058\n",
      "Batch: 10500,train loss is: 0.00013689679650659496\n",
      "test loss is 0.00014007807234308454\n",
      "Batch: 10600,train loss is: 8.448888022676441e-05\n",
      "test loss is 0.00014041244276076425\n",
      "Batch: 10700,train loss is: 0.00010447516330955794\n",
      "test loss is 0.00013510940223729033\n",
      "Batch: 10800,train loss is: 0.00011968966010755762\n",
      "test loss is 0.00014471853905167708\n",
      "Batch: 10900,train loss is: 0.00012425012950129783\n",
      "test loss is 0.00013748124224333532\n",
      "Batch: 11000,train loss is: 0.00016354892668264486\n",
      "test loss is 0.0001368153358437632\n",
      "Batch: 11100,train loss is: 0.00016432011914751756\n",
      "test loss is 0.0001396027937549735\n",
      "Batch: 11200,train loss is: 0.0001751155078379444\n",
      "test loss is 0.00013808712614197044\n",
      "Batch: 11300,train loss is: 8.857598917530613e-05\n",
      "test loss is 0.0001451834543156404\n",
      "Batch: 11400,train loss is: 0.000269486987088006\n",
      "test loss is 0.00013706267410893452\n",
      "Batch: 11500,train loss is: 7.453988696657232e-05\n",
      "test loss is 0.00013802505394413492\n",
      "Batch: 11600,train loss is: 0.00018315797042660055\n",
      "test loss is 0.0001478198720614671\n",
      "Batch: 11700,train loss is: 0.00010958008466904725\n",
      "test loss is 0.00013815861334022137\n",
      "Batch: 11800,train loss is: 0.00011740888159964204\n",
      "test loss is 0.0001370702515333111\n",
      "Batch: 11900,train loss is: 0.00010533196482672329\n",
      "test loss is 0.00015343393066464834\n",
      "Batch: 12000,train loss is: 0.00011654872039618034\n",
      "test loss is 0.00014254615547490347\n",
      "Batch: 12100,train loss is: 0.00017398871574061066\n",
      "test loss is 0.00013748658453559932\n",
      "Batch: 12200,train loss is: 9.716927188054999e-05\n",
      "test loss is 0.00013970281831523687\n",
      "Batch: 12300,train loss is: 9.97021338474529e-05\n",
      "test loss is 0.00014398209182011457\n",
      "Batch: 12400,train loss is: 0.00011779019478942437\n",
      "test loss is 0.00013700039658290779\n",
      "Batch: 12500,train loss is: 0.0002625824249449045\n",
      "test loss is 0.00014052461376478162\n",
      "Batch: 12600,train loss is: 8.007454141241056e-05\n",
      "test loss is 0.00013622325168689717\n",
      "Batch: 12700,train loss is: 0.00010872757835278956\n",
      "test loss is 0.00013813663934401693\n",
      "Batch: 12800,train loss is: 5.56554517555095e-05\n",
      "test loss is 0.00013869809636132921\n",
      "Batch: 12900,train loss is: 0.00019123088289464668\n",
      "test loss is 0.00014572656281334445\n",
      "Batch: 13000,train loss is: 9.256709687205257e-05\n",
      "test loss is 0.00013504331596109046\n",
      "Batch: 13100,train loss is: 0.00020374756836834617\n",
      "test loss is 0.00013467113829075398\n",
      "Batch: 13200,train loss is: 0.00010774014366834927\n",
      "test loss is 0.00015258846158568064\n",
      "Batch: 13300,train loss is: 0.00013787300772721007\n",
      "test loss is 0.00016658092149560277\n",
      "Batch: 13400,train loss is: 0.00010062351778924017\n",
      "test loss is 0.00013557897554298778\n",
      "Batch: 13500,train loss is: 7.850488638153977e-05\n",
      "test loss is 0.00014130614516868508\n",
      "Batch: 13600,train loss is: 7.932316554325364e-05\n",
      "test loss is 0.0001489327539680641\n",
      "Batch: 13700,train loss is: 0.00012070912547667649\n",
      "test loss is 0.00013543571246011618\n",
      "Batch: 13800,train loss is: 7.962931343597773e-05\n",
      "test loss is 0.0001476880835326738\n",
      "Batch: 13900,train loss is: 6.984713524062942e-05\n",
      "test loss is 0.00013870507329778845\n",
      "Batch: 14000,train loss is: 0.0001371909612044177\n",
      "test loss is 0.0001441559218212271\n",
      "Batch: 14100,train loss is: 8.33410797726483e-05\n",
      "test loss is 0.0001365573899700506\n",
      "Batch: 14200,train loss is: 8.005240124428775e-05\n",
      "test loss is 0.00013722414805289901\n",
      "Batch: 14300,train loss is: 9.521453511796719e-05\n",
      "test loss is 0.0001364995152233995\n",
      "Batch: 14400,train loss is: 0.00020543419908956842\n",
      "test loss is 0.00013537897556747293\n",
      "Batch: 14500,train loss is: 0.00010189895509153116\n",
      "test loss is 0.00013568603367266097\n",
      "Batch: 14600,train loss is: 0.00017333421440539476\n",
      "test loss is 0.00014583812428649393\n",
      "Batch: 14700,train loss is: 0.00010490932561346034\n",
      "test loss is 0.00013544372792017096\n",
      "Batch: 14800,train loss is: 0.0001936239637750665\n",
      "test loss is 0.0001391413763249513\n",
      "Batch: 14900,train loss is: 9.582235700527618e-05\n",
      "test loss is 0.00013315891461507386\n",
      "Batch: 15000,train loss is: 0.00010975332629233032\n",
      "test loss is 0.00014009281396377418\n",
      "Batch: 15100,train loss is: 0.00016598384056013156\n",
      "test loss is 0.0001383884179700409\n",
      "Batch: 15200,train loss is: 0.00017070506320046873\n",
      "test loss is 0.00013778012225841568\n",
      "Batch: 15300,train loss is: 0.00016694213426939115\n",
      "test loss is 0.00013785380943604256\n",
      "Batch: 15400,train loss is: 8.085273512252469e-05\n",
      "test loss is 0.00013465064169150516\n",
      "Batch: 15500,train loss is: 0.00010524739191944757\n",
      "test loss is 0.00014213446181977042\n",
      "Batch: 15600,train loss is: 0.00024125039255985014\n",
      "test loss is 0.00013659564293810278\n",
      "Batch: 15700,train loss is: 8.285597217882402e-05\n",
      "test loss is 0.00014057123790736298\n",
      "Batch: 15800,train loss is: 8.271859965330797e-05\n",
      "test loss is 0.00014149914722371213\n",
      "Batch: 15900,train loss is: 0.00016210536819610198\n",
      "test loss is 0.0001407338014960908\n",
      "Batch: 16000,train loss is: 0.00016537203959905402\n",
      "test loss is 0.00013563008406638287\n",
      "Batch: 16100,train loss is: 0.00022445740584749584\n",
      "test loss is 0.0001342344003724317\n",
      "Batch: 16200,train loss is: 0.00010682762135400128\n",
      "test loss is 0.00013725883274468776\n",
      "Batch: 16300,train loss is: 0.00011085406767979708\n",
      "test loss is 0.00014243426024006023\n",
      "Batch: 16400,train loss is: 8.335549711738044e-05\n",
      "test loss is 0.00014159878526923117\n",
      "Batch: 16500,train loss is: 9.733146073358892e-05\n",
      "test loss is 0.0001359501718109388\n",
      "Batch: 16600,train loss is: 0.00010055558277484847\n",
      "test loss is 0.0001362161250745552\n",
      "Batch: 16700,train loss is: 0.0002218820505712252\n",
      "test loss is 0.00014472738716384987\n",
      "Batch: 16800,train loss is: 0.00015085412521781478\n",
      "test loss is 0.0001398791797372403\n",
      "Batch: 16900,train loss is: 0.00010841833519911283\n",
      "test loss is 0.0001483791339167961\n",
      "Batch: 17000,train loss is: 0.00012101508925124537\n",
      "test loss is 0.00014393323894466606\n",
      "Batch: 17100,train loss is: 0.00011488467903591875\n",
      "test loss is 0.00013517214397222433\n",
      "Batch: 17200,train loss is: 7.200104106162965e-05\n",
      "test loss is 0.00013701011760869193\n",
      "Batch: 17300,train loss is: 8.11805480985494e-05\n",
      "test loss is 0.00014739412659719745\n",
      "Batch: 17400,train loss is: 6.632432263618139e-05\n",
      "test loss is 0.00014401004409955696\n",
      "Batch: 17500,train loss is: 0.00013202830701624196\n",
      "test loss is 0.00013808101074955809\n",
      "Batch: 17600,train loss is: 9.48149819936568e-05\n",
      "test loss is 0.00013883516878904282\n",
      "Batch: 17700,train loss is: 6.696946875982055e-05\n",
      "test loss is 0.00013349648081417706\n",
      "Batch: 17800,train loss is: 0.00027778180112390745\n",
      "test loss is 0.00013765487906939477\n",
      "Batch: 17900,train loss is: 6.443004033139368e-05\n",
      "test loss is 0.00013659009065651188\n",
      "Batch: 18000,train loss is: 9.879042076088056e-05\n",
      "test loss is 0.0001405228577039372\n",
      "Batch: 18100,train loss is: 0.0002340903078907946\n",
      "test loss is 0.0001367957921988498\n",
      "Batch: 18200,train loss is: 0.00010239073472108092\n",
      "test loss is 0.00013461720578318695\n",
      "Batch: 18300,train loss is: 0.00014384761275650036\n",
      "test loss is 0.00013218653614783553\n",
      "Batch: 18400,train loss is: 0.00012276477873400524\n",
      "test loss is 0.00013265844271642145\n",
      "Batch: 18500,train loss is: 0.00019153111463857697\n",
      "test loss is 0.00016515752141977482\n",
      "Batch: 18600,train loss is: 9.486128526327589e-05\n",
      "test loss is 0.00014070882473557703\n",
      "Batch: 18700,train loss is: 0.00017382280513674118\n",
      "test loss is 0.0001397764652065045\n",
      "Batch: 18800,train loss is: 9.816233596575633e-05\n",
      "test loss is 0.0001356510146167834\n",
      "Batch: 18900,train loss is: 0.00015160391788192375\n",
      "test loss is 0.00014239608777313266\n",
      "Batch: 19000,train loss is: 7.073220542520624e-05\n",
      "test loss is 0.00013812695941984083\n",
      "Batch: 19100,train loss is: 0.0001665425770281156\n",
      "test loss is 0.00014436405542391292\n",
      "Batch: 19200,train loss is: 0.00012596795448511778\n",
      "test loss is 0.00013402606740430494\n",
      "Batch: 19300,train loss is: 0.0001385249819290575\n",
      "test loss is 0.00013330524493448408\n",
      "Batch: 19400,train loss is: 0.00010842971295532171\n",
      "test loss is 0.0001365086042041911\n",
      "Batch: 19500,train loss is: 9.80938607388664e-05\n",
      "test loss is 0.00016043287856812256\n",
      "Batch: 19600,train loss is: 0.00010882038094431924\n",
      "test loss is 0.00013767321067567227\n",
      "Batch: 19700,train loss is: 0.0001611006945702961\n",
      "test loss is 0.00013601947688627025\n",
      "Batch: 19800,train loss is: 9.279358685582587e-05\n",
      "test loss is 0.00013966454018852993\n",
      "Batch: 19900,train loss is: 7.890283665133897e-05\n",
      "test loss is 0.00014847867568014108\n",
      "Batch: 20000,train loss is: 0.00021279176086343973\n",
      "test loss is 0.00015234594998670946\n",
      "Batch: 20100,train loss is: 0.00018270174827116103\n",
      "test loss is 0.00014930258637679087\n",
      "Batch: 20200,train loss is: 8.984549234363586e-05\n",
      "test loss is 0.00013806798776228623\n",
      "Batch: 20300,train loss is: 5.951619937946774e-05\n",
      "test loss is 0.00015249737900750341\n",
      "Batch: 20400,train loss is: 0.00014669004559796054\n",
      "test loss is 0.0001545327629644615\n",
      "Batch: 20500,train loss is: 0.00012724227937733725\n",
      "test loss is 0.000132660913812019\n",
      "Batch: 20600,train loss is: 0.00021944527840604217\n",
      "test loss is 0.00013994753722938635\n",
      "Batch: 20700,train loss is: 0.0001306868185888877\n",
      "test loss is 0.0001414446588980201\n",
      "Batch: 20800,train loss is: 8.827153165753828e-05\n",
      "test loss is 0.000141375144570218\n",
      "Batch: 20900,train loss is: 0.0001021336768989698\n",
      "test loss is 0.00013989587156888693\n",
      "Batch: 21000,train loss is: 4.9361294762086156e-05\n",
      "test loss is 0.00013213032870904788\n",
      "Batch: 21100,train loss is: 9.446195333068711e-05\n",
      "test loss is 0.00013210915766758905\n",
      "Batch: 21200,train loss is: 8.413111204793781e-05\n",
      "test loss is 0.00014169769455235796\n",
      "Batch: 21300,train loss is: 0.00011030400514496612\n",
      "test loss is 0.00013936035133801973\n",
      "Batch: 21400,train loss is: 0.00013558865906546864\n",
      "test loss is 0.00013338042771486174\n",
      "Batch: 21500,train loss is: 0.00010394214893829563\n",
      "test loss is 0.00014359204235600296\n",
      "Batch: 21600,train loss is: 0.00012208125660192234\n",
      "test loss is 0.00014288960720707018\n",
      "Batch: 21700,train loss is: 8.765611114343682e-05\n",
      "test loss is 0.00014006771803868253\n",
      "Batch: 21800,train loss is: 0.00016160135190705275\n",
      "test loss is 0.0001358491445256795\n",
      "Batch: 21900,train loss is: 0.0001138404077296321\n",
      "test loss is 0.00014976413839230338\n",
      "Batch: 22000,train loss is: 0.00014463559751140637\n",
      "test loss is 0.0001612425804785996\n",
      "Batch: 22100,train loss is: 8.025896091595133e-05\n",
      "test loss is 0.00013381412360026463\n",
      "Batch: 22200,train loss is: 6.772800383091165e-05\n",
      "test loss is 0.00013719049289809878\n",
      "Batch: 22300,train loss is: 0.00011474268476801816\n",
      "test loss is 0.0001545300546458585\n",
      "Batch: 22400,train loss is: 0.00012777118951630453\n",
      "test loss is 0.00013731167490909897\n",
      "Batch: 22500,train loss is: 8.50969046131178e-05\n",
      "test loss is 0.00013747200193189816\n",
      "Batch: 22600,train loss is: 0.00013635560721471918\n",
      "test loss is 0.00014188447000156848\n",
      "Batch: 22700,train loss is: 7.540662678620454e-05\n",
      "test loss is 0.0001315207478503503\n",
      "Batch: 22800,train loss is: 0.0001187607000777829\n",
      "test loss is 0.0001414727249078901\n",
      "Batch: 22900,train loss is: 8.987919149671345e-05\n",
      "test loss is 0.00013392681686301305\n",
      "Batch: 23000,train loss is: 0.0002286189447457379\n",
      "test loss is 0.0001460149478886863\n",
      "Batch: 23100,train loss is: 0.00010125652491786915\n",
      "test loss is 0.0001337192920572332\n",
      "Batch: 23200,train loss is: 9.115725194833414e-05\n",
      "test loss is 0.00013222002647923493\n",
      "Batch: 23300,train loss is: 7.345624750382705e-05\n",
      "test loss is 0.00013416106417976658\n",
      "Batch: 23400,train loss is: 9.624207395368926e-05\n",
      "test loss is 0.00013625923454631196\n",
      "Batch: 23500,train loss is: 0.00018895307609431822\n",
      "test loss is 0.00013478169226193655\n",
      "Batch: 23600,train loss is: 7.886616680741613e-05\n",
      "test loss is 0.00014069925821863808\n",
      "Batch: 23700,train loss is: 0.00015151819725840715\n",
      "test loss is 0.00013114649097407084\n",
      "Batch: 23800,train loss is: 0.0002097162946907968\n",
      "test loss is 0.00013581624067318164\n",
      "Batch: 23900,train loss is: 0.00044266416630488013\n",
      "test loss is 0.00013550369882995035\n",
      "Batch: 24000,train loss is: 9.118690594832919e-05\n",
      "test loss is 0.00016273296202534658\n",
      "Batch: 24100,train loss is: 0.0001268112069626005\n",
      "test loss is 0.00013387450777858288\n",
      "Batch: 24200,train loss is: 0.00021642071737509083\n",
      "test loss is 0.00014342058974103802\n",
      "Batch: 24300,train loss is: 0.00013398593933622256\n",
      "test loss is 0.000175759541759365\n",
      "Batch: 24400,train loss is: 0.00012484607737143474\n",
      "test loss is 0.00013797881258637666\n",
      "Batch: 24500,train loss is: 0.00016745010258130004\n",
      "test loss is 0.00013947825791050302\n",
      "Batch: 24600,train loss is: 0.00012379610913271306\n",
      "test loss is 0.00015771768721244147\n",
      "Batch: 24700,train loss is: 9.112672651895931e-05\n",
      "test loss is 0.00013462493074079926\n",
      "Batch: 24800,train loss is: 0.00016274248201217373\n",
      "test loss is 0.0001385094493655204\n",
      "Batch: 24900,train loss is: 5.833858271309269e-05\n",
      "test loss is 0.00013325985173150373\n",
      "Batch: 25000,train loss is: 0.00010059444949872038\n",
      "test loss is 0.00013382359339944203\n",
      "Batch: 25100,train loss is: 9.184548742560064e-05\n",
      "test loss is 0.00013453612850480246\n",
      "Batch: 25200,train loss is: 0.0001385795139373081\n",
      "test loss is 0.00014011666909428337\n",
      "Batch: 25300,train loss is: 0.0001170686227344096\n",
      "test loss is 0.00013119196442585185\n",
      "Batch: 25400,train loss is: 0.00011128141277720724\n",
      "test loss is 0.00014373152398641244\n",
      "Batch: 25500,train loss is: 8.704565300774978e-05\n",
      "test loss is 0.0001318000586008821\n",
      "Batch: 25600,train loss is: 0.00012047399377494334\n",
      "test loss is 0.00013784426142845826\n",
      "Batch: 25700,train loss is: 0.00014726772178312404\n",
      "test loss is 0.00013190665790936893\n",
      "Batch: 25800,train loss is: 9.31986033136694e-05\n",
      "test loss is 0.00013745814687117348\n",
      "Batch: 25900,train loss is: 6.911598491925973e-05\n",
      "test loss is 0.00013516540544294742\n",
      "Batch: 26000,train loss is: 0.0001344667795910145\n",
      "test loss is 0.00013663093902694797\n",
      "Batch: 26100,train loss is: 0.0002109058699634826\n",
      "test loss is 0.0001329991391803616\n",
      "Batch: 26200,train loss is: 0.00025115366823697633\n",
      "test loss is 0.00013737737688408784\n",
      "Batch: 26300,train loss is: 0.00015747875714418105\n",
      "test loss is 0.00013188568159622278\n",
      "Batch: 26400,train loss is: 9.68576114617483e-05\n",
      "test loss is 0.00013117315018110252\n",
      "Batch: 26500,train loss is: 0.00013180648421099649\n",
      "test loss is 0.00013730202796795253\n",
      "Batch: 26600,train loss is: 0.0001790970565364984\n",
      "test loss is 0.00013546707400585568\n",
      "Batch: 26700,train loss is: 6.570537107233078e-05\n",
      "test loss is 0.00013757232198964426\n",
      "Batch: 26800,train loss is: 0.0002013731216358527\n",
      "test loss is 0.00013623174142411048\n",
      "Batch: 26900,train loss is: 7.250860192837168e-05\n",
      "test loss is 0.00014422124947640282\n",
      "Batch: 27000,train loss is: 0.0001189337838344877\n",
      "test loss is 0.00014786979425738763\n",
      "Batch: 27100,train loss is: 7.780529922106696e-05\n",
      "test loss is 0.00014711647730600202\n",
      "Batch: 27200,train loss is: 0.00010398276021139043\n",
      "test loss is 0.00013374789011453867\n",
      "Batch: 27300,train loss is: 0.00014457255696215317\n",
      "test loss is 0.00013563812992782142\n",
      "Batch: 27400,train loss is: 0.00010480229471547105\n",
      "test loss is 0.00013696037350981332\n",
      "Batch: 27500,train loss is: 0.00010336687347415508\n",
      "test loss is 0.00014802297175553838\n",
      "Batch: 27600,train loss is: 0.0001103604480946939\n",
      "test loss is 0.00014407017673034135\n",
      "Batch: 27700,train loss is: 0.0003876245943139948\n",
      "test loss is 0.00015159415221942685\n",
      "Batch: 27800,train loss is: 0.00010244390025755143\n",
      "test loss is 0.0001354069758613368\n",
      "Batch: 27900,train loss is: 0.00016368846650064028\n",
      "test loss is 0.00013430669075591283\n",
      "Batch: 28000,train loss is: 8.959141984797632e-05\n",
      "test loss is 0.000140962044013025\n",
      "Batch: 28100,train loss is: 0.00018865554459886167\n",
      "test loss is 0.00014915928808329054\n",
      "Batch: 28200,train loss is: 0.0001418047136595923\n",
      "test loss is 0.0001376895599566528\n",
      "Batch: 28300,train loss is: 0.0004009772281901434\n",
      "test loss is 0.0001340559712862073\n",
      "Batch: 28400,train loss is: 0.00011964613714188715\n",
      "test loss is 0.00013531495609235832\n",
      "Batch: 28500,train loss is: 0.0001472717771114764\n",
      "test loss is 0.00014290216348227983\n",
      "Batch: 28600,train loss is: 9.048533145375057e-05\n",
      "test loss is 0.00013490242805833067\n",
      "Batch: 28700,train loss is: 0.00012040480000384368\n",
      "test loss is 0.0001352464992820819\n",
      "Batch: 28800,train loss is: 0.00018010401935946986\n",
      "test loss is 0.0001314097379970534\n",
      "Batch: 28900,train loss is: 0.00014923612853881739\n",
      "test loss is 0.00016286324736403755\n",
      "Batch: 29000,train loss is: 0.00012406653229403726\n",
      "test loss is 0.00013430701743506865\n",
      "Batch: 29100,train loss is: 0.00010567863343223203\n",
      "test loss is 0.0001482617815143277\n",
      "Batch: 29200,train loss is: 0.0001088749779324859\n",
      "test loss is 0.00016172484578614433\n",
      "Batch: 29300,train loss is: 0.00010973310991192436\n",
      "test loss is 0.0001332421104913735\n",
      "Batch: 29400,train loss is: 0.00016527624625741006\n",
      "test loss is 0.0001411791992910639\n",
      "Batch: 29500,train loss is: 7.714033140503122e-05\n",
      "test loss is 0.00013431975974054637\n",
      "Batch: 29600,train loss is: 0.00022941135965741496\n",
      "test loss is 0.00015199189283375544\n",
      "Batch: 29700,train loss is: 8.225134502044376e-05\n",
      "test loss is 0.00013265854129382293\n",
      "Batch: 29800,train loss is: 0.0002473253793932526\n",
      "test loss is 0.00013190222987273094\n",
      "Batch: 29900,train loss is: 0.0003257235343445609\n",
      "test loss is 0.00013457914669169068\n",
      "Batch: 30000,train loss is: 7.946132900122054e-05\n",
      "test loss is 0.00013734528920656338\n",
      "Batch: 30100,train loss is: 9.693695088577184e-05\n",
      "test loss is 0.00014317277266502704\n",
      "Batch: 30200,train loss is: 0.00011159619087591012\n",
      "test loss is 0.00013020555066084444\n",
      "Batch: 30300,train loss is: 9.389075989839494e-05\n",
      "test loss is 0.00013426988214292445\n",
      "Batch: 30400,train loss is: 9.645662153718778e-05\n",
      "test loss is 0.0001410280783585075\n",
      "Batch: 30500,train loss is: 0.00010648480266038242\n",
      "test loss is 0.00013430733598623982\n",
      "Batch: 30600,train loss is: 8.558617489539816e-05\n",
      "test loss is 0.00013222864466589672\n",
      "Batch: 30700,train loss is: 9.878498792800714e-05\n",
      "test loss is 0.0001342953337401774\n",
      "Batch: 30800,train loss is: 8.941118348538287e-05\n",
      "test loss is 0.000136467566770736\n",
      "Batch: 30900,train loss is: 0.0001133625490681189\n",
      "test loss is 0.00013661635120588482\n",
      "Batch: 31000,train loss is: 6.804707456876624e-05\n",
      "test loss is 0.00013617902013173149\n",
      "Batch: 31100,train loss is: 0.00010468638117065326\n",
      "test loss is 0.00013748868901548088\n",
      "Batch: 31200,train loss is: 8.674715373553648e-05\n",
      "test loss is 0.0001334232553752996\n",
      "Batch: 31300,train loss is: 0.00013885980710726052\n",
      "test loss is 0.00013103237177269979\n",
      "Batch: 31400,train loss is: 0.00010324705044394738\n",
      "test loss is 0.00015491157513869219\n",
      "Batch: 31500,train loss is: 7.755930904427321e-05\n",
      "test loss is 0.0001428321094866415\n",
      "Batch: 31600,train loss is: 0.00010296565795605635\n",
      "test loss is 0.00013139167779544687\n",
      "Batch: 31700,train loss is: 8.544102676638578e-05\n",
      "test loss is 0.0001333732367674567\n",
      "Batch: 31800,train loss is: 0.000135931941293448\n",
      "test loss is 0.0001322656579918797\n",
      "Batch: 31900,train loss is: 0.00015158283697116714\n",
      "test loss is 0.000143785952039034\n",
      "Batch: 32000,train loss is: 0.0005151910804879558\n",
      "test loss is 0.00013336690363863402\n",
      "Batch: 32100,train loss is: 5.240482446882549e-05\n",
      "test loss is 0.00013218331074912795\n",
      "Batch: 32200,train loss is: 9.872875791450746e-05\n",
      "test loss is 0.0001302036058961074\n",
      "Batch: 32300,train loss is: 9.006897373940517e-05\n",
      "test loss is 0.00013192651219711218\n",
      "Batch: 32400,train loss is: 0.00010820991293780602\n",
      "test loss is 0.0001400806790006491\n",
      "Batch: 32500,train loss is: 0.0001510830662568824\n",
      "test loss is 0.00013461981795171208\n",
      "Batch: 32600,train loss is: 7.957045068348838e-05\n",
      "test loss is 0.00013602167295218454\n",
      "Batch: 32700,train loss is: 7.882900715277144e-05\n",
      "test loss is 0.0001323172506408271\n",
      "Batch: 32800,train loss is: 8.809065994161854e-05\n",
      "test loss is 0.0001468709241818974\n",
      "Batch: 32900,train loss is: 0.00013288850326811127\n",
      "test loss is 0.00013480218323028614\n",
      "Batch: 33000,train loss is: 6.070392498267438e-05\n",
      "test loss is 0.00013268307977098648\n",
      "Batch: 33100,train loss is: 9.090847457033322e-05\n",
      "test loss is 0.00014461554062153854\n",
      "Batch: 33200,train loss is: 0.00011890471060772175\n",
      "test loss is 0.00015224723555894168\n",
      "Batch: 33300,train loss is: 0.00010712595693343889\n",
      "test loss is 0.00015719638428572059\n",
      "Batch: 33400,train loss is: 9.252013939827565e-05\n",
      "test loss is 0.00013280902622625146\n",
      "Batch: 33500,train loss is: 0.00010182308236710129\n",
      "test loss is 0.00013370816599330736\n",
      "Batch: 33600,train loss is: 7.926258477589134e-05\n",
      "test loss is 0.0001354630290901651\n",
      "Batch: 33700,train loss is: 9.741820964986429e-05\n",
      "test loss is 0.00013924263239119573\n",
      "Batch: 33800,train loss is: 0.00011997726577724627\n",
      "test loss is 0.00013539243118825954\n",
      "Batch: 33900,train loss is: 0.00010794319033749677\n",
      "test loss is 0.00013211167843219125\n",
      "-----------------------Epoch: 2----------------------------------\n",
      "Batch: 0,train loss is: 0.0001212976610021914\n",
      "test loss is 0.00013752936430130562\n",
      "Batch: 100,train loss is: 0.0001512630729783705\n",
      "test loss is 0.00013441184390581686\n",
      "Batch: 200,train loss is: 0.00011835691778761272\n",
      "test loss is 0.00013134004770401335\n",
      "Batch: 300,train loss is: 8.987695764474927e-05\n",
      "test loss is 0.0001334521749443152\n",
      "Batch: 400,train loss is: 8.762552288592884e-05\n",
      "test loss is 0.0001297327587609882\n",
      "Batch: 500,train loss is: 0.00013556300423195043\n",
      "test loss is 0.00015722156380350424\n",
      "Batch: 600,train loss is: 0.00011196877445530049\n",
      "test loss is 0.0001304351719979481\n",
      "Batch: 700,train loss is: 7.778477960177205e-05\n",
      "test loss is 0.00013436295710480924\n",
      "Batch: 800,train loss is: 0.00017538149127084953\n",
      "test loss is 0.00013635200719312762\n",
      "Batch: 900,train loss is: 6.013951588594739e-05\n",
      "test loss is 0.00013871233693995994\n",
      "Batch: 1000,train loss is: 0.0001048548483295056\n",
      "test loss is 0.00013375090346260675\n",
      "Batch: 1100,train loss is: 0.00025274381860554066\n",
      "test loss is 0.0001362711277702777\n",
      "Batch: 1200,train loss is: 5.31210844005667e-05\n",
      "test loss is 0.00013683192086092616\n",
      "Batch: 1300,train loss is: 0.00011912425664958291\n",
      "test loss is 0.00013439322968144318\n",
      "Batch: 1400,train loss is: 0.00010437704064594675\n",
      "test loss is 0.000131017051529472\n",
      "Batch: 1500,train loss is: 0.0001022883707932019\n",
      "test loss is 0.00013616018702073383\n",
      "Batch: 1600,train loss is: 6.89464406299956e-05\n",
      "test loss is 0.00013582561948567625\n",
      "Batch: 1700,train loss is: 0.00014977914812261032\n",
      "test loss is 0.00014866539115496988\n",
      "Batch: 1800,train loss is: 8.13752701583277e-05\n",
      "test loss is 0.0001299842573205479\n",
      "Batch: 1900,train loss is: 6.533450944117854e-05\n",
      "test loss is 0.00013695781581278375\n",
      "Batch: 2000,train loss is: 0.0001355220990643965\n",
      "test loss is 0.00013463864838320777\n",
      "Batch: 2100,train loss is: 7.830846962346448e-05\n",
      "test loss is 0.00013476533236441682\n",
      "Batch: 2200,train loss is: 0.00013711669384547384\n",
      "test loss is 0.00013489863617744893\n",
      "Batch: 2300,train loss is: 0.00010351319295853828\n",
      "test loss is 0.0001310284741783209\n",
      "Batch: 2400,train loss is: 0.000115065552140853\n",
      "test loss is 0.0001323573001772966\n",
      "Batch: 2500,train loss is: 0.00013582397197149304\n",
      "test loss is 0.000130117930324116\n",
      "Batch: 2600,train loss is: 9.018020565536639e-05\n",
      "test loss is 0.00013179646984256722\n",
      "Batch: 2700,train loss is: 0.00015096222126773074\n",
      "test loss is 0.0001398471015986072\n",
      "Batch: 2800,train loss is: 0.00010934714015694316\n",
      "test loss is 0.00013809915282911883\n",
      "Batch: 2900,train loss is: 0.00012605699537178994\n",
      "test loss is 0.000130968399433873\n",
      "Batch: 3000,train loss is: 9.888204102314642e-05\n",
      "test loss is 0.00013376416335250023\n",
      "Batch: 3100,train loss is: 8.715883840584171e-05\n",
      "test loss is 0.00012941501041402825\n",
      "Batch: 3200,train loss is: 0.00019572451340893802\n",
      "test loss is 0.000136490675104142\n",
      "Batch: 3300,train loss is: 0.00010491345820373277\n",
      "test loss is 0.00013143594994666092\n",
      "Batch: 3400,train loss is: 0.0001025145795414536\n",
      "test loss is 0.00014830653603847862\n",
      "Batch: 3500,train loss is: 9.720594891789552e-05\n",
      "test loss is 0.00013815492096674653\n",
      "Batch: 3600,train loss is: 8.587173837061503e-05\n",
      "test loss is 0.0001357835893444393\n",
      "Batch: 3700,train loss is: 6.228822207122312e-05\n",
      "test loss is 0.00012948155664773518\n",
      "Batch: 3800,train loss is: 7.959278852067632e-05\n",
      "test loss is 0.0001321805583556094\n",
      "Batch: 3900,train loss is: 0.0001217463515512998\n",
      "test loss is 0.00013400805624756059\n",
      "Batch: 4000,train loss is: 6.13169461565727e-05\n",
      "test loss is 0.00013395997978721545\n",
      "Batch: 4100,train loss is: 0.00013499081774664894\n",
      "test loss is 0.00014109813453683085\n",
      "Batch: 4200,train loss is: 0.00012762253186738263\n",
      "test loss is 0.00014468261785727712\n",
      "Batch: 4300,train loss is: 0.00010634171678024999\n",
      "test loss is 0.00013129432998205848\n",
      "Batch: 4400,train loss is: 8.113271160220709e-05\n",
      "test loss is 0.00013360962581777656\n",
      "Batch: 4500,train loss is: 0.00010019173233650172\n",
      "test loss is 0.0001302607161236737\n",
      "Batch: 4600,train loss is: 9.487476559977321e-05\n",
      "test loss is 0.00013262108991867117\n",
      "Batch: 4700,train loss is: 0.00017015070213668117\n",
      "test loss is 0.0001410873620006018\n",
      "Batch: 4800,train loss is: 0.00029569659108732223\n",
      "test loss is 0.00015302875876375602\n",
      "Batch: 4900,train loss is: 0.0001062730069231257\n",
      "test loss is 0.00013450759666420073\n",
      "Batch: 5000,train loss is: 7.00122979280849e-05\n",
      "test loss is 0.00013314073752634814\n",
      "Batch: 5100,train loss is: 7.881121547579648e-05\n",
      "test loss is 0.00013057083716787844\n",
      "Batch: 5200,train loss is: 9.258167389730717e-05\n",
      "test loss is 0.00013854037933170663\n",
      "Batch: 5300,train loss is: 0.00010499744422552991\n",
      "test loss is 0.0001370992901379685\n",
      "Batch: 5400,train loss is: 0.00022641992462517452\n",
      "test loss is 0.00014062917962355564\n",
      "Batch: 5500,train loss is: 8.574261917877059e-05\n",
      "test loss is 0.00015099419620962038\n",
      "Batch: 5600,train loss is: 0.00016653296942949523\n",
      "test loss is 0.00013329733310265884\n",
      "Batch: 5700,train loss is: 0.00013872560023522177\n",
      "test loss is 0.00013957554742804887\n",
      "Batch: 5800,train loss is: 0.00017738412544115488\n",
      "test loss is 0.00013632458313151072\n",
      "Batch: 5900,train loss is: 0.00037660989697545634\n",
      "test loss is 0.00013662787344476612\n",
      "Batch: 6000,train loss is: 0.00017127065190310488\n",
      "test loss is 0.00013722551459354924\n",
      "Batch: 6100,train loss is: 7.147429118256519e-05\n",
      "test loss is 0.00013326580409815707\n",
      "Batch: 6200,train loss is: 9.395082280118914e-05\n",
      "test loss is 0.00013139261458763\n",
      "Batch: 6300,train loss is: 0.0001555162431085372\n",
      "test loss is 0.0001543467381185215\n",
      "Batch: 6400,train loss is: 9.611619130672949e-05\n",
      "test loss is 0.00013378611172951317\n",
      "Batch: 6500,train loss is: 7.668053636893834e-05\n",
      "test loss is 0.00013665049695979047\n",
      "Batch: 6600,train loss is: 0.00029520769481238815\n",
      "test loss is 0.000130315393102758\n",
      "Batch: 6700,train loss is: 0.00010488801102960593\n",
      "test loss is 0.0001311157660721072\n",
      "Batch: 6800,train loss is: 0.00015434312599095265\n",
      "test loss is 0.00013563060415959917\n",
      "Batch: 6900,train loss is: 0.00010281546553509653\n",
      "test loss is 0.00013241536244733675\n",
      "Batch: 7000,train loss is: 0.0001414977070338552\n",
      "test loss is 0.00013672984047492383\n",
      "Batch: 7100,train loss is: 0.00014418918521530102\n",
      "test loss is 0.00015388619785701026\n",
      "Batch: 7200,train loss is: 8.093605729247399e-05\n",
      "test loss is 0.00012923069884209659\n",
      "Batch: 7300,train loss is: 0.0002447087828980914\n",
      "test loss is 0.0001357729343851877\n",
      "Batch: 7400,train loss is: 9.874367391058363e-05\n",
      "test loss is 0.0001672460638299777\n",
      "Batch: 7500,train loss is: 0.0001272985161076533\n",
      "test loss is 0.0001376969751411168\n",
      "Batch: 7600,train loss is: 0.0001944309599875191\n",
      "test loss is 0.00015384730907870797\n",
      "Batch: 7700,train loss is: 0.00018448828799878743\n",
      "test loss is 0.0001312237974486502\n",
      "Batch: 7800,train loss is: 6.771015688430222e-05\n",
      "test loss is 0.0001286622329495303\n",
      "Batch: 7900,train loss is: 8.335903631282491e-05\n",
      "test loss is 0.00012937160522067645\n",
      "Batch: 8000,train loss is: 5.3601813245690276e-05\n",
      "test loss is 0.0001342550713054908\n",
      "Batch: 8100,train loss is: 0.00021187990700136515\n",
      "test loss is 0.00013683529510455445\n",
      "Batch: 8200,train loss is: 0.00010379914101619395\n",
      "test loss is 0.00012945805884163013\n",
      "Batch: 8300,train loss is: 7.144682111556703e-05\n",
      "test loss is 0.00013489254916119553\n",
      "Batch: 8400,train loss is: 0.00012685482789502728\n",
      "test loss is 0.00013829510085983963\n",
      "Batch: 8500,train loss is: 0.000239539657340138\n",
      "test loss is 0.0001382700711215743\n",
      "Batch: 8600,train loss is: 0.00010046420444975337\n",
      "test loss is 0.00014720650867881992\n",
      "Batch: 8700,train loss is: 0.00012208546840098463\n",
      "test loss is 0.0001424367077747562\n",
      "Batch: 8800,train loss is: 8.919468583089949e-05\n",
      "test loss is 0.00013064926506083088\n",
      "Batch: 8900,train loss is: 0.00011951661707184272\n",
      "test loss is 0.0001273038287247423\n",
      "Batch: 9000,train loss is: 8.026726775298647e-05\n",
      "test loss is 0.00013136880631838248\n",
      "Batch: 9100,train loss is: 0.00010653470566925507\n",
      "test loss is 0.00013256266447495287\n",
      "Batch: 9200,train loss is: 0.0001389383819184817\n",
      "test loss is 0.00013418882733718122\n",
      "Batch: 9300,train loss is: 0.0001685160447604154\n",
      "test loss is 0.00013044807499595405\n",
      "Batch: 9400,train loss is: 0.00014295172871649087\n",
      "test loss is 0.00014073424145390274\n",
      "Batch: 9500,train loss is: 7.720453468030908e-05\n",
      "test loss is 0.0001380666421579295\n",
      "Batch: 9600,train loss is: 9.083878099991902e-05\n",
      "test loss is 0.00013052929204479096\n",
      "Batch: 9700,train loss is: 6.955493708546717e-05\n",
      "test loss is 0.00013299697562882195\n",
      "Batch: 9800,train loss is: 7.447102175660119e-05\n",
      "test loss is 0.00013085113021798619\n",
      "Batch: 9900,train loss is: 7.928419304364863e-05\n",
      "test loss is 0.00013026969762787544\n",
      "Batch: 10000,train loss is: 7.149753311278617e-05\n",
      "test loss is 0.00013793384117351034\n",
      "Batch: 10100,train loss is: 0.000216921084407833\n",
      "test loss is 0.00013143797499633254\n",
      "Batch: 10200,train loss is: 0.0003483254538308543\n",
      "test loss is 0.00013076968947567057\n",
      "Batch: 10300,train loss is: 0.00013034893744517798\n",
      "test loss is 0.0001350092144035184\n",
      "Batch: 10400,train loss is: 6.018116492288365e-05\n",
      "test loss is 0.00013575887988622235\n",
      "Batch: 10500,train loss is: 0.00012680495305896535\n",
      "test loss is 0.00013249001103576592\n",
      "Batch: 10600,train loss is: 8.191269933722276e-05\n",
      "test loss is 0.00013297579850411506\n",
      "Batch: 10700,train loss is: 0.00010031341641627244\n",
      "test loss is 0.00012796475967334636\n",
      "Batch: 10800,train loss is: 0.00010881410975104991\n",
      "test loss is 0.00013655169035323085\n",
      "Batch: 10900,train loss is: 0.00011537248636290308\n",
      "test loss is 0.00013105430187808484\n",
      "Batch: 11000,train loss is: 0.00015365732562674296\n",
      "test loss is 0.00012895713999457923\n",
      "Batch: 11100,train loss is: 0.00015288062719334114\n",
      "test loss is 0.0001320546873626255\n",
      "Batch: 11200,train loss is: 0.00016112475750538097\n",
      "test loss is 0.0001304902791979427\n",
      "Batch: 11300,train loss is: 8.593755520040244e-05\n",
      "test loss is 0.0001377299807085971\n",
      "Batch: 11400,train loss is: 0.00024213941797222857\n",
      "test loss is 0.00012939678416799678\n",
      "Batch: 11500,train loss is: 7.092089628077561e-05\n",
      "test loss is 0.00013061470729564345\n",
      "Batch: 11600,train loss is: 0.0001675238966165339\n",
      "test loss is 0.0001384955380816616\n",
      "Batch: 11700,train loss is: 0.00010395933464626147\n",
      "test loss is 0.00013011613390556605\n",
      "Batch: 11800,train loss is: 0.00011000106475849753\n",
      "test loss is 0.00012938577669849272\n",
      "Batch: 11900,train loss is: 0.00011011670523354607\n",
      "test loss is 0.00014628363257528025\n",
      "Batch: 12000,train loss is: 0.00010767325061469963\n",
      "test loss is 0.0001337490682021631\n",
      "Batch: 12100,train loss is: 0.00015693069710265626\n",
      "test loss is 0.00013002079875868068\n",
      "Batch: 12200,train loss is: 9.285650319554497e-05\n",
      "test loss is 0.00013156968626316628\n",
      "Batch: 12300,train loss is: 9.324593926760298e-05\n",
      "test loss is 0.0001370056660337746\n",
      "Batch: 12400,train loss is: 0.0001075952096699056\n",
      "test loss is 0.0001305095167545683\n",
      "Batch: 12500,train loss is: 0.0002324134296364633\n",
      "test loss is 0.0001331734480518976\n",
      "Batch: 12600,train loss is: 7.214895460602409e-05\n",
      "test loss is 0.0001286935518239817\n",
      "Batch: 12700,train loss is: 0.00010542977188310668\n",
      "test loss is 0.000132110204981369\n",
      "Batch: 12800,train loss is: 5.257140506036953e-05\n",
      "test loss is 0.0001313430375546032\n",
      "Batch: 12900,train loss is: 0.00018467639967422367\n",
      "test loss is 0.00013743401566679707\n",
      "Batch: 13000,train loss is: 9.219736870032554e-05\n",
      "test loss is 0.00012753559410804576\n",
      "Batch: 13100,train loss is: 0.0001958809483675311\n",
      "test loss is 0.00012694437474353615\n",
      "Batch: 13200,train loss is: 0.00010819218647295307\n",
      "test loss is 0.00014547080917267738\n",
      "Batch: 13300,train loss is: 0.00013203835659501508\n",
      "test loss is 0.00015679543246580703\n",
      "Batch: 13400,train loss is: 9.661524870135444e-05\n",
      "test loss is 0.00012753426321263322\n",
      "Batch: 13500,train loss is: 7.197538059741585e-05\n",
      "test loss is 0.0001325887233208799\n",
      "Batch: 13600,train loss is: 7.606353973682413e-05\n",
      "test loss is 0.0001425152909578158\n",
      "Batch: 13700,train loss is: 0.00012045928613580252\n",
      "test loss is 0.0001282460520597712\n",
      "Batch: 13800,train loss is: 7.629141214122664e-05\n",
      "test loss is 0.00014084617206463256\n",
      "Batch: 13900,train loss is: 6.617409761276724e-05\n",
      "test loss is 0.00013186037254373428\n",
      "Batch: 14000,train loss is: 0.00012672171042240124\n",
      "test loss is 0.000135196352053811\n",
      "Batch: 14100,train loss is: 7.771915204299726e-05\n",
      "test loss is 0.00012998118642295963\n",
      "Batch: 14200,train loss is: 7.723878281532623e-05\n",
      "test loss is 0.0001300197211091254\n",
      "Batch: 14300,train loss is: 9.095901830122242e-05\n",
      "test loss is 0.00012928189856333273\n",
      "Batch: 14400,train loss is: 0.00020132613333445813\n",
      "test loss is 0.0001283919411461711\n",
      "Batch: 14500,train loss is: 9.65585765356379e-05\n",
      "test loss is 0.00012870993839489322\n",
      "Batch: 14600,train loss is: 0.000156628688358139\n",
      "test loss is 0.00013823365470631775\n",
      "Batch: 14700,train loss is: 0.00010461995772567651\n",
      "test loss is 0.0001287661326748243\n",
      "Batch: 14800,train loss is: 0.0001896251833406173\n",
      "test loss is 0.00013208112230354306\n",
      "Batch: 14900,train loss is: 9.08366514473256e-05\n",
      "test loss is 0.00012630811939090867\n",
      "Batch: 15000,train loss is: 0.00010839810621224534\n",
      "test loss is 0.00013197266652863142\n",
      "Batch: 15100,train loss is: 0.00015747447278421356\n",
      "test loss is 0.0001329476814667945\n",
      "Batch: 15200,train loss is: 0.00016370302457753476\n",
      "test loss is 0.0001320553901895673\n",
      "Batch: 15300,train loss is: 0.00015558231448292739\n",
      "test loss is 0.00013066289387036575\n",
      "Batch: 15400,train loss is: 7.625255816569114e-05\n",
      "test loss is 0.0001273748972124046\n",
      "Batch: 15500,train loss is: 9.562933248609066e-05\n",
      "test loss is 0.0001354240599870646\n",
      "Batch: 15600,train loss is: 0.00022481918556853205\n",
      "test loss is 0.00013022765719449597\n",
      "Batch: 15700,train loss is: 8.07691482577731e-05\n",
      "test loss is 0.00013512534917243412\n",
      "Batch: 15800,train loss is: 8.234454929976358e-05\n",
      "test loss is 0.00013599310276220572\n",
      "Batch: 15900,train loss is: 0.0001477202692749967\n",
      "test loss is 0.0001339016634442363\n",
      "Batch: 16000,train loss is: 0.00016308797180706477\n",
      "test loss is 0.0001287743376190302\n",
      "Batch: 16100,train loss is: 0.00021049902655111068\n",
      "test loss is 0.00012745645958465196\n",
      "Batch: 16200,train loss is: 9.529106998264763e-05\n",
      "test loss is 0.0001304477662835152\n",
      "Batch: 16300,train loss is: 0.00010865750629172869\n",
      "test loss is 0.0001359549769679479\n",
      "Batch: 16400,train loss is: 7.663443873865573e-05\n",
      "test loss is 0.0001338094061880667\n",
      "Batch: 16500,train loss is: 9.195290554132895e-05\n",
      "test loss is 0.0001285039158489887\n",
      "Batch: 16600,train loss is: 9.238489839127946e-05\n",
      "test loss is 0.0001292922742391434\n",
      "Batch: 16700,train loss is: 0.00021349467453366072\n",
      "test loss is 0.0001375317761709057\n",
      "Batch: 16800,train loss is: 0.00015025728151846614\n",
      "test loss is 0.00013536975538292013\n",
      "Batch: 16900,train loss is: 0.00010072328206448313\n",
      "test loss is 0.00013996051081574927\n",
      "Batch: 17000,train loss is: 0.0001178666380334494\n",
      "test loss is 0.00013770920249216485\n",
      "Batch: 17100,train loss is: 0.0001146639309072838\n",
      "test loss is 0.00012858306565219558\n",
      "Batch: 17200,train loss is: 6.787971050425904e-05\n",
      "test loss is 0.00013005180013532484\n",
      "Batch: 17300,train loss is: 8.114406542378024e-05\n",
      "test loss is 0.00013950486532714488\n",
      "Batch: 17400,train loss is: 6.412059280589908e-05\n",
      "test loss is 0.0001372369317138558\n",
      "Batch: 17500,train loss is: 0.00012728381729868382\n",
      "test loss is 0.00013015855131804307\n",
      "Batch: 17600,train loss is: 8.554455202494995e-05\n",
      "test loss is 0.00013132062967702898\n",
      "Batch: 17700,train loss is: 6.213712646714672e-05\n",
      "test loss is 0.0001267288743414211\n",
      "Batch: 17800,train loss is: 0.0002704784830539957\n",
      "test loss is 0.00013087359969142238\n",
      "Batch: 17900,train loss is: 5.7680165934384976e-05\n",
      "test loss is 0.0001288550097756537\n",
      "Batch: 18000,train loss is: 9.264353290853926e-05\n",
      "test loss is 0.00013378825061014182\n",
      "Batch: 18100,train loss is: 0.0002165124545952981\n",
      "test loss is 0.0001301060822071112\n",
      "Batch: 18200,train loss is: 9.997379979973793e-05\n",
      "test loss is 0.0001278418955553418\n",
      "Batch: 18300,train loss is: 0.00014124529513327656\n",
      "test loss is 0.00012570187604257568\n",
      "Batch: 18400,train loss is: 0.00011583073994141591\n",
      "test loss is 0.0001255199703897519\n",
      "Batch: 18500,train loss is: 0.00017370308030096945\n",
      "test loss is 0.00015687587017925177\n",
      "Batch: 18600,train loss is: 8.995974919694112e-05\n",
      "test loss is 0.00013614229358046453\n",
      "Batch: 18700,train loss is: 0.00016431469471313247\n",
      "test loss is 0.00013261128216761177\n",
      "Batch: 18800,train loss is: 9.45767100401484e-05\n",
      "test loss is 0.00012889629425846287\n",
      "Batch: 18900,train loss is: 0.00014601993764135494\n",
      "test loss is 0.00013420627519142764\n",
      "Batch: 19000,train loss is: 6.617237473775647e-05\n",
      "test loss is 0.00013121841383830678\n",
      "Batch: 19100,train loss is: 0.00015589968576732127\n",
      "test loss is 0.00013757312833575226\n",
      "Batch: 19200,train loss is: 0.00011850076812376432\n",
      "test loss is 0.00012722915895637708\n",
      "Batch: 19300,train loss is: 0.00012900982640074146\n",
      "test loss is 0.00012726191331707457\n",
      "Batch: 19400,train loss is: 0.00010206604162625908\n",
      "test loss is 0.0001303094116789404\n",
      "Batch: 19500,train loss is: 9.219763413184791e-05\n",
      "test loss is 0.0001502387885712862\n",
      "Batch: 19600,train loss is: 0.00010101481632787955\n",
      "test loss is 0.00013130290236552244\n",
      "Batch: 19700,train loss is: 0.00015509458921352488\n",
      "test loss is 0.0001291336068680515\n",
      "Batch: 19800,train loss is: 8.541287768256519e-05\n",
      "test loss is 0.00013238845818914656\n",
      "Batch: 19900,train loss is: 7.512555432914062e-05\n",
      "test loss is 0.00014034959984645772\n",
      "Batch: 20000,train loss is: 0.00020132846155518792\n",
      "test loss is 0.00014321146279382623\n",
      "Batch: 20100,train loss is: 0.00016736449493361287\n",
      "test loss is 0.0001433228964329766\n",
      "Batch: 20200,train loss is: 8.669614582117378e-05\n",
      "test loss is 0.00013322715438856815\n",
      "Batch: 20300,train loss is: 5.614507362295536e-05\n",
      "test loss is 0.00014455306739412152\n",
      "Batch: 20400,train loss is: 0.0001353367855309675\n",
      "test loss is 0.00014481944231818707\n",
      "Batch: 20500,train loss is: 0.00011998834199585873\n",
      "test loss is 0.00012590282574991043\n",
      "Batch: 20600,train loss is: 0.00021577056603461006\n",
      "test loss is 0.00013347086424468714\n",
      "Batch: 20700,train loss is: 0.00012772537341560863\n",
      "test loss is 0.000134771969629835\n",
      "Batch: 20800,train loss is: 8.641944852930548e-05\n",
      "test loss is 0.00013398256778621157\n",
      "Batch: 20900,train loss is: 9.843523680765573e-05\n",
      "test loss is 0.00013324406135708326\n",
      "Batch: 21000,train loss is: 4.7150620667846336e-05\n",
      "test loss is 0.00012585710732118845\n",
      "Batch: 21100,train loss is: 8.908592325136314e-05\n",
      "test loss is 0.00012570591498224068\n",
      "Batch: 21200,train loss is: 7.656580589387724e-05\n",
      "test loss is 0.000134652754693752\n",
      "Batch: 21300,train loss is: 0.00010379737433522163\n",
      "test loss is 0.0001334702415779982\n",
      "Batch: 21400,train loss is: 0.00013091739876564014\n",
      "test loss is 0.0001262972612220191\n",
      "Batch: 21500,train loss is: 0.00010010388393076399\n",
      "test loss is 0.00013558343178024182\n",
      "Batch: 21600,train loss is: 0.00012060071688281695\n",
      "test loss is 0.00013796048552963105\n",
      "Batch: 21700,train loss is: 8.306222578251493e-05\n",
      "test loss is 0.00013366529458985597\n",
      "Batch: 21800,train loss is: 0.0001586061555677502\n",
      "test loss is 0.00012905443770250944\n",
      "Batch: 21900,train loss is: 0.0001078764075009881\n",
      "test loss is 0.0001420230612282856\n",
      "Batch: 22000,train loss is: 0.00013198565648406816\n",
      "test loss is 0.0001549865311916645\n",
      "Batch: 22100,train loss is: 7.984493879203934e-05\n",
      "test loss is 0.00012678045051740659\n",
      "Batch: 22200,train loss is: 6.562692427047356e-05\n",
      "test loss is 0.00013117342463656213\n",
      "Batch: 22300,train loss is: 0.00010839468389819512\n",
      "test loss is 0.0001460740355948034\n",
      "Batch: 22400,train loss is: 0.00012122678360465808\n",
      "test loss is 0.00013044892996884574\n",
      "Batch: 22500,train loss is: 8.478156700550064e-05\n",
      "test loss is 0.00013075433891200378\n",
      "Batch: 22600,train loss is: 0.0001257622076647945\n",
      "test loss is 0.0001344718679609586\n",
      "Batch: 22700,train loss is: 7.386735334405951e-05\n",
      "test loss is 0.00012508544845383517\n",
      "Batch: 22800,train loss is: 0.00011944829631382662\n",
      "test loss is 0.00013645801897814512\n",
      "Batch: 22900,train loss is: 7.673958693544241e-05\n",
      "test loss is 0.0001292973099345098\n",
      "Batch: 23000,train loss is: 0.0002053985797053486\n",
      "test loss is 0.00013690512790268046\n",
      "Batch: 23100,train loss is: 9.885822085040568e-05\n",
      "test loss is 0.00012652107110024878\n",
      "Batch: 23200,train loss is: 8.563799184020843e-05\n",
      "test loss is 0.0001254009778254855\n",
      "Batch: 23300,train loss is: 6.936838671506574e-05\n",
      "test loss is 0.00012748523718437312\n",
      "Batch: 23400,train loss is: 9.241514008632243e-05\n",
      "test loss is 0.00012995686085604826\n",
      "Batch: 23500,train loss is: 0.00017278763195821515\n",
      "test loss is 0.00012827345880599134\n",
      "Batch: 23600,train loss is: 7.613549660934809e-05\n",
      "test loss is 0.000135201268599377\n",
      "Batch: 23700,train loss is: 0.0001476421174127912\n",
      "test loss is 0.00012463493276419896\n",
      "Batch: 23800,train loss is: 0.00020026911932047705\n",
      "test loss is 0.0001292794424633413\n",
      "Batch: 23900,train loss is: 0.00042584586445949867\n",
      "test loss is 0.0001284520237254165\n",
      "Batch: 24000,train loss is: 8.890741347516427e-05\n",
      "test loss is 0.00015288634838894614\n",
      "Batch: 24100,train loss is: 0.00012007254170442476\n",
      "test loss is 0.00012756207122730724\n",
      "Batch: 24200,train loss is: 0.00020505193068468593\n",
      "test loss is 0.00013597870542523765\n",
      "Batch: 24300,train loss is: 0.00012598801291280085\n",
      "test loss is 0.0001627671537725052\n",
      "Batch: 24400,train loss is: 0.0001241670849355435\n",
      "test loss is 0.0001293903407889513\n",
      "Batch: 24500,train loss is: 0.0001571584242938873\n",
      "test loss is 0.00013397535614461146\n",
      "Batch: 24600,train loss is: 0.00011775341197474298\n",
      "test loss is 0.00015108493244976427\n",
      "Batch: 24700,train loss is: 8.748224762943177e-05\n",
      "test loss is 0.00012765020149646221\n",
      "Batch: 24800,train loss is: 0.00016084458066791026\n",
      "test loss is 0.00013166488467958586\n",
      "Batch: 24900,train loss is: 5.5238542133980986e-05\n",
      "test loss is 0.00012615345400043517\n",
      "Batch: 25000,train loss is: 9.673395748204102e-05\n",
      "test loss is 0.00012728720385230972\n",
      "Batch: 25100,train loss is: 8.265441307830677e-05\n",
      "test loss is 0.00012723459538541477\n",
      "Batch: 25200,train loss is: 0.00012919107524461516\n",
      "test loss is 0.00013276124401941498\n",
      "Batch: 25300,train loss is: 0.00011497605507977197\n",
      "test loss is 0.00012481420166187072\n",
      "Batch: 25400,train loss is: 0.000106156175046406\n",
      "test loss is 0.00013684448307694155\n",
      "Batch: 25500,train loss is: 8.31136440821523e-05\n",
      "test loss is 0.0001256492646369391\n",
      "Batch: 25600,train loss is: 0.00011733034567767822\n",
      "test loss is 0.00013238997321665983\n",
      "Batch: 25700,train loss is: 0.00014069478112109252\n",
      "test loss is 0.00012617119642409474\n",
      "Batch: 25800,train loss is: 9.031004308278239e-05\n",
      "test loss is 0.0001309026854496859\n",
      "Batch: 25900,train loss is: 6.684507900231703e-05\n",
      "test loss is 0.00012819698740585507\n",
      "Batch: 26000,train loss is: 0.00012823655532554499\n",
      "test loss is 0.00013027102506619562\n",
      "Batch: 26100,train loss is: 0.0002009832548722561\n",
      "test loss is 0.00012587724220072534\n",
      "Batch: 26200,train loss is: 0.00023014143376809428\n",
      "test loss is 0.00013130249846423178\n",
      "Batch: 26300,train loss is: 0.00015023496781874505\n",
      "test loss is 0.00012520323522963904\n",
      "Batch: 26400,train loss is: 9.212440540690568e-05\n",
      "test loss is 0.00012450877097791642\n",
      "Batch: 26500,train loss is: 0.00012408401106609547\n",
      "test loss is 0.00012887052600143764\n",
      "Batch: 26600,train loss is: 0.00017378680987981788\n",
      "test loss is 0.00012802424865056544\n",
      "Batch: 26700,train loss is: 6.246819289948794e-05\n",
      "test loss is 0.00013092457590793303\n",
      "Batch: 26800,train loss is: 0.00019049440951955575\n",
      "test loss is 0.00012924239763306694\n",
      "Batch: 26900,train loss is: 6.735112211430383e-05\n",
      "test loss is 0.00014009936031743103\n",
      "Batch: 27000,train loss is: 0.00011281125242602224\n",
      "test loss is 0.00014079648281299154\n",
      "Batch: 27100,train loss is: 7.191595188281034e-05\n",
      "test loss is 0.00014149510717982564\n",
      "Batch: 27200,train loss is: 0.00010180261644258023\n",
      "test loss is 0.0001269243493083151\n",
      "Batch: 27300,train loss is: 0.00013505463866429605\n",
      "test loss is 0.0001268866910270262\n",
      "Batch: 27400,train loss is: 0.00010163610140592978\n",
      "test loss is 0.00013021447825613977\n",
      "Batch: 27500,train loss is: 9.673109474581295e-05\n",
      "test loss is 0.00014290713790597855\n",
      "Batch: 27600,train loss is: 0.00010925489979806999\n",
      "test loss is 0.00014210946897848456\n",
      "Batch: 27700,train loss is: 0.00035608521416757936\n",
      "test loss is 0.00014649019719617423\n",
      "Batch: 27800,train loss is: 9.8987791775063e-05\n",
      "test loss is 0.00012904172448251495\n",
      "Batch: 27900,train loss is: 0.00015110258025435438\n",
      "test loss is 0.0001274098020940348\n",
      "Batch: 28000,train loss is: 8.363692894704133e-05\n",
      "test loss is 0.00013334039398702366\n",
      "Batch: 28100,train loss is: 0.00018229715891518485\n",
      "test loss is 0.0001424401453716022\n",
      "Batch: 28200,train loss is: 0.00013115354627172426\n",
      "test loss is 0.00013130341921075998\n",
      "Batch: 28300,train loss is: 0.0003712640620213665\n",
      "test loss is 0.00012738686292468307\n",
      "Batch: 28400,train loss is: 0.00011623243383153724\n",
      "test loss is 0.00012896615226579622\n",
      "Batch: 28500,train loss is: 0.00013091415731127367\n",
      "test loss is 0.00013566704544952507\n",
      "Batch: 28600,train loss is: 8.782633223731045e-05\n",
      "test loss is 0.0001282616316570807\n",
      "Batch: 28700,train loss is: 0.00011302110098086099\n",
      "test loss is 0.0001283979266711257\n",
      "Batch: 28800,train loss is: 0.00017515618700997095\n",
      "test loss is 0.0001248721184544382\n",
      "Batch: 28900,train loss is: 0.00014010052907968763\n",
      "test loss is 0.0001526936557254442\n",
      "Batch: 29000,train loss is: 0.00011889509130308669\n",
      "test loss is 0.00012742892435526216\n",
      "Batch: 29100,train loss is: 0.00010236735276627972\n",
      "test loss is 0.00014063720865171647\n",
      "Batch: 29200,train loss is: 0.00010694081018284327\n",
      "test loss is 0.00015747129375667525\n",
      "Batch: 29300,train loss is: 0.0001048038436340964\n",
      "test loss is 0.00012707290592658198\n",
      "Batch: 29400,train loss is: 0.00015805767555309274\n",
      "test loss is 0.00013502300526030544\n",
      "Batch: 29500,train loss is: 7.163161489512312e-05\n",
      "test loss is 0.00012738997600740624\n",
      "Batch: 29600,train loss is: 0.00022056707384079707\n",
      "test loss is 0.00014308935497301466\n",
      "Batch: 29700,train loss is: 7.875310396182843e-05\n",
      "test loss is 0.00012603599246128902\n",
      "Batch: 29800,train loss is: 0.0002361597691118801\n",
      "test loss is 0.00012528161410611514\n",
      "Batch: 29900,train loss is: 0.0003095571754160364\n",
      "test loss is 0.0001277219362702307\n",
      "Batch: 30000,train loss is: 7.844976614389074e-05\n",
      "test loss is 0.00013050491360467302\n",
      "Batch: 30100,train loss is: 9.550112060750423e-05\n",
      "test loss is 0.0001370749553088858\n",
      "Batch: 30200,train loss is: 0.00010206180854200348\n",
      "test loss is 0.00012373950549647754\n",
      "Batch: 30300,train loss is: 9.348117247465603e-05\n",
      "test loss is 0.000127370004680312\n",
      "Batch: 30400,train loss is: 9.377978529316379e-05\n",
      "test loss is 0.00013481592011027534\n",
      "Batch: 30500,train loss is: 0.0001008798974063701\n",
      "test loss is 0.0001272750994488876\n",
      "Batch: 30600,train loss is: 8.183271786723172e-05\n",
      "test loss is 0.00012593684063807155\n",
      "Batch: 30700,train loss is: 9.600897258753662e-05\n",
      "test loss is 0.0001292375947804811\n",
      "Batch: 30800,train loss is: 8.356285415157331e-05\n",
      "test loss is 0.00012997058658390293\n",
      "Batch: 30900,train loss is: 0.0001068221100909196\n",
      "test loss is 0.00012960880696922877\n",
      "Batch: 31000,train loss is: 6.324811616175818e-05\n",
      "test loss is 0.00012978122037011502\n",
      "Batch: 31100,train loss is: 9.983447037785007e-05\n",
      "test loss is 0.0001307170433837954\n",
      "Batch: 31200,train loss is: 8.68675827632922e-05\n",
      "test loss is 0.0001275629720003644\n",
      "Batch: 31300,train loss is: 0.00013490900283941732\n",
      "test loss is 0.00012487515762304554\n",
      "Batch: 31400,train loss is: 9.90918107451915e-05\n",
      "test loss is 0.00014886753166676976\n",
      "Batch: 31500,train loss is: 7.146043825369311e-05\n",
      "test loss is 0.00013495694559660792\n",
      "Batch: 31600,train loss is: 9.768884117185434e-05\n",
      "test loss is 0.00012545497021747453\n",
      "Batch: 31700,train loss is: 7.881479829402043e-05\n",
      "test loss is 0.00012671486469088253\n",
      "Batch: 31800,train loss is: 0.00013289551774070988\n",
      "test loss is 0.0001257420950037929\n",
      "Batch: 31900,train loss is: 0.00014227097888823092\n",
      "test loss is 0.0001359342449467586\n",
      "Batch: 32000,train loss is: 0.00049327450873442\n",
      "test loss is 0.00012693119813293318\n",
      "Batch: 32100,train loss is: 4.700891354976316e-05\n",
      "test loss is 0.00012681814754674634\n",
      "Batch: 32200,train loss is: 9.383695714758087e-05\n",
      "test loss is 0.00012394492385632395\n",
      "Batch: 32300,train loss is: 8.517962991409597e-05\n",
      "test loss is 0.0001259585727680681\n",
      "Batch: 32400,train loss is: 0.00010197751440395945\n",
      "test loss is 0.00013341400158010605\n",
      "Batch: 32500,train loss is: 0.000145282148043509\n",
      "test loss is 0.00012866501101916887\n",
      "Batch: 32600,train loss is: 7.590427750352986e-05\n",
      "test loss is 0.0001298080368540461\n",
      "Batch: 32700,train loss is: 7.431989735178557e-05\n",
      "test loss is 0.00012560295403688958\n",
      "Batch: 32800,train loss is: 8.639794934164835e-05\n",
      "test loss is 0.00013976395391996393\n",
      "Batch: 32900,train loss is: 0.00011963296301156369\n",
      "test loss is 0.00012833220624476532\n",
      "Batch: 33000,train loss is: 5.8609293142268266e-05\n",
      "test loss is 0.0001260597003671681\n",
      "Batch: 33100,train loss is: 8.625424287296035e-05\n",
      "test loss is 0.00013828932130982487\n",
      "Batch: 33200,train loss is: 0.00011066037041887866\n",
      "test loss is 0.00014452724534720998\n",
      "Batch: 33300,train loss is: 0.00010487225428518679\n",
      "test loss is 0.00014965642249731157\n",
      "Batch: 33400,train loss is: 8.759047671529148e-05\n",
      "test loss is 0.00012641838378078425\n",
      "Batch: 33500,train loss is: 9.415414934975944e-05\n",
      "test loss is 0.00012751117348880795\n",
      "Batch: 33600,train loss is: 7.251118614697847e-05\n",
      "test loss is 0.00012908133465749087\n",
      "Batch: 33700,train loss is: 9.38559938216368e-05\n",
      "test loss is 0.00013336563552825774\n",
      "Batch: 33800,train loss is: 0.00011534790447792519\n",
      "test loss is 0.00012898208866674187\n",
      "Batch: 33900,train loss is: 9.965592686419781e-05\n",
      "test loss is 0.0001259880306188737\n",
      "-----------------------Epoch: 3----------------------------------\n",
      "Batch: 0,train loss is: 0.00011568705229979968\n",
      "test loss is 0.0001308867234755318\n",
      "Batch: 100,train loss is: 0.00014386122678184006\n",
      "test loss is 0.00012758661663746934\n",
      "Batch: 200,train loss is: 0.00011212207232075512\n",
      "test loss is 0.00012481445652108653\n",
      "Batch: 300,train loss is: 8.94815516718652e-05\n",
      "test loss is 0.0001274505507112206\n",
      "Batch: 400,train loss is: 8.033510932040509e-05\n",
      "test loss is 0.0001233317799044301\n",
      "Batch: 500,train loss is: 0.0001318867616998852\n",
      "test loss is 0.00014964269764806553\n",
      "Batch: 600,train loss is: 0.0001041865626436876\n",
      "test loss is 0.00012391414972966818\n",
      "Batch: 700,train loss is: 7.463947649515751e-05\n",
      "test loss is 0.0001284216718905171\n",
      "Batch: 800,train loss is: 0.000161633859688328\n",
      "test loss is 0.000131177713361061\n",
      "Batch: 900,train loss is: 5.921996154478446e-05\n",
      "test loss is 0.0001320525778680347\n",
      "Batch: 1000,train loss is: 0.00010204594294141176\n",
      "test loss is 0.00012856311499837304\n",
      "Batch: 1100,train loss is: 0.00023702491062626654\n",
      "test loss is 0.00012986240188839833\n",
      "Batch: 1200,train loss is: 5.262016598231719e-05\n",
      "test loss is 0.00013251237839453895\n",
      "Batch: 1300,train loss is: 0.00011486431472948911\n",
      "test loss is 0.00012895443040258038\n",
      "Batch: 1400,train loss is: 9.891414476252865e-05\n",
      "test loss is 0.00012531223081116145\n",
      "Batch: 1500,train loss is: 9.81228497671611e-05\n",
      "test loss is 0.00012916336074794411\n",
      "Batch: 1600,train loss is: 6.843467413639698e-05\n",
      "test loss is 0.00012900841486033517\n",
      "Batch: 1700,train loss is: 0.0001431078028115692\n",
      "test loss is 0.0001414527390426012\n",
      "Batch: 1800,train loss is: 7.579749431677417e-05\n",
      "test loss is 0.0001245040705217283\n",
      "Batch: 1900,train loss is: 6.202136269345212e-05\n",
      "test loss is 0.00013048243200727838\n",
      "Batch: 2000,train loss is: 0.0001300472549507098\n",
      "test loss is 0.00012813993963611977\n",
      "Batch: 2100,train loss is: 7.784772011321048e-05\n",
      "test loss is 0.0001293443075853673\n",
      "Batch: 2200,train loss is: 0.000132748802418625\n",
      "test loss is 0.00012891938944498013\n",
      "Batch: 2300,train loss is: 9.880148418439064e-05\n",
      "test loss is 0.00012572515379720293\n",
      "Batch: 2400,train loss is: 0.00010786799626540805\n",
      "test loss is 0.0001257865248682186\n",
      "Batch: 2500,train loss is: 0.0001256831376737809\n",
      "test loss is 0.0001233304837698077\n",
      "Batch: 2600,train loss is: 8.533282891177497e-05\n",
      "test loss is 0.00012486378251608404\n",
      "Batch: 2700,train loss is: 0.00014470968329807887\n",
      "test loss is 0.0001316316196017464\n",
      "Batch: 2800,train loss is: 0.00010812866083183986\n",
      "test loss is 0.00013396849186106814\n",
      "Batch: 2900,train loss is: 0.00012430805646875302\n",
      "test loss is 0.00012403808846773957\n",
      "Batch: 3000,train loss is: 9.129656589667781e-05\n",
      "test loss is 0.0001274268510258339\n",
      "Batch: 3100,train loss is: 8.21287927196668e-05\n",
      "test loss is 0.0001234725619241739\n",
      "Batch: 3200,train loss is: 0.00018007529802188357\n",
      "test loss is 0.0001309469078061225\n",
      "Batch: 3300,train loss is: 0.00010163803981312486\n",
      "test loss is 0.00012536220029850932\n",
      "Batch: 3400,train loss is: 9.759157150331442e-05\n",
      "test loss is 0.00014402670463248843\n",
      "Batch: 3500,train loss is: 9.057112998429078e-05\n",
      "test loss is 0.00013254114232027033\n",
      "Batch: 3600,train loss is: 8.059112158162244e-05\n",
      "test loss is 0.0001291837566531897\n",
      "Batch: 3700,train loss is: 6.293417574283367e-05\n",
      "test loss is 0.0001232982260866304\n",
      "Batch: 3800,train loss is: 7.539078966168752e-05\n",
      "test loss is 0.00012599813145443536\n",
      "Batch: 3900,train loss is: 0.0001179131155156846\n",
      "test loss is 0.00012734887390736667\n",
      "Batch: 4000,train loss is: 5.847662735640551e-05\n",
      "test loss is 0.00012779107625161926\n",
      "Batch: 4100,train loss is: 0.00012455642696471405\n",
      "test loss is 0.00013364484332948758\n",
      "Batch: 4200,train loss is: 0.0001227172266085274\n",
      "test loss is 0.000140290034521486\n",
      "Batch: 4300,train loss is: 9.943660239732483e-05\n",
      "test loss is 0.00012541320890268928\n",
      "Batch: 4400,train loss is: 7.629079859752272e-05\n",
      "test loss is 0.0001278523165855328\n",
      "Batch: 4500,train loss is: 8.948798680864845e-05\n",
      "test loss is 0.00012354819602423272\n",
      "Batch: 4600,train loss is: 8.961359050008581e-05\n",
      "test loss is 0.00012682307384947106\n",
      "Batch: 4700,train loss is: 0.00016275868527071478\n",
      "test loss is 0.00013393883598673318\n",
      "Batch: 4800,train loss is: 0.0002806370923824497\n",
      "test loss is 0.00014727447142616368\n",
      "Batch: 4900,train loss is: 0.00010297377537092731\n",
      "test loss is 0.0001292604129661796\n",
      "Batch: 5000,train loss is: 6.670424818066123e-05\n",
      "test loss is 0.00012740113878386184\n",
      "Batch: 5100,train loss is: 7.478005354489478e-05\n",
      "test loss is 0.00012388194857496231\n",
      "Batch: 5200,train loss is: 9.058258085847886e-05\n",
      "test loss is 0.00013376743383262206\n",
      "Batch: 5300,train loss is: 9.891121483370812e-05\n",
      "test loss is 0.00013145340969758463\n",
      "Batch: 5400,train loss is: 0.00022111175980222154\n",
      "test loss is 0.0001337198133103898\n",
      "Batch: 5500,train loss is: 8.248992866591807e-05\n",
      "test loss is 0.00014415071536068628\n",
      "Batch: 5600,train loss is: 0.0001615788080105194\n",
      "test loss is 0.0001271563072985907\n",
      "Batch: 5700,train loss is: 0.00013657040065182145\n",
      "test loss is 0.0001338667758697363\n",
      "Batch: 5800,train loss is: 0.00016795088493884368\n",
      "test loss is 0.00013119600261256504\n",
      "Batch: 5900,train loss is: 0.00033172672196049834\n",
      "test loss is 0.00013188382284966648\n",
      "Batch: 6000,train loss is: 0.00016705178886310885\n",
      "test loss is 0.00013151728988307373\n",
      "Batch: 6100,train loss is: 6.903862622744326e-05\n",
      "test loss is 0.00012692704801897195\n",
      "Batch: 6200,train loss is: 8.913614393129304e-05\n",
      "test loss is 0.00012467181188052537\n",
      "Batch: 6300,train loss is: 0.00014779102619661853\n",
      "test loss is 0.00014710086901462924\n",
      "Batch: 6400,train loss is: 8.808366546600479e-05\n",
      "test loss is 0.00012739934139323594\n",
      "Batch: 6500,train loss is: 7.537786419278745e-05\n",
      "test loss is 0.0001304073041260518\n",
      "Batch: 6600,train loss is: 0.0002576224933646509\n",
      "test loss is 0.00012453794657316694\n",
      "Batch: 6700,train loss is: 0.0001028128962296309\n",
      "test loss is 0.00012532751793268893\n",
      "Batch: 6800,train loss is: 0.00014503474778703392\n",
      "test loss is 0.00012962775195803804\n",
      "Batch: 6900,train loss is: 0.00010083236878971609\n",
      "test loss is 0.00012691787990957616\n",
      "Batch: 7000,train loss is: 0.00013261054086621192\n",
      "test loss is 0.00012949441893638096\n",
      "Batch: 7100,train loss is: 0.00013839788751717318\n",
      "test loss is 0.00014582435171496178\n",
      "Batch: 7200,train loss is: 7.626107739327295e-05\n",
      "test loss is 0.00012299874225981817\n",
      "Batch: 7300,train loss is: 0.00023718300104208354\n",
      "test loss is 0.0001298704671553469\n",
      "Batch: 7400,train loss is: 9.21427479604882e-05\n",
      "test loss is 0.00015800798169187042\n",
      "Batch: 7500,train loss is: 0.00011505576222868296\n",
      "test loss is 0.00013074631669659419\n",
      "Batch: 7600,train loss is: 0.00018828460136112332\n",
      "test loss is 0.00014912188269511142\n",
      "Batch: 7700,train loss is: 0.00017882999397998776\n",
      "test loss is 0.00012425383333736226\n",
      "Batch: 7800,train loss is: 6.55673014041115e-05\n",
      "test loss is 0.00012329370836322473\n",
      "Batch: 7900,train loss is: 7.64722885421691e-05\n",
      "test loss is 0.00012295868216231215\n",
      "Batch: 8000,train loss is: 5.065083072311343e-05\n",
      "test loss is 0.00012923142158855006\n",
      "Batch: 8100,train loss is: 0.00018924357129988852\n",
      "test loss is 0.00012908313548097813\n",
      "Batch: 8200,train loss is: 9.971396589764469e-05\n",
      "test loss is 0.0001234525601232827\n",
      "Batch: 8300,train loss is: 6.926717606870208e-05\n",
      "test loss is 0.00012899788325871852\n",
      "Batch: 8400,train loss is: 0.00012449846044017336\n",
      "test loss is 0.00013279077052532042\n",
      "Batch: 8500,train loss is: 0.0002299511477219878\n",
      "test loss is 0.0001329822779297852\n",
      "Batch: 8600,train loss is: 9.905949897827858e-05\n",
      "test loss is 0.00014290278274582587\n",
      "Batch: 8700,train loss is: 0.00011877527577114793\n",
      "test loss is 0.00013636321487484114\n",
      "Batch: 8800,train loss is: 8.53005529901041e-05\n",
      "test loss is 0.00012466025196707477\n",
      "Batch: 8900,train loss is: 0.00011151996185035694\n",
      "test loss is 0.00012151927667129144\n",
      "Batch: 9000,train loss is: 7.618195969481309e-05\n",
      "test loss is 0.00012520708511349656\n",
      "Batch: 9100,train loss is: 0.00010232917445600272\n",
      "test loss is 0.00012713113856439105\n",
      "Batch: 9200,train loss is: 0.00013618296731256424\n",
      "test loss is 0.00012874633278077172\n",
      "Batch: 9300,train loss is: 0.00016088238218061975\n",
      "test loss is 0.0001248288326890828\n",
      "Batch: 9400,train loss is: 0.00013421775906292325\n",
      "test loss is 0.00013264222697776317\n",
      "Batch: 9500,train loss is: 7.325286822055926e-05\n",
      "test loss is 0.00013177542636632207\n",
      "Batch: 9600,train loss is: 8.867618143260482e-05\n",
      "test loss is 0.00012450873172096245\n",
      "Batch: 9700,train loss is: 6.821512868025392e-05\n",
      "test loss is 0.00012701230721130115\n",
      "Batch: 9800,train loss is: 7.116598689456409e-05\n",
      "test loss is 0.000124635462293872\n",
      "Batch: 9900,train loss is: 7.463808714390915e-05\n",
      "test loss is 0.00012351828753181092\n",
      "Batch: 10000,train loss is: 7.001402099057921e-05\n",
      "test loss is 0.00013173806144524248\n",
      "Batch: 10100,train loss is: 0.0002078123880956877\n",
      "test loss is 0.00012579639764859102\n",
      "Batch: 10200,train loss is: 0.00032666262948854416\n",
      "test loss is 0.00012500245434252257\n",
      "Batch: 10300,train loss is: 0.00012204239102492756\n",
      "test loss is 0.0001282678275516461\n",
      "Batch: 10400,train loss is: 5.6876406881209296e-05\n",
      "test loss is 0.00012975280314921878\n",
      "Batch: 10500,train loss is: 0.00011851120973236059\n",
      "test loss is 0.00012634469268510453\n",
      "Batch: 10600,train loss is: 7.888603742861956e-05\n",
      "test loss is 0.00012672299080047038\n",
      "Batch: 10700,train loss is: 9.624026642863855e-05\n",
      "test loss is 0.00012203638545885529\n",
      "Batch: 10800,train loss is: 0.00010190043701609666\n",
      "test loss is 0.00012932959354379402\n",
      "Batch: 10900,train loss is: 0.00010751951579047713\n",
      "test loss is 0.0001249168591383843\n",
      "Batch: 11000,train loss is: 0.00014883440678018073\n",
      "test loss is 0.00012263107082385248\n",
      "Batch: 11100,train loss is: 0.00014524826170140523\n",
      "test loss is 0.00012610006435009172\n",
      "Batch: 11200,train loss is: 0.0001506090123616437\n",
      "test loss is 0.00012417494940018098\n",
      "Batch: 11300,train loss is: 8.323010509596435e-05\n",
      "test loss is 0.0001318351993920339\n",
      "Batch: 11400,train loss is: 0.00022463857866953925\n",
      "test loss is 0.0001230438350367085\n",
      "Batch: 11500,train loss is: 6.803330437479447e-05\n",
      "test loss is 0.00012472488512223228\n",
      "Batch: 11600,train loss is: 0.00015233939956512633\n",
      "test loss is 0.00013107622192826915\n",
      "Batch: 11700,train loss is: 9.790178180023797e-05\n",
      "test loss is 0.00012332278130451334\n",
      "Batch: 11800,train loss is: 0.0001057006175079797\n",
      "test loss is 0.0001233587070397484\n",
      "Batch: 11900,train loss is: 0.00011132254630454055\n",
      "test loss is 0.00013933879827274554\n",
      "Batch: 12000,train loss is: 0.00010129735307256901\n",
      "test loss is 0.00012727741028943418\n",
      "Batch: 12100,train loss is: 0.00014544954871023697\n",
      "test loss is 0.00012464584780406737\n",
      "Batch: 12200,train loss is: 8.691254419207493e-05\n",
      "test loss is 0.0001245260768015681\n",
      "Batch: 12300,train loss is: 8.876736996206797e-05\n",
      "test loss is 0.00013091069127589227\n",
      "Batch: 12400,train loss is: 9.972363815381243e-05\n",
      "test loss is 0.00012505383186040513\n",
      "Batch: 12500,train loss is: 0.00021141769909466848\n",
      "test loss is 0.00012703772868112985\n",
      "Batch: 12600,train loss is: 6.625541419857016e-05\n",
      "test loss is 0.00012296960260811793\n",
      "Batch: 12700,train loss is: 0.000100972994266786\n",
      "test loss is 0.00012635610656642948\n",
      "Batch: 12800,train loss is: 5.1268782475827674e-05\n",
      "test loss is 0.00012509381301073106\n",
      "Batch: 12900,train loss is: 0.00017639946604995881\n",
      "test loss is 0.00012977452213231493\n",
      "Batch: 13000,train loss is: 9.117853422913555e-05\n",
      "test loss is 0.00012152286097174676\n",
      "Batch: 13100,train loss is: 0.0001905280029831361\n",
      "test loss is 0.00012091603411703486\n",
      "Batch: 13200,train loss is: 0.00010746643137400091\n",
      "test loss is 0.00013921695182567985\n",
      "Batch: 13300,train loss is: 0.00012399724829612548\n",
      "test loss is 0.00014928712219626985\n",
      "Batch: 13400,train loss is: 9.378948404637571e-05\n",
      "test loss is 0.0001213094406523767\n",
      "Batch: 13500,train loss is: 6.694022612032906e-05\n",
      "test loss is 0.0001258204783738721\n",
      "Batch: 13600,train loss is: 7.329335365007589e-05\n",
      "test loss is 0.00013737141172913482\n",
      "Batch: 13700,train loss is: 0.00011910603937221223\n",
      "test loss is 0.0001227335863323694\n",
      "Batch: 13800,train loss is: 7.321855742184782e-05\n",
      "test loss is 0.00013484617714180325\n",
      "Batch: 13900,train loss is: 6.306740762959732e-05\n",
      "test loss is 0.00012593283522457097\n",
      "Batch: 14000,train loss is: 0.00011991496325010016\n",
      "test loss is 0.0001279922222799272\n",
      "Batch: 14100,train loss is: 7.480088515280078e-05\n",
      "test loss is 0.00012451437278616524\n",
      "Batch: 14200,train loss is: 7.486253632928498e-05\n",
      "test loss is 0.00012442192741001605\n",
      "Batch: 14300,train loss is: 8.85590595867597e-05\n",
      "test loss is 0.00012346947954579695\n",
      "Batch: 14400,train loss is: 0.00018932666062222817\n",
      "test loss is 0.0001224232891778862\n",
      "Batch: 14500,train loss is: 9.202204225784017e-05\n",
      "test loss is 0.00012327578634794084\n",
      "Batch: 14600,train loss is: 0.0001482748542589074\n",
      "test loss is 0.0001324729601877699\n",
      "Batch: 14700,train loss is: 0.00010289308055495461\n",
      "test loss is 0.00012321550194273798\n",
      "Batch: 14800,train loss is: 0.0001835603940604507\n",
      "test loss is 0.0001260681023021608\n",
      "Batch: 14900,train loss is: 8.753125276153303e-05\n",
      "test loss is 0.00012096302843429511\n",
      "Batch: 15000,train loss is: 0.00010674143013306847\n",
      "test loss is 0.00012516700614472023\n",
      "Batch: 15100,train loss is: 0.00015106863473586274\n",
      "test loss is 0.00012852699420661743\n",
      "Batch: 15200,train loss is: 0.00015705976428290547\n",
      "test loss is 0.0001272479629910762\n",
      "Batch: 15300,train loss is: 0.00014608908371177058\n",
      "test loss is 0.00012428919770485263\n",
      "Batch: 15400,train loss is: 7.15836909182078e-05\n",
      "test loss is 0.00012139432669399594\n",
      "Batch: 15500,train loss is: 8.950486465648799e-05\n",
      "test loss is 0.00012896060223019067\n",
      "Batch: 15600,train loss is: 0.0002152291419947781\n",
      "test loss is 0.00012497047692536773\n",
      "Batch: 15700,train loss is: 7.849297590942248e-05\n",
      "test loss is 0.00012906953746903712\n",
      "Batch: 15800,train loss is: 8.105708141683987e-05\n",
      "test loss is 0.00013110150943217212\n",
      "Batch: 15900,train loss is: 0.00013918252935524026\n",
      "test loss is 0.0001280011390268929\n",
      "Batch: 16000,train loss is: 0.00016011490279188315\n",
      "test loss is 0.00012348215258837627\n",
      "Batch: 16100,train loss is: 0.0001978747369513666\n",
      "test loss is 0.00012210492256830018\n",
      "Batch: 16200,train loss is: 8.71983845342676e-05\n",
      "test loss is 0.00012498865664042444\n",
      "Batch: 16300,train loss is: 0.00010768014893699453\n",
      "test loss is 0.00013089390698184744\n",
      "Batch: 16400,train loss is: 7.205911209461111e-05\n",
      "test loss is 0.00012774340879389653\n",
      "Batch: 16500,train loss is: 8.588868325936844e-05\n",
      "test loss is 0.0001225698081845141\n",
      "Batch: 16600,train loss is: 8.660762230860328e-05\n",
      "test loss is 0.00012318606338162046\n",
      "Batch: 16700,train loss is: 0.000205819874100776\n",
      "test loss is 0.00013151892045686086\n",
      "Batch: 16800,train loss is: 0.0001484287857510709\n",
      "test loss is 0.00013176300100115363\n",
      "Batch: 16900,train loss is: 9.724287091087109e-05\n",
      "test loss is 0.00013290747977853174\n",
      "Batch: 17000,train loss is: 0.00011581077106216887\n",
      "test loss is 0.00013290921118088312\n",
      "Batch: 17100,train loss is: 0.0001135957570975987\n",
      "test loss is 0.00012319909384005725\n",
      "Batch: 17200,train loss is: 6.574395674993752e-05\n",
      "test loss is 0.00012454552464746842\n",
      "Batch: 17300,train loss is: 7.889731341759173e-05\n",
      "test loss is 0.00013307150286587528\n",
      "Batch: 17400,train loss is: 6.147971784479077e-05\n",
      "test loss is 0.0001316346267242467\n",
      "Batch: 17500,train loss is: 0.00012056113619495278\n",
      "test loss is 0.00012363468584317226\n",
      "Batch: 17600,train loss is: 7.70951328388318e-05\n",
      "test loss is 0.0001254042542345063\n",
      "Batch: 17700,train loss is: 5.8160613651040964e-05\n",
      "test loss is 0.00012108728798180977\n",
      "Batch: 17800,train loss is: 0.0002628458315622751\n",
      "test loss is 0.00012551675505390552\n",
      "Batch: 17900,train loss is: 5.351323358839413e-05\n",
      "test loss is 0.00012255077725637593\n",
      "Batch: 18000,train loss is: 8.777235107961116e-05\n",
      "test loss is 0.0001282820227053477\n",
      "Batch: 18100,train loss is: 0.00020250370427860864\n",
      "test loss is 0.00012504363666993883\n",
      "Batch: 18200,train loss is: 9.873854721675025e-05\n",
      "test loss is 0.00012233389561163658\n",
      "Batch: 18300,train loss is: 0.00013912994515433578\n",
      "test loss is 0.00012045855542589526\n",
      "Batch: 18400,train loss is: 0.00011223357435068544\n",
      "test loss is 0.00011978118512546831\n",
      "Batch: 18500,train loss is: 0.00016523712788994355\n",
      "test loss is 0.00015022629212987226\n",
      "Batch: 18600,train loss is: 8.476732901100267e-05\n",
      "test loss is 0.0001329511627337604\n",
      "Batch: 18700,train loss is: 0.00015632716361704217\n",
      "test loss is 0.00012699104920182422\n",
      "Batch: 18800,train loss is: 9.028535873877948e-05\n",
      "test loss is 0.00012344029388535425\n",
      "Batch: 18900,train loss is: 0.0001416696791761652\n",
      "test loss is 0.00012791753073312782\n",
      "Batch: 19000,train loss is: 6.336966210128528e-05\n",
      "test loss is 0.00012561168704949705\n",
      "Batch: 19100,train loss is: 0.00014655751628229628\n",
      "test loss is 0.0001315695461430805\n",
      "Batch: 19200,train loss is: 0.00011131282525867258\n",
      "test loss is 0.0001214115451623099\n",
      "Batch: 19300,train loss is: 0.00012479942697856485\n",
      "test loss is 0.0001222886942251671\n",
      "Batch: 19400,train loss is: 9.46913714151838e-05\n",
      "test loss is 0.00012507961644885805\n",
      "Batch: 19500,train loss is: 8.48588738676654e-05\n",
      "test loss is 0.00014234128237277868\n",
      "Batch: 19600,train loss is: 9.489293194459151e-05\n",
      "test loss is 0.00012571616064466212\n",
      "Batch: 19700,train loss is: 0.00015428511135075229\n",
      "test loss is 0.00012378403975426647\n",
      "Batch: 19800,train loss is: 7.874863955199436e-05\n",
      "test loss is 0.00012664550784760513\n",
      "Batch: 19900,train loss is: 7.276641880887734e-05\n",
      "test loss is 0.00013408564091621953\n",
      "Batch: 20000,train loss is: 0.0001919858590690859\n",
      "test loss is 0.00013469600550733727\n",
      "Batch: 20100,train loss is: 0.00015663829742469985\n",
      "test loss is 0.00013794230517931086\n",
      "Batch: 20200,train loss is: 8.27697072873815e-05\n",
      "test loss is 0.00012933505205594233\n",
      "Batch: 20300,train loss is: 5.3571329241566565e-05\n",
      "test loss is 0.0001376030854841747\n",
      "Batch: 20400,train loss is: 0.00012430247465224596\n",
      "test loss is 0.00013701580413200668\n",
      "Batch: 20500,train loss is: 0.00011378427823231377\n",
      "test loss is 0.00012041384256068316\n",
      "Batch: 20600,train loss is: 0.00021163130110136419\n",
      "test loss is 0.00012759952880929737\n",
      "Batch: 20700,train loss is: 0.00012435196288673706\n",
      "test loss is 0.0001299427162400313\n",
      "Batch: 20800,train loss is: 8.722012164454265e-05\n",
      "test loss is 0.00012820775178797579\n",
      "Batch: 20900,train loss is: 9.519015682581576e-05\n",
      "test loss is 0.00012780853519342282\n",
      "Batch: 21000,train loss is: 4.5634875817686926e-05\n",
      "test loss is 0.00012072074492505158\n",
      "Batch: 21100,train loss is: 8.432680673917041e-05\n",
      "test loss is 0.00012060725069730115\n",
      "Batch: 21200,train loss is: 7.069080442651664e-05\n",
      "test loss is 0.0001280778088095868\n",
      "Batch: 21300,train loss is: 9.89714036611985e-05\n",
      "test loss is 0.00012882956280320715\n",
      "Batch: 21400,train loss is: 0.00012480102172526591\n",
      "test loss is 0.0001204391051131821\n",
      "Batch: 21500,train loss is: 9.718139203553703e-05\n",
      "test loss is 0.00012894485048719006\n",
      "Batch: 21600,train loss is: 0.00011919972122833825\n",
      "test loss is 0.00013362377061135164\n",
      "Batch: 21700,train loss is: 7.940261520028242e-05\n",
      "test loss is 0.00012836318189446346\n",
      "Batch: 21800,train loss is: 0.0001537247420645421\n",
      "test loss is 0.00012284343169834374\n",
      "Batch: 21900,train loss is: 0.00010087439106851946\n",
      "test loss is 0.00013483859631837174\n",
      "Batch: 22000,train loss is: 0.00012215867518391963\n",
      "test loss is 0.00014820012124236077\n",
      "Batch: 22100,train loss is: 7.627637683001709e-05\n",
      "test loss is 0.00012093806090614241\n",
      "Batch: 22200,train loss is: 6.525880007440354e-05\n",
      "test loss is 0.00012662605566530618\n",
      "Batch: 22300,train loss is: 0.00010255431254939314\n",
      "test loss is 0.00013892385974453302\n",
      "Batch: 22400,train loss is: 0.00011459441725860849\n",
      "test loss is 0.0001248150527435413\n",
      "Batch: 22500,train loss is: 8.363870232115725e-05\n",
      "test loss is 0.00012453699457458736\n",
      "Batch: 22600,train loss is: 0.0001180721939317776\n",
      "test loss is 0.0001286239499751867\n",
      "Batch: 22700,train loss is: 7.14001095976601e-05\n",
      "test loss is 0.00011973689580022751\n",
      "Batch: 22800,train loss is: 0.00011934753343310328\n",
      "test loss is 0.0001320675041013232\n",
      "Batch: 22900,train loss is: 6.782286331200267e-05\n",
      "test loss is 0.00012567374653273155\n",
      "Batch: 23000,train loss is: 0.00018871824816721322\n",
      "test loss is 0.00012978557306348395\n",
      "Batch: 23100,train loss is: 9.454322617474844e-05\n",
      "test loss is 0.00012025786438955459\n",
      "Batch: 23200,train loss is: 8.238463670970581e-05\n",
      "test loss is 0.00012007478784907781\n",
      "Batch: 23300,train loss is: 6.578673494172234e-05\n",
      "test loss is 0.00012186878888559863\n",
      "Batch: 23400,train loss is: 9.057296818764784e-05\n",
      "test loss is 0.0001247991794951828\n",
      "Batch: 23500,train loss is: 0.00015913874812176332\n",
      "test loss is 0.00012301733724763773\n",
      "Batch: 23600,train loss is: 7.446237908584763e-05\n",
      "test loss is 0.00013030289547513661\n",
      "Batch: 23700,train loss is: 0.00014624456947360818\n",
      "test loss is 0.0001196682336978499\n",
      "Batch: 23800,train loss is: 0.00018920710988541326\n",
      "test loss is 0.00012383520474750339\n",
      "Batch: 23900,train loss is: 0.0004025137706867238\n",
      "test loss is 0.0001231458788884219\n",
      "Batch: 24000,train loss is: 8.770000510705018e-05\n",
      "test loss is 0.00014458065756694767\n",
      "Batch: 24100,train loss is: 0.00011611777227512123\n",
      "test loss is 0.00012208183042492252\n",
      "Batch: 24200,train loss is: 0.00019377860189549671\n",
      "test loss is 0.00013001652197223483\n",
      "Batch: 24300,train loss is: 0.00011797158118017045\n",
      "test loss is 0.0001517637125534732\n",
      "Batch: 24400,train loss is: 0.00012366529754282283\n",
      "test loss is 0.00012302888124333987\n",
      "Batch: 24500,train loss is: 0.00014923315283496915\n",
      "test loss is 0.00012882364873611116\n",
      "Batch: 24600,train loss is: 0.00011366129249535385\n",
      "test loss is 0.0001448887407546224\n",
      "Batch: 24700,train loss is: 8.392748204261674e-05\n",
      "test loss is 0.0001222188331628731\n",
      "Batch: 24800,train loss is: 0.00015697255864542057\n",
      "test loss is 0.00012568291527859478\n",
      "Batch: 24900,train loss is: 5.259454579769659e-05\n",
      "test loss is 0.00012038394560662516\n",
      "Batch: 25000,train loss is: 9.401071702236099e-05\n",
      "test loss is 0.00012192001656018646\n",
      "Batch: 25100,train loss is: 7.552879034648883e-05\n",
      "test loss is 0.00012141738082372124\n",
      "Batch: 25200,train loss is: 0.00012229472463908277\n",
      "test loss is 0.00012707238317338998\n",
      "Batch: 25300,train loss is: 0.00010894734559718809\n",
      "test loss is 0.00011951977601855312\n",
      "Batch: 25400,train loss is: 0.00010146779292093363\n",
      "test loss is 0.00013078788582783884\n",
      "Batch: 25500,train loss is: 7.962871453929913e-05\n",
      "test loss is 0.00012051418538699465\n",
      "Batch: 25600,train loss is: 0.00011615373909551473\n",
      "test loss is 0.00012743281928371555\n",
      "Batch: 25700,train loss is: 0.00013520430470385914\n",
      "test loss is 0.0001210500456649507\n",
      "Batch: 25800,train loss is: 8.624541593023096e-05\n",
      "test loss is 0.00012542150055060478\n",
      "Batch: 25900,train loss is: 6.439459414880861e-05\n",
      "test loss is 0.00012271940515846066\n",
      "Batch: 26000,train loss is: 0.00012278566541670214\n",
      "test loss is 0.00012433281883180098\n",
      "Batch: 26100,train loss is: 0.00018827939441763412\n",
      "test loss is 0.00011991308024846716\n",
      "Batch: 26200,train loss is: 0.00021203175656294606\n",
      "test loss is 0.0001255846753774408\n",
      "Batch: 26300,train loss is: 0.00014517852891117716\n",
      "test loss is 0.0001199528008483294\n",
      "Batch: 26400,train loss is: 8.699756285746525e-05\n",
      "test loss is 0.00011911892118121584\n",
      "Batch: 26500,train loss is: 0.00011773074639012131\n",
      "test loss is 0.0001222782460089249\n",
      "Batch: 26600,train loss is: 0.0001686915262985306\n",
      "test loss is 0.00012167766147788936\n",
      "Batch: 26700,train loss is: 6.054710645163484e-05\n",
      "test loss is 0.00012600686600417238\n",
      "Batch: 26800,train loss is: 0.0001822300540277858\n",
      "test loss is 0.00012340082443639875\n",
      "Batch: 26900,train loss is: 6.1860459046421e-05\n",
      "test loss is 0.0001354560800859076\n",
      "Batch: 27000,train loss is: 0.00010964605166221674\n",
      "test loss is 0.00013451795204657058\n",
      "Batch: 27100,train loss is: 6.481327931750774e-05\n",
      "test loss is 0.00013504025868101653\n",
      "Batch: 27200,train loss is: 9.913653792795445e-05\n",
      "test loss is 0.00012115204395989926\n",
      "Batch: 27300,train loss is: 0.0001286876635727281\n",
      "test loss is 0.00012110552783440418\n",
      "Batch: 27400,train loss is: 9.851796084010184e-05\n",
      "test loss is 0.00012441330157254943\n",
      "Batch: 27500,train loss is: 9.073179138370685e-05\n",
      "test loss is 0.0001376943946577683\n",
      "Batch: 27600,train loss is: 0.00010624506326594528\n",
      "test loss is 0.00013779987096774506\n",
      "Batch: 27700,train loss is: 0.0003376295833856841\n",
      "test loss is 0.00014059443851193953\n",
      "Batch: 27800,train loss is: 9.687981921230271e-05\n",
      "test loss is 0.00012345738619781493\n",
      "Batch: 27900,train loss is: 0.00014778525874249886\n",
      "test loss is 0.00012171082379732198\n",
      "Batch: 28000,train loss is: 7.961245295822122e-05\n",
      "test loss is 0.00012754369696808108\n",
      "Batch: 28100,train loss is: 0.00017687031791994165\n",
      "test loss is 0.0001366958273281424\n",
      "Batch: 28200,train loss is: 0.0001237065104966406\n",
      "test loss is 0.00012615079619509312\n",
      "Batch: 28300,train loss is: 0.000343098266131718\n",
      "test loss is 0.00012190104122079869\n",
      "Batch: 28400,train loss is: 0.00011013652096377688\n",
      "test loss is 0.00012318562267111798\n",
      "Batch: 28500,train loss is: 0.00011913700277635465\n",
      "test loss is 0.0001304516616635622\n",
      "Batch: 28600,train loss is: 8.454780981038211e-05\n",
      "test loss is 0.0001229653695079472\n",
      "Batch: 28700,train loss is: 0.00010775428143086886\n",
      "test loss is 0.00012279438255375884\n",
      "Batch: 28800,train loss is: 0.0001709166642886246\n",
      "test loss is 0.00011941869049385208\n",
      "Batch: 28900,train loss is: 0.00013094415170166504\n",
      "test loss is 0.00014369461471531916\n",
      "Batch: 29000,train loss is: 0.0001120916248087122\n",
      "test loss is 0.0001215860670332689\n",
      "Batch: 29100,train loss is: 9.846195757800992e-05\n",
      "test loss is 0.00013422264777123482\n",
      "Batch: 29200,train loss is: 0.00010481578536384169\n",
      "test loss is 0.00015346778555558498\n",
      "Batch: 29300,train loss is: 0.0001028084629734162\n",
      "test loss is 0.00012212129446294618\n",
      "Batch: 29400,train loss is: 0.0001535210953887957\n",
      "test loss is 0.0001300914836868579\n",
      "Batch: 29500,train loss is: 6.7746666623558e-05\n",
      "test loss is 0.00012163237482434547\n",
      "Batch: 29600,train loss is: 0.0002135833931037589\n",
      "test loss is 0.00013600192795458997\n",
      "Batch: 29700,train loss is: 7.561183181515651e-05\n",
      "test loss is 0.00012050300484941867\n",
      "Batch: 29800,train loss is: 0.00022238663042383876\n",
      "test loss is 0.00012000739995139299\n",
      "Batch: 29900,train loss is: 0.00029528268193986684\n",
      "test loss is 0.00012218969658312365\n",
      "Batch: 30000,train loss is: 7.781333450271285e-05\n",
      "test loss is 0.00012492469282201287\n",
      "Batch: 30100,train loss is: 9.517290231247783e-05\n",
      "test loss is 0.00013150007449357683\n",
      "Batch: 30200,train loss is: 9.452582583463487e-05\n",
      "test loss is 0.00011827014052853163\n",
      "Batch: 30300,train loss is: 9.166459563065353e-05\n",
      "test loss is 0.0001215242843299415\n",
      "Batch: 30400,train loss is: 9.122554218082761e-05\n",
      "test loss is 0.00012963449086589467\n",
      "Batch: 30500,train loss is: 9.561668466013848e-05\n",
      "test loss is 0.00012172033266732832\n",
      "Batch: 30600,train loss is: 7.788063732817287e-05\n",
      "test loss is 0.00012093163799913731\n",
      "Batch: 30700,train loss is: 9.14028895830362e-05\n",
      "test loss is 0.00012486855378005814\n",
      "Batch: 30800,train loss is: 7.939846791924427e-05\n",
      "test loss is 0.00012450437156097911\n",
      "Batch: 30900,train loss is: 0.00010156422915474941\n",
      "test loss is 0.00012364722947730787\n",
      "Batch: 31000,train loss is: 5.878535989795846e-05\n",
      "test loss is 0.00012459525888331544\n",
      "Batch: 31100,train loss is: 9.608071034281047e-05\n",
      "test loss is 0.000124846706095452\n",
      "Batch: 31200,train loss is: 8.627730912707652e-05\n",
      "test loss is 0.00012237554738572635\n",
      "Batch: 31300,train loss is: 0.00013251061946872484\n",
      "test loss is 0.0001197608223055029\n",
      "Batch: 31400,train loss is: 9.587109520269848e-05\n",
      "test loss is 0.00014392541781780308\n",
      "Batch: 31500,train loss is: 6.750569122430917e-05\n",
      "test loss is 0.00012931577199556827\n",
      "Batch: 31600,train loss is: 9.162683570232708e-05\n",
      "test loss is 0.0001203238006214477\n",
      "Batch: 31700,train loss is: 7.284920076766379e-05\n",
      "test loss is 0.0001213298593350934\n",
      "Batch: 31800,train loss is: 0.0001323102155103195\n",
      "test loss is 0.00012042672956749209\n",
      "Batch: 31900,train loss is: 0.00013367461257840523\n",
      "test loss is 0.00012919753653487754\n",
      "Batch: 32000,train loss is: 0.00048095731537383177\n",
      "test loss is 0.00012218813052422762\n",
      "Batch: 32100,train loss is: 4.4192608478643454e-05\n",
      "test loss is 0.00012241839861247997\n",
      "Batch: 32200,train loss is: 8.794151521742069e-05\n",
      "test loss is 0.000118829608685193\n",
      "Batch: 32300,train loss is: 8.119953549607926e-05\n",
      "test loss is 0.00012087222880508991\n",
      "Batch: 32400,train loss is: 9.733359302002357e-05\n",
      "test loss is 0.0001281029725364395\n",
      "Batch: 32500,train loss is: 0.00014013397299236349\n",
      "test loss is 0.0001234635525381022\n",
      "Batch: 32600,train loss is: 7.345337233672162e-05\n",
      "test loss is 0.00012442487064947118\n",
      "Batch: 32700,train loss is: 6.88396298416368e-05\n",
      "test loss is 0.00012031246515429874\n",
      "Batch: 32800,train loss is: 8.619490779611196e-05\n",
      "test loss is 0.00013440252025670942\n",
      "Batch: 32900,train loss is: 0.00010969860557559114\n",
      "test loss is 0.00012316392433801523\n",
      "Batch: 33000,train loss is: 5.6572979873291624e-05\n",
      "test loss is 0.0001208017453147583\n",
      "Batch: 33100,train loss is: 8.273560665914338e-05\n",
      "test loss is 0.00013313830112033766\n",
      "Batch: 33200,train loss is: 0.00010354401618604103\n",
      "test loss is 0.00013770491209088816\n",
      "Batch: 33300,train loss is: 9.83026510658289e-05\n",
      "test loss is 0.000141806348169311\n",
      "Batch: 33400,train loss is: 8.196255381640064e-05\n",
      "test loss is 0.00012151457625646204\n",
      "Batch: 33500,train loss is: 8.756796120571035e-05\n",
      "test loss is 0.0001221673955659263\n",
      "Batch: 33600,train loss is: 6.589277523359021e-05\n",
      "test loss is 0.0001243947576724278\n",
      "Batch: 33700,train loss is: 8.984685079553044e-05\n",
      "test loss is 0.0001282679653418345\n",
      "Batch: 33800,train loss is: 0.00011232200446850667\n",
      "test loss is 0.00012388811237777332\n",
      "Batch: 33900,train loss is: 9.39271365427807e-05\n",
      "test loss is 0.0001210752587969138\n",
      "-----------------------Epoch: 4----------------------------------\n",
      "Batch: 0,train loss is: 0.00011419933019089113\n",
      "test loss is 0.0001252001572398986\n",
      "Batch: 100,train loss is: 0.00013995465472803372\n",
      "test loss is 0.00012183055777080215\n",
      "Batch: 200,train loss is: 0.00010667906081876745\n",
      "test loss is 0.000119635189088206\n",
      "Batch: 300,train loss is: 8.884745226926888e-05\n",
      "test loss is 0.0001225275225389691\n",
      "Batch: 400,train loss is: 7.517959739211046e-05\n",
      "test loss is 0.00011858804026832555\n",
      "Batch: 500,train loss is: 0.00012878064192206712\n",
      "test loss is 0.00014295291879784066\n",
      "Batch: 600,train loss is: 9.733491791483148e-05\n",
      "test loss is 0.00011891714860792105\n",
      "Batch: 700,train loss is: 7.271231331356179e-05\n",
      "test loss is 0.00012326931476454897\n",
      "Batch: 800,train loss is: 0.0001511277116841489\n",
      "test loss is 0.0001264949999419769\n",
      "Batch: 900,train loss is: 5.854421624934833e-05\n",
      "test loss is 0.0001267698054154084\n",
      "Batch: 1000,train loss is: 9.900035678199699e-05\n",
      "test loss is 0.00012413160092304503\n",
      "Batch: 1100,train loss is: 0.00022092390914479036\n",
      "test loss is 0.00012496022534126921\n",
      "Batch: 1200,train loss is: 5.327800373836e-05\n",
      "test loss is 0.00012835901563257912\n",
      "Batch: 1300,train loss is: 0.00011290917458300041\n",
      "test loss is 0.00012405101634272732\n",
      "Batch: 1400,train loss is: 9.296267678891297e-05\n",
      "test loss is 0.00012033662428748398\n",
      "Batch: 1500,train loss is: 9.350649478293108e-05\n",
      "test loss is 0.0001233704508836073\n",
      "Batch: 1600,train loss is: 6.885244508777717e-05\n",
      "test loss is 0.00012341772162452266\n",
      "Batch: 1700,train loss is: 0.00014239420249295543\n",
      "test loss is 0.00013489542797568454\n",
      "Batch: 1800,train loss is: 7.190874672984012e-05\n",
      "test loss is 0.00011970272593078262\n",
      "Batch: 1900,train loss is: 5.8849368040609516e-05\n",
      "test loss is 0.00012491055321035523\n",
      "Batch: 2000,train loss is: 0.00012645347817362174\n",
      "test loss is 0.00012240126448034969\n",
      "Batch: 2100,train loss is: 7.888732870164517e-05\n",
      "test loss is 0.0001249271090430587\n",
      "Batch: 2200,train loss is: 0.00013310152011168148\n",
      "test loss is 0.00012362233944880095\n",
      "Batch: 2300,train loss is: 9.580431867862539e-05\n",
      "test loss is 0.0001215722003930228\n",
      "Batch: 2400,train loss is: 0.00010221034372839966\n",
      "test loss is 0.00012059740302454731\n",
      "Batch: 2500,train loss is: 0.00011614919585479443\n",
      "test loss is 0.00011791414131940241\n",
      "Batch: 2600,train loss is: 8.002295452562475e-05\n",
      "test loss is 0.00011960716968669117\n",
      "Batch: 2700,train loss is: 0.00013974439104506725\n",
      "test loss is 0.00012405088157908367\n",
      "Batch: 2800,train loss is: 0.00010651565991187235\n",
      "test loss is 0.00013058270147627848\n",
      "Batch: 2900,train loss is: 0.00012203830459850854\n",
      "test loss is 0.00011865085426580267\n",
      "Batch: 3000,train loss is: 8.601016763910832e-05\n",
      "test loss is 0.00012197482577371987\n",
      "Batch: 3100,train loss is: 7.80589686967391e-05\n",
      "test loss is 0.00011870158521898956\n",
      "Batch: 3200,train loss is: 0.00016913889649632343\n",
      "test loss is 0.00012547496979303316\n",
      "Batch: 3300,train loss is: 9.81158378129709e-05\n",
      "test loss is 0.00012022420934880727\n",
      "Batch: 3400,train loss is: 9.418931090987201e-05\n",
      "test loss is 0.0001393652226282712\n",
      "Batch: 3500,train loss is: 8.406081074714056e-05\n",
      "test loss is 0.0001277151327511568\n",
      "Batch: 3600,train loss is: 7.589015846996855e-05\n",
      "test loss is 0.0001237275178571911\n",
      "Batch: 3700,train loss is: 6.170343768908586e-05\n",
      "test loss is 0.0001178783565460474\n",
      "Batch: 3800,train loss is: 7.208105830587108e-05\n",
      "test loss is 0.00012101151752130921\n",
      "Batch: 3900,train loss is: 0.00011564903644939597\n",
      "test loss is 0.00012210274812738786\n",
      "Batch: 4000,train loss is: 5.511075950460223e-05\n",
      "test loss is 0.00012261609731877205\n",
      "Batch: 4100,train loss is: 0.00011750418266412374\n",
      "test loss is 0.00012776279649220266\n",
      "Batch: 4200,train loss is: 0.00012252780168701673\n",
      "test loss is 0.00013589967868373327\n",
      "Batch: 4300,train loss is: 9.270359274008225e-05\n",
      "test loss is 0.00012004582339956318\n",
      "Batch: 4400,train loss is: 7.247192167383737e-05\n",
      "test loss is 0.00012324842630450261\n",
      "Batch: 4500,train loss is: 8.2588629596905e-05\n",
      "test loss is 0.00011811430753717329\n",
      "Batch: 4600,train loss is: 8.564630426625695e-05\n",
      "test loss is 0.00012188152596724179\n",
      "Batch: 4700,train loss is: 0.00015902608572228674\n",
      "test loss is 0.00012862755147846101\n",
      "Batch: 4800,train loss is: 0.0002699140664291628\n",
      "test loss is 0.00014143893419993526\n",
      "Batch: 4900,train loss is: 9.935068948053005e-05\n",
      "test loss is 0.00012360084303725954\n",
      "Batch: 5000,train loss is: 6.450276956774349e-05\n",
      "test loss is 0.00012310120253992285\n",
      "Batch: 5100,train loss is: 7.250016925604004e-05\n",
      "test loss is 0.0001186720493758529\n",
      "Batch: 5200,train loss is: 8.944177197552364e-05\n",
      "test loss is 0.00012938992066091032\n",
      "Batch: 5300,train loss is: 9.479948297301249e-05\n",
      "test loss is 0.00012675158828448114\n",
      "Batch: 5400,train loss is: 0.00021367358602219412\n",
      "test loss is 0.0001280848966593116\n",
      "Batch: 5500,train loss is: 7.82326924400228e-05\n",
      "test loss is 0.0001387200572358254\n",
      "Batch: 5600,train loss is: 0.00015769583299838014\n",
      "test loss is 0.00012194664517385466\n",
      "Batch: 5700,train loss is: 0.00013534380497602153\n",
      "test loss is 0.0001286293702046774\n",
      "Batch: 5800,train loss is: 0.00016100577945156962\n",
      "test loss is 0.00012678997320396455\n",
      "Batch: 5900,train loss is: 0.0003013488967983596\n",
      "test loss is 0.00012800013515870955\n",
      "Batch: 6000,train loss is: 0.00016455512158603062\n",
      "test loss is 0.00012701335499138786\n",
      "Batch: 6100,train loss is: 6.785357241513134e-05\n",
      "test loss is 0.0001216812473065863\n",
      "Batch: 6200,train loss is: 8.623224152473925e-05\n",
      "test loss is 0.00011926360838334468\n",
      "Batch: 6300,train loss is: 0.0001406161743950979\n",
      "test loss is 0.00014077155231568146\n",
      "Batch: 6400,train loss is: 8.027050568062175e-05\n",
      "test loss is 0.00012198339913085886\n",
      "Batch: 6500,train loss is: 7.540406287182146e-05\n",
      "test loss is 0.00012457557975149844\n",
      "Batch: 6600,train loss is: 0.00023312070018538712\n",
      "test loss is 0.00011994376461765423\n",
      "Batch: 6700,train loss is: 0.00010013930853596723\n",
      "test loss is 0.00012029494043434356\n",
      "Batch: 6800,train loss is: 0.0001367829178762957\n",
      "test loss is 0.00012437939933211277\n",
      "Batch: 6900,train loss is: 0.00010001175772396432\n",
      "test loss is 0.00012227754761971765\n",
      "Batch: 7000,train loss is: 0.00012581733388343296\n",
      "test loss is 0.00012298202032547527\n",
      "Batch: 7100,train loss is: 0.00013520293452999322\n",
      "test loss is 0.000139010372783604\n",
      "Batch: 7200,train loss is: 7.256876159789895e-05\n",
      "test loss is 0.00011756552224106344\n",
      "Batch: 7300,train loss is: 0.00023133653579105563\n",
      "test loss is 0.00012531833999249523\n",
      "Batch: 7400,train loss is: 8.704631336603959e-05\n",
      "test loss is 0.00015069179906494057\n",
      "Batch: 7500,train loss is: 0.00010777501196688657\n",
      "test loss is 0.0001252046137049801\n",
      "Batch: 7600,train loss is: 0.0001765029991172608\n",
      "test loss is 0.0001441134723252553\n",
      "Batch: 7700,train loss is: 0.0001730563194198362\n",
      "test loss is 0.00011846442912011907\n",
      "Batch: 7800,train loss is: 6.373506754871223e-05\n",
      "test loss is 0.00011892462099763475\n",
      "Batch: 7900,train loss is: 7.092932504516832e-05\n",
      "test loss is 0.00011785788305571924\n",
      "Batch: 8000,train loss is: 4.8021854850837686e-05\n",
      "test loss is 0.0001256503424935809\n",
      "Batch: 8100,train loss is: 0.00017406102566835463\n",
      "test loss is 0.00012313963589953483\n",
      "Batch: 8200,train loss is: 9.668313552000934e-05\n",
      "test loss is 0.00011860528865745107\n",
      "Batch: 8300,train loss is: 6.738655186783467e-05\n",
      "test loss is 0.00012377818744474997\n",
      "Batch: 8400,train loss is: 0.0001250362621246663\n",
      "test loss is 0.00012825918202380521\n",
      "Batch: 8500,train loss is: 0.0002232294637300706\n",
      "test loss is 0.00012834246471649278\n",
      "Batch: 8600,train loss is: 9.800222281373189e-05\n",
      "test loss is 0.00013919095593133452\n",
      "Batch: 8700,train loss is: 0.00011149358196969265\n",
      "test loss is 0.00012903778956056767\n",
      "Batch: 8800,train loss is: 8.516200745597142e-05\n",
      "test loss is 0.00011965461670210775\n",
      "Batch: 8900,train loss is: 0.00010482306585550528\n",
      "test loss is 0.00011663450712035147\n",
      "Batch: 9000,train loss is: 7.255683640357693e-05\n",
      "test loss is 0.00011992977297584689\n",
      "Batch: 9100,train loss is: 9.823257959093322e-05\n",
      "test loss is 0.0001227124706901649\n",
      "Batch: 9200,train loss is: 0.0001317448810985978\n",
      "test loss is 0.00012430985888172262\n",
      "Batch: 9300,train loss is: 0.00015390319040680834\n",
      "test loss is 0.00011990839893575665\n",
      "Batch: 9400,train loss is: 0.0001260901657937277\n",
      "test loss is 0.00012565943754372046\n",
      "Batch: 9500,train loss is: 7.044174290427861e-05\n",
      "test loss is 0.0001269942114912674\n",
      "Batch: 9600,train loss is: 8.589173581845885e-05\n",
      "test loss is 0.00011953611484919054\n",
      "Batch: 9700,train loss is: 6.86679155995502e-05\n",
      "test loss is 0.00012192734187078845\n",
      "Batch: 9800,train loss is: 6.845642835864862e-05\n",
      "test loss is 0.00011916766267506671\n",
      "Batch: 9900,train loss is: 7.107983766199741e-05\n",
      "test loss is 0.00011798171798610821\n",
      "Batch: 10000,train loss is: 6.89201713863629e-05\n",
      "test loss is 0.00012655182287616565\n",
      "Batch: 10100,train loss is: 0.00020244452636971962\n",
      "test loss is 0.0001211328539853948\n",
      "Batch: 10200,train loss is: 0.0003109119388299902\n",
      "test loss is 0.00012007806535806148\n",
      "Batch: 10300,train loss is: 0.0001148753117968278\n",
      "test loss is 0.00012238224093750444\n",
      "Batch: 10400,train loss is: 5.358590045924756e-05\n",
      "test loss is 0.00012449707535950984\n",
      "Batch: 10500,train loss is: 0.0001102562420496765\n",
      "test loss is 0.00012123234375071369\n",
      "Batch: 10600,train loss is: 7.667084811324708e-05\n",
      "test loss is 0.00012177793186129995\n",
      "Batch: 10700,train loss is: 9.191861732799868e-05\n",
      "test loss is 0.00011702496615230278\n",
      "Batch: 10800,train loss is: 9.741567544823544e-05\n",
      "test loss is 0.00012323739556287109\n",
      "Batch: 10900,train loss is: 0.00010227541930943948\n",
      "test loss is 0.00011982589535414792\n",
      "Batch: 11000,train loss is: 0.00014583315270222104\n",
      "test loss is 0.00011728847090605363\n",
      "Batch: 11100,train loss is: 0.00013748839222230713\n",
      "test loss is 0.00012094846896334083\n",
      "Batch: 11200,train loss is: 0.00014335589807419692\n",
      "test loss is 0.00011866155731796322\n",
      "Batch: 11300,train loss is: 7.959136112133637e-05\n",
      "test loss is 0.00012673826410236245\n",
      "Batch: 11400,train loss is: 0.00021168909280143552\n",
      "test loss is 0.00011777652002726922\n",
      "Batch: 11500,train loss is: 6.50837786032418e-05\n",
      "test loss is 0.00012009574344477577\n",
      "Batch: 11600,train loss is: 0.0001403783329880789\n",
      "test loss is 0.0001241737294973079\n",
      "Batch: 11700,train loss is: 9.039645929825204e-05\n",
      "test loss is 0.00011780147532683005\n",
      "Batch: 11800,train loss is: 0.00010266055322301739\n",
      "test loss is 0.00011812489121686087\n",
      "Batch: 11900,train loss is: 0.00011292552483951207\n",
      "test loss is 0.00013371002563394586\n",
      "Batch: 12000,train loss is: 9.658050590670876e-05\n",
      "test loss is 0.00012206562022760695\n",
      "Batch: 12100,train loss is: 0.000134911243590231\n",
      "test loss is 0.00011936494552507581\n",
      "Batch: 12200,train loss is: 8.210406084617475e-05\n",
      "test loss is 0.00011931157797843326\n",
      "Batch: 12300,train loss is: 8.31535001926298e-05\n",
      "test loss is 0.00012514719015073583\n",
      "Batch: 12400,train loss is: 9.340396559118887e-05\n",
      "test loss is 0.00012020768706364581\n",
      "Batch: 12500,train loss is: 0.00019303602503936059\n",
      "test loss is 0.00012157194221740981\n",
      "Batch: 12600,train loss is: 6.203951783527697e-05\n",
      "test loss is 0.00011815366132098295\n",
      "Batch: 12700,train loss is: 9.702812917138278e-05\n",
      "test loss is 0.00012166873068726032\n",
      "Batch: 12800,train loss is: 5.020465297107201e-05\n",
      "test loss is 0.00011998155924920953\n",
      "Batch: 12900,train loss is: 0.00016757617060414538\n",
      "test loss is 0.00012289195491273588\n",
      "Batch: 13000,train loss is: 9.072052435885615e-05\n",
      "test loss is 0.0001164404212651996\n",
      "Batch: 13100,train loss is: 0.00018688018483899968\n",
      "test loss is 0.00011589879352273636\n",
      "Batch: 13200,train loss is: 0.00010569006922353634\n",
      "test loss is 0.00013404176276803114\n",
      "Batch: 13300,train loss is: 0.00011752716044707601\n",
      "test loss is 0.0001453371738770657\n",
      "Batch: 13400,train loss is: 9.001252526255406e-05\n",
      "test loss is 0.00011630633908089376\n",
      "Batch: 13500,train loss is: 6.355072314890537e-05\n",
      "test loss is 0.00012043273420199219\n",
      "Batch: 13600,train loss is: 7.098209451155638e-05\n",
      "test loss is 0.00013218885935556933\n",
      "Batch: 13700,train loss is: 0.00011769766375611688\n",
      "test loss is 0.00011805189330701896\n",
      "Batch: 13800,train loss is: 7.070328029701713e-05\n",
      "test loss is 0.00012924190374102604\n",
      "Batch: 13900,train loss is: 6.020917634612993e-05\n",
      "test loss is 0.00012127609383523915\n",
      "Batch: 14000,train loss is: 0.00011411580678517538\n",
      "test loss is 0.00012186439317315756\n",
      "Batch: 14100,train loss is: 7.356794212384371e-05\n",
      "test loss is 0.00011989437095830247\n",
      "Batch: 14200,train loss is: 7.250872237019485e-05\n",
      "test loss is 0.0001193256105959669\n",
      "Batch: 14300,train loss is: 8.58084409276213e-05\n",
      "test loss is 0.00011838351240101926\n",
      "Batch: 14400,train loss is: 0.00017494064795845252\n",
      "test loss is 0.00011734734126235143\n",
      "Batch: 14500,train loss is: 8.632827179934154e-05\n",
      "test loss is 0.00011844471263398126\n",
      "Batch: 14600,train loss is: 0.0001390382894726811\n",
      "test loss is 0.0001271707459117796\n",
      "Batch: 14700,train loss is: 0.00010320391998208184\n",
      "test loss is 0.00011880601336585885\n",
      "Batch: 14800,train loss is: 0.00018052035557282117\n",
      "test loss is 0.00012176505360600758\n",
      "Batch: 14900,train loss is: 8.48057720745593e-05\n",
      "test loss is 0.00011626749271317186\n",
      "Batch: 15000,train loss is: 0.0001073106870450829\n",
      "test loss is 0.00011994656672617568\n",
      "Batch: 15100,train loss is: 0.0001457168856239621\n",
      "test loss is 0.00012533120155248058\n",
      "Batch: 15200,train loss is: 0.000144525874900137\n",
      "test loss is 0.00012284537873212388\n",
      "Batch: 15300,train loss is: 0.0001400088819841323\n",
      "test loss is 0.00011890924916470782\n",
      "Batch: 15400,train loss is: 6.872445353830926e-05\n",
      "test loss is 0.00011669526597915113\n",
      "Batch: 15500,train loss is: 8.457978121300197e-05\n",
      "test loss is 0.00012382652820710062\n",
      "Batch: 15600,train loss is: 0.00020452745671302555\n",
      "test loss is 0.00012062430992796897\n",
      "Batch: 15700,train loss is: 7.777008276634666e-05\n",
      "test loss is 0.00012466809226829536\n",
      "Batch: 15800,train loss is: 8.04614261549535e-05\n",
      "test loss is 0.0001270135389419836\n",
      "Batch: 15900,train loss is: 0.00012825293110143248\n",
      "test loss is 0.0001234804242288018\n",
      "Batch: 16000,train loss is: 0.00015582760351329856\n",
      "test loss is 0.00011900163746547856\n",
      "Batch: 16100,train loss is: 0.00018927550432933871\n",
      "test loss is 0.00011722428748159182\n",
      "Batch: 16200,train loss is: 8.083340663258449e-05\n",
      "test loss is 0.00012051220258896156\n",
      "Batch: 16300,train loss is: 0.00010583262690640029\n",
      "test loss is 0.00012624216383696563\n",
      "Batch: 16400,train loss is: 6.869360391164613e-05\n",
      "test loss is 0.00012267039312464359\n",
      "Batch: 16500,train loss is: 8.144425132246944e-05\n",
      "test loss is 0.00011773689922615754\n",
      "Batch: 16600,train loss is: 8.144730058591921e-05\n",
      "test loss is 0.0001176913075399568\n",
      "Batch: 16700,train loss is: 0.00019769710828387586\n",
      "test loss is 0.00012627721112388898\n",
      "Batch: 16800,train loss is: 0.0001487687101944831\n",
      "test loss is 0.00012946169010390532\n",
      "Batch: 16900,train loss is: 9.262944811439461e-05\n",
      "test loss is 0.0001269721241148616\n",
      "Batch: 17000,train loss is: 0.00011469215108594557\n",
      "test loss is 0.00012906203418586138\n",
      "Batch: 17100,train loss is: 0.00011231046050485966\n",
      "test loss is 0.00011870384574907866\n",
      "Batch: 17200,train loss is: 6.399525896820165e-05\n",
      "test loss is 0.00011989172235145458\n",
      "Batch: 17300,train loss is: 7.768033187836855e-05\n",
      "test loss is 0.00012749662676040546\n",
      "Batch: 17400,train loss is: 5.88272764754792e-05\n",
      "test loss is 0.00012753412033363236\n",
      "Batch: 17500,train loss is: 0.00011519204907131639\n",
      "test loss is 0.00011808973568397724\n",
      "Batch: 17600,train loss is: 7.109594954714313e-05\n",
      "test loss is 0.00012051795113630768\n",
      "Batch: 17700,train loss is: 5.5941690780343705e-05\n",
      "test loss is 0.00011628463777006217\n",
      "Batch: 17800,train loss is: 0.0002545279626745363\n",
      "test loss is 0.0001211841440738764\n",
      "Batch: 17900,train loss is: 5.0421250138513707e-05\n",
      "test loss is 0.00011664426181654524\n",
      "Batch: 18000,train loss is: 8.356370959545499e-05\n",
      "test loss is 0.00012386441749928076\n",
      "Batch: 18100,train loss is: 0.0001906389150393234\n",
      "test loss is 0.00012065127442753413\n",
      "Batch: 18200,train loss is: 9.743405269763167e-05\n",
      "test loss is 0.00011758470624440141\n",
      "Batch: 18300,train loss is: 0.00013484262380803548\n",
      "test loss is 0.00011584306521816761\n",
      "Batch: 18400,train loss is: 0.00010887334233411087\n",
      "test loss is 0.00011492154777128671\n",
      "Batch: 18500,train loss is: 0.00015514015573636592\n",
      "test loss is 0.0001440025724264093\n",
      "Batch: 18600,train loss is: 8.063194405548681e-05\n",
      "test loss is 0.00013008081586080654\n",
      "Batch: 18700,train loss is: 0.0001464831012638148\n",
      "test loss is 0.00012210219925622002\n",
      "Batch: 18800,train loss is: 8.740410383073073e-05\n",
      "test loss is 0.00011897484111129506\n",
      "Batch: 18900,train loss is: 0.00013706407341036175\n",
      "test loss is 0.00012312977048630023\n",
      "Batch: 19000,train loss is: 6.107725357189548e-05\n",
      "test loss is 0.00012105271700403975\n",
      "Batch: 19100,train loss is: 0.00014213982196934033\n",
      "test loss is 0.00012728671711078837\n",
      "Batch: 19200,train loss is: 0.00010612700145359688\n",
      "test loss is 0.00011647258134252378\n",
      "Batch: 19300,train loss is: 0.00012242670902646213\n",
      "test loss is 0.00011812857392588863\n",
      "Batch: 19400,train loss is: 8.696649951580537e-05\n",
      "test loss is 0.00012041191942683757\n",
      "Batch: 19500,train loss is: 7.919523644930769e-05\n",
      "test loss is 0.0001355678795454715\n",
      "Batch: 19600,train loss is: 9.006179446962849e-05\n",
      "test loss is 0.0001208869412849512\n",
      "Batch: 19700,train loss is: 0.00015440715064461632\n",
      "test loss is 0.00011876629678593054\n",
      "Batch: 19800,train loss is: 7.394399610789894e-05\n",
      "test loss is 0.00012111589119993866\n",
      "Batch: 19900,train loss is: 7.098667685963075e-05\n",
      "test loss is 0.0001285144716683135\n",
      "Batch: 20000,train loss is: 0.00018703407225805394\n",
      "test loss is 0.00012760203068100582\n",
      "Batch: 20100,train loss is: 0.00014648793748815852\n",
      "test loss is 0.00013355178759426702\n",
      "Batch: 20200,train loss is: 7.854685518725625e-05\n",
      "test loss is 0.00012522026740710684\n",
      "Batch: 20300,train loss is: 5.193652230117871e-05\n",
      "test loss is 0.0001322192641885114\n",
      "Batch: 20400,train loss is: 0.0001156191042426118\n",
      "test loss is 0.0001314649153734346\n",
      "Batch: 20500,train loss is: 0.00010830233563358539\n",
      "test loss is 0.00011586662146207657\n",
      "Batch: 20600,train loss is: 0.0002083429407455687\n",
      "test loss is 0.0001226113049343892\n",
      "Batch: 20700,train loss is: 0.00012080236885163206\n",
      "test loss is 0.00012485615223637182\n",
      "Batch: 20800,train loss is: 8.832209447564707e-05\n",
      "test loss is 0.00012362234792057805\n",
      "Batch: 20900,train loss is: 9.14382930409051e-05\n",
      "test loss is 0.00012284583081849233\n",
      "Batch: 21000,train loss is: 4.4063510790963476e-05\n",
      "test loss is 0.00011626222392196606\n",
      "Batch: 21100,train loss is: 8.00184865278303e-05\n",
      "test loss is 0.00011607444143679558\n",
      "Batch: 21200,train loss is: 6.576823879025279e-05\n",
      "test loss is 0.00012260338493806397\n",
      "Batch: 21300,train loss is: 9.379907285133152e-05\n",
      "test loss is 0.0001238487456424161\n",
      "Batch: 21400,train loss is: 0.00011965907651880244\n",
      "test loss is 0.00011536820548069155\n",
      "Batch: 21500,train loss is: 9.28643861463487e-05\n",
      "test loss is 0.00012228963343994802\n",
      "Batch: 21600,train loss is: 0.0001188768619059994\n",
      "test loss is 0.00012974976788465194\n",
      "Batch: 21700,train loss is: 7.629738273096455e-05\n",
      "test loss is 0.00012386932466781496\n",
      "Batch: 21800,train loss is: 0.00015264834764238154\n",
      "test loss is 0.00011765749772176879\n",
      "Batch: 21900,train loss is: 9.406907938133032e-05\n",
      "test loss is 0.00012867789142438984\n",
      "Batch: 22000,train loss is: 0.00011401693738116489\n",
      "test loss is 0.0001430634631171475\n",
      "Batch: 22100,train loss is: 7.198670619897773e-05\n",
      "test loss is 0.00011598772937653648\n",
      "Batch: 22200,train loss is: 6.421093369077838e-05\n",
      "test loss is 0.0001228742505814082\n",
      "Batch: 22300,train loss is: 9.714241894347174e-05\n",
      "test loss is 0.0001324906144429428\n",
      "Batch: 22400,train loss is: 0.00010772321120661926\n",
      "test loss is 0.00012017433579606804\n",
      "Batch: 22500,train loss is: 8.310646484258149e-05\n",
      "test loss is 0.00011937414923118294\n",
      "Batch: 22600,train loss is: 0.00011261514253106295\n",
      "test loss is 0.0001234291358303477\n",
      "Batch: 22700,train loss is: 6.942145104837142e-05\n",
      "test loss is 0.00011528346619772588\n",
      "Batch: 22800,train loss is: 0.0001198148006406714\n",
      "test loss is 0.0001280753908026133\n",
      "Batch: 22900,train loss is: 6.229012308384077e-05\n",
      "test loss is 0.00012171062370132217\n",
      "Batch: 23000,train loss is: 0.00017413487443584393\n",
      "test loss is 0.0001231621315392497\n",
      "Batch: 23100,train loss is: 9.159442268235275e-05\n",
      "test loss is 0.00011533619501798113\n",
      "Batch: 23200,train loss is: 7.983212364093543e-05\n",
      "test loss is 0.00011565319083448503\n",
      "Batch: 23300,train loss is: 6.162891624917382e-05\n",
      "test loss is 0.00011686169664745408\n",
      "Batch: 23400,train loss is: 8.893986686878975e-05\n",
      "test loss is 0.00011998765648477252\n",
      "Batch: 23500,train loss is: 0.00014863601755800153\n",
      "test loss is 0.00011863864742635998\n",
      "Batch: 23600,train loss is: 7.334195227503477e-05\n",
      "test loss is 0.00012588252657800094\n",
      "Batch: 23700,train loss is: 0.00014456297968643987\n",
      "test loss is 0.00011565408358539424\n",
      "Batch: 23800,train loss is: 0.00017893647496123547\n",
      "test loss is 0.00011908795779260037\n",
      "Batch: 23900,train loss is: 0.00038279536573406403\n",
      "test loss is 0.00011850065456324377\n",
      "Batch: 24000,train loss is: 8.695324885454932e-05\n",
      "test loss is 0.00013686094365467227\n",
      "Batch: 24100,train loss is: 0.00011130292192011284\n",
      "test loss is 0.0001171443716623466\n",
      "Batch: 24200,train loss is: 0.00018435842749541282\n",
      "test loss is 0.0001249689959700707\n",
      "Batch: 24300,train loss is: 0.0001113813712473102\n",
      "test loss is 0.00014275722155828946\n",
      "Batch: 24400,train loss is: 0.0001246729272869321\n",
      "test loss is 0.00011807565000626749\n",
      "Batch: 24500,train loss is: 0.00014120252100946522\n",
      "test loss is 0.00012419223212232054\n",
      "Batch: 24600,train loss is: 0.00010942999727573053\n",
      "test loss is 0.000139395634615476\n",
      "Batch: 24700,train loss is: 8.144325434276152e-05\n",
      "test loss is 0.00011730671633174818\n",
      "Batch: 24800,train loss is: 0.00015146421068834502\n",
      "test loss is 0.00012047662480836095\n",
      "Batch: 24900,train loss is: 5.0958507988219444e-05\n",
      "test loss is 0.00011549977749470661\n",
      "Batch: 25000,train loss is: 9.12933224004415e-05\n",
      "test loss is 0.00011726585685894126\n",
      "Batch: 25100,train loss is: 6.984288097764087e-05\n",
      "test loss is 0.00011638655877307742\n",
      "Batch: 25200,train loss is: 0.00011579812526020696\n",
      "test loss is 0.00012172522396533525\n",
      "Batch: 25300,train loss is: 0.00010302551765003991\n",
      "test loss is 0.00011490575977448033\n",
      "Batch: 25400,train loss is: 9.799681037497733e-05\n",
      "test loss is 0.00012569996157507892\n",
      "Batch: 25500,train loss is: 7.691924027540832e-05\n",
      "test loss is 0.00011588261938515605\n",
      "Batch: 25600,train loss is: 0.00011369433797700135\n",
      "test loss is 0.00012291542652747214\n",
      "Batch: 25700,train loss is: 0.00012884356841312092\n",
      "test loss is 0.00011658294845872558\n",
      "Batch: 25800,train loss is: 8.270527667649175e-05\n",
      "test loss is 0.0001202097718976291\n",
      "Batch: 25900,train loss is: 6.008849589148718e-05\n",
      "test loss is 0.00011733660031719927\n",
      "Batch: 26000,train loss is: 0.00011710494836743722\n",
      "test loss is 0.00011944585729040172\n",
      "Batch: 26100,train loss is: 0.00018418668680793689\n",
      "test loss is 0.0001153193730185442\n",
      "Batch: 26200,train loss is: 0.00019762924488009263\n",
      "test loss is 0.00012058713161452049\n",
      "Batch: 26300,train loss is: 0.00014095912194410156\n",
      "test loss is 0.0001152021747418664\n",
      "Batch: 26400,train loss is: 8.219520588664566e-05\n",
      "test loss is 0.00011428273667502452\n",
      "Batch: 26500,train loss is: 0.00011179507903466494\n",
      "test loss is 0.00011691289551879774\n",
      "Batch: 26600,train loss is: 0.00016230666862609882\n",
      "test loss is 0.00011631833944942777\n",
      "Batch: 26700,train loss is: 5.923020416510449e-05\n",
      "test loss is 0.00012197110620610271\n",
      "Batch: 26800,train loss is: 0.0001764750790620484\n",
      "test loss is 0.00011850138742090141\n",
      "Batch: 26900,train loss is: 5.813613880147909e-05\n",
      "test loss is 0.0001308461682322913\n",
      "Batch: 27000,train loss is: 0.00010534004270384509\n",
      "test loss is 0.00012928890192451253\n",
      "Batch: 27100,train loss is: 6.150699073476828e-05\n",
      "test loss is 0.000131003861857326\n",
      "Batch: 27200,train loss is: 9.659819319137383e-05\n",
      "test loss is 0.00011602888401629756\n",
      "Batch: 27300,train loss is: 0.0001211422014536399\n",
      "test loss is 0.00011588351084119708\n",
      "Batch: 27400,train loss is: 9.627449407716646e-05\n",
      "test loss is 0.00011960608691318591\n",
      "Batch: 27500,train loss is: 8.557290727673085e-05\n",
      "test loss is 0.00013236356410560696\n",
      "Batch: 27600,train loss is: 0.00010301944382043255\n",
      "test loss is 0.00013442275934934153\n",
      "Batch: 27700,train loss is: 0.000325021416520133\n",
      "test loss is 0.0001352096164127807\n",
      "Batch: 27800,train loss is: 9.416443113699538e-05\n",
      "test loss is 0.00011868105732904204\n",
      "Batch: 27900,train loss is: 0.00014216649845122566\n",
      "test loss is 0.00011695725040450178\n",
      "Batch: 28000,train loss is: 7.612895679673315e-05\n",
      "test loss is 0.00012248235841596041\n",
      "Batch: 28100,train loss is: 0.0001723517740739766\n",
      "test loss is 0.0001312581151556294\n",
      "Batch: 28200,train loss is: 0.00011722987509231292\n",
      "test loss is 0.00012164709493129465\n",
      "Batch: 28300,train loss is: 0.00032724111588169604\n",
      "test loss is 0.00011709199366976657\n",
      "Batch: 28400,train loss is: 0.00010421269666276148\n",
      "test loss is 0.00011786398583377344\n",
      "Batch: 28500,train loss is: 0.00011053476702403978\n",
      "test loss is 0.00012517270733031444\n",
      "Batch: 28600,train loss is: 8.264398470835395e-05\n",
      "test loss is 0.00011841347327347083\n",
      "Batch: 28700,train loss is: 0.0001033996907187177\n",
      "test loss is 0.00011795118495390016\n",
      "Batch: 28800,train loss is: 0.00016948677378492876\n",
      "test loss is 0.00011473762692348536\n",
      "Batch: 28900,train loss is: 0.0001241877650674682\n",
      "test loss is 0.00013451947413684973\n",
      "Batch: 29000,train loss is: 0.00010589402704674257\n",
      "test loss is 0.00011663924045040026\n",
      "Batch: 29100,train loss is: 9.429879546570073e-05\n",
      "test loss is 0.00012808743769565273\n",
      "Batch: 29200,train loss is: 0.00010307736798063587\n",
      "test loss is 0.00014965971598014525\n",
      "Batch: 29300,train loss is: 9.880967611157524e-05\n",
      "test loss is 0.00011780180886823439\n",
      "Batch: 29400,train loss is: 0.00014734485351291086\n",
      "test loss is 0.00012528103194773315\n",
      "Batch: 29500,train loss is: 6.475469584393348e-05\n",
      "test loss is 0.00011678109068918291\n",
      "Batch: 29600,train loss is: 0.00021173661111956982\n",
      "test loss is 0.00012977975914406036\n",
      "Batch: 29700,train loss is: 7.196048575421225e-05\n",
      "test loss is 0.0001155805572517811\n",
      "Batch: 29800,train loss is: 0.00021158808185801453\n",
      "test loss is 0.00011532195049610113\n",
      "Batch: 29900,train loss is: 0.0002846593601506641\n",
      "test loss is 0.00011714777176942801\n",
      "Batch: 30000,train loss is: 7.810885792101679e-05\n",
      "test loss is 0.000120034746333975\n",
      "Batch: 30100,train loss is: 9.555501218301431e-05\n",
      "test loss is 0.00012715148110737908\n",
      "Batch: 30200,train loss is: 8.76397490402967e-05\n",
      "test loss is 0.00011357397238157861\n",
      "Batch: 30300,train loss is: 8.975026904766617e-05\n",
      "test loss is 0.00011679138560513127\n",
      "Batch: 30400,train loss is: 8.748343667727361e-05\n",
      "test loss is 0.00012510805729150503\n",
      "Batch: 30500,train loss is: 9.02036937367063e-05\n",
      "test loss is 0.00011670684283082364\n",
      "Batch: 30600,train loss is: 7.42498407304372e-05\n",
      "test loss is 0.00011651340299705859\n",
      "Batch: 30700,train loss is: 8.874600344422171e-05\n",
      "test loss is 0.00012099357454753936\n",
      "Batch: 30800,train loss is: 7.57476120688077e-05\n",
      "test loss is 0.0001196918845693454\n",
      "Batch: 30900,train loss is: 9.699052935328947e-05\n",
      "test loss is 0.00011840208942538356\n",
      "Batch: 31000,train loss is: 5.528532188190822e-05\n",
      "test loss is 0.00011986605017998252\n",
      "Batch: 31100,train loss is: 9.256043195373333e-05\n",
      "test loss is 0.00012007050610610742\n",
      "Batch: 31200,train loss is: 8.556489089771814e-05\n",
      "test loss is 0.0001175871213425797\n",
      "Batch: 31300,train loss is: 0.00012922227058571935\n",
      "test loss is 0.00011517873111385842\n",
      "Batch: 31400,train loss is: 9.37941604412265e-05\n",
      "test loss is 0.0001394685248963555\n",
      "Batch: 31500,train loss is: 6.420210394193012e-05\n",
      "test loss is 0.0001240110500593633\n",
      "Batch: 31600,train loss is: 8.790362087829367e-05\n",
      "test loss is 0.00011558266707010283\n",
      "Batch: 31700,train loss is: 6.832882320380546e-05\n",
      "test loss is 0.00011653728655360449\n",
      "Batch: 31800,train loss is: 0.0001296118139942027\n",
      "test loss is 0.00011559338374921799\n",
      "Batch: 31900,train loss is: 0.00012788421177132632\n",
      "test loss is 0.000124226259735546\n",
      "Batch: 32000,train loss is: 0.00046352356983474664\n",
      "test loss is 0.00011789100034841304\n",
      "Batch: 32100,train loss is: 4.222090868994147e-05\n",
      "test loss is 0.00011867608128658268\n",
      "Batch: 32200,train loss is: 8.35073601163019e-05\n",
      "test loss is 0.00011428595985171774\n",
      "Batch: 32300,train loss is: 7.771966771463836e-05\n",
      "test loss is 0.00011630726205362485\n",
      "Batch: 32400,train loss is: 9.25005589890059e-05\n",
      "test loss is 0.00012380860152625913\n",
      "Batch: 32500,train loss is: 0.0001349686456764367\n",
      "test loss is 0.00011879651030371941\n",
      "Batch: 32600,train loss is: 7.125921372725051e-05\n",
      "test loss is 0.00011974716015146604\n",
      "Batch: 32700,train loss is: 6.676021505753011e-05\n",
      "test loss is 0.00011591924143104223\n",
      "Batch: 32800,train loss is: 8.475137644706024e-05\n",
      "test loss is 0.00012887868477663182\n",
      "Batch: 32900,train loss is: 0.0001019135816362517\n",
      "test loss is 0.00011829362345981745\n",
      "Batch: 33000,train loss is: 5.516259385045137e-05\n",
      "test loss is 0.00011610095687407489\n",
      "Batch: 33100,train loss is: 8.036659106329444e-05\n",
      "test loss is 0.00012904611414301485\n",
      "Batch: 33200,train loss is: 9.856619105244458e-05\n",
      "test loss is 0.00013175744397568362\n",
      "Batch: 33300,train loss is: 9.242667950105673e-05\n",
      "test loss is 0.00013414544684477108\n",
      "Batch: 33400,train loss is: 7.664348446264515e-05\n",
      "test loss is 0.00011698158563318585\n",
      "Batch: 33500,train loss is: 8.15414807944312e-05\n",
      "test loss is 0.0001175763494930277\n",
      "Batch: 33600,train loss is: 6.284436361881863e-05\n",
      "test loss is 0.00012011988371511493\n",
      "Batch: 33700,train loss is: 8.629220918649252e-05\n",
      "test loss is 0.0001243371939807434\n",
      "Batch: 33800,train loss is: 0.000109515676045655\n",
      "test loss is 0.0001198771710006799\n",
      "Batch: 33900,train loss is: 8.692669302612726e-05\n",
      "test loss is 0.00011708931801458509\n",
      "-----------------------Epoch: 5----------------------------------\n",
      "Batch: 0,train loss is: 0.00011142452403402166\n",
      "test loss is 0.00012041281755916154\n",
      "Batch: 100,train loss is: 0.00013903307702500649\n",
      "test loss is 0.00011667950286163432\n",
      "Batch: 200,train loss is: 0.0001031127405677592\n",
      "test loss is 0.00011513089989762912\n",
      "Batch: 300,train loss is: 8.633543412057577e-05\n",
      "test loss is 0.00011787199081404676\n",
      "Batch: 400,train loss is: 7.198493887575177e-05\n",
      "test loss is 0.00011433672196642489\n",
      "Batch: 500,train loss is: 0.0001280193557492203\n",
      "test loss is 0.00013782991478858322\n",
      "Batch: 600,train loss is: 9.160274703892789e-05\n",
      "test loss is 0.00011481624963717917\n",
      "Batch: 700,train loss is: 7.134200022632529e-05\n",
      "test loss is 0.00011847246733072104\n",
      "Batch: 800,train loss is: 0.000142937197035966\n",
      "test loss is 0.00012241110772789112\n",
      "Batch: 900,train loss is: 5.7497171970159394e-05\n",
      "test loss is 0.0001218344725786801\n",
      "Batch: 1000,train loss is: 9.617715728696467e-05\n",
      "test loss is 0.0001200975063537369\n",
      "Batch: 1100,train loss is: 0.00020899933166361817\n",
      "test loss is 0.00012106890529528387\n",
      "Batch: 1200,train loss is: 5.294916145548546e-05\n",
      "test loss is 0.0001244649432180567\n",
      "Batch: 1300,train loss is: 0.0001124827698634789\n",
      "test loss is 0.00011949299098908642\n",
      "Batch: 1400,train loss is: 8.728956642331825e-05\n",
      "test loss is 0.00011594249050486503\n",
      "Batch: 1500,train loss is: 9.109024717134257e-05\n",
      "test loss is 0.00011824669247360627\n",
      "Batch: 1600,train loss is: 6.761563578364438e-05\n",
      "test loss is 0.00011838146683856934\n",
      "Batch: 1700,train loss is: 0.00014034352581993758\n",
      "test loss is 0.00012900783188201026\n",
      "Batch: 1800,train loss is: 6.900418453044398e-05\n",
      "test loss is 0.00011561215868179027\n",
      "Batch: 1900,train loss is: 5.6320951185647665e-05\n",
      "test loss is 0.00012022597776210175\n",
      "Batch: 2000,train loss is: 0.00012373142289096712\n",
      "test loss is 0.00011779163412421188\n",
      "Batch: 2100,train loss is: 7.792828272707685e-05\n",
      "test loss is 0.00012114819131612825\n",
      "Batch: 2200,train loss is: 0.00013103705790936854\n",
      "test loss is 0.00011908921281225194\n",
      "Batch: 2300,train loss is: 9.24750771002554e-05\n",
      "test loss is 0.00011730846093629236\n",
      "Batch: 2400,train loss is: 9.680662062959453e-05\n",
      "test loss is 0.0001162924021531313\n",
      "Batch: 2500,train loss is: 0.00010834029310418641\n",
      "test loss is 0.00011326462585621485\n",
      "Batch: 2600,train loss is: 7.484126623236106e-05\n",
      "test loss is 0.00011481348279044242\n",
      "Batch: 2700,train loss is: 0.00013863132339457098\n",
      "test loss is 0.00011790259122352504\n",
      "Batch: 2800,train loss is: 0.0001057239647165401\n",
      "test loss is 0.0001275062178163322\n",
      "Batch: 2900,train loss is: 0.00011940175214875531\n",
      "test loss is 0.00011401457372754472\n",
      "Batch: 3000,train loss is: 8.210480770510859e-05\n",
      "test loss is 0.00011768059947357551\n",
      "Batch: 3100,train loss is: 7.48534202332522e-05\n",
      "test loss is 0.0001142707886485343\n",
      "Batch: 3200,train loss is: 0.0001582938936567338\n",
      "test loss is 0.00012114102137169925\n",
      "Batch: 3300,train loss is: 9.577782490924153e-05\n",
      "test loss is 0.00011559052877479082\n",
      "Batch: 3400,train loss is: 9.09475547502077e-05\n",
      "test loss is 0.00013498340274607884\n",
      "Batch: 3500,train loss is: 7.915537756272816e-05\n",
      "test loss is 0.0001234389360912826\n",
      "Batch: 3600,train loss is: 7.114462013547797e-05\n",
      "test loss is 0.00011858574146585774\n",
      "Batch: 3700,train loss is: 6.090705047769713e-05\n",
      "test loss is 0.00011320194159364668\n",
      "Batch: 3800,train loss is: 6.882558570789677e-05\n",
      "test loss is 0.00011578995118282077\n",
      "Batch: 3900,train loss is: 0.00011389743212801188\n",
      "test loss is 0.0001176857135510295\n",
      "Batch: 4000,train loss is: 5.2421423734085036e-05\n",
      "test loss is 0.00011797298707799133\n",
      "Batch: 4100,train loss is: 0.00011129991306219236\n",
      "test loss is 0.00012260819921604038\n",
      "Batch: 4200,train loss is: 0.00012156332186046993\n",
      "test loss is 0.00013225165327678779\n",
      "Batch: 4300,train loss is: 8.55617923276799e-05\n",
      "test loss is 0.00011535983894599708\n",
      "Batch: 4400,train loss is: 6.893402762750039e-05\n",
      "test loss is 0.00011943421498217183\n",
      "Batch: 4500,train loss is: 7.511687283802276e-05\n",
      "test loss is 0.00011355027942452103\n",
      "Batch: 4600,train loss is: 8.397961005287737e-05\n",
      "test loss is 0.00011720174701695573\n",
      "Batch: 4700,train loss is: 0.000154515656191321\n",
      "test loss is 0.00012333589407592015\n",
      "Batch: 4800,train loss is: 0.0002514615172445894\n",
      "test loss is 0.0001368062397086912\n",
      "Batch: 4900,train loss is: 9.65099212021246e-05\n",
      "test loss is 0.00011954773433703602\n",
      "Batch: 5000,train loss is: 6.267405123265183e-05\n",
      "test loss is 0.00011973070757042915\n",
      "Batch: 5100,train loss is: 6.97458808950392e-05\n",
      "test loss is 0.00011418489034318429\n",
      "Batch: 5200,train loss is: 8.743255904566885e-05\n",
      "test loss is 0.00012543628270358714\n",
      "Batch: 5300,train loss is: 9.054887179809311e-05\n",
      "test loss is 0.0001221367566399156\n",
      "Batch: 5400,train loss is: 0.0002093973786560177\n",
      "test loss is 0.0001229327983413765\n",
      "Batch: 5500,train loss is: 7.715743830817658e-05\n",
      "test loss is 0.00013424189928730592\n",
      "Batch: 5600,train loss is: 0.00015404814793222463\n",
      "test loss is 0.00011735605592314427\n",
      "Batch: 5700,train loss is: 0.00013648425137813698\n",
      "test loss is 0.00012459260562194802\n",
      "Batch: 5800,train loss is: 0.00015410744335689547\n",
      "test loss is 0.00012272855494249214\n",
      "Batch: 5900,train loss is: 0.0002758672960885432\n",
      "test loss is 0.0001243374897902705\n",
      "Batch: 6000,train loss is: 0.00016067834751350005\n",
      "test loss is 0.00012264064111659013\n",
      "Batch: 6100,train loss is: 6.647851007456198e-05\n",
      "test loss is 0.00011690027584177945\n",
      "Batch: 6200,train loss is: 8.362299705532026e-05\n",
      "test loss is 0.00011447497851020569\n",
      "Batch: 6300,train loss is: 0.00013265468283720752\n",
      "test loss is 0.00013425110973702288\n",
      "Batch: 6400,train loss is: 7.350981212251517e-05\n",
      "test loss is 0.00011690399271063691\n",
      "Batch: 6500,train loss is: 7.530540563802574e-05\n",
      "test loss is 0.00011939633230202847\n",
      "Batch: 6600,train loss is: 0.00021552944890603642\n",
      "test loss is 0.00011568452326114547\n",
      "Batch: 6700,train loss is: 9.860335465796697e-05\n",
      "test loss is 0.00011590204664821055\n",
      "Batch: 6800,train loss is: 0.00013400002835017017\n",
      "test loss is 0.0001197432780339485\n",
      "Batch: 6900,train loss is: 0.0001040748344807581\n",
      "test loss is 0.0001181379136219969\n",
      "Batch: 7000,train loss is: 0.00012090416249818572\n",
      "test loss is 0.00011773733375921203\n",
      "Batch: 7100,train loss is: 0.00012883545383368368\n",
      "test loss is 0.00013297197356013552\n",
      "Batch: 7200,train loss is: 6.813197065007167e-05\n",
      "test loss is 0.00011290485323085056\n",
      "Batch: 7300,train loss is: 0.00022413678193513673\n",
      "test loss is 0.00012125716859714823\n",
      "Batch: 7400,train loss is: 8.423561765803418e-05\n",
      "test loss is 0.00014432537653268594\n",
      "Batch: 7500,train loss is: 0.00010015011505785088\n",
      "test loss is 0.00012000678617164256\n",
      "Batch: 7600,train loss is: 0.00017139382451726335\n",
      "test loss is 0.00014116743245891486\n",
      "Batch: 7700,train loss is: 0.00016946411752167447\n",
      "test loss is 0.0001135915802143092\n",
      "Batch: 7800,train loss is: 6.233901773448833e-05\n",
      "test loss is 0.00011449012109056119\n",
      "Batch: 7900,train loss is: 6.396458563876182e-05\n",
      "test loss is 0.00011341913817499584\n",
      "Batch: 8000,train loss is: 4.527956012961262e-05\n",
      "test loss is 0.00012181918512512048\n",
      "Batch: 8100,train loss is: 0.00016050286182990196\n",
      "test loss is 0.000118493009282609\n",
      "Batch: 8200,train loss is: 9.60054168086566e-05\n",
      "test loss is 0.00011409104408955497\n",
      "Batch: 8300,train loss is: 6.627410995657786e-05\n",
      "test loss is 0.00011901463847068409\n",
      "Batch: 8400,train loss is: 0.0001248531070174166\n",
      "test loss is 0.0001241062939089533\n",
      "Batch: 8500,train loss is: 0.00021944758412343734\n",
      "test loss is 0.00012486434156124725\n",
      "Batch: 8600,train loss is: 9.432490726351581e-05\n",
      "test loss is 0.00013465287007537308\n",
      "Batch: 8700,train loss is: 0.00010551855242114717\n",
      "test loss is 0.0001226441291158968\n",
      "Batch: 8800,train loss is: 8.330773939988414e-05\n",
      "test loss is 0.00011532262437819317\n",
      "Batch: 8900,train loss is: 9.743731013617954e-05\n",
      "test loss is 0.00011234074880582804\n",
      "Batch: 9000,train loss is: 7.031687322110145e-05\n",
      "test loss is 0.0001151267462139485\n",
      "Batch: 9100,train loss is: 9.450149470249606e-05\n",
      "test loss is 0.00011852331444716478\n",
      "Batch: 9200,train loss is: 0.00012661703860903156\n",
      "test loss is 0.0001203153217233214\n",
      "Batch: 9300,train loss is: 0.0001461505750001135\n",
      "test loss is 0.00011557373498827452\n",
      "Batch: 9400,train loss is: 0.00011881926186395031\n",
      "test loss is 0.0001204890030675392\n",
      "Batch: 9500,train loss is: 6.872219168153291e-05\n",
      "test loss is 0.00012217386370782162\n",
      "Batch: 9600,train loss is: 8.259189954369461e-05\n",
      "test loss is 0.00011513185784441098\n",
      "Batch: 9700,train loss is: 6.991870263910645e-05\n",
      "test loss is 0.00011737027681470188\n",
      "Batch: 9800,train loss is: 6.623806474166012e-05\n",
      "test loss is 0.00011438934787314097\n",
      "Batch: 9900,train loss is: 6.771740808832651e-05\n",
      "test loss is 0.00011281089458488044\n",
      "Batch: 10000,train loss is: 6.819817030877705e-05\n",
      "test loss is 0.00012250349615224615\n",
      "Batch: 10100,train loss is: 0.0001986282574974302\n",
      "test loss is 0.00011689820626800714\n",
      "Batch: 10200,train loss is: 0.00029414562823028073\n",
      "test loss is 0.0001158703996789745\n",
      "Batch: 10300,train loss is: 0.00010767964524334843\n",
      "test loss is 0.00011701550049382274\n",
      "Batch: 10400,train loss is: 5.161284746330162e-05\n",
      "test loss is 0.0001200243774530253\n",
      "Batch: 10500,train loss is: 0.00010383994762049776\n",
      "test loss is 0.00011672333065578655\n",
      "Batch: 10600,train loss is: 7.532337343573311e-05\n",
      "test loss is 0.00011726058814603449\n",
      "Batch: 10700,train loss is: 8.627455128431045e-05\n",
      "test loss is 0.00011262991830237118\n",
      "Batch: 10800,train loss is: 9.383554671251598e-05\n",
      "test loss is 0.00011862106165970771\n",
      "Batch: 10900,train loss is: 9.807610485779378e-05\n",
      "test loss is 0.00011535315255228528\n",
      "Batch: 11000,train loss is: 0.00013708706112234645\n",
      "test loss is 0.00011268775595748125\n",
      "Batch: 11100,train loss is: 0.00013199690188410223\n",
      "test loss is 0.00011624065984091906\n",
      "Batch: 11200,train loss is: 0.00013606467769045646\n",
      "test loss is 0.00011395779171622184\n",
      "Batch: 11300,train loss is: 7.67208030316914e-05\n",
      "test loss is 0.00012218597265542823\n",
      "Batch: 11400,train loss is: 0.00019949686070175088\n",
      "test loss is 0.00011318553168596108\n",
      "Batch: 11500,train loss is: 6.286697220042038e-05\n",
      "test loss is 0.00011633785865744683\n",
      "Batch: 11600,train loss is: 0.0001305445555086434\n",
      "test loss is 0.00011808669091687014\n",
      "Batch: 11700,train loss is: 8.346343453823054e-05\n",
      "test loss is 0.00011313785306297768\n",
      "Batch: 11800,train loss is: 9.915966623798017e-05\n",
      "test loss is 0.00011341684329544931\n",
      "Batch: 11900,train loss is: 0.00011274797534106373\n",
      "test loss is 0.00012859460604839335\n",
      "Batch: 12000,train loss is: 9.29614334972292e-05\n",
      "test loss is 0.00011740971256196276\n",
      "Batch: 12100,train loss is: 0.00012636584029572734\n",
      "test loss is 0.0001142966263111518\n",
      "Batch: 12200,train loss is: 7.696863620541964e-05\n",
      "test loss is 0.00011497560291409723\n",
      "Batch: 12300,train loss is: 7.905248316998158e-05\n",
      "test loss is 0.00011981101968981774\n",
      "Batch: 12400,train loss is: 8.870881420756452e-05\n",
      "test loss is 0.00011540914476943915\n",
      "Batch: 12500,train loss is: 0.0001828683711918008\n",
      "test loss is 0.00011669086463903367\n",
      "Batch: 12600,train loss is: 6.028675351491718e-05\n",
      "test loss is 0.0001139658899714594\n",
      "Batch: 12700,train loss is: 9.236857120049368e-05\n",
      "test loss is 0.0001171942560253783\n",
      "Batch: 12800,train loss is: 4.913555958886838e-05\n",
      "test loss is 0.00011516581703965715\n",
      "Batch: 12900,train loss is: 0.00016156618272970214\n",
      "test loss is 0.00011713890549815907\n",
      "Batch: 13000,train loss is: 9.047173097455921e-05\n",
      "test loss is 0.0001122532611802803\n",
      "Batch: 13100,train loss is: 0.00018390533664913668\n",
      "test loss is 0.00011159043715474194\n",
      "Batch: 13200,train loss is: 0.00010535715350670666\n",
      "test loss is 0.0001294794397155891\n",
      "Batch: 13300,train loss is: 0.00011195017165193187\n",
      "test loss is 0.0001415493366388368\n",
      "Batch: 13400,train loss is: 8.661113314504265e-05\n",
      "test loss is 0.000111960141135027\n",
      "Batch: 13500,train loss is: 6.112808400431632e-05\n",
      "test loss is 0.00011593494101713876\n",
      "Batch: 13600,train loss is: 6.93204981419931e-05\n",
      "test loss is 0.00012749764469097635\n",
      "Batch: 13700,train loss is: 0.00011272203123256136\n",
      "test loss is 0.00011408019259693446\n",
      "Batch: 13800,train loss is: 6.925793844263507e-05\n",
      "test loss is 0.00012439744032722715\n",
      "Batch: 13900,train loss is: 5.7954665807468994e-05\n",
      "test loss is 0.0001168166408789684\n",
      "Batch: 14000,train loss is: 0.00010951783703710138\n",
      "test loss is 0.00011635474626585966\n",
      "Batch: 14100,train loss is: 7.17498204993474e-05\n",
      "test loss is 0.00011557756690644112\n",
      "Batch: 14200,train loss is: 7.034056164219401e-05\n",
      "test loss is 0.000114883618550539\n",
      "Batch: 14300,train loss is: 8.357511648730344e-05\n",
      "test loss is 0.0001142251852630501\n",
      "Batch: 14400,train loss is: 0.00016807428232708217\n",
      "test loss is 0.00011296132261992258\n",
      "Batch: 14500,train loss is: 8.200977322535342e-05\n",
      "test loss is 0.0001139021879240047\n",
      "Batch: 14600,train loss is: 0.00013363153246383135\n",
      "test loss is 0.00012207534922892776\n",
      "Batch: 14700,train loss is: 0.00010273335456599381\n",
      "test loss is 0.00011485211264637266\n",
      "Batch: 14800,train loss is: 0.00017664982921971014\n",
      "test loss is 0.00011798689660764217\n",
      "Batch: 14900,train loss is: 8.195921880096505e-05\n",
      "test loss is 0.00011213706341539254\n",
      "Batch: 15000,train loss is: 0.00010937268503380168\n",
      "test loss is 0.00011539104411015214\n",
      "Batch: 15100,train loss is: 0.00013931821256181319\n",
      "test loss is 0.00012181571902817997\n",
      "Batch: 15200,train loss is: 0.00013513111299729094\n",
      "test loss is 0.00011887374355106917\n",
      "Batch: 15300,train loss is: 0.00013500487003227196\n",
      "test loss is 0.00011437029867375312\n",
      "Batch: 15400,train loss is: 6.65410279116179e-05\n",
      "test loss is 0.0001126327836624163\n",
      "Batch: 15500,train loss is: 8.134904360605096e-05\n",
      "test loss is 0.00011878671015299566\n",
      "Batch: 15600,train loss is: 0.0001911361498737718\n",
      "test loss is 0.00011683645979496345\n",
      "Batch: 15700,train loss is: 7.607714424913812e-05\n",
      "test loss is 0.00011932765342686221\n",
      "Batch: 15800,train loss is: 8.067863671933621e-05\n",
      "test loss is 0.00012270241620523036\n",
      "Batch: 15900,train loss is: 0.00011879158694356474\n",
      "test loss is 0.00011925602492911026\n",
      "Batch: 16000,train loss is: 0.00014991655682398896\n",
      "test loss is 0.00011501986934476686\n",
      "Batch: 16100,train loss is: 0.00018207824444440172\n",
      "test loss is 0.00011322435244406648\n",
      "Batch: 16200,train loss is: 7.631520227649243e-05\n",
      "test loss is 0.00011670350110342394\n",
      "Batch: 16300,train loss is: 0.00010328997768600962\n",
      "test loss is 0.00012251283639784296\n",
      "Batch: 16400,train loss is: 6.725090067710319e-05\n",
      "test loss is 0.00011895916157034674\n",
      "Batch: 16500,train loss is: 7.744180113429827e-05\n",
      "test loss is 0.00011369819699035856\n",
      "Batch: 16600,train loss is: 7.693549844753197e-05\n",
      "test loss is 0.00011290288723499639\n",
      "Batch: 16700,train loss is: 0.00018967194124324375\n",
      "test loss is 0.00012113083352392068\n",
      "Batch: 16800,train loss is: 0.0001474555365747694\n",
      "test loss is 0.0001272849616173522\n",
      "Batch: 16900,train loss is: 8.751707734226234e-05\n",
      "test loss is 0.00012252309137996689\n",
      "Batch: 17000,train loss is: 0.00011362129715946498\n",
      "test loss is 0.00012563805879446666\n",
      "Batch: 17100,train loss is: 0.00011094650837361056\n",
      "test loss is 0.00011482711702084765\n",
      "Batch: 17200,train loss is: 6.174317123681442e-05\n",
      "test loss is 0.00011567981988336824\n",
      "Batch: 17300,train loss is: 7.548070625494805e-05\n",
      "test loss is 0.00012239690425806297\n",
      "Batch: 17400,train loss is: 5.6630696396769594e-05\n",
      "test loss is 0.000124012640146416\n",
      "Batch: 17500,train loss is: 0.00011094268295556521\n",
      "test loss is 0.00011327931736895203\n",
      "Batch: 17600,train loss is: 6.739358910520107e-05\n",
      "test loss is 0.000116428058510989\n",
      "Batch: 17700,train loss is: 5.4579579827287584e-05\n",
      "test loss is 0.00011200183985735138\n",
      "Batch: 17800,train loss is: 0.0002465319918855633\n",
      "test loss is 0.00011699896174928165\n",
      "Batch: 17900,train loss is: 4.7727108226461856e-05\n",
      "test loss is 0.00011189817261148667\n",
      "Batch: 18000,train loss is: 8.034197483395941e-05\n",
      "test loss is 0.00011975784975651906\n",
      "Batch: 18100,train loss is: 0.00018035635961351958\n",
      "test loss is 0.00011638696384632322\n",
      "Batch: 18200,train loss is: 9.536477159688283e-05\n",
      "test loss is 0.00011336958472124184\n",
      "Batch: 18300,train loss is: 0.00013127195823014865\n",
      "test loss is 0.00011164567601377807\n",
      "Batch: 18400,train loss is: 0.00010648282099471973\n",
      "test loss is 0.00011058578328297731\n",
      "Batch: 18500,train loss is: 0.0001489128934092629\n",
      "test loss is 0.0001387828620949363\n",
      "Batch: 18600,train loss is: 7.844368061724268e-05\n",
      "test loss is 0.00012774173094208616\n",
      "Batch: 18700,train loss is: 0.0001383175188198835\n",
      "test loss is 0.00011793457547075456\n",
      "Batch: 18800,train loss is: 8.56132785779896e-05\n",
      "test loss is 0.00011468944900604742\n",
      "Batch: 18900,train loss is: 0.00013305172011194414\n",
      "test loss is 0.00011863308511739761\n",
      "Batch: 19000,train loss is: 5.748887117066038e-05\n",
      "test loss is 0.00011645939376209142\n",
      "Batch: 19100,train loss is: 0.000134728315660352\n",
      "test loss is 0.00012292947349237338\n",
      "Batch: 19200,train loss is: 0.00010145819387866318\n",
      "test loss is 0.0001119761974430764\n",
      "Batch: 19300,train loss is: 0.0001203454935791523\n",
      "test loss is 0.00011445793040464674\n",
      "Batch: 19400,train loss is: 8.08714282748506e-05\n",
      "test loss is 0.00011599120797668593\n",
      "Batch: 19500,train loss is: 7.591691908173218e-05\n",
      "test loss is 0.00012996599191148297\n",
      "Batch: 19600,train loss is: 8.471473932320215e-05\n",
      "test loss is 0.00011627673597372314\n",
      "Batch: 19700,train loss is: 0.00015464768184812896\n",
      "test loss is 0.00011454304829844233\n",
      "Batch: 19800,train loss is: 7.090673559935881e-05\n",
      "test loss is 0.00011606477362112631\n",
      "Batch: 19900,train loss is: 6.953671281103712e-05\n",
      "test loss is 0.00012316326104046328\n",
      "Batch: 20000,train loss is: 0.00017957637309425447\n",
      "test loss is 0.00012112753810651721\n",
      "Batch: 20100,train loss is: 0.00013567024958663172\n",
      "test loss is 0.00012917654099454509\n",
      "Batch: 20200,train loss is: 7.596749652041848e-05\n",
      "test loss is 0.0001212593122181973\n",
      "Batch: 20300,train loss is: 5.0675395920457625e-05\n",
      "test loss is 0.00012749165070869084\n",
      "Batch: 20400,train loss is: 0.0001084640365642102\n",
      "test loss is 0.00012601275806020345\n",
      "Batch: 20500,train loss is: 0.00010271553815834084\n",
      "test loss is 0.00011178295064486388\n",
      "Batch: 20600,train loss is: 0.0002016060797664497\n",
      "test loss is 0.00011807403093635181\n",
      "Batch: 20700,train loss is: 0.0001167542935396832\n",
      "test loss is 0.000120622878212377\n",
      "Batch: 20800,train loss is: 9.034118879998635e-05\n",
      "test loss is 0.0001201249728701727\n",
      "Batch: 20900,train loss is: 8.920355287193449e-05\n",
      "test loss is 0.00011849313336918055\n",
      "Batch: 21000,train loss is: 4.333630713033602e-05\n",
      "test loss is 0.00011201539647663773\n",
      "Batch: 21100,train loss is: 7.751048375786846e-05\n",
      "test loss is 0.00011196822246870953\n",
      "Batch: 21200,train loss is: 6.20597124873428e-05\n",
      "test loss is 0.00011736002053687935\n",
      "Batch: 21300,train loss is: 8.934933469526979e-05\n",
      "test loss is 0.00011980513399907728\n",
      "Batch: 21400,train loss is: 0.00011531496637318633\n",
      "test loss is 0.00011107354887083631\n",
      "Batch: 21500,train loss is: 8.822504795652504e-05\n",
      "test loss is 0.00011717853926482372\n",
      "Batch: 21600,train loss is: 0.00012076853329457512\n",
      "test loss is 0.00012631487350945412\n",
      "Batch: 21700,train loss is: 7.287279131473297e-05\n",
      "test loss is 0.00011974680971353818\n",
      "Batch: 21800,train loss is: 0.00015202130651296989\n",
      "test loss is 0.00011281902072083591\n",
      "Batch: 21900,train loss is: 8.680308087753721e-05\n",
      "test loss is 0.00012306710774097958\n",
      "Batch: 22000,train loss is: 0.00010509255160373434\n",
      "test loss is 0.00013844424210786185\n",
      "Batch: 22100,train loss is: 6.797452363581546e-05\n",
      "test loss is 0.00011182084791288\n",
      "Batch: 22200,train loss is: 6.284161936911076e-05\n",
      "test loss is 0.00011896684991753114\n",
      "Batch: 22300,train loss is: 9.302692278424574e-05\n",
      "test loss is 0.00012660583056084402\n",
      "Batch: 22400,train loss is: 0.00010355213351032995\n",
      "test loss is 0.00011660233658415109\n",
      "Batch: 22500,train loss is: 8.202443831934066e-05\n",
      "test loss is 0.00011520562775178212\n",
      "Batch: 22600,train loss is: 0.00010574257850546827\n",
      "test loss is 0.00011864979596371833\n",
      "Batch: 22700,train loss is: 6.755336743856106e-05\n",
      "test loss is 0.00011147100782649996\n",
      "Batch: 22800,train loss is: 0.00011730712889026548\n",
      "test loss is 0.00012433219920073168\n",
      "Batch: 22900,train loss is: 5.8208425061120764e-05\n",
      "test loss is 0.00011789027642606608\n",
      "Batch: 23000,train loss is: 0.0001612557226755997\n",
      "test loss is 0.00011745111597271689\n",
      "Batch: 23100,train loss is: 8.764588564506598e-05\n",
      "test loss is 0.00011087610805396066\n",
      "Batch: 23200,train loss is: 7.691708810628864e-05\n",
      "test loss is 0.00011153265289827794\n",
      "Batch: 23300,train loss is: 5.812514413194368e-05\n",
      "test loss is 0.00011235014308425535\n",
      "Batch: 23400,train loss is: 8.652338059868135e-05\n",
      "test loss is 0.00011570688554922254\n",
      "Batch: 23500,train loss is: 0.00013675486740651624\n",
      "test loss is 0.00011464844522238404\n",
      "Batch: 23600,train loss is: 7.195191137571898e-05\n",
      "test loss is 0.00012222960869435098\n",
      "Batch: 23700,train loss is: 0.00014300612777433323\n",
      "test loss is 0.00011231895442336493\n",
      "Batch: 23800,train loss is: 0.00016593205762434083\n",
      "test loss is 0.00011473885375096157\n",
      "Batch: 23900,train loss is: 0.0003672045157744654\n",
      "test loss is 0.00011433837563356561\n",
      "Batch: 24000,train loss is: 8.518023035428627e-05\n",
      "test loss is 0.00013094043505675106\n",
      "Batch: 24100,train loss is: 0.00010720384243408284\n",
      "test loss is 0.00011282492611651654\n",
      "Batch: 24200,train loss is: 0.00017912511275995345\n",
      "test loss is 0.00012043294802196063\n",
      "Batch: 24300,train loss is: 0.0001028516316377764\n",
      "test loss is 0.00013447832054617602\n",
      "Batch: 24400,train loss is: 0.0001273192702764039\n",
      "test loss is 0.00011392262835778572\n",
      "Batch: 24500,train loss is: 0.00013261164326908243\n",
      "test loss is 0.00012004862192014926\n",
      "Batch: 24600,train loss is: 0.0001053209889548371\n",
      "test loss is 0.0001339379911398231\n",
      "Batch: 24700,train loss is: 7.967736561299321e-05\n",
      "test loss is 0.00011280240550729594\n",
      "Batch: 24800,train loss is: 0.00014766347387524872\n",
      "test loss is 0.0001156957214058084\n",
      "Batch: 24900,train loss is: 4.9790000054663965e-05\n",
      "test loss is 0.00011107119405443671\n",
      "Batch: 25000,train loss is: 8.885091162052853e-05\n",
      "test loss is 0.00011337435867734641\n",
      "Batch: 25100,train loss is: 6.520318680283362e-05\n",
      "test loss is 0.00011197225470740079\n",
      "Batch: 25200,train loss is: 0.00011141337498323118\n",
      "test loss is 0.00011700859941530728\n",
      "Batch: 25300,train loss is: 9.732892420895297e-05\n",
      "test loss is 0.00011081231683826936\n",
      "Batch: 25400,train loss is: 9.422844002403277e-05\n",
      "test loss is 0.0001206914105289149\n",
      "Batch: 25500,train loss is: 7.447550601103538e-05\n",
      "test loss is 0.00011176841879374754\n",
      "Batch: 25600,train loss is: 0.00011004004147911471\n",
      "test loss is 0.00011858086159601767\n",
      "Batch: 25700,train loss is: 0.00012520665259106232\n",
      "test loss is 0.0001124843952614694\n",
      "Batch: 25800,train loss is: 7.891969722332204e-05\n",
      "test loss is 0.00011557966460044412\n",
      "Batch: 25900,train loss is: 5.587147313537521e-05\n",
      "test loss is 0.00011294622128745022\n",
      "Batch: 26000,train loss is: 0.00011375228674019052\n",
      "test loss is 0.00011512935622584184\n",
      "Batch: 26100,train loss is: 0.0001780996901759265\n",
      "test loss is 0.00011127216162957607\n",
      "Batch: 26200,train loss is: 0.00018414831032621717\n",
      "test loss is 0.00011615031459148886\n",
      "Batch: 26300,train loss is: 0.00013607171290303815\n",
      "test loss is 0.00011137235301161895\n",
      "Batch: 26400,train loss is: 7.845038485561971e-05\n",
      "test loss is 0.0001101120111540543\n",
      "Batch: 26500,train loss is: 0.00010823149403846976\n",
      "test loss is 0.00011225089406087624\n",
      "Batch: 26600,train loss is: 0.00015711802315599726\n",
      "test loss is 0.00011183824602023435\n",
      "Batch: 26700,train loss is: 5.8636065442966475e-05\n",
      "test loss is 0.00011792223735316933\n",
      "Batch: 26800,train loss is: 0.00017061993962427384\n",
      "test loss is 0.00011429663190511088\n",
      "Batch: 26900,train loss is: 5.48596671593046e-05\n",
      "test loss is 0.0001260516768742629\n",
      "Batch: 27000,train loss is: 0.00010262839592973775\n",
      "test loss is 0.00012396554721549337\n",
      "Batch: 27100,train loss is: 5.9146932237006556e-05\n",
      "test loss is 0.00012751072216622542\n",
      "Batch: 27200,train loss is: 9.265617755866181e-05\n",
      "test loss is 0.00011178458981202278\n",
      "Batch: 27300,train loss is: 0.0001152268514300129\n",
      "test loss is 0.00011166583814446928\n",
      "Batch: 27400,train loss is: 9.469239796564276e-05\n",
      "test loss is 0.00011483442828764862\n",
      "Batch: 27500,train loss is: 8.279318324227422e-05\n",
      "test loss is 0.00012694908022881465\n",
      "Batch: 27600,train loss is: 9.802191551659682e-05\n",
      "test loss is 0.00013090907145792954\n",
      "Batch: 27700,train loss is: 0.00031901337706603426\n",
      "test loss is 0.00012944170926007956\n",
      "Batch: 27800,train loss is: 9.026593589133693e-05\n",
      "test loss is 0.00011449799661390597\n",
      "Batch: 27900,train loss is: 0.0001366403125844356\n",
      "test loss is 0.0001126201689704402\n",
      "Batch: 28000,train loss is: 7.378834614600246e-05\n",
      "test loss is 0.00011885419686259705\n",
      "Batch: 28100,train loss is: 0.00017134591791835587\n",
      "test loss is 0.0001260717095507949\n",
      "Batch: 28200,train loss is: 0.00011421161117092325\n",
      "test loss is 0.00011787821393596935\n",
      "Batch: 28300,train loss is: 0.0003109244814967878\n",
      "test loss is 0.00011305883677080159\n",
      "Batch: 28400,train loss is: 0.00010221057808239621\n",
      "test loss is 0.00011382384717086137\n",
      "Batch: 28500,train loss is: 0.00010298051493822386\n",
      "test loss is 0.00012068416009836642\n",
      "Batch: 28600,train loss is: 7.883186222663063e-05\n",
      "test loss is 0.00011444276662189402\n",
      "Batch: 28700,train loss is: 0.00010027389048387004\n",
      "test loss is 0.00011393768492588678\n",
      "Batch: 28800,train loss is: 0.00016613173702945827\n",
      "test loss is 0.00011079553268354786\n",
      "Batch: 28900,train loss is: 0.00011917840318475777\n",
      "test loss is 0.00012716721743153425\n",
      "Batch: 29000,train loss is: 0.00010021942373131648\n",
      "test loss is 0.00011244418131344189\n",
      "Batch: 29100,train loss is: 9.117125642883884e-05\n",
      "test loss is 0.00012282779285349244\n",
      "Batch: 29200,train loss is: 0.00010094759806131637\n",
      "test loss is 0.00014666546362153776\n",
      "Batch: 29300,train loss is: 9.777549249137077e-05\n",
      "test loss is 0.00011403365132089591\n",
      "Batch: 29400,train loss is: 0.0001408030654680721\n",
      "test loss is 0.00012116212564594144\n",
      "Batch: 29500,train loss is: 6.263421287938339e-05\n",
      "test loss is 0.00011237667268397175\n",
      "Batch: 29600,train loss is: 0.00020774373950300395\n",
      "test loss is 0.00012459445246849245\n",
      "Batch: 29700,train loss is: 6.93407101990271e-05\n",
      "test loss is 0.00011134030867068443\n",
      "Batch: 29800,train loss is: 0.00020280806162691328\n",
      "test loss is 0.00011110674784331629\n",
      "Batch: 29900,train loss is: 0.00027767197290258733\n",
      "test loss is 0.00011259264057875993\n",
      "Batch: 30000,train loss is: 7.782654433138325e-05\n",
      "test loss is 0.00011542609594891478\n",
      "Batch: 30100,train loss is: 9.278082229204221e-05\n",
      "test loss is 0.00012358318991386255\n",
      "Batch: 30200,train loss is: 8.124413946593392e-05\n",
      "test loss is 0.00010971360669750977\n",
      "Batch: 30300,train loss is: 8.841819907434797e-05\n",
      "test loss is 0.000112613579113758\n",
      "Batch: 30400,train loss is: 8.483519884531486e-05\n",
      "test loss is 0.00012075612709403057\n",
      "Batch: 30500,train loss is: 8.541539761208174e-05\n",
      "test loss is 0.00011219079287590304\n",
      "Batch: 30600,train loss is: 7.112696152264671e-05\n",
      "test loss is 0.00011302891132378261\n",
      "Batch: 30700,train loss is: 8.558941197808204e-05\n",
      "test loss is 0.00011777601962559942\n",
      "Batch: 30800,train loss is: 7.298039675483399e-05\n",
      "test loss is 0.00011584669571413741\n",
      "Batch: 30900,train loss is: 9.193341217587769e-05\n",
      "test loss is 0.00011378832677175789\n",
      "Batch: 31000,train loss is: 5.274489611285405e-05\n",
      "test loss is 0.00011567299391479274\n",
      "Batch: 31100,train loss is: 8.966396784178015e-05\n",
      "test loss is 0.00011576143058901509\n",
      "Batch: 31200,train loss is: 8.245377548772891e-05\n",
      "test loss is 0.00011335022594722495\n",
      "Batch: 31300,train loss is: 0.00012621555451415809\n",
      "test loss is 0.00011108494337115688\n",
      "Batch: 31400,train loss is: 9.132108282965804e-05\n",
      "test loss is 0.00013544901353056122\n",
      "Batch: 31500,train loss is: 6.257432502492601e-05\n",
      "test loss is 0.0001195015668906749\n",
      "Batch: 31600,train loss is: 8.444703892334584e-05\n",
      "test loss is 0.00011159995389254976\n",
      "Batch: 31700,train loss is: 6.573110854292164e-05\n",
      "test loss is 0.0001125086192550309\n",
      "Batch: 31800,train loss is: 0.00012850812070132198\n",
      "test loss is 0.00011151326312289533\n",
      "Batch: 31900,train loss is: 0.00012044126622629628\n",
      "test loss is 0.00011976225252860702\n",
      "Batch: 32000,train loss is: 0.00044304506640236596\n",
      "test loss is 0.0001144463849478898\n",
      "Batch: 32100,train loss is: 4.053259115690097e-05\n",
      "test loss is 0.00011549089211390528\n",
      "Batch: 32200,train loss is: 7.799976321055871e-05\n",
      "test loss is 0.00011050646158499077\n",
      "Batch: 32300,train loss is: 7.45812082622736e-05\n",
      "test loss is 0.00011236908908506576\n",
      "Batch: 32400,train loss is: 8.818010619116276e-05\n",
      "test loss is 0.00011915947622905547\n",
      "Batch: 32500,train loss is: 0.00013140840708601977\n",
      "test loss is 0.0001149125364544384\n",
      "Batch: 32600,train loss is: 6.891050723621381e-05\n",
      "test loss is 0.0001157100286051423\n",
      "Batch: 32700,train loss is: 6.450807630288667e-05\n",
      "test loss is 0.00011216013589368584\n",
      "Batch: 32800,train loss is: 8.38489746498762e-05\n",
      "test loss is 0.0001251546428770852\n",
      "Batch: 32900,train loss is: 9.496455470501389e-05\n",
      "test loss is 0.00011395912189352914\n",
      "Batch: 33000,train loss is: 5.368033783417153e-05\n",
      "test loss is 0.00011214939047809003\n",
      "Batch: 33100,train loss is: 7.833687191031548e-05\n",
      "test loss is 0.00012531061690375232\n",
      "Batch: 33200,train loss is: 9.324302224928527e-05\n",
      "test loss is 0.00012598738535437798\n",
      "Batch: 33300,train loss is: 8.654881758151242e-05\n",
      "test loss is 0.00012770144624529332\n",
      "Batch: 33400,train loss is: 7.24534018073415e-05\n",
      "test loss is 0.00011329264118086209\n",
      "Batch: 33500,train loss is: 7.715188700958214e-05\n",
      "test loss is 0.00011341850106171525\n",
      "Batch: 33600,train loss is: 6.041429418345487e-05\n",
      "test loss is 0.00011637440238565798\n",
      "Batch: 33700,train loss is: 8.308364098521581e-05\n",
      "test loss is 0.00012101480439966025\n",
      "Batch: 33800,train loss is: 0.00010551223616347399\n",
      "test loss is 0.0001157511129616039\n",
      "Batch: 33900,train loss is: 8.281771371311221e-05\n",
      "test loss is 0.00011360383538931267\n",
      "-----------------------Epoch: 6----------------------------------\n",
      "Batch: 0,train loss is: 0.00010904506310463404\n",
      "test loss is 0.00011625879846613292\n",
      "Batch: 100,train loss is: 0.00013762993082502037\n",
      "test loss is 0.0001123845608854791\n",
      "Batch: 200,train loss is: 0.00010077822007372944\n",
      "test loss is 0.00011131823449718562\n",
      "Batch: 300,train loss is: 8.334469849679257e-05\n",
      "test loss is 0.00011385175772656745\n",
      "Batch: 400,train loss is: 6.84449897211416e-05\n",
      "test loss is 0.00011066334162866947\n",
      "Batch: 500,train loss is: 0.0001259588302303041\n",
      "test loss is 0.00013257887900616034\n",
      "Batch: 600,train loss is: 8.597068968043802e-05\n",
      "test loss is 0.00011136129708765681\n",
      "Batch: 700,train loss is: 6.995260436962181e-05\n",
      "test loss is 0.00011450823289637609\n",
      "Batch: 800,train loss is: 0.00013509723711867722\n",
      "test loss is 0.00011821443328300276\n",
      "Batch: 900,train loss is: 5.649519810693869e-05\n",
      "test loss is 0.00011761430135201993\n",
      "Batch: 1000,train loss is: 9.336189199305105e-05\n",
      "test loss is 0.00011661458514040228\n",
      "Batch: 1100,train loss is: 0.00020140315079630893\n",
      "test loss is 0.00011768963663324488\n",
      "Batch: 1200,train loss is: 5.2600790115194515e-05\n",
      "test loss is 0.00012095646394426106\n",
      "Batch: 1300,train loss is: 0.00011180532507335484\n",
      "test loss is 0.00011512498838559645\n",
      "Batch: 1400,train loss is: 8.258453825919275e-05\n",
      "test loss is 0.00011222823092564126\n",
      "Batch: 1500,train loss is: 8.74311677848229e-05\n",
      "test loss is 0.00011393016618602606\n",
      "Batch: 1600,train loss is: 6.687041967290656e-05\n",
      "test loss is 0.00011388347919089249\n",
      "Batch: 1700,train loss is: 0.00013955520622614966\n",
      "test loss is 0.00012335391245434297\n",
      "Batch: 1800,train loss is: 6.597800967133266e-05\n",
      "test loss is 0.00011153561839786694\n",
      "Batch: 1900,train loss is: 5.369113986565122e-05\n",
      "test loss is 0.00011618948570596649\n",
      "Batch: 2000,train loss is: 0.00012146935188256001\n",
      "test loss is 0.00011345994103177713\n",
      "Batch: 2100,train loss is: 7.831234918557105e-05\n",
      "test loss is 0.00011773024723184372\n",
      "Batch: 2200,train loss is: 0.00013072274662712186\n",
      "test loss is 0.00011499151502404481\n",
      "Batch: 2300,train loss is: 8.891205640231612e-05\n",
      "test loss is 0.00011353487617260653\n",
      "Batch: 2400,train loss is: 9.019687097093364e-05\n",
      "test loss is 0.00011221433491492181\n",
      "Batch: 2500,train loss is: 0.00010326775209985719\n",
      "test loss is 0.00010965138428449063\n",
      "Batch: 2600,train loss is: 6.992477409113515e-05\n",
      "test loss is 0.00011050967507013287\n",
      "Batch: 2700,train loss is: 0.00013602079282539446\n",
      "test loss is 0.000112763354867021\n",
      "Batch: 2800,train loss is: 0.00010382266540956388\n",
      "test loss is 0.00012433628132249865\n",
      "Batch: 2900,train loss is: 0.00011719008444854196\n",
      "test loss is 0.00010982467523698404\n",
      "Batch: 3000,train loss is: 7.673779012674155e-05\n",
      "test loss is 0.00011377044535961226\n",
      "Batch: 3100,train loss is: 7.187619519686739e-05\n",
      "test loss is 0.00011047379057357728\n",
      "Batch: 3200,train loss is: 0.0001490738135312245\n",
      "test loss is 0.00011681702448243977\n",
      "Batch: 3300,train loss is: 9.38109076314405e-05\n",
      "test loss is 0.00011137777533815357\n",
      "Batch: 3400,train loss is: 9.025110698819943e-05\n",
      "test loss is 0.00013151930648320397\n",
      "Batch: 3500,train loss is: 7.409577771087011e-05\n",
      "test loss is 0.00011942384112660347\n",
      "Batch: 3600,train loss is: 6.650461627712823e-05\n",
      "test loss is 0.00011432218820703123\n",
      "Batch: 3700,train loss is: 6.055567381066749e-05\n",
      "test loss is 0.00010928746196888886\n",
      "Batch: 3800,train loss is: 6.593467654698285e-05\n",
      "test loss is 0.00011099277158071723\n",
      "Batch: 3900,train loss is: 0.00011274327298533272\n",
      "test loss is 0.00011366837765875402\n",
      "Batch: 4000,train loss is: 4.9190837486196517e-05\n",
      "test loss is 0.0001135697598595551\n",
      "Batch: 4100,train loss is: 0.00010689048619379927\n",
      "test loss is 0.00011796070522947564\n",
      "Batch: 4200,train loss is: 0.00011915405077048073\n",
      "test loss is 0.00012850716446718337\n",
      "Batch: 4300,train loss is: 7.949719418724507e-05\n",
      "test loss is 0.00011137226861731671\n",
      "Batch: 4400,train loss is: 6.601705733609987e-05\n",
      "test loss is 0.00011620932074993606\n",
      "Batch: 4500,train loss is: 7.059149870742627e-05\n",
      "test loss is 0.00010938009278087834\n",
      "Batch: 4600,train loss is: 8.184619390087869e-05\n",
      "test loss is 0.00011320534374125929\n",
      "Batch: 4700,train loss is: 0.0001496009820122337\n",
      "test loss is 0.00011874194462613764\n",
      "Batch: 4800,train loss is: 0.00024251057438065817\n",
      "test loss is 0.00013239959462815802\n",
      "Batch: 4900,train loss is: 9.508480742375012e-05\n",
      "test loss is 0.00011548082440555512\n",
      "Batch: 5000,train loss is: 6.116320145327313e-05\n",
      "test loss is 0.00011655576531474045\n",
      "Batch: 5100,train loss is: 6.870798324706133e-05\n",
      "test loss is 0.00011046618673604962\n",
      "Batch: 5200,train loss is: 8.478607143730831e-05\n",
      "test loss is 0.0001210611432446507\n",
      "Batch: 5300,train loss is: 8.867804629310709e-05\n",
      "test loss is 0.00011773085559631191\n",
      "Batch: 5400,train loss is: 0.0002035756981337074\n",
      "test loss is 0.00011826500534326491\n",
      "Batch: 5500,train loss is: 7.585954090733216e-05\n",
      "test loss is 0.00013046238505550937\n",
      "Batch: 5600,train loss is: 0.0001490798208635062\n",
      "test loss is 0.00011348483128161776\n",
      "Batch: 5700,train loss is: 0.0001338501860141362\n",
      "test loss is 0.00012022249883171804\n",
      "Batch: 5800,train loss is: 0.00014723328826251694\n",
      "test loss is 0.00011896281737640277\n",
      "Batch: 5900,train loss is: 0.0002547890412816317\n",
      "test loss is 0.00012040122885667635\n",
      "Batch: 6000,train loss is: 0.00015757530578995577\n",
      "test loss is 0.00011921294598456909\n",
      "Batch: 6100,train loss is: 6.568536824967955e-05\n",
      "test loss is 0.00011279757002340476\n",
      "Batch: 6200,train loss is: 8.205890443478796e-05\n",
      "test loss is 0.00011016149169819815\n",
      "Batch: 6300,train loss is: 0.00012420986631981403\n",
      "test loss is 0.00012860779280023568\n",
      "Batch: 6400,train loss is: 6.839780116662915e-05\n",
      "test loss is 0.0001127361792511537\n",
      "Batch: 6500,train loss is: 7.498693486541947e-05\n",
      "test loss is 0.00011479860165526198\n",
      "Batch: 6600,train loss is: 0.00020122995531847402\n",
      "test loss is 0.00011197061266058279\n",
      "Batch: 6700,train loss is: 9.564329702707757e-05\n",
      "test loss is 0.00011210405440855229\n",
      "Batch: 6800,train loss is: 0.00012872005852332516\n",
      "test loss is 0.00011576734483643433\n",
      "Batch: 6900,train loss is: 0.00010771975999836498\n",
      "test loss is 0.00011490119898089868\n",
      "Batch: 7000,train loss is: 0.0001174355944232792\n",
      "test loss is 0.0001130783245926358\n",
      "Batch: 7100,train loss is: 0.0001254808074432053\n",
      "test loss is 0.00012764017933094586\n",
      "Batch: 7200,train loss is: 6.430387504006745e-05\n",
      "test loss is 0.00010893653212624838\n",
      "Batch: 7300,train loss is: 0.00022009288001932306\n",
      "test loss is 0.00011825443583333878\n",
      "Batch: 7400,train loss is: 8.186059159483706e-05\n",
      "test loss is 0.00013743153744375124\n",
      "Batch: 7500,train loss is: 9.446601282985725e-05\n",
      "test loss is 0.00011539215998274563\n",
      "Batch: 7600,train loss is: 0.00016356600326422832\n",
      "test loss is 0.00013799235663871997\n",
      "Batch: 7700,train loss is: 0.00016598717846689595\n",
      "test loss is 0.000109319574637291\n",
      "Batch: 7800,train loss is: 6.15316400801644e-05\n",
      "test loss is 0.00011069221683010047\n",
      "Batch: 7900,train loss is: 5.7185127630675684e-05\n",
      "test loss is 0.00010923496443858552\n",
      "Batch: 8000,train loss is: 4.296027368293359e-05\n",
      "test loss is 0.00011901273427114452\n",
      "Batch: 8100,train loss is: 0.00015039428810699183\n",
      "test loss is 0.00011421885947206219\n",
      "Batch: 8200,train loss is: 9.818740055325555e-05\n",
      "test loss is 0.00010998794376362185\n",
      "Batch: 8300,train loss is: 6.563573253565046e-05\n",
      "test loss is 0.00011510265548593664\n",
      "Batch: 8400,train loss is: 0.0001237491635071762\n",
      "test loss is 0.00012070040824061974\n",
      "Batch: 8500,train loss is: 0.00021240242693048138\n",
      "test loss is 0.00012128701215613258\n",
      "Batch: 8600,train loss is: 9.457443840993813e-05\n",
      "test loss is 0.0001318381842377565\n",
      "Batch: 8700,train loss is: 0.00010000545882607184\n",
      "test loss is 0.00011772407716499895\n",
      "Batch: 8800,train loss is: 8.44061393922759e-05\n",
      "test loss is 0.00011128959581990665\n",
      "Batch: 8900,train loss is: 9.423514815350048e-05\n",
      "test loss is 0.00010836778364591307\n",
      "Batch: 9000,train loss is: 6.876055718483471e-05\n",
      "test loss is 0.00011081245986783646\n",
      "Batch: 9100,train loss is: 9.135290517073638e-05\n",
      "test loss is 0.00011506918195788568\n",
      "Batch: 9200,train loss is: 0.00012248446474254963\n",
      "test loss is 0.00011607971091479837\n",
      "Batch: 9300,train loss is: 0.00013934596260852788\n",
      "test loss is 0.00011168615285650141\n",
      "Batch: 9400,train loss is: 0.00011340759616872172\n",
      "test loss is 0.0001164071452203891\n",
      "Batch: 9500,train loss is: 6.684759470555289e-05\n",
      "test loss is 0.00011818466229206303\n",
      "Batch: 9600,train loss is: 7.962264647039866e-05\n",
      "test loss is 0.00011090323040019544\n",
      "Batch: 9700,train loss is: 7.102913675568577e-05\n",
      "test loss is 0.00011317080728146427\n",
      "Batch: 9800,train loss is: 6.34967350231423e-05\n",
      "test loss is 0.00010984435389315882\n",
      "Batch: 9900,train loss is: 6.473870016610747e-05\n",
      "test loss is 0.00010839033615927309\n",
      "Batch: 10000,train loss is: 6.61306335202042e-05\n",
      "test loss is 0.00011847434809003517\n",
      "Batch: 10100,train loss is: 0.00019552008542215294\n",
      "test loss is 0.00011320795554515\n",
      "Batch: 10200,train loss is: 0.00027886811521984006\n",
      "test loss is 0.00011214766916424144\n",
      "Batch: 10300,train loss is: 0.00010116015607064726\n",
      "test loss is 0.00011262277469802148\n",
      "Batch: 10400,train loss is: 4.8893573549189324e-05\n",
      "test loss is 0.00011602815313411476\n",
      "Batch: 10500,train loss is: 9.968034056028952e-05\n",
      "test loss is 0.00011256547561653738\n",
      "Batch: 10600,train loss is: 7.532525560087268e-05\n",
      "test loss is 0.00011328569865193359\n",
      "Batch: 10700,train loss is: 8.222383716710357e-05\n",
      "test loss is 0.00010873114521119852\n",
      "Batch: 10800,train loss is: 9.10853829976407e-05\n",
      "test loss is 0.00011505028590387863\n",
      "Batch: 10900,train loss is: 9.509233569219778e-05\n",
      "test loss is 0.00011137100069526402\n",
      "Batch: 11000,train loss is: 0.0001284243363948512\n",
      "test loss is 0.00010853831748050312\n",
      "Batch: 11100,train loss is: 0.00012810952966889425\n",
      "test loss is 0.00011210963490776538\n",
      "Batch: 11200,train loss is: 0.00013227309407408763\n",
      "test loss is 0.00010934218560816717\n",
      "Batch: 11300,train loss is: 7.367950097188156e-05\n",
      "test loss is 0.00011790439579824918\n",
      "Batch: 11400,train loss is: 0.00019286323422915228\n",
      "test loss is 0.0001090353639967679\n",
      "Batch: 11500,train loss is: 6.134716378204268e-05\n",
      "test loss is 0.00011375721448157843\n",
      "Batch: 11600,train loss is: 0.00012304621643623983\n",
      "test loss is 0.00011282959121054329\n",
      "Batch: 11700,train loss is: 7.853256301734124e-05\n",
      "test loss is 0.00010884699291030155\n",
      "Batch: 11800,train loss is: 9.693898519019077e-05\n",
      "test loss is 0.00010941100210559852\n",
      "Batch: 11900,train loss is: 0.00011041237055527415\n",
      "test loss is 0.00012341966751204612\n",
      "Batch: 12000,train loss is: 9.033702334280256e-05\n",
      "test loss is 0.00011348849107123071\n",
      "Batch: 12100,train loss is: 0.00012184667315022532\n",
      "test loss is 0.00010951889243424078\n",
      "Batch: 12200,train loss is: 7.439114728738483e-05\n",
      "test loss is 0.00011096501334640419\n",
      "Batch: 12300,train loss is: 7.53475800864627e-05\n",
      "test loss is 0.00011550291847898867\n",
      "Batch: 12400,train loss is: 8.370474523681765e-05\n",
      "test loss is 0.00011119775098925852\n",
      "Batch: 12500,train loss is: 0.00017341699550627198\n",
      "test loss is 0.00011258443973743896\n",
      "Batch: 12600,train loss is: 5.9158915829661746e-05\n",
      "test loss is 0.00011053913620484956\n",
      "Batch: 12700,train loss is: 8.687983683230573e-05\n",
      "test loss is 0.00011236432907824485\n",
      "Batch: 12800,train loss is: 4.843708841718738e-05\n",
      "test loss is 0.00011096340503373421\n",
      "Batch: 12900,train loss is: 0.0001521839814076705\n",
      "test loss is 0.00011164543850755918\n",
      "Batch: 13000,train loss is: 8.934825965801721e-05\n",
      "test loss is 0.00010871839291486079\n",
      "Batch: 13100,train loss is: 0.00018128965299942028\n",
      "test loss is 0.00010784155857808549\n",
      "Batch: 13200,train loss is: 0.00010144888123598582\n",
      "test loss is 0.00012429003634923104\n",
      "Batch: 13300,train loss is: 0.00010793412193129515\n",
      "test loss is 0.00013724212362674565\n",
      "Batch: 13400,train loss is: 8.317363340255412e-05\n",
      "test loss is 0.00010816587089830273\n",
      "Batch: 13500,train loss is: 5.9852816888707206e-05\n",
      "test loss is 0.0001121469843631411\n",
      "Batch: 13600,train loss is: 6.773874347206541e-05\n",
      "test loss is 0.00012365261831175878\n",
      "Batch: 13700,train loss is: 0.00010908369212373082\n",
      "test loss is 0.00011048468940552054\n",
      "Batch: 13800,train loss is: 6.676400877355255e-05\n",
      "test loss is 0.00011983733859080181\n",
      "Batch: 13900,train loss is: 5.5537696836384975e-05\n",
      "test loss is 0.00011284132526368233\n",
      "Batch: 14000,train loss is: 0.0001057313639244359\n",
      "test loss is 0.00011174549067972283\n",
      "Batch: 14100,train loss is: 6.956345871317069e-05\n",
      "test loss is 0.00011164916582831369\n",
      "Batch: 14200,train loss is: 6.778894917364454e-05\n",
      "test loss is 0.00011075602220072136\n",
      "Batch: 14300,train loss is: 8.264765023844704e-05\n",
      "test loss is 0.00011076858391169212\n",
      "Batch: 14400,train loss is: 0.00015951703144328553\n",
      "test loss is 0.00010895483647054948\n",
      "Batch: 14500,train loss is: 7.714426551251145e-05\n",
      "test loss is 0.00010987549238026594\n",
      "Batch: 14600,train loss is: 0.00012972342252528054\n",
      "test loss is 0.0001181030739730618\n",
      "Batch: 14700,train loss is: 0.00010310233471334928\n",
      "test loss is 0.00011152849572867326\n",
      "Batch: 14800,train loss is: 0.0001719848913288053\n",
      "test loss is 0.0001143871580292852\n",
      "Batch: 14900,train loss is: 7.882545673867742e-05\n",
      "test loss is 0.00010841945706256444\n",
      "Batch: 15000,train loss is: 0.00011079738233779967\n",
      "test loss is 0.00011154116162860462\n",
      "Batch: 15100,train loss is: 0.0001338455919834563\n",
      "test loss is 0.00011793543176782148\n",
      "Batch: 15200,train loss is: 0.0001282024585137592\n",
      "test loss is 0.00011522269969719862\n",
      "Batch: 15300,train loss is: 0.00013063180317735457\n",
      "test loss is 0.00011009439066267734\n",
      "Batch: 15400,train loss is: 6.426095150566622e-05\n",
      "test loss is 0.0001088629767352023\n",
      "Batch: 15500,train loss is: 7.878989962699847e-05\n",
      "test loss is 0.00011427043794589143\n",
      "Batch: 15600,train loss is: 0.00017946120605253133\n",
      "test loss is 0.00011304930247510904\n",
      "Batch: 15700,train loss is: 7.66602390847679e-05\n",
      "test loss is 0.00011553112919793412\n",
      "Batch: 15800,train loss is: 8.04129409073821e-05\n",
      "test loss is 0.00011875120573000657\n",
      "Batch: 15900,train loss is: 0.00011322755543492146\n",
      "test loss is 0.00011518839115739217\n",
      "Batch: 16000,train loss is: 0.00014364838903922525\n",
      "test loss is 0.00011173313922952084\n",
      "Batch: 16100,train loss is: 0.0001764582936797529\n",
      "test loss is 0.0001096305080581165\n",
      "Batch: 16200,train loss is: 7.165591773894432e-05\n",
      "test loss is 0.00011338110538766651\n",
      "Batch: 16300,train loss is: 0.00010235201061190344\n",
      "test loss is 0.00011913765891650866\n",
      "Batch: 16400,train loss is: 6.583204411953606e-05\n",
      "test loss is 0.00011550864670659083\n",
      "Batch: 16500,train loss is: 7.344267618111633e-05\n",
      "test loss is 0.00010983260685803136\n",
      "Batch: 16600,train loss is: 7.239142557038487e-05\n",
      "test loss is 0.00010869560747586615\n",
      "Batch: 16700,train loss is: 0.0001817867578239972\n",
      "test loss is 0.00011620840652135026\n",
      "Batch: 16800,train loss is: 0.00014406818453046203\n",
      "test loss is 0.00012399624148509038\n",
      "Batch: 16900,train loss is: 8.33888603315049e-05\n",
      "test loss is 0.00011830474517469334\n",
      "Batch: 17000,train loss is: 0.0001120298239968993\n",
      "test loss is 0.0001220948363762848\n",
      "Batch: 17100,train loss is: 0.00010954351358007231\n",
      "test loss is 0.00011165145040628178\n",
      "Batch: 17200,train loss is: 6.062758692591421e-05\n",
      "test loss is 0.00011181051275733071\n",
      "Batch: 17300,train loss is: 7.323440681554566e-05\n",
      "test loss is 0.00011758153649248374\n",
      "Batch: 17400,train loss is: 5.454258459958541e-05\n",
      "test loss is 0.00012064067955638391\n",
      "Batch: 17500,train loss is: 0.00010862352354316542\n",
      "test loss is 0.00010892036343133442\n",
      "Batch: 17600,train loss is: 6.46248018280386e-05\n",
      "test loss is 0.00011284694986095855\n",
      "Batch: 17700,train loss is: 5.230082295161993e-05\n",
      "test loss is 0.00010812270358571057\n",
      "Batch: 17800,train loss is: 0.00023887059115829302\n",
      "test loss is 0.00011292743989551824\n",
      "Batch: 17900,train loss is: 4.557183448690859e-05\n",
      "test loss is 0.00010797977075952823\n",
      "Batch: 18000,train loss is: 7.670749052625546e-05\n",
      "test loss is 0.00011633707007452445\n",
      "Batch: 18100,train loss is: 0.0001718792976578172\n",
      "test loss is 0.0001121672692714158\n",
      "Batch: 18200,train loss is: 9.410760941966265e-05\n",
      "test loss is 0.00010958365735704968\n",
      "Batch: 18300,train loss is: 0.00012477215068619615\n",
      "test loss is 0.00010815232366727853\n",
      "Batch: 18400,train loss is: 0.00010426835565431862\n",
      "test loss is 0.00010687559717906365\n",
      "Batch: 18500,train loss is: 0.00014127177325209863\n",
      "test loss is 0.0001354391588329573\n",
      "Batch: 18600,train loss is: 7.538782474561923e-05\n",
      "test loss is 0.00012629397995584093\n",
      "Batch: 18700,train loss is: 0.00013274855824826196\n",
      "test loss is 0.00011431725378127948\n",
      "Batch: 18800,train loss is: 8.59378235597699e-05\n",
      "test loss is 0.00011083946193800968\n",
      "Batch: 18900,train loss is: 0.0001283014074149925\n",
      "test loss is 0.00011429534244967386\n",
      "Batch: 19000,train loss is: 5.5561620833582635e-05\n",
      "test loss is 0.00011219775672208049\n",
      "Batch: 19100,train loss is: 0.00012882622300272766\n",
      "test loss is 0.00011976952258719078\n",
      "Batch: 19200,train loss is: 9.699944777261182e-05\n",
      "test loss is 0.00010792838664172245\n",
      "Batch: 19300,train loss is: 0.0001203062242558442\n",
      "test loss is 0.00011126587273527001\n",
      "Batch: 19400,train loss is: 7.510510716154745e-05\n",
      "test loss is 0.00011188465526876583\n",
      "Batch: 19500,train loss is: 7.142764590821661e-05\n",
      "test loss is 0.00012414201297357105\n",
      "Batch: 19600,train loss is: 8.075987213493998e-05\n",
      "test loss is 0.00011227322185708975\n",
      "Batch: 19700,train loss is: 0.00015260027284630447\n",
      "test loss is 0.00011080549288070341\n",
      "Batch: 19800,train loss is: 6.872577441335123e-05\n",
      "test loss is 0.00011193067727328184\n",
      "Batch: 19900,train loss is: 6.699989605219853e-05\n",
      "test loss is 0.00011882904981798884\n",
      "Batch: 20000,train loss is: 0.00017208978830565364\n",
      "test loss is 0.00011547305059709614\n",
      "Batch: 20100,train loss is: 0.0001277224457266391\n",
      "test loss is 0.0001245967199022052\n",
      "Batch: 20200,train loss is: 7.53858053106254e-05\n",
      "test loss is 0.00011770850625547968\n",
      "Batch: 20300,train loss is: 4.88386968256517e-05\n",
      "test loss is 0.00012338979923757934\n",
      "Batch: 20400,train loss is: 0.0001038545893029838\n",
      "test loss is 0.00012105573683115428\n",
      "Batch: 20500,train loss is: 9.928820226623745e-05\n",
      "test loss is 0.00010799792700502549\n",
      "Batch: 20600,train loss is: 0.00019451696690215385\n",
      "test loss is 0.00011382813853936703\n",
      "Batch: 20700,train loss is: 0.00011338781139290742\n",
      "test loss is 0.00011632475728039392\n",
      "Batch: 20800,train loss is: 8.966078008480938e-05\n",
      "test loss is 0.0001168670101278119\n",
      "Batch: 20900,train loss is: 8.596511968491379e-05\n",
      "test loss is 0.00011391413701505354\n",
      "Batch: 21000,train loss is: 4.236012400536886e-05\n",
      "test loss is 0.00010789591137382044\n",
      "Batch: 21100,train loss is: 7.429280437775908e-05\n",
      "test loss is 0.00010838706720498767\n",
      "Batch: 21200,train loss is: 5.832795976657516e-05\n",
      "test loss is 0.00011250123682082915\n",
      "Batch: 21300,train loss is: 8.648896588399761e-05\n",
      "test loss is 0.00011557476930252068\n",
      "Batch: 21400,train loss is: 0.00011216872605270762\n",
      "test loss is 0.0001072339226771834\n",
      "Batch: 21500,train loss is: 8.489993724597647e-05\n",
      "test loss is 0.00011264878910509448\n",
      "Batch: 21600,train loss is: 0.00012212801997596304\n",
      "test loss is 0.00012307408960472827\n",
      "Batch: 21700,train loss is: 7.039703034320425e-05\n",
      "test loss is 0.00011591211328893272\n",
      "Batch: 21800,train loss is: 0.00015009834558228074\n",
      "test loss is 0.00010873537526691836\n",
      "Batch: 21900,train loss is: 7.842029906497118e-05\n",
      "test loss is 0.00011764801217610669\n",
      "Batch: 22000,train loss is: 9.936580589163054e-05\n",
      "test loss is 0.00013340869338006523\n",
      "Batch: 22100,train loss is: 6.435342271947376e-05\n",
      "test loss is 0.00010783492964290665\n",
      "Batch: 22200,train loss is: 6.239816576414717e-05\n",
      "test loss is 0.00011609369335538668\n",
      "Batch: 22300,train loss is: 8.833564082455549e-05\n",
      "test loss is 0.00012203738675445039\n",
      "Batch: 22400,train loss is: 9.844326027499423e-05\n",
      "test loss is 0.00011317966688916406\n",
      "Batch: 22500,train loss is: 7.921438267939679e-05\n",
      "test loss is 0.00011122253104071037\n",
      "Batch: 22600,train loss is: 0.00010054158394496073\n",
      "test loss is 0.00011448971665013952\n",
      "Batch: 22700,train loss is: 6.555577086937223e-05\n",
      "test loss is 0.00010801926538361363\n",
      "Batch: 22800,train loss is: 0.0001158738460659287\n",
      "test loss is 0.00012032729360366104\n",
      "Batch: 22900,train loss is: 5.50794882737507e-05\n",
      "test loss is 0.00011466317191862767\n",
      "Batch: 23000,train loss is: 0.00015195571832822785\n",
      "test loss is 0.00011258882125583797\n",
      "Batch: 23100,train loss is: 8.316730806833946e-05\n",
      "test loss is 0.00010698001014721091\n",
      "Batch: 23200,train loss is: 7.42522172425982e-05\n",
      "test loss is 0.00010792712242138003\n",
      "Batch: 23300,train loss is: 5.551640352287532e-05\n",
      "test loss is 0.00010825610480535652\n",
      "Batch: 23400,train loss is: 8.388701845665193e-05\n",
      "test loss is 0.00011182940074373941\n",
      "Batch: 23500,train loss is: 0.0001266301874965892\n",
      "test loss is 0.0001111448227814836\n",
      "Batch: 23600,train loss is: 6.991720233842005e-05\n",
      "test loss is 0.0001183084682542836\n",
      "Batch: 23700,train loss is: 0.00014084306049151848\n",
      "test loss is 0.00010934792276064709\n",
      "Batch: 23800,train loss is: 0.00015646908716047942\n",
      "test loss is 0.00011099180141005806\n",
      "Batch: 23900,train loss is: 0.00035471002430707154\n",
      "test loss is 0.0001104349175246468\n",
      "Batch: 24000,train loss is: 8.18219549774609e-05\n",
      "test loss is 0.00012534210221782166\n",
      "Batch: 24100,train loss is: 0.00010438364442552063\n",
      "test loss is 0.00010895448677971699\n",
      "Batch: 24200,train loss is: 0.000170789992303775\n",
      "test loss is 0.00011647252187802627\n",
      "Batch: 24300,train loss is: 9.7118413587294e-05\n",
      "test loss is 0.00012789423346035853\n",
      "Batch: 24400,train loss is: 0.00012648907822128657\n",
      "test loss is 0.0001102689635569828\n",
      "Batch: 24500,train loss is: 0.00012746050986317347\n",
      "test loss is 0.00011633288017025243\n",
      "Batch: 24600,train loss is: 0.00010198195760393796\n",
      "test loss is 0.0001284669630271006\n",
      "Batch: 24700,train loss is: 7.812106194487518e-05\n",
      "test loss is 0.00010898733496813231\n",
      "Batch: 24800,train loss is: 0.0001431369304098308\n",
      "test loss is 0.00011166880054220165\n",
      "Batch: 24900,train loss is: 4.851947152068329e-05\n",
      "test loss is 0.00010712953202148841\n",
      "Batch: 25000,train loss is: 8.671703502922978e-05\n",
      "test loss is 0.00010958410368465346\n",
      "Batch: 25100,train loss is: 6.176865130158437e-05\n",
      "test loss is 0.00010814000993821985\n",
      "Batch: 25200,train loss is: 0.00010711020741118395\n",
      "test loss is 0.00011266345511887546\n",
      "Batch: 25300,train loss is: 9.433005372121048e-05\n",
      "test loss is 0.00010729206349579001\n",
      "Batch: 25400,train loss is: 9.033752942291336e-05\n",
      "test loss is 0.00011634403358201314\n",
      "Batch: 25500,train loss is: 7.15080896152335e-05\n",
      "test loss is 0.00010831827254004643\n",
      "Batch: 25600,train loss is: 0.00010697390552229134\n",
      "test loss is 0.00011494034625100924\n",
      "Batch: 25700,train loss is: 0.0001222731687627412\n",
      "test loss is 0.00010871139203221182\n",
      "Batch: 25800,train loss is: 7.577033796170505e-05\n",
      "test loss is 0.00011148043108627303\n",
      "Batch: 25900,train loss is: 5.199712627547841e-05\n",
      "test loss is 0.00010890756370359862\n",
      "Batch: 26000,train loss is: 0.00011416371015208147\n",
      "test loss is 0.00011121243871973587\n",
      "Batch: 26100,train loss is: 0.00017149673441287772\n",
      "test loss is 0.00010734923313041663\n",
      "Batch: 26200,train loss is: 0.0001750708549300609\n",
      "test loss is 0.00011243205881669852\n",
      "Batch: 26300,train loss is: 0.0001315943148367736\n",
      "test loss is 0.00010766099282633966\n",
      "Batch: 26400,train loss is: 7.467709871231861e-05\n",
      "test loss is 0.00010640426901966864\n",
      "Batch: 26500,train loss is: 0.00010609319385469277\n",
      "test loss is 0.00010811427218264512\n",
      "Batch: 26600,train loss is: 0.00015372307674351468\n",
      "test loss is 0.00010750550179034027\n",
      "Batch: 26700,train loss is: 5.8666870419412244e-05\n",
      "test loss is 0.0001147046062268336\n",
      "Batch: 26800,train loss is: 0.00016358943613526246\n",
      "test loss is 0.00011031308557401486\n",
      "Batch: 26900,train loss is: 5.222856095338334e-05\n",
      "test loss is 0.0001223049333075503\n",
      "Batch: 27000,train loss is: 9.760945891052497e-05\n",
      "test loss is 0.00011901382045664221\n",
      "Batch: 27100,train loss is: 5.637784027597183e-05\n",
      "test loss is 0.00012484267946970376\n",
      "Batch: 27200,train loss is: 8.993860109640824e-05\n",
      "test loss is 0.0001078736644344294\n",
      "Batch: 27300,train loss is: 0.00010926565861176263\n",
      "test loss is 0.00010772446541315643\n",
      "Batch: 27400,train loss is: 9.302207425695469e-05\n",
      "test loss is 0.00011071143415653783\n",
      "Batch: 27500,train loss is: 7.989410272119915e-05\n",
      "test loss is 0.0001230583242368753\n",
      "Batch: 27600,train loss is: 9.441957589744511e-05\n",
      "test loss is 0.00012837260852742455\n",
      "Batch: 27700,train loss is: 0.0003065454014941118\n",
      "test loss is 0.00012488367401625376\n",
      "Batch: 27800,train loss is: 8.745237789704042e-05\n",
      "test loss is 0.00011090491633282397\n",
      "Batch: 27900,train loss is: 0.0001326185139703473\n",
      "test loss is 0.00010855232472804465\n",
      "Batch: 28000,train loss is: 7.215357157987685e-05\n",
      "test loss is 0.00011573649909585232\n",
      "Batch: 28100,train loss is: 0.0001686791526336522\n",
      "test loss is 0.00012103055699755305\n",
      "Batch: 28200,train loss is: 0.00011217309604358263\n",
      "test loss is 0.00011474163814795323\n",
      "Batch: 28300,train loss is: 0.0002956829019339546\n",
      "test loss is 0.00010923720265437211\n",
      "Batch: 28400,train loss is: 9.752268061375479e-05\n",
      "test loss is 0.00010984802514991637\n",
      "Batch: 28500,train loss is: 9.882156451633009e-05\n",
      "test loss is 0.00011704144164765616\n",
      "Batch: 28600,train loss is: 7.568812131567162e-05\n",
      "test loss is 0.00011058658919321425\n",
      "Batch: 28700,train loss is: 9.638004058781265e-05\n",
      "test loss is 0.00010966623287069887\n",
      "Batch: 28800,train loss is: 0.00016376458132336323\n",
      "test loss is 0.0001068989617112081\n",
      "Batch: 28900,train loss is: 0.00011634740611758302\n",
      "test loss is 0.00012095816766620888\n",
      "Batch: 29000,train loss is: 9.513055616421255e-05\n",
      "test loss is 0.00010844608977869893\n",
      "Batch: 29100,train loss is: 8.885003126739379e-05\n",
      "test loss is 0.0001179258068535876\n",
      "Batch: 29200,train loss is: 9.949325036106467e-05\n",
      "test loss is 0.0001440720866504661\n",
      "Batch: 29300,train loss is: 9.819391522401004e-05\n",
      "test loss is 0.00011078997750964633\n",
      "Batch: 29400,train loss is: 0.0001362265787349139\n",
      "test loss is 0.00011733874352903971\n",
      "Batch: 29500,train loss is: 6.169269711728243e-05\n",
      "test loss is 0.00010846372198784991\n",
      "Batch: 29600,train loss is: 0.00020253181450271106\n",
      "test loss is 0.00011940048502151723\n",
      "Batch: 29700,train loss is: 6.674465891865832e-05\n",
      "test loss is 0.0001072795160237591\n",
      "Batch: 29800,train loss is: 0.0002003033054545536\n",
      "test loss is 0.00010741003552152508\n",
      "Batch: 29900,train loss is: 0.0002678473201562914\n",
      "test loss is 0.00010865677791169308\n",
      "Batch: 30000,train loss is: 7.792912685669887e-05\n",
      "test loss is 0.00011159458600557074\n",
      "Batch: 30100,train loss is: 9.041314737064146e-05\n",
      "test loss is 0.00012084221191910644\n",
      "Batch: 30200,train loss is: 7.680659705703438e-05\n",
      "test loss is 0.00010648975154217844\n",
      "Batch: 30300,train loss is: 8.788729421445362e-05\n",
      "test loss is 0.00010861878459370188\n",
      "Batch: 30400,train loss is: 8.249311762259441e-05\n",
      "test loss is 0.00011689844765104619\n",
      "Batch: 30500,train loss is: 8.107095191685224e-05\n",
      "test loss is 0.00010818954744443202\n",
      "Batch: 30600,train loss is: 6.801951252343535e-05\n",
      "test loss is 0.00010939159883440532\n",
      "Batch: 30700,train loss is: 8.322726678504933e-05\n",
      "test loss is 0.00011430071882960531\n",
      "Batch: 30800,train loss is: 7.134842744363792e-05\n",
      "test loss is 0.00011210888132118224\n",
      "Batch: 30900,train loss is: 8.814909026089163e-05\n",
      "test loss is 0.0001097085607867941\n",
      "Batch: 31000,train loss is: 5.068937327006567e-05\n",
      "test loss is 0.00011162856300312933\n",
      "Batch: 31100,train loss is: 8.703820606471153e-05\n",
      "test loss is 0.00011171680351106493\n",
      "Batch: 31200,train loss is: 7.902320765914306e-05\n",
      "test loss is 0.00010938575754406786\n",
      "Batch: 31300,train loss is: 0.000123056612226733\n",
      "test loss is 0.00010735891828249509\n",
      "Batch: 31400,train loss is: 8.85147730070636e-05\n",
      "test loss is 0.0001312624592953367\n",
      "Batch: 31500,train loss is: 6.113946842263283e-05\n",
      "test loss is 0.00011567515113537119\n",
      "Batch: 31600,train loss is: 8.170375674366072e-05\n",
      "test loss is 0.00010780692377986821\n",
      "Batch: 31700,train loss is: 6.324651473435628e-05\n",
      "test loss is 0.00010922368270249886\n",
      "Batch: 31800,train loss is: 0.00012912729631334917\n",
      "test loss is 0.00010802310206974825\n",
      "Batch: 31900,train loss is: 0.00011360200725706873\n",
      "test loss is 0.00011554584786731372\n",
      "Batch: 32000,train loss is: 0.0004225188140139144\n",
      "test loss is 0.00011126472803166907\n",
      "Batch: 32100,train loss is: 3.852155442640782e-05\n",
      "test loss is 0.00011294533077700146\n",
      "Batch: 32200,train loss is: 7.416884762507469e-05\n",
      "test loss is 0.00010720712116431356\n",
      "Batch: 32300,train loss is: 7.23291193921451e-05\n",
      "test loss is 0.00010897295838441032\n",
      "Batch: 32400,train loss is: 8.355826581931719e-05\n",
      "test loss is 0.00011477274272254872\n",
      "Batch: 32500,train loss is: 0.0001263967484682483\n",
      "test loss is 0.00011142698038965263\n",
      "Batch: 32600,train loss is: 6.746844763704409e-05\n",
      "test loss is 0.00011174248510358988\n",
      "Batch: 32700,train loss is: 6.18888613656168e-05\n",
      "test loss is 0.00010840887386327971\n",
      "Batch: 32800,train loss is: 8.149507581263703e-05\n",
      "test loss is 0.00012166412654504046\n",
      "Batch: 32900,train loss is: 8.831856475309302e-05\n",
      "test loss is 0.00010979766460356045\n",
      "Batch: 33000,train loss is: 5.212307615684464e-05\n",
      "test loss is 0.00010850525949174941\n",
      "Batch: 33100,train loss is: 7.550055653911504e-05\n",
      "test loss is 0.00012162362007979962\n",
      "Batch: 33200,train loss is: 8.877383479041481e-05\n",
      "test loss is 0.00012050878831621639\n",
      "Batch: 33300,train loss is: 8.201922031342419e-05\n",
      "test loss is 0.00012195716336345648\n",
      "Batch: 33400,train loss is: 6.828759671114762e-05\n",
      "test loss is 0.00010992122672382189\n",
      "Batch: 33500,train loss is: 7.350018401968832e-05\n",
      "test loss is 0.00010932354877913319\n",
      "Batch: 33600,train loss is: 5.807023925483976e-05\n",
      "test loss is 0.00011282233588374288\n",
      "Batch: 33700,train loss is: 7.999920373016852e-05\n",
      "test loss is 0.0001179549671116727\n",
      "Batch: 33800,train loss is: 0.00010145483029544798\n",
      "test loss is 0.00011227528274443562\n",
      "Batch: 33900,train loss is: 7.970847778403406e-05\n",
      "test loss is 0.00011043375937193883\n",
      "-----------------------Epoch: 7----------------------------------\n",
      "Batch: 0,train loss is: 0.00010666746379821267\n",
      "test loss is 0.00011264730591556212\n",
      "Batch: 100,train loss is: 0.0001353627925066989\n",
      "test loss is 0.00010830027062770264\n",
      "Batch: 200,train loss is: 0.00010025754528493899\n",
      "test loss is 0.00010776627462742539\n",
      "Batch: 300,train loss is: 8.255985799928293e-05\n",
      "test loss is 0.00011005488792607033\n",
      "Batch: 400,train loss is: 6.56293734675318e-05\n",
      "test loss is 0.0001072267153385224\n",
      "Batch: 500,train loss is: 0.00012356843808332345\n",
      "test loss is 0.000128291727445775\n",
      "Batch: 600,train loss is: 8.167976881436052e-05\n",
      "test loss is 0.00010787892175110247\n",
      "Batch: 700,train loss is: 6.810295519791446e-05\n",
      "test loss is 0.00011088784022656817\n",
      "Batch: 800,train loss is: 0.00012797393712389585\n",
      "test loss is 0.00011416764031369065\n",
      "Batch: 900,train loss is: 5.5757515680762186e-05\n",
      "test loss is 0.00011361816215199587\n",
      "Batch: 1000,train loss is: 8.924189324738335e-05\n",
      "test loss is 0.00011249332158661005\n",
      "Batch: 1100,train loss is: 0.0001914320508808477\n",
      "test loss is 0.00011570542824985279\n",
      "Batch: 1200,train loss is: 5.225969715062346e-05\n",
      "test loss is 0.00011777349554712193\n",
      "Batch: 1300,train loss is: 0.0001109561190991186\n",
      "test loss is 0.00011086326798531682\n",
      "Batch: 1400,train loss is: 7.835018399303972e-05\n",
      "test loss is 0.00010858767797505185\n",
      "Batch: 1500,train loss is: 8.326049983723112e-05\n",
      "test loss is 0.00010992682685647571\n",
      "Batch: 1600,train loss is: 6.506030907153956e-05\n",
      "test loss is 0.0001098408686705021\n",
      "Batch: 1700,train loss is: 0.00013860465181225916\n",
      "test loss is 0.00011861685155432576\n",
      "Batch: 1800,train loss is: 6.315462155749284e-05\n",
      "test loss is 0.00010776827542962064\n",
      "Batch: 1900,train loss is: 5.182275416136775e-05\n",
      "test loss is 0.00011284090577948122\n",
      "Batch: 2000,train loss is: 0.00011923497691322245\n",
      "test loss is 0.00010927802083912782\n",
      "Batch: 2100,train loss is: 7.765644570169623e-05\n",
      "test loss is 0.00011447850777146996\n",
      "Batch: 2200,train loss is: 0.0001289830476931562\n",
      "test loss is 0.00011103447330157291\n",
      "Batch: 2300,train loss is: 8.60028555185259e-05\n",
      "test loss is 0.00010975887024040154\n",
      "Batch: 2400,train loss is: 8.493494669842665e-05\n",
      "test loss is 0.00010851532616582475\n",
      "Batch: 2500,train loss is: 0.00010044860937044409\n",
      "test loss is 0.00010635070128019015\n",
      "Batch: 2600,train loss is: 6.639551399253686e-05\n",
      "test loss is 0.00010669905550987877\n",
      "Batch: 2700,train loss is: 0.00013137065747667964\n",
      "test loss is 0.00010855433474528114\n",
      "Batch: 2800,train loss is: 0.00010416760567185961\n",
      "test loss is 0.00012144380843939195\n",
      "Batch: 2900,train loss is: 0.00011310822685310885\n",
      "test loss is 0.00010630307721020077\n",
      "Batch: 3000,train loss is: 7.224188261641993e-05\n",
      "test loss is 0.00010998702063968879\n",
      "Batch: 3100,train loss is: 6.850547684352226e-05\n",
      "test loss is 0.0001070164193664494\n",
      "Batch: 3200,train loss is: 0.00014108989943364508\n",
      "test loss is 0.00011304682314745599\n",
      "Batch: 3300,train loss is: 9.140501753636657e-05\n",
      "test loss is 0.00010729403532190075\n",
      "Batch: 3400,train loss is: 9.085177300116376e-05\n",
      "test loss is 0.0001289640282042412\n",
      "Batch: 3500,train loss is: 6.995712923735515e-05\n",
      "test loss is 0.0001155746627988511\n",
      "Batch: 3600,train loss is: 6.251076413425672e-05\n",
      "test loss is 0.00011014180584659403\n",
      "Batch: 3700,train loss is: 5.882038356777511e-05\n",
      "test loss is 0.00010562854843971172\n",
      "Batch: 3800,train loss is: 6.373076255672969e-05\n",
      "test loss is 0.0001066648566080968\n",
      "Batch: 3900,train loss is: 0.00011031293918315781\n",
      "test loss is 0.00011011689174408911\n",
      "Batch: 4000,train loss is: 4.6639537605318774e-05\n",
      "test loss is 0.00010939588852184225\n",
      "Batch: 4100,train loss is: 0.00010215291178585807\n",
      "test loss is 0.00011414142641960215\n",
      "Batch: 4200,train loss is: 0.00011639312551414825\n",
      "test loss is 0.00012488945548476822\n",
      "Batch: 4300,train loss is: 7.49285308581235e-05\n",
      "test loss is 0.00010771861917759207\n",
      "Batch: 4400,train loss is: 6.270265476684324e-05\n",
      "test loss is 0.00011266325890047075\n",
      "Batch: 4500,train loss is: 6.77060441185295e-05\n",
      "test loss is 0.00010578073086987244\n",
      "Batch: 4600,train loss is: 7.915449299377256e-05\n",
      "test loss is 0.00010960401804577298\n",
      "Batch: 4700,train loss is: 0.0001441248337491222\n",
      "test loss is 0.0001147643772309422\n",
      "Batch: 4800,train loss is: 0.00023203371172377487\n",
      "test loss is 0.00012829184897350938\n",
      "Batch: 4900,train loss is: 9.3408547898166e-05\n",
      "test loss is 0.0001122068631856827\n",
      "Batch: 5000,train loss is: 5.9384506002155885e-05\n",
      "test loss is 0.00011346348856062083\n",
      "Batch: 5100,train loss is: 6.756142140765078e-05\n",
      "test loss is 0.00010683612160122745\n",
      "Batch: 5200,train loss is: 8.206319989883501e-05\n",
      "test loss is 0.00011721730866340754\n",
      "Batch: 5300,train loss is: 8.660455143423959e-05\n",
      "test loss is 0.00011384316077762572\n",
      "Batch: 5400,train loss is: 0.00019894497397559587\n",
      "test loss is 0.00011431873312067429\n",
      "Batch: 5500,train loss is: 7.391243161372625e-05\n",
      "test loss is 0.00012588650875847485\n",
      "Batch: 5600,train loss is: 0.0001428400897970091\n",
      "test loss is 0.00010954667715724102\n",
      "Batch: 5700,train loss is: 0.00012986734974919373\n",
      "test loss is 0.00011656145321268679\n",
      "Batch: 5800,train loss is: 0.00014158251751903857\n",
      "test loss is 0.00011586560542310157\n",
      "Batch: 5900,train loss is: 0.00024058711479479778\n",
      "test loss is 0.00011613507350970895\n",
      "Batch: 6000,train loss is: 0.0001554532194469411\n",
      "test loss is 0.00011633148830679328\n",
      "Batch: 6100,train loss is: 6.448425075594781e-05\n",
      "test loss is 0.0001090818496594896\n",
      "Batch: 6200,train loss is: 8.068468860390948e-05\n",
      "test loss is 0.00010635731294661351\n",
      "Batch: 6300,train loss is: 0.000118465625454497\n",
      "test loss is 0.0001235257142403741\n",
      "Batch: 6400,train loss is: 6.40740132558915e-05\n",
      "test loss is 0.00010908323431199838\n",
      "Batch: 6500,train loss is: 7.460431416355421e-05\n",
      "test loss is 0.00011042806280304393\n",
      "Batch: 6600,train loss is: 0.00019254738296457867\n",
      "test loss is 0.00010834200366632529\n",
      "Batch: 6700,train loss is: 9.234566192080225e-05\n",
      "test loss is 0.00010844007093774332\n",
      "Batch: 6800,train loss is: 0.00012087591557401839\n",
      "test loss is 0.00011190364125622102\n",
      "Batch: 6900,train loss is: 0.0001107969620189902\n",
      "test loss is 0.00011193095282797762\n",
      "Batch: 7000,train loss is: 0.00011205658706280119\n",
      "test loss is 0.00010891855793440288\n",
      "Batch: 7100,train loss is: 0.00011972874709549109\n",
      "test loss is 0.00012262663060886935\n",
      "Batch: 7200,train loss is: 6.142851326562756e-05\n",
      "test loss is 0.0001053157282698669\n",
      "Batch: 7300,train loss is: 0.00021399206303143265\n",
      "test loss is 0.00011526785365891673\n",
      "Batch: 7400,train loss is: 7.894152197556537e-05\n",
      "test loss is 0.00013050195927727832\n",
      "Batch: 7500,train loss is: 9.075958749244047e-05\n",
      "test loss is 0.00011100356997521075\n",
      "Batch: 7600,train loss is: 0.00015943669729973404\n",
      "test loss is 0.0001349819475364597\n",
      "Batch: 7700,train loss is: 0.0001646684220856768\n",
      "test loss is 0.00010544215089472549\n",
      "Batch: 7800,train loss is: 6.061998615568765e-05\n",
      "test loss is 0.00010685254159191885\n",
      "Batch: 7900,train loss is: 5.1549542806708954e-05\n",
      "test loss is 0.00010552272358227173\n",
      "Batch: 8000,train loss is: 4.133683625849847e-05\n",
      "test loss is 0.00011660523291101728\n",
      "Batch: 8100,train loss is: 0.00014589983071266498\n",
      "test loss is 0.0001103428483218035\n",
      "Batch: 8200,train loss is: 0.00010045805858359687\n",
      "test loss is 0.00010611509601265201\n",
      "Batch: 8300,train loss is: 6.499326014106995e-05\n",
      "test loss is 0.00011165691408307791\n",
      "Batch: 8400,train loss is: 0.0001238790085772117\n",
      "test loss is 0.00011770313906222699\n",
      "Batch: 8500,train loss is: 0.0002030648989894953\n",
      "test loss is 0.00011736346399505411\n",
      "Batch: 8600,train loss is: 9.358886797978157e-05\n",
      "test loss is 0.000128727863142102\n",
      "Batch: 8700,train loss is: 9.574112892332796e-05\n",
      "test loss is 0.00011310619311239864\n",
      "Batch: 8800,train loss is: 8.436658438403442e-05\n",
      "test loss is 0.0001079199876870952\n",
      "Batch: 8900,train loss is: 8.98603663501471e-05\n",
      "test loss is 0.00010477398701358453\n",
      "Batch: 9000,train loss is: 6.615849285632502e-05\n",
      "test loss is 0.00010675390489892384\n",
      "Batch: 9100,train loss is: 8.839369619381727e-05\n",
      "test loss is 0.0001120098047696952\n",
      "Batch: 9200,train loss is: 0.00011732646941323507\n",
      "test loss is 0.00011218985242706266\n",
      "Batch: 9300,train loss is: 0.0001320015626254953\n",
      "test loss is 0.00010813665824864447\n",
      "Batch: 9400,train loss is: 0.00010544602481207513\n",
      "test loss is 0.00011211716304358976\n",
      "Batch: 9500,train loss is: 6.509328243607174e-05\n",
      "test loss is 0.00011419163167516175\n",
      "Batch: 9600,train loss is: 7.680310170494498e-05\n",
      "test loss is 0.00010727467148551497\n",
      "Batch: 9700,train loss is: 7.260821682976383e-05\n",
      "test loss is 0.00010961631095620054\n",
      "Batch: 9800,train loss is: 6.03176669844269e-05\n",
      "test loss is 0.0001058915360864903\n",
      "Batch: 9900,train loss is: 6.265049825025558e-05\n",
      "test loss is 0.00010446572111144596\n",
      "Batch: 10000,train loss is: 6.306920978959549e-05\n",
      "test loss is 0.00011442079922077748\n",
      "Batch: 10100,train loss is: 0.0001931487022058351\n",
      "test loss is 0.00010966050447452408\n",
      "Batch: 10200,train loss is: 0.00026631461065591\n",
      "test loss is 0.00010868157433423528\n",
      "Batch: 10300,train loss is: 9.343932255058292e-05\n",
      "test loss is 0.00010829481837644223\n",
      "Batch: 10400,train loss is: 4.6649429016816864e-05\n",
      "test loss is 0.0001130025140892978\n",
      "Batch: 10500,train loss is: 9.702606581127418e-05\n",
      "test loss is 0.00010891883289909108\n",
      "Batch: 10600,train loss is: 7.329568648846898e-05\n",
      "test loss is 0.00010929394893076522\n",
      "Batch: 10700,train loss is: 7.678084446435641e-05\n",
      "test loss is 0.00010493436069798504\n",
      "Batch: 10800,train loss is: 9.000719752327799e-05\n",
      "test loss is 0.00011144929239668462\n",
      "Batch: 10900,train loss is: 9.120256424452107e-05\n",
      "test loss is 0.0001073684422337643\n",
      "Batch: 11000,train loss is: 0.00012097848044904747\n",
      "test loss is 0.00010469315937916858\n",
      "Batch: 11100,train loss is: 0.00012478203065020428\n",
      "test loss is 0.00010824826082192764\n",
      "Batch: 11200,train loss is: 0.0001259130676679664\n",
      "test loss is 0.0001051974171287347\n",
      "Batch: 11300,train loss is: 7.06770473653679e-05\n",
      "test loss is 0.00011363622625112899\n",
      "Batch: 11400,train loss is: 0.00018572507051957068\n",
      "test loss is 0.00010552933362454647\n",
      "Batch: 11500,train loss is: 6.091825231602826e-05\n",
      "test loss is 0.0001118764183198941\n",
      "Batch: 11600,train loss is: 0.00011716281703906904\n",
      "test loss is 0.0001079395270947947\n",
      "Batch: 11700,train loss is: 7.39761484576606e-05\n",
      "test loss is 0.00010496745416644179\n",
      "Batch: 11800,train loss is: 9.366379554183635e-05\n",
      "test loss is 0.00010575803348785549\n",
      "Batch: 11900,train loss is: 0.0001062559200666305\n",
      "test loss is 0.00011803122545890729\n",
      "Batch: 12000,train loss is: 8.88351782885285e-05\n",
      "test loss is 0.00010953284667985473\n",
      "Batch: 12100,train loss is: 0.00012014428782325326\n",
      "test loss is 0.00010568356342010374\n",
      "Batch: 12200,train loss is: 7.007239437776368e-05\n",
      "test loss is 0.00010721264820732452\n",
      "Batch: 12300,train loss is: 7.183397082917732e-05\n",
      "test loss is 0.00011097550793911843\n",
      "Batch: 12400,train loss is: 7.94058573325871e-05\n",
      "test loss is 0.00010730951893379781\n",
      "Batch: 12500,train loss is: 0.0001670985262955301\n",
      "test loss is 0.00010864621846976706\n",
      "Batch: 12600,train loss is: 5.876222734689467e-05\n",
      "test loss is 0.00010744875613866162\n",
      "Batch: 12700,train loss is: 8.226962475657085e-05\n",
      "test loss is 0.00010839789075366117\n",
      "Batch: 12800,train loss is: 4.724266412270707e-05\n",
      "test loss is 0.00010715264616182226\n",
      "Batch: 12900,train loss is: 0.00014245389657766098\n",
      "test loss is 0.00010685408657629824\n",
      "Batch: 13000,train loss is: 8.829649920692498e-05\n",
      "test loss is 0.00010547578051116113\n",
      "Batch: 13100,train loss is: 0.00018047334962268243\n",
      "test loss is 0.0001043888922028559\n",
      "Batch: 13200,train loss is: 9.828495637101302e-05\n",
      "test loss is 0.00011955693332902888\n",
      "Batch: 13300,train loss is: 0.00010439443831804965\n",
      "test loss is 0.00013413967373706532\n",
      "Batch: 13400,train loss is: 8.020899683640257e-05\n",
      "test loss is 0.00010462931959568171\n",
      "Batch: 13500,train loss is: 5.845566108584375e-05\n",
      "test loss is 0.0001086434113112996\n",
      "Batch: 13600,train loss is: 6.613288512817592e-05\n",
      "test loss is 0.00011990816487383059\n",
      "Batch: 13700,train loss is: 0.00010565045911963213\n",
      "test loss is 0.00010716663279879944\n",
      "Batch: 13800,train loss is: 6.516417170563036e-05\n",
      "test loss is 0.00011542243797801575\n",
      "Batch: 13900,train loss is: 5.184686531254146e-05\n",
      "test loss is 0.00010879611340079889\n",
      "Batch: 14000,train loss is: 0.00010224074605559244\n",
      "test loss is 0.00010753282785399472\n",
      "Batch: 14100,train loss is: 6.699072634340473e-05\n",
      "test loss is 0.00010809165337516858\n",
      "Batch: 14200,train loss is: 6.562761917187077e-05\n",
      "test loss is 0.00010695351019843452\n",
      "Batch: 14300,train loss is: 8.192075061110324e-05\n",
      "test loss is 0.00010761146023395188\n",
      "Batch: 14400,train loss is: 0.00015151776158963055\n",
      "test loss is 0.00010527372839056033\n",
      "Batch: 14500,train loss is: 7.37626383347816e-05\n",
      "test loss is 0.00010624915321936282\n",
      "Batch: 14600,train loss is: 0.00012373803342306886\n",
      "test loss is 0.00011463785106189691\n",
      "Batch: 14700,train loss is: 0.00010152832883157051\n",
      "test loss is 0.00010796042423167926\n",
      "Batch: 14800,train loss is: 0.00016738399419335346\n",
      "test loss is 0.00011094158866654884\n",
      "Batch: 14900,train loss is: 7.826176593917092e-05\n",
      "test loss is 0.00010530556562535788\n",
      "Batch: 15000,train loss is: 0.00011101571282210228\n",
      "test loss is 0.00010816267819420305\n",
      "Batch: 15100,train loss is: 0.0001285609547480448\n",
      "test loss is 0.00011430312720366178\n",
      "Batch: 15200,train loss is: 0.00012330841051950698\n",
      "test loss is 0.00011218545590641905\n",
      "Batch: 15300,train loss is: 0.00012488202716510902\n",
      "test loss is 0.00010605249829519232\n",
      "Batch: 15400,train loss is: 6.262111078179054e-05\n",
      "test loss is 0.00010571571229956105\n",
      "Batch: 15500,train loss is: 7.638007680505426e-05\n",
      "test loss is 0.00010987543355490374\n",
      "Batch: 15600,train loss is: 0.0001694487392466732\n",
      "test loss is 0.00010947966886229481\n",
      "Batch: 15700,train loss is: 7.701696983709728e-05\n",
      "test loss is 0.00011224484795820213\n",
      "Batch: 15800,train loss is: 7.937140296449369e-05\n",
      "test loss is 0.00011488097576445906\n",
      "Batch: 15900,train loss is: 0.00011132259843864235\n",
      "test loss is 0.0001113780969780413\n",
      "Batch: 16000,train loss is: 0.00013843832229993352\n",
      "test loss is 0.00010870123328467608\n",
      "Batch: 16100,train loss is: 0.0001700230086307992\n",
      "test loss is 0.00010615603341608535\n",
      "Batch: 16200,train loss is: 6.842025877678192e-05\n",
      "test loss is 0.00011017472196292954\n",
      "Batch: 16300,train loss is: 0.00010269324326605311\n",
      "test loss is 0.00011532369378014377\n",
      "Batch: 16400,train loss is: 6.390340558120106e-05\n",
      "test loss is 0.00011208072066206768\n",
      "Batch: 16500,train loss is: 6.965320396686413e-05\n",
      "test loss is 0.00010650114411385593\n",
      "Batch: 16600,train loss is: 6.832573625633413e-05\n",
      "test loss is 0.00010512587178916184\n",
      "Batch: 16700,train loss is: 0.00017404559354819793\n",
      "test loss is 0.00011187968794354866\n",
      "Batch: 16800,train loss is: 0.00014293787695725285\n",
      "test loss is 0.0001216169784383006\n",
      "Batch: 16900,train loss is: 8.014618849137605e-05\n",
      "test loss is 0.00011400380048486589\n",
      "Batch: 17000,train loss is: 0.00010946404015504226\n",
      "test loss is 0.00011886556287096434\n",
      "Batch: 17100,train loss is: 0.00010693696445749555\n",
      "test loss is 0.00010876572670164399\n",
      "Batch: 17200,train loss is: 5.939224868240589e-05\n",
      "test loss is 0.00010849288715538536\n",
      "Batch: 17300,train loss is: 7.152831833965939e-05\n",
      "test loss is 0.00011357656851006351\n",
      "Batch: 17400,train loss is: 5.1239756411307405e-05\n",
      "test loss is 0.00011793010040995275\n",
      "Batch: 17500,train loss is: 0.00010584667332697718\n",
      "test loss is 0.00010491115367267312\n",
      "Batch: 17600,train loss is: 6.168842133938906e-05\n",
      "test loss is 0.00010932877797302973\n",
      "Batch: 17700,train loss is: 5.074087513897793e-05\n",
      "test loss is 0.00010479321847153737\n",
      "Batch: 17800,train loss is: 0.0002316414457356303\n",
      "test loss is 0.00010899086941285749\n",
      "Batch: 17900,train loss is: 4.353301143663346e-05\n",
      "test loss is 0.0001044101786507088\n",
      "Batch: 18000,train loss is: 7.38173351830228e-05\n",
      "test loss is 0.00011394461561500599\n",
      "Batch: 18100,train loss is: 0.00016261992054552153\n",
      "test loss is 0.00010885105135904097\n",
      "Batch: 18200,train loss is: 9.219549591291212e-05\n",
      "test loss is 0.00010613400209341723\n",
      "Batch: 18300,train loss is: 0.0001184759902434127\n",
      "test loss is 0.00010442639780759865\n",
      "Batch: 18400,train loss is: 0.00010168718678621367\n",
      "test loss is 0.00010335786861252768\n",
      "Batch: 18500,train loss is: 0.00013417551276898105\n",
      "test loss is 0.00013082707015757693\n",
      "Batch: 18600,train loss is: 7.314845345953289e-05\n",
      "test loss is 0.00012478722161123268\n",
      "Batch: 18700,train loss is: 0.00012884120217910956\n",
      "test loss is 0.00011057686663371903\n",
      "Batch: 18800,train loss is: 8.4769725321316e-05\n",
      "test loss is 0.0001072856539105428\n",
      "Batch: 18900,train loss is: 0.0001248320143038973\n",
      "test loss is 0.00011038637356331236\n",
      "Batch: 19000,train loss is: 5.395145211035687e-05\n",
      "test loss is 0.00010815521126043708\n",
      "Batch: 19100,train loss is: 0.00012833232066801045\n",
      "test loss is 0.00011833797303268562\n",
      "Batch: 19200,train loss is: 9.258747573526868e-05\n",
      "test loss is 0.00010432264824640006\n",
      "Batch: 19300,train loss is: 0.0001199955010443361\n",
      "test loss is 0.0001081233812253559\n",
      "Batch: 19400,train loss is: 7.015598419059715e-05\n",
      "test loss is 0.00010809112458994351\n",
      "Batch: 19500,train loss is: 6.921115664849005e-05\n",
      "test loss is 0.00011907404330486171\n",
      "Batch: 19600,train loss is: 7.658496481391048e-05\n",
      "test loss is 0.00010843590961951849\n",
      "Batch: 19700,train loss is: 0.00015019434303147972\n",
      "test loss is 0.00010705326693669111\n",
      "Batch: 19800,train loss is: 6.674758362419311e-05\n",
      "test loss is 0.00010802759317824201\n",
      "Batch: 19900,train loss is: 6.492214876085785e-05\n",
      "test loss is 0.00011445766361837754\n",
      "Batch: 20000,train loss is: 0.0001654174655424212\n",
      "test loss is 0.00011095422065544865\n",
      "Batch: 20100,train loss is: 0.00011852631718324841\n",
      "test loss is 0.00012042776140413263\n",
      "Batch: 20200,train loss is: 7.524738647486747e-05\n",
      "test loss is 0.00011388554527869814\n",
      "Batch: 20300,train loss is: 4.706553885445718e-05\n",
      "test loss is 0.00011842675786468668\n",
      "Batch: 20400,train loss is: 0.0001001214580451726\n",
      "test loss is 0.00011709942037804836\n",
      "Batch: 20500,train loss is: 9.686119986961629e-05\n",
      "test loss is 0.00010449202455189527\n",
      "Batch: 20600,train loss is: 0.00018911931462881613\n",
      "test loss is 0.00010974624363169283\n",
      "Batch: 20700,train loss is: 0.00011124961167674463\n",
      "test loss is 0.0001128643136852167\n",
      "Batch: 20800,train loss is: 8.960217824198424e-05\n",
      "test loss is 0.00011388947300524742\n",
      "Batch: 20900,train loss is: 8.293551000210258e-05\n",
      "test loss is 0.00010978212087482613\n",
      "Batch: 21000,train loss is: 4.091945018628129e-05\n",
      "test loss is 0.00010428884858114657\n",
      "Batch: 21100,train loss is: 7.131506191439228e-05\n",
      "test loss is 0.00010512864609838168\n",
      "Batch: 21200,train loss is: 5.4522984824243915e-05\n",
      "test loss is 0.000108268866053493\n",
      "Batch: 21300,train loss is: 8.388690578262197e-05\n",
      "test loss is 0.00011172424243332358\n",
      "Batch: 21400,train loss is: 0.00010818796782647467\n",
      "test loss is 0.0001034346980985726\n",
      "Batch: 21500,train loss is: 8.210112465214682e-05\n",
      "test loss is 0.00010847280272269139\n",
      "Batch: 21600,train loss is: 0.00012522052619516423\n",
      "test loss is 0.00012025709552835934\n",
      "Batch: 21700,train loss is: 6.883076374638753e-05\n",
      "test loss is 0.00011223066074866489\n",
      "Batch: 21800,train loss is: 0.00014785056402231671\n",
      "test loss is 0.00010496032230556985\n",
      "Batch: 21900,train loss is: 7.244545609904536e-05\n",
      "test loss is 0.00011320173433645279\n",
      "Batch: 22000,train loss is: 9.305689584345706e-05\n",
      "test loss is 0.00012888379411153696\n",
      "Batch: 22100,train loss is: 6.206790199350816e-05\n",
      "test loss is 0.00010419043083397291\n",
      "Batch: 22200,train loss is: 6.160691004641632e-05\n",
      "test loss is 0.00011287371557263499\n",
      "Batch: 22300,train loss is: 8.544911498528772e-05\n",
      "test loss is 0.00011779867502513144\n",
      "Batch: 22400,train loss is: 9.33781070979568e-05\n",
      "test loss is 0.000109963198399234\n",
      "Batch: 22500,train loss is: 7.751445951186961e-05\n",
      "test loss is 0.00010738875294983269\n",
      "Batch: 22600,train loss is: 9.603878056906921e-05\n",
      "test loss is 0.00011017012065781975\n",
      "Batch: 22700,train loss is: 6.441603805691869e-05\n",
      "test loss is 0.00010470547079523869\n",
      "Batch: 22800,train loss is: 0.00011521885263814135\n",
      "test loss is 0.00011623680778517091\n",
      "Batch: 22900,train loss is: 5.273954004248201e-05\n",
      "test loss is 0.00011153656622133173\n",
      "Batch: 23000,train loss is: 0.00014444640659812353\n",
      "test loss is 0.00010824746238327346\n",
      "Batch: 23100,train loss is: 8.039280359955291e-05\n",
      "test loss is 0.00010328259830829453\n",
      "Batch: 23200,train loss is: 7.212976522789574e-05\n",
      "test loss is 0.00010440118641588619\n",
      "Batch: 23300,train loss is: 5.277755203069835e-05\n",
      "test loss is 0.00010468580988713933\n",
      "Batch: 23400,train loss is: 8.145705376859024e-05\n",
      "test loss is 0.00010826762159641839\n",
      "Batch: 23500,train loss is: 0.00011725381683818967\n",
      "test loss is 0.00010792479688014574\n",
      "Batch: 23600,train loss is: 6.797538242906769e-05\n",
      "test loss is 0.00011475709252229674\n",
      "Batch: 23700,train loss is: 0.00013939357443100805\n",
      "test loss is 0.00010663240286097642\n",
      "Batch: 23800,train loss is: 0.00014787098732431606\n",
      "test loss is 0.00010744895003293187\n",
      "Batch: 23900,train loss is: 0.00034024233241800993\n",
      "test loss is 0.00010710865878776721\n",
      "Batch: 24000,train loss is: 7.691621892875899e-05\n",
      "test loss is 0.00011964523195491007\n",
      "Batch: 24100,train loss is: 0.00010248711273855337\n",
      "test loss is 0.00010521012691987275\n",
      "Batch: 24200,train loss is: 0.0001650286335615873\n",
      "test loss is 0.00011290596107038716\n",
      "Batch: 24300,train loss is: 9.324788849607562e-05\n",
      "test loss is 0.00012217508096164491\n",
      "Batch: 24400,train loss is: 0.00012688672280362093\n",
      "test loss is 0.00010704601710989482\n",
      "Batch: 24500,train loss is: 0.00012230007438015955\n",
      "test loss is 0.00011283057704259218\n",
      "Batch: 24600,train loss is: 9.809580338814583e-05\n",
      "test loss is 0.00012372015861055778\n",
      "Batch: 24700,train loss is: 7.61565849416229e-05\n",
      "test loss is 0.00010541189233221854\n",
      "Batch: 24800,train loss is: 0.0001376925536783192\n",
      "test loss is 0.00010754058680916436\n",
      "Batch: 24900,train loss is: 4.771848989831454e-05\n",
      "test loss is 0.00010355679639314435\n",
      "Batch: 25000,train loss is: 8.458607364731654e-05\n",
      "test loss is 0.00010630503697295042\n",
      "Batch: 25100,train loss is: 5.819854096186538e-05\n",
      "test loss is 0.00010454069254024538\n",
      "Batch: 25200,train loss is: 0.00010369461047182165\n",
      "test loss is 0.00010884817055659108\n",
      "Batch: 25300,train loss is: 9.048829663569662e-05\n",
      "test loss is 0.00010388804297140232\n",
      "Batch: 25400,train loss is: 8.621047734450182e-05\n",
      "test loss is 0.00011210140343380937\n",
      "Batch: 25500,train loss is: 6.814294395259258e-05\n",
      "test loss is 0.00010513208730674945\n",
      "Batch: 25600,train loss is: 0.0001040251212664829\n",
      "test loss is 0.00011155690825677868\n",
      "Batch: 25700,train loss is: 0.00011832109156728097\n",
      "test loss is 0.00010530161431699356\n",
      "Batch: 25800,train loss is: 7.397854897612422e-05\n",
      "test loss is 0.00010765789743703655\n",
      "Batch: 25900,train loss is: 4.831395229899652e-05\n",
      "test loss is 0.00010544175788731346\n",
      "Batch: 26000,train loss is: 0.00011250067234294835\n",
      "test loss is 0.00010751516070923127\n",
      "Batch: 26100,train loss is: 0.00016778895247310789\n",
      "test loss is 0.00010411215112825728\n",
      "Batch: 26200,train loss is: 0.00016627025966249958\n",
      "test loss is 0.00010870706608927495\n",
      "Batch: 26300,train loss is: 0.00012663647057923694\n",
      "test loss is 0.00010451494976483475\n",
      "Batch: 26400,train loss is: 7.184612929836243e-05\n",
      "test loss is 0.00010298174206218727\n",
      "Batch: 26500,train loss is: 0.00010442997109909096\n",
      "test loss is 0.00010452454207358097\n",
      "Batch: 26600,train loss is: 0.0001482357919375402\n",
      "test loss is 0.00010385045767447317\n",
      "Batch: 26700,train loss is: 5.8095077349188746e-05\n",
      "test loss is 0.00011067729880645305\n",
      "Batch: 26800,train loss is: 0.00015662483840792345\n",
      "test loss is 0.00010634481289830945\n",
      "Batch: 26900,train loss is: 4.9493411361681584e-05\n",
      "test loss is 0.00011875326058402183\n",
      "Batch: 27000,train loss is: 9.339654890222424e-05\n",
      "test loss is 0.00011506824056508008\n",
      "Batch: 27100,train loss is: 5.435667917801962e-05\n",
      "test loss is 0.00012159508433933354\n",
      "Batch: 27200,train loss is: 8.715163658274448e-05\n",
      "test loss is 0.0001045296929524686\n",
      "Batch: 27300,train loss is: 0.00010271058730508164\n",
      "test loss is 0.00010413093053767822\n",
      "Batch: 27400,train loss is: 9.05316066462008e-05\n",
      "test loss is 0.00010679807724305272\n",
      "Batch: 27500,train loss is: 7.64036416142442e-05\n",
      "test loss is 0.00011852308853433879\n",
      "Batch: 27600,train loss is: 9.197528591542535e-05\n",
      "test loss is 0.00012532702713701367\n",
      "Batch: 27700,train loss is: 0.0002952995868414803\n",
      "test loss is 0.00012006382231440994\n",
      "Batch: 27800,train loss is: 8.441028613092436e-05\n",
      "test loss is 0.00010739244231442963\n",
      "Batch: 27900,train loss is: 0.000127834381766469\n",
      "test loss is 0.00010485891009512234\n",
      "Batch: 28000,train loss is: 7.11083018616864e-05\n",
      "test loss is 0.00011329296508112894\n",
      "Batch: 28100,train loss is: 0.0001657756780692213\n",
      "test loss is 0.00011694386353607107\n",
      "Batch: 28200,train loss is: 0.00011124746434607914\n",
      "test loss is 0.00011148253684059668\n",
      "Batch: 28300,train loss is: 0.00028793947128146917\n",
      "test loss is 0.00010578385378724067\n",
      "Batch: 28400,train loss is: 9.380420894531077e-05\n",
      "test loss is 0.00010607756871860291\n",
      "Batch: 28500,train loss is: 9.612865427497315e-05\n",
      "test loss is 0.00011311341114324037\n",
      "Batch: 28600,train loss is: 7.440723728131703e-05\n",
      "test loss is 0.0001070009571334825\n",
      "Batch: 28700,train loss is: 9.365188904459912e-05\n",
      "test loss is 0.00010583587872363999\n",
      "Batch: 28800,train loss is: 0.00016072524746590992\n",
      "test loss is 0.00010337608654215022\n",
      "Batch: 28900,train loss is: 0.00011078774881120345\n",
      "test loss is 0.00011480976133825858\n",
      "Batch: 29000,train loss is: 9.045180091043044e-05\n",
      "test loss is 0.00010482513439317427\n",
      "Batch: 29100,train loss is: 8.630377815317654e-05\n",
      "test loss is 0.00011343175033170237\n",
      "Batch: 29200,train loss is: 9.914722893434559e-05\n",
      "test loss is 0.0001420116082828688\n",
      "Batch: 29300,train loss is: 9.788995018651268e-05\n",
      "test loss is 0.00010762207195251868\n",
      "Batch: 29400,train loss is: 0.0001332016513559764\n",
      "test loss is 0.00011362075417042138\n",
      "Batch: 29500,train loss is: 6.066080690841437e-05\n",
      "test loss is 0.00010474990213019828\n",
      "Batch: 29600,train loss is: 0.00019474378338698063\n",
      "test loss is 0.0001148131643268836\n",
      "Batch: 29700,train loss is: 6.509053930937662e-05\n",
      "test loss is 0.0001035469141135476\n",
      "Batch: 29800,train loss is: 0.00019353041001303836\n",
      "test loss is 0.00010401993005520957\n",
      "Batch: 29900,train loss is: 0.0002571893771020136\n",
      "test loss is 0.00010502853206939904\n",
      "Batch: 30000,train loss is: 7.675987777997364e-05\n",
      "test loss is 0.00010769878065628721\n",
      "Batch: 30100,train loss is: 8.721754858696706e-05\n",
      "test loss is 0.00011762972550902453\n",
      "Batch: 30200,train loss is: 7.252393251922698e-05\n",
      "test loss is 0.00010351046353542955\n",
      "Batch: 30300,train loss is: 8.77572551265717e-05\n",
      "test loss is 0.00010505942973587193\n",
      "Batch: 30400,train loss is: 7.911500070512741e-05\n",
      "test loss is 0.00011317138436512205\n",
      "Batch: 30500,train loss is: 7.78710582085779e-05\n",
      "test loss is 0.00010468053266298976\n",
      "Batch: 30600,train loss is: 6.53336557855146e-05\n",
      "test loss is 0.00010609105177780219\n",
      "Batch: 30700,train loss is: 8.114519821770094e-05\n",
      "test loss is 0.00011083268845894873\n",
      "Batch: 30800,train loss is: 6.788037364483795e-05\n",
      "test loss is 0.0001083279636491975\n",
      "Batch: 30900,train loss is: 8.355961266925199e-05\n",
      "test loss is 0.00010586861919346661\n",
      "Batch: 31000,train loss is: 4.813352900329401e-05\n",
      "test loss is 0.00010814711647804954\n",
      "Batch: 31100,train loss is: 8.477791137815056e-05\n",
      "test loss is 0.00010792331979986201\n",
      "Batch: 31200,train loss is: 7.592684353086548e-05\n",
      "test loss is 0.00010599183893945698\n",
      "Batch: 31300,train loss is: 0.00012022429780864057\n",
      "test loss is 0.00010393494433385732\n",
      "Batch: 31400,train loss is: 8.664493019968614e-05\n",
      "test loss is 0.0001279288974263001\n",
      "Batch: 31500,train loss is: 5.996668993976974e-05\n",
      "test loss is 0.00011175657433396814\n",
      "Batch: 31600,train loss is: 7.888576905350302e-05\n",
      "test loss is 0.00010438077786744209\n",
      "Batch: 31700,train loss is: 6.0540651583314003e-05\n",
      "test loss is 0.00010656988419594395\n",
      "Batch: 31800,train loss is: 0.00012966838744576484\n",
      "test loss is 0.00010474047005002666\n",
      "Batch: 31900,train loss is: 0.00010894135776409958\n",
      "test loss is 0.0001116548013989865\n",
      "Batch: 32000,train loss is: 0.0004146729142814416\n",
      "test loss is 0.0001084644050803058\n",
      "Batch: 32100,train loss is: 3.73014591478158e-05\n",
      "test loss is 0.00011011106033954029\n",
      "Batch: 32200,train loss is: 7.015473987322363e-05\n",
      "test loss is 0.00010406180384894482\n",
      "Batch: 32300,train loss is: 6.975578635966414e-05\n",
      "test loss is 0.00010583753695772778\n",
      "Batch: 32400,train loss is: 7.879355354904952e-05\n",
      "test loss is 0.00011099801468252345\n",
      "Batch: 32500,train loss is: 0.00012003407787731634\n",
      "test loss is 0.00010836182151546072\n",
      "Batch: 32600,train loss is: 6.603904628697911e-05\n",
      "test loss is 0.00010828789036646877\n",
      "Batch: 32700,train loss is: 5.8893711711491736e-05\n",
      "test loss is 0.00010484469086234456\n",
      "Batch: 32800,train loss is: 8.04809155465155e-05\n",
      "test loss is 0.00011822145881550962\n",
      "Batch: 32900,train loss is: 8.286005471233094e-05\n",
      "test loss is 0.00010629311926270142\n",
      "Batch: 33000,train loss is: 5.0522332892201484e-05\n",
      "test loss is 0.00010533737586650358\n",
      "Batch: 33100,train loss is: 7.298639025395974e-05\n",
      "test loss is 0.00011799431841633365\n",
      "Batch: 33200,train loss is: 8.441398728190858e-05\n",
      "test loss is 0.00011558004891053415\n",
      "Batch: 33300,train loss is: 7.961052171375175e-05\n",
      "test loss is 0.0001175521765295582\n",
      "Batch: 33400,train loss is: 6.413641904852987e-05\n",
      "test loss is 0.00010636075595593222\n",
      "Batch: 33500,train loss is: 7.014374641689174e-05\n",
      "test loss is 0.00010561785962514467\n",
      "Batch: 33600,train loss is: 5.5304401651489064e-05\n",
      "test loss is 0.00011012016123012326\n",
      "Batch: 33700,train loss is: 7.768590469699813e-05\n",
      "test loss is 0.00011464177913334355\n",
      "Batch: 33800,train loss is: 9.79253478397521e-05\n",
      "test loss is 0.0001084791236458561\n",
      "Batch: 33900,train loss is: 7.783660697979254e-05\n",
      "test loss is 0.00010771151917969705\n",
      "-----------------------Epoch: 8----------------------------------\n",
      "Batch: 0,train loss is: 0.00010572654568051625\n",
      "test loss is 0.00010899692230457567\n",
      "Batch: 100,train loss is: 0.00013285965564393084\n",
      "test loss is 0.00010464170509458166\n",
      "Batch: 200,train loss is: 9.984661317169223e-05\n",
      "test loss is 0.00010451151713271002\n",
      "Batch: 300,train loss is: 8.154741713106394e-05\n",
      "test loss is 0.00010681415840203916\n",
      "Batch: 400,train loss is: 6.403078392661311e-05\n",
      "test loss is 0.00010415055086538774\n",
      "Batch: 500,train loss is: 0.00012042121624359884\n",
      "test loss is 0.0001240400520272507\n",
      "Batch: 600,train loss is: 7.824460855413995e-05\n",
      "test loss is 0.00010464886381547224\n",
      "Batch: 700,train loss is: 6.742405735646867e-05\n",
      "test loss is 0.00010745045288124156\n",
      "Batch: 800,train loss is: 0.00012177541192209331\n",
      "test loss is 0.00011035092687513521\n",
      "Batch: 900,train loss is: 5.5152567008955125e-05\n",
      "test loss is 0.00010995431608804105\n",
      "Batch: 1000,train loss is: 8.67240517734864e-05\n",
      "test loss is 0.00010893944631130289\n",
      "Batch: 1100,train loss is: 0.0001813662548142314\n",
      "test loss is 0.00011361617813276452\n",
      "Batch: 1200,train loss is: 5.3443568190296546e-05\n",
      "test loss is 0.00011484826588499987\n",
      "Batch: 1300,train loss is: 0.00010963068058440023\n",
      "test loss is 0.0001068569157581042\n",
      "Batch: 1400,train loss is: 7.618309314715093e-05\n",
      "test loss is 0.00010525275920226925\n",
      "Batch: 1500,train loss is: 8.082231537932939e-05\n",
      "test loss is 0.00010618004003405154\n",
      "Batch: 1600,train loss is: 6.419251578355265e-05\n",
      "test loss is 0.00010588609803819324\n",
      "Batch: 1700,train loss is: 0.00013849217029484362\n",
      "test loss is 0.00011478867298556133\n",
      "Batch: 1800,train loss is: 6.115515789109657e-05\n",
      "test loss is 0.00010439086968562494\n",
      "Batch: 1900,train loss is: 5.021281844238534e-05\n",
      "test loss is 0.000109361985729303\n",
      "Batch: 2000,train loss is: 0.00011745009901815734\n",
      "test loss is 0.00010573284794723545\n",
      "Batch: 2100,train loss is: 7.610871367206824e-05\n",
      "test loss is 0.00011132093522531478\n",
      "Batch: 2200,train loss is: 0.00012739926079257297\n",
      "test loss is 0.00010756480285782393\n",
      "Batch: 2300,train loss is: 8.402027772980946e-05\n",
      "test loss is 0.00010634327620771139\n",
      "Batch: 2400,train loss is: 8.058033525895706e-05\n",
      "test loss is 0.00010520546323541161\n",
      "Batch: 2500,train loss is: 9.699397511996437e-05\n",
      "test loss is 0.0001030717001787838\n",
      "Batch: 2600,train loss is: 6.483737855534563e-05\n",
      "test loss is 0.0001032297726264048\n",
      "Batch: 2700,train loss is: 0.0001264752819608213\n",
      "test loss is 0.00010490173423278115\n",
      "Batch: 2800,train loss is: 0.00010362498091389423\n",
      "test loss is 0.0001177132839588105\n",
      "Batch: 2900,train loss is: 0.00011005397025205952\n",
      "test loss is 0.00010274650155436501\n",
      "Batch: 3000,train loss is: 6.806517422454295e-05\n",
      "test loss is 0.00010649186123929611\n",
      "Batch: 3100,train loss is: 6.538382207782977e-05\n",
      "test loss is 0.00010384197547321976\n",
      "Batch: 3200,train loss is: 0.00013323044932851203\n",
      "test loss is 0.00010937834395233426\n",
      "Batch: 3300,train loss is: 9.000727151919894e-05\n",
      "test loss is 0.00010353340799267584\n",
      "Batch: 3400,train loss is: 9.003633729453081e-05\n",
      "test loss is 0.0001264092526965365\n",
      "Batch: 3500,train loss is: 6.656328614840232e-05\n",
      "test loss is 0.00011186996125398823\n",
      "Batch: 3600,train loss is: 5.957533944410146e-05\n",
      "test loss is 0.00010664750213924498\n",
      "Batch: 3700,train loss is: 5.8208063578242e-05\n",
      "test loss is 0.00010235090549986303\n",
      "Batch: 3800,train loss is: 6.136206359580659e-05\n",
      "test loss is 0.00010247702564679042\n",
      "Batch: 3900,train loss is: 0.00010709546202789495\n",
      "test loss is 0.00010673056861429099\n",
      "Batch: 4000,train loss is: 4.447063047111483e-05\n",
      "test loss is 0.00010576040184362793\n",
      "Batch: 4100,train loss is: 9.898889796859582e-05\n",
      "test loss is 0.00011106228049104868\n",
      "Batch: 4200,train loss is: 0.00011292583956862698\n",
      "test loss is 0.00012113963587189148\n",
      "Batch: 4300,train loss is: 7.083254339694277e-05\n",
      "test loss is 0.00010432018900168871\n",
      "Batch: 4400,train loss is: 6.0589198911234765e-05\n",
      "test loss is 0.00010979569802644143\n",
      "Batch: 4500,train loss is: 6.495802248166266e-05\n",
      "test loss is 0.00010235221303209783\n",
      "Batch: 4600,train loss is: 7.63623013407241e-05\n",
      "test loss is 0.00010614925011356181\n",
      "Batch: 4700,train loss is: 0.00013930605874509108\n",
      "test loss is 0.00011114292317578725\n",
      "Batch: 4800,train loss is: 0.00022888537124697916\n",
      "test loss is 0.00012470784549970036\n",
      "Batch: 4900,train loss is: 9.0844166033819e-05\n",
      "test loss is 0.00010911562694402575\n",
      "Batch: 5000,train loss is: 5.8401917864873284e-05\n",
      "test loss is 0.00011046510295136435\n",
      "Batch: 5100,train loss is: 6.639113091561592e-05\n",
      "test loss is 0.00010346296209740078\n",
      "Batch: 5200,train loss is: 8.004804670384336e-05\n",
      "test loss is 0.00011426996054371667\n",
      "Batch: 5300,train loss is: 8.439513790424521e-05\n",
      "test loss is 0.00011019558887075626\n",
      "Batch: 5400,train loss is: 0.00019519907877364346\n",
      "test loss is 0.00011044810962578185\n",
      "Batch: 5500,train loss is: 7.293476343353837e-05\n",
      "test loss is 0.00012220104245041631\n",
      "Batch: 5600,train loss is: 0.0001356930202606357\n",
      "test loss is 0.00010606518980670415\n",
      "Batch: 5700,train loss is: 0.00012580881284157642\n",
      "test loss is 0.00011290384612253304\n",
      "Batch: 5800,train loss is: 0.00013892753701050657\n",
      "test loss is 0.0001138883351453356\n",
      "Batch: 5900,train loss is: 0.000230211199494196\n",
      "test loss is 0.00011243390433922991\n",
      "Batch: 6000,train loss is: 0.00015155002734855265\n",
      "test loss is 0.00011282144015209975\n",
      "Batch: 6100,train loss is: 6.21037159303286e-05\n",
      "test loss is 0.0001054242697201554\n",
      "Batch: 6200,train loss is: 7.921242905294642e-05\n",
      "test loss is 0.00010276040412303671\n",
      "Batch: 6300,train loss is: 0.00011315283742116963\n",
      "test loss is 0.00011824857366471042\n",
      "Batch: 6400,train loss is: 6.047542970747551e-05\n",
      "test loss is 0.00010540932947003499\n",
      "Batch: 6500,train loss is: 7.425609684175367e-05\n",
      "test loss is 0.00010645568450432466\n",
      "Batch: 6600,train loss is: 0.00018948046177812096\n",
      "test loss is 0.00010508699363946662\n",
      "Batch: 6700,train loss is: 8.942633585661604e-05\n",
      "test loss is 0.00010546266470726647\n",
      "Batch: 6800,train loss is: 0.00011592588534324564\n",
      "test loss is 0.00010844594742278712\n",
      "Batch: 6900,train loss is: 0.00011140945503532033\n",
      "test loss is 0.00010906276328811595\n",
      "Batch: 7000,train loss is: 0.00010836274733119719\n",
      "test loss is 0.00010495740391487292\n",
      "Batch: 7100,train loss is: 0.00011595132121520125\n",
      "test loss is 0.00011792196690619343\n",
      "Batch: 7200,train loss is: 5.870013593631809e-05\n",
      "test loss is 0.00010201784770237137\n",
      "Batch: 7300,train loss is: 0.000205622694768688\n",
      "test loss is 0.0001120533809790911\n",
      "Batch: 7400,train loss is: 7.735123964472175e-05\n",
      "test loss is 0.00012498249136370728\n",
      "Batch: 7500,train loss is: 8.697155502334726e-05\n",
      "test loss is 0.00010715333072810322\n",
      "Batch: 7600,train loss is: 0.00015947126078104027\n",
      "test loss is 0.00013249835098625833\n",
      "Batch: 7700,train loss is: 0.00016368310702016546\n",
      "test loss is 0.00010168172209018923\n",
      "Batch: 7800,train loss is: 5.9576008891351915e-05\n",
      "test loss is 0.0001036667540303202\n",
      "Batch: 7900,train loss is: 4.730520693957776e-05\n",
      "test loss is 0.0001020896523355511\n",
      "Batch: 8000,train loss is: 4.022836052327194e-05\n",
      "test loss is 0.00011437097571189789\n",
      "Batch: 8100,train loss is: 0.00014006413787494783\n",
      "test loss is 0.00010679778468989306\n",
      "Batch: 8200,train loss is: 9.955323624541352e-05\n",
      "test loss is 0.0001025874299251971\n",
      "Batch: 8300,train loss is: 6.425177610943595e-05\n",
      "test loss is 0.00010861415224953018\n",
      "Batch: 8400,train loss is: 0.00012340739829758298\n",
      "test loss is 0.00011521541938342751\n",
      "Batch: 8500,train loss is: 0.00019476181172730514\n",
      "test loss is 0.00011430223907803263\n",
      "Batch: 8600,train loss is: 9.438626422924974e-05\n",
      "test loss is 0.00012647115030595257\n",
      "Batch: 8700,train loss is: 9.189075585807963e-05\n",
      "test loss is 0.00010923542502124343\n",
      "Batch: 8800,train loss is: 8.40620798247293e-05\n",
      "test loss is 0.00010453062486225363\n",
      "Batch: 8900,train loss is: 8.47903789280682e-05\n",
      "test loss is 0.00010140439154384312\n",
      "Batch: 9000,train loss is: 6.397606484891524e-05\n",
      "test loss is 0.00010317010834444139\n",
      "Batch: 9100,train loss is: 8.53339913765558e-05\n",
      "test loss is 0.00010929645499469857\n",
      "Batch: 9200,train loss is: 0.00011316222676254544\n",
      "test loss is 0.00010863106803124329\n",
      "Batch: 9300,train loss is: 0.00012503388648889627\n",
      "test loss is 0.00010470493433186693\n",
      "Batch: 9400,train loss is: 9.902896933902402e-05\n",
      "test loss is 0.00010812941888119634\n",
      "Batch: 9500,train loss is: 6.316480696533134e-05\n",
      "test loss is 0.00011065247624014184\n",
      "Batch: 9600,train loss is: 7.304178519800631e-05\n",
      "test loss is 0.00010402493403179992\n",
      "Batch: 9700,train loss is: 7.38631053355026e-05\n",
      "test loss is 0.00010585021441697884\n",
      "Batch: 9800,train loss is: 5.8057764720526144e-05\n",
      "test loss is 0.00010237188077731886\n",
      "Batch: 9900,train loss is: 6.060601794884112e-05\n",
      "test loss is 0.00010078896148066242\n",
      "Batch: 10000,train loss is: 6.053699446272335e-05\n",
      "test loss is 0.00011079072081090206\n",
      "Batch: 10100,train loss is: 0.00018861480817906721\n",
      "test loss is 0.00010666846183946057\n",
      "Batch: 10200,train loss is: 0.0002591890957218824\n",
      "test loss is 0.00010542964026339246\n",
      "Batch: 10300,train loss is: 8.714004201922161e-05\n",
      "test loss is 0.00010432676998728695\n",
      "Batch: 10400,train loss is: 4.493620239964635e-05\n",
      "test loss is 0.00011011391786146863\n",
      "Batch: 10500,train loss is: 9.470377220742401e-05\n",
      "test loss is 0.00010529642788843782\n",
      "Batch: 10600,train loss is: 7.132910397898389e-05\n",
      "test loss is 0.00010537586599181931\n",
      "Batch: 10700,train loss is: 7.176530684322898e-05\n",
      "test loss is 0.0001013032497880806\n",
      "Batch: 10800,train loss is: 8.903856640065572e-05\n",
      "test loss is 0.00010793794100237734\n",
      "Batch: 10900,train loss is: 8.998271153347414e-05\n",
      "test loss is 0.00010364300165202977\n",
      "Batch: 11000,train loss is: 0.00011639032155930971\n",
      "test loss is 0.00010104053289245725\n",
      "Batch: 11100,train loss is: 0.00012049336638774087\n",
      "test loss is 0.00010456043427869615\n",
      "Batch: 11200,train loss is: 0.00012119963556177483\n",
      "test loss is 0.00010159532763576157\n",
      "Batch: 11300,train loss is: 6.694488855843315e-05\n",
      "test loss is 0.00010937005063442237\n",
      "Batch: 11400,train loss is: 0.0001805700567924822\n",
      "test loss is 0.00010229514382450653\n",
      "Batch: 11500,train loss is: 6.060062632504172e-05\n",
      "test loss is 0.00011025996247546646\n",
      "Batch: 11600,train loss is: 0.00011070493074359716\n",
      "test loss is 0.00010361206929829175\n",
      "Batch: 11700,train loss is: 6.977257721340977e-05\n",
      "test loss is 0.00010141490500320019\n",
      "Batch: 11800,train loss is: 9.13301258442699e-05\n",
      "test loss is 0.0001025944705407698\n",
      "Batch: 11900,train loss is: 0.00010582498797607347\n",
      "test loss is 0.00011382079425778437\n",
      "Batch: 12000,train loss is: 8.548719993992436e-05\n",
      "test loss is 0.00010585049967420147\n",
      "Batch: 12100,train loss is: 0.00011911811964543669\n",
      "test loss is 0.00010240665705256768\n",
      "Batch: 12200,train loss is: 6.55513851161612e-05\n",
      "test loss is 0.00010406260528623526\n",
      "Batch: 12300,train loss is: 6.82696749114807e-05\n",
      "test loss is 0.00010687761014016878\n",
      "Batch: 12400,train loss is: 7.6360296308636e-05\n",
      "test loss is 0.00010384011038568842\n",
      "Batch: 12500,train loss is: 0.00016137422673608958\n",
      "test loss is 0.00010499444667026403\n",
      "Batch: 12600,train loss is: 5.737302364083328e-05\n",
      "test loss is 0.00010460905182812273\n",
      "Batch: 12700,train loss is: 7.65517231052929e-05\n",
      "test loss is 0.00010461004014207413\n",
      "Batch: 12800,train loss is: 4.591810572585105e-05\n",
      "test loss is 0.00010352939710165422\n",
      "Batch: 12900,train loss is: 0.00013596987866428345\n",
      "test loss is 0.00010296409669588066\n",
      "Batch: 13000,train loss is: 8.654820540824214e-05\n",
      "test loss is 0.00010231960161457971\n",
      "Batch: 13100,train loss is: 0.00017585233456969153\n",
      "test loss is 0.00010115873218016944\n",
      "Batch: 13200,train loss is: 9.683446178039844e-05\n",
      "test loss is 0.00011580786561616983\n",
      "Batch: 13300,train loss is: 0.00010154796451576692\n",
      "test loss is 0.000132393789333513\n",
      "Batch: 13400,train loss is: 7.797497090281375e-05\n",
      "test loss is 0.00010156542521375561\n",
      "Batch: 13500,train loss is: 5.801083561984385e-05\n",
      "test loss is 0.00010546026703134132\n",
      "Batch: 13600,train loss is: 6.53238421919005e-05\n",
      "test loss is 0.00011643180376617894\n",
      "Batch: 13700,train loss is: 0.00010424313770184923\n",
      "test loss is 0.00010375848442598696\n",
      "Batch: 13800,train loss is: 6.323086145842328e-05\n",
      "test loss is 0.00011170455998292097\n",
      "Batch: 13900,train loss is: 4.904262886154831e-05\n",
      "test loss is 0.00010478835873164793\n",
      "Batch: 14000,train loss is: 9.90527508067869e-05\n",
      "test loss is 0.00010357711175722876\n",
      "Batch: 14100,train loss is: 6.460613616387355e-05\n",
      "test loss is 0.00010471860377813248\n",
      "Batch: 14200,train loss is: 6.419017713032695e-05\n",
      "test loss is 0.00010353643517489584\n",
      "Batch: 14300,train loss is: 8.20487273730716e-05\n",
      "test loss is 0.00010466723653569773\n",
      "Batch: 14400,train loss is: 0.00014579948865172895\n",
      "test loss is 0.0001021455792675878\n",
      "Batch: 14500,train loss is: 6.983679124369368e-05\n",
      "test loss is 0.00010292902386322714\n",
      "Batch: 14600,train loss is: 0.00011714469795357551\n",
      "test loss is 0.00011114274140029326\n",
      "Batch: 14700,train loss is: 0.00010057916268868657\n",
      "test loss is 0.00010474995179131308\n",
      "Batch: 14800,train loss is: 0.0001616352337492653\n",
      "test loss is 0.00010764930054601163\n",
      "Batch: 14900,train loss is: 7.626547036379804e-05\n",
      "test loss is 0.00010221580578681729\n",
      "Batch: 15000,train loss is: 0.00011011158428834342\n",
      "test loss is 0.00010489484068261453\n",
      "Batch: 15100,train loss is: 0.00012494244816112134\n",
      "test loss is 0.00011066057933308496\n",
      "Batch: 15200,train loss is: 0.00012076562041004593\n",
      "test loss is 0.00010942965687651053\n",
      "Batch: 15300,train loss is: 0.00012111257995146129\n",
      "test loss is 0.00010250754979401835\n",
      "Batch: 15400,train loss is: 6.053796614733258e-05\n",
      "test loss is 0.00010261421061678183\n",
      "Batch: 15500,train loss is: 7.417163287028572e-05\n",
      "test loss is 0.00010610691872220171\n",
      "Batch: 15600,train loss is: 0.00015951839956477798\n",
      "test loss is 0.00010605265396997932\n",
      "Batch: 15700,train loss is: 7.622709845091206e-05\n",
      "test loss is 0.00010854964744387391\n",
      "Batch: 15800,train loss is: 7.812345872278443e-05\n",
      "test loss is 0.00011113593355401889\n",
      "Batch: 15900,train loss is: 0.00011031020026633472\n",
      "test loss is 0.0001082850651913809\n",
      "Batch: 16000,train loss is: 0.00013465724506698745\n",
      "test loss is 0.00010555253188814905\n",
      "Batch: 16100,train loss is: 0.00017002080582866286\n",
      "test loss is 0.00010288399263353492\n",
      "Batch: 16200,train loss is: 6.52217503901952e-05\n",
      "test loss is 0.0001072014495657924\n",
      "Batch: 16300,train loss is: 0.00010099378501132095\n",
      "test loss is 0.00011169303195352193\n",
      "Batch: 16400,train loss is: 6.24199524581299e-05\n",
      "test loss is 0.00010910234707969602\n",
      "Batch: 16500,train loss is: 6.73255556846474e-05\n",
      "test loss is 0.0001030242004733733\n",
      "Batch: 16600,train loss is: 6.48530085709741e-05\n",
      "test loss is 0.00010204545919850667\n",
      "Batch: 16700,train loss is: 0.00016609754193689825\n",
      "test loss is 0.00010758617901808074\n",
      "Batch: 16800,train loss is: 0.00014154516264261205\n",
      "test loss is 0.00011894336950617985\n",
      "Batch: 16900,train loss is: 7.565083128092767e-05\n",
      "test loss is 0.00011036040347787254\n",
      "Batch: 17000,train loss is: 0.00010770008287336415\n",
      "test loss is 0.00011635320752313607\n",
      "Batch: 17100,train loss is: 0.00010543327272252014\n",
      "test loss is 0.00010601445503718773\n",
      "Batch: 17200,train loss is: 5.8104140993905385e-05\n",
      "test loss is 0.00010531057948242538\n",
      "Batch: 17300,train loss is: 6.941852698080278e-05\n",
      "test loss is 0.00010951862616835034\n",
      "Batch: 17400,train loss is: 4.8066285922239915e-05\n",
      "test loss is 0.00011472675331857213\n",
      "Batch: 17500,train loss is: 0.00010612281177093946\n",
      "test loss is 0.00010143842749471065\n",
      "Batch: 17600,train loss is: 5.853391750661352e-05\n",
      "test loss is 0.00010602623849202434\n",
      "Batch: 17700,train loss is: 4.955887271440864e-05\n",
      "test loss is 0.00010192865413132043\n",
      "Batch: 17800,train loss is: 0.00022517746412988157\n",
      "test loss is 0.00010553525198131922\n",
      "Batch: 17900,train loss is: 4.2085497162940585e-05\n",
      "test loss is 0.00010126719028102705\n",
      "Batch: 18000,train loss is: 7.124267908959534e-05\n",
      "test loss is 0.00011176415838875168\n",
      "Batch: 18100,train loss is: 0.00015646895427097172\n",
      "test loss is 0.00010557234005062863\n",
      "Batch: 18200,train loss is: 9.113376406768039e-05\n",
      "test loss is 0.00010308552410741566\n",
      "Batch: 18300,train loss is: 0.00011240497550595046\n",
      "test loss is 0.00010120633950391225\n",
      "Batch: 18400,train loss is: 9.840968799854176e-05\n",
      "test loss is 9.999464406084415e-05\n",
      "Batch: 18500,train loss is: 0.00012906726918249483\n",
      "test loss is 0.00012550592199685908\n",
      "Batch: 18600,train loss is: 7.156624344841064e-05\n",
      "test loss is 0.00012308412626158775\n",
      "Batch: 18700,train loss is: 0.00012454439669951631\n",
      "test loss is 0.00010703086025327268\n",
      "Batch: 18800,train loss is: 8.34345422076338e-05\n",
      "test loss is 0.00010392881112296367\n",
      "Batch: 18900,train loss is: 0.00012252809431284213\n",
      "test loss is 0.00010700007487845249\n",
      "Batch: 19000,train loss is: 5.2654877432381656e-05\n",
      "test loss is 0.00010451267563014708\n",
      "Batch: 19100,train loss is: 0.0001264523033036974\n",
      "test loss is 0.00011652060030453602\n",
      "Batch: 19200,train loss is: 8.784802756254371e-05\n",
      "test loss is 0.0001010728222575156\n",
      "Batch: 19300,train loss is: 0.0001192493427195821\n",
      "test loss is 0.00010538160171884649\n",
      "Batch: 19400,train loss is: 6.678201908358867e-05\n",
      "test loss is 0.00010459273926730906\n",
      "Batch: 19500,train loss is: 6.43603646590495e-05\n",
      "test loss is 0.00011360561163268998\n",
      "Batch: 19600,train loss is: 7.292280689966612e-05\n",
      "test loss is 0.00010503415467827637\n",
      "Batch: 19700,train loss is: 0.00014738294299341048\n",
      "test loss is 0.00010361455960241714\n",
      "Batch: 19800,train loss is: 6.446086205623764e-05\n",
      "test loss is 0.00010434963545133939\n",
      "Batch: 19900,train loss is: 6.390280863074685e-05\n",
      "test loss is 0.00011075951322891671\n",
      "Batch: 20000,train loss is: 0.00015836284521104644\n",
      "test loss is 0.00010678933197637984\n",
      "Batch: 20100,train loss is: 0.00011209151095612464\n",
      "test loss is 0.00011727530868532884\n",
      "Batch: 20200,train loss is: 7.405358714616959e-05\n",
      "test loss is 0.00011001449068907818\n",
      "Batch: 20300,train loss is: 4.470198348203811e-05\n",
      "test loss is 0.00011343996469431992\n",
      "Batch: 20400,train loss is: 9.575915083913162e-05\n",
      "test loss is 0.0001134135917619761\n",
      "Batch: 20500,train loss is: 9.448514975986585e-05\n",
      "test loss is 0.00010143863367500993\n",
      "Batch: 20600,train loss is: 0.00018261159148001024\n",
      "test loss is 0.00010610527712906961\n",
      "Batch: 20700,train loss is: 0.00010544001849981048\n",
      "test loss is 0.00010981964217023997\n",
      "Batch: 20800,train loss is: 9.037621810892322e-05\n",
      "test loss is 0.0001117099420424226\n",
      "Batch: 20900,train loss is: 7.996771364114961e-05\n",
      "test loss is 0.00010579370719436475\n",
      "Batch: 21000,train loss is: 3.986976955850617e-05\n",
      "test loss is 0.00010079685610961196\n",
      "Batch: 21100,train loss is: 6.886084032879302e-05\n",
      "test loss is 0.0001020673725678718\n",
      "Batch: 21200,train loss is: 5.2185677574260143e-05\n",
      "test loss is 0.00010471309264309506\n",
      "Batch: 21300,train loss is: 7.997580298788763e-05\n",
      "test loss is 0.00010839706082879103\n",
      "Batch: 21400,train loss is: 0.00010454068863226701\n",
      "test loss is 0.00010011210893836411\n",
      "Batch: 21500,train loss is: 7.835917604494938e-05\n",
      "test loss is 0.00010431173078513327\n",
      "Batch: 21600,train loss is: 0.00012692010139638395\n",
      "test loss is 0.0001168490459529009\n",
      "Batch: 21700,train loss is: 6.815563285023435e-05\n",
      "test loss is 0.00010893093217550834\n",
      "Batch: 21800,train loss is: 0.0001443319679101997\n",
      "test loss is 0.00010138727918975936\n",
      "Batch: 21900,train loss is: 6.63783915990498e-05\n",
      "test loss is 0.00010941727979649282\n",
      "Batch: 22000,train loss is: 8.672917975496601e-05\n",
      "test loss is 0.00012409451076981237\n",
      "Batch: 22100,train loss is: 5.972594381313642e-05\n",
      "test loss is 0.00010067773215232878\n",
      "Batch: 22200,train loss is: 6.074627708391805e-05\n",
      "test loss is 0.00010998899262702768\n",
      "Batch: 22300,train loss is: 8.253820816522136e-05\n",
      "test loss is 0.00011451652739860072\n",
      "Batch: 22400,train loss is: 8.924818087222606e-05\n",
      "test loss is 0.00010692957062591042\n",
      "Batch: 22500,train loss is: 7.586310068323729e-05\n",
      "test loss is 0.00010418661364002625\n",
      "Batch: 22600,train loss is: 9.073880415418034e-05\n",
      "test loss is 0.00010652612411707682\n",
      "Batch: 22700,train loss is: 6.317242458849995e-05\n",
      "test loss is 0.00010171369371088596\n",
      "Batch: 22800,train loss is: 0.00011248735438359491\n",
      "test loss is 0.00011233072814814304\n",
      "Batch: 22900,train loss is: 5.1034930168893095e-05\n",
      "test loss is 0.00010866634775260557\n",
      "Batch: 23000,train loss is: 0.00013760321129171102\n",
      "test loss is 0.0001041993080157555\n",
      "Batch: 23100,train loss is: 7.716302991650571e-05\n",
      "test loss is 9.974577689324753e-05\n",
      "Batch: 23200,train loss is: 7.00263030769551e-05\n",
      "test loss is 0.00010110929265962982\n",
      "Batch: 23300,train loss is: 5.055428687925284e-05\n",
      "test loss is 0.00010148984267473986\n",
      "Batch: 23400,train loss is: 7.73902705700956e-05\n",
      "test loss is 0.00010537305872960504\n",
      "Batch: 23500,train loss is: 0.00011054049577530945\n",
      "test loss is 0.00010478572222539874\n",
      "Batch: 23600,train loss is: 6.618861480176676e-05\n",
      "test loss is 0.00011126357738079691\n",
      "Batch: 23700,train loss is: 0.00013645045462434236\n",
      "test loss is 0.00010398246890643586\n",
      "Batch: 23800,train loss is: 0.0001406777564635661\n",
      "test loss is 0.00010431286427606619\n",
      "Batch: 23900,train loss is: 0.00032143495234610156\n",
      "test loss is 0.0001040201349154033\n",
      "Batch: 24000,train loss is: 7.517062790325859e-05\n",
      "test loss is 0.0001153273433390419\n",
      "Batch: 24100,train loss is: 0.00010088516844758323\n",
      "test loss is 0.00010178129116843431\n",
      "Batch: 24200,train loss is: 0.00015415282431632788\n",
      "test loss is 0.00010964873925304633\n",
      "Batch: 24300,train loss is: 9.03234308423971e-05\n",
      "test loss is 0.00011746053555108097\n",
      "Batch: 24400,train loss is: 0.00012897040801891776\n",
      "test loss is 0.00010376101002853738\n",
      "Batch: 24500,train loss is: 0.00011887414533403578\n",
      "test loss is 0.00010995806168683392\n",
      "Batch: 24600,train loss is: 9.493966375551597e-05\n",
      "test loss is 0.0001192936742934896\n",
      "Batch: 24700,train loss is: 7.391199426804694e-05\n",
      "test loss is 0.00010214764267580199\n",
      "Batch: 24800,train loss is: 0.00013265972378618263\n",
      "test loss is 0.00010397655453845196\n",
      "Batch: 24900,train loss is: 4.6615160566995565e-05\n",
      "test loss is 0.0001005490496030898\n",
      "Batch: 25000,train loss is: 8.325654721859581e-05\n",
      "test loss is 0.00010348643161466963\n",
      "Batch: 25100,train loss is: 5.515008764535706e-05\n",
      "test loss is 0.00010130400545383997\n",
      "Batch: 25200,train loss is: 0.00010150500641947362\n",
      "test loss is 0.00010542952036148777\n",
      "Batch: 25300,train loss is: 8.702459722797462e-05\n",
      "test loss is 0.00010072401507136591\n",
      "Batch: 25400,train loss is: 8.206540464031363e-05\n",
      "test loss is 0.00010864683482585736\n",
      "Batch: 25500,train loss is: 6.650498879543877e-05\n",
      "test loss is 0.00010227066187838506\n",
      "Batch: 25600,train loss is: 0.000102033666394727\n",
      "test loss is 0.00010848965634480583\n",
      "Batch: 25700,train loss is: 0.00011495158520194985\n",
      "test loss is 0.00010221799563021987\n",
      "Batch: 25800,train loss is: 7.259375761058709e-05\n",
      "test loss is 0.00010423255891446122\n",
      "Batch: 25900,train loss is: 4.555783034698029e-05\n",
      "test loss is 0.00010241218358055559\n",
      "Batch: 26000,train loss is: 0.0001050494858698665\n",
      "test loss is 0.00010406265625018778\n",
      "Batch: 26100,train loss is: 0.00016157634730284742\n",
      "test loss is 0.00010079228821314711\n",
      "Batch: 26200,train loss is: 0.0001590233059471122\n",
      "test loss is 0.0001057682735392159\n",
      "Batch: 26300,train loss is: 0.00012160846068850876\n",
      "test loss is 0.0001016116952479781\n",
      "Batch: 26400,train loss is: 6.977483748099934e-05\n",
      "test loss is 9.974038433978986e-05\n",
      "Batch: 26500,train loss is: 0.00010177794365024976\n",
      "test loss is 0.00010122285093767453\n",
      "Batch: 26600,train loss is: 0.0001441594092353128\n",
      "test loss is 0.0001006891026292063\n",
      "Batch: 26700,train loss is: 5.842760104566656e-05\n",
      "test loss is 0.00010727066013550536\n",
      "Batch: 26800,train loss is: 0.00015196977411021927\n",
      "test loss is 0.00010313926370646338\n",
      "Batch: 26900,train loss is: 4.79372346248256e-05\n",
      "test loss is 0.00011564725953521104\n",
      "Batch: 27000,train loss is: 9.14087346277145e-05\n",
      "test loss is 0.00011186313713272019\n",
      "Batch: 27100,train loss is: 5.2377126088401605e-05\n",
      "test loss is 0.00011826115217473493\n",
      "Batch: 27200,train loss is: 8.542577513566897e-05\n",
      "test loss is 0.00010144171534935566\n",
      "Batch: 27300,train loss is: 9.77363318419537e-05\n",
      "test loss is 0.000100961872338981\n",
      "Batch: 27400,train loss is: 8.840886823411725e-05\n",
      "test loss is 0.00010326610097559919\n",
      "Batch: 27500,train loss is: 7.340654193901087e-05\n",
      "test loss is 0.00011440962653993549\n",
      "Batch: 27600,train loss is: 8.998513670332185e-05\n",
      "test loss is 0.00012216653889466934\n",
      "Batch: 27700,train loss is: 0.0002822382542560303\n",
      "test loss is 0.00011463034190957997\n",
      "Batch: 27800,train loss is: 8.003370272400442e-05\n",
      "test loss is 0.00010450309867288372\n",
      "Batch: 27900,train loss is: 0.00012281902713038437\n",
      "test loss is 0.00010135637437408837\n",
      "Batch: 28000,train loss is: 6.966070146751552e-05\n",
      "test loss is 0.00011063175934143637\n",
      "Batch: 28100,train loss is: 0.00016317962125192846\n",
      "test loss is 0.00011299830172364251\n",
      "Batch: 28200,train loss is: 0.00010935362263642128\n",
      "test loss is 0.00010830748739015981\n",
      "Batch: 28300,train loss is: 0.00028300660651104005\n",
      "test loss is 0.00010252322794034467\n",
      "Batch: 28400,train loss is: 9.100892564789168e-05\n",
      "test loss is 0.00010273298214388595\n",
      "Batch: 28500,train loss is: 9.274836215004009e-05\n",
      "test loss is 0.00010941952099259357\n",
      "Batch: 28600,train loss is: 7.230699115685919e-05\n",
      "test loss is 0.0001039496297229475\n",
      "Batch: 28700,train loss is: 9.027034957401992e-05\n",
      "test loss is 0.00010209602241402827\n",
      "Batch: 28800,train loss is: 0.00015932943198224736\n",
      "test loss is 0.00010007372426599061\n",
      "Batch: 28900,train loss is: 0.00010609366621796924\n",
      "test loss is 0.00010928718368350127\n",
      "Batch: 29000,train loss is: 8.661663463802005e-05\n",
      "test loss is 0.00010144312249206492\n",
      "Batch: 29100,train loss is: 8.473384824447096e-05\n",
      "test loss is 0.00010937473306211368\n",
      "Batch: 29200,train loss is: 9.761422822467063e-05\n",
      "test loss is 0.00013936210303272867\n",
      "Batch: 29300,train loss is: 9.523055423570135e-05\n",
      "test loss is 0.00010481391172394229\n",
      "Batch: 29400,train loss is: 0.00012734119062672404\n",
      "test loss is 0.0001104398943854542\n",
      "Batch: 29500,train loss is: 6.000546277857923e-05\n",
      "test loss is 0.00010154565288323344\n",
      "Batch: 29600,train loss is: 0.0001899398916105594\n",
      "test loss is 0.0001106524411441597\n",
      "Batch: 29700,train loss is: 6.326537803545911e-05\n",
      "test loss is 0.00010021959535178672\n",
      "Batch: 29800,train loss is: 0.00018734535839685882\n",
      "test loss is 0.00010105062470330107\n",
      "Batch: 29900,train loss is: 0.00024755095042499784\n",
      "test loss is 0.00010164619091660823\n",
      "Batch: 30000,train loss is: 7.463667524987e-05\n",
      "test loss is 0.0001040700240946198\n",
      "Batch: 30100,train loss is: 8.474028186894445e-05\n",
      "test loss is 0.00011435905300514074\n",
      "Batch: 30200,train loss is: 6.950000563373916e-05\n",
      "test loss is 0.00010101996067907584\n",
      "Batch: 30300,train loss is: 8.525875096026769e-05\n",
      "test loss is 0.00010119820703326543\n",
      "Batch: 30400,train loss is: 7.602679134663596e-05\n",
      "test loss is 0.00010960022464990814\n",
      "Batch: 30500,train loss is: 7.346471809119679e-05\n",
      "test loss is 0.00010135490549896961\n",
      "Batch: 30600,train loss is: 6.296977997653193e-05\n",
      "test loss is 0.00010283387527796471\n",
      "Batch: 30700,train loss is: 7.702803316940084e-05\n",
      "test loss is 0.00010764656984914935\n",
      "Batch: 30800,train loss is: 6.452017209178539e-05\n",
      "test loss is 0.00010485718074211778\n",
      "Batch: 30900,train loss is: 7.978422998366774e-05\n",
      "test loss is 0.00010239775582813722\n",
      "Batch: 31000,train loss is: 4.6591585537847065e-05\n",
      "test loss is 0.00010511439112082627\n",
      "Batch: 31100,train loss is: 8.201722147839722e-05\n",
      "test loss is 0.00010430139847835893\n",
      "Batch: 31200,train loss is: 7.406985456424478e-05\n",
      "test loss is 0.00010280317820665729\n",
      "Batch: 31300,train loss is: 0.00011844207547170762\n",
      "test loss is 0.00010081604620256112\n",
      "Batch: 31400,train loss is: 8.538680272275152e-05\n",
      "test loss is 0.00012412802247826913\n",
      "Batch: 31500,train loss is: 5.783296751543198e-05\n",
      "test loss is 0.00010832492840390143\n",
      "Batch: 31600,train loss is: 7.652230497926958e-05\n",
      "test loss is 0.00010109095239769648\n",
      "Batch: 31700,train loss is: 5.921223802353424e-05\n",
      "test loss is 0.00010411935153697733\n",
      "Batch: 31800,train loss is: 0.00012976261299603502\n",
      "test loss is 0.00010140604023947363\n",
      "Batch: 31900,train loss is: 0.00010528676009619288\n",
      "test loss is 0.00010793539160482797\n",
      "Batch: 32000,train loss is: 0.00039657707487609096\n",
      "test loss is 0.0001055562833214078\n",
      "Batch: 32100,train loss is: 3.5986752277996506e-05\n",
      "test loss is 0.00010801880331984502\n",
      "Batch: 32200,train loss is: 6.674923424378236e-05\n",
      "test loss is 0.00010088412521959338\n",
      "Batch: 32300,train loss is: 6.759989653660365e-05\n",
      "test loss is 0.00010278105573814308\n",
      "Batch: 32400,train loss is: 7.489307225451089e-05\n",
      "test loss is 0.00010730177473090415\n",
      "Batch: 32500,train loss is: 0.00011481823187404694\n",
      "test loss is 0.0001054298600162913\n",
      "Batch: 32600,train loss is: 6.555509649383998e-05\n",
      "test loss is 0.00010517367032642545\n",
      "Batch: 32700,train loss is: 5.624738605104027e-05\n",
      "test loss is 0.00010167706343264394\n",
      "Batch: 32800,train loss is: 7.861833998700521e-05\n",
      "test loss is 0.00011600994967944471\n",
      "Batch: 32900,train loss is: 7.83938734909899e-05\n",
      "test loss is 0.00010291421097913997\n",
      "Batch: 33000,train loss is: 4.958735504106367e-05\n",
      "test loss is 0.00010243329680886625\n",
      "Batch: 33100,train loss is: 7.083433235440771e-05\n",
      "test loss is 0.000114843763771075\n",
      "Batch: 33200,train loss is: 8.102993339632926e-05\n",
      "test loss is 0.00011120249680072832\n",
      "Batch: 33300,train loss is: 7.622325104422673e-05\n",
      "test loss is 0.00011339192333431784\n",
      "Batch: 33400,train loss is: 6.157406402031177e-05\n",
      "test loss is 0.00010312198989392786\n",
      "Batch: 33500,train loss is: 6.73992146100449e-05\n",
      "test loss is 0.00010232096209399206\n",
      "Batch: 33600,train loss is: 5.363249466434837e-05\n",
      "test loss is 0.0001072567744445272\n",
      "Batch: 33700,train loss is: 7.441516941169798e-05\n",
      "test loss is 0.00011182275479939839\n",
      "Batch: 33800,train loss is: 9.316217259434236e-05\n",
      "test loss is 0.00010526553630047002\n",
      "Batch: 33900,train loss is: 7.423816022858268e-05\n",
      "test loss is 0.00010509970264908264\n",
      "-----------------------Epoch: 9----------------------------------\n",
      "Batch: 0,train loss is: 0.00010470410624496036\n",
      "test loss is 0.00010572582904086957\n",
      "Batch: 100,train loss is: 0.00013094717900981304\n",
      "test loss is 0.0001012035293587091\n",
      "Batch: 200,train loss is: 9.850698646242475e-05\n",
      "test loss is 0.00010166243497497324\n",
      "Batch: 300,train loss is: 7.900475086766446e-05\n",
      "test loss is 0.00010365782188913973\n",
      "Batch: 400,train loss is: 6.260042451033817e-05\n",
      "test loss is 0.00010152904845865569\n",
      "Batch: 500,train loss is: 0.00011743819785285858\n",
      "test loss is 0.00012018818743116699\n",
      "Batch: 600,train loss is: 7.479170146349692e-05\n",
      "test loss is 0.00010138722334564619\n",
      "Batch: 700,train loss is: 6.722066125168184e-05\n",
      "test loss is 0.00010421890407341511\n",
      "Batch: 800,train loss is: 0.00011586058385342906\n",
      "test loss is 0.00010640285868256917\n",
      "Batch: 900,train loss is: 5.4621066296820035e-05\n",
      "test loss is 0.00010668253339823265\n",
      "Batch: 1000,train loss is: 8.560407069085346e-05\n",
      "test loss is 0.00010622233477774727\n",
      "Batch: 1100,train loss is: 0.00017358006135493475\n",
      "test loss is 0.00011222971123353984\n",
      "Batch: 1200,train loss is: 5.270145674818324e-05\n",
      "test loss is 0.00011195234757849957\n",
      "Batch: 1300,train loss is: 0.00010760968121596299\n",
      "test loss is 0.00010297018456428218\n",
      "Batch: 1400,train loss is: 7.402728441201626e-05\n",
      "test loss is 0.00010209687897634133\n",
      "Batch: 1500,train loss is: 7.878734498302299e-05\n",
      "test loss is 0.00010287687392254907\n",
      "Batch: 1600,train loss is: 6.329225401902283e-05\n",
      "test loss is 0.0001025419296670944\n",
      "Batch: 1700,train loss is: 0.00013821069560434533\n",
      "test loss is 0.00011079988106200826\n",
      "Batch: 1800,train loss is: 5.889767541663061e-05\n",
      "test loss is 0.00010114267153062856\n",
      "Batch: 1900,train loss is: 4.8662313034728024e-05\n",
      "test loss is 0.00010635748049441377\n",
      "Batch: 2000,train loss is: 0.00011475101573223439\n",
      "test loss is 0.00010260161435312073\n",
      "Batch: 2100,train loss is: 7.439558779112573e-05\n",
      "test loss is 0.00010786516409772765\n",
      "Batch: 2200,train loss is: 0.00012540184917831203\n",
      "test loss is 0.00010460104862052703\n",
      "Batch: 2300,train loss is: 8.168245842494699e-05\n",
      "test loss is 0.00010304460742146352\n",
      "Batch: 2400,train loss is: 7.764437760036454e-05\n",
      "test loss is 0.00010195382705792644\n",
      "Batch: 2500,train loss is: 9.306516011491267e-05\n",
      "test loss is 0.00010004695638718361\n",
      "Batch: 2600,train loss is: 6.331229472585511e-05\n",
      "test loss is 0.00010006291867073125\n",
      "Batch: 2700,train loss is: 0.00012153369139106453\n",
      "test loss is 0.00010126625157105556\n",
      "Batch: 2800,train loss is: 0.00010209443608000255\n",
      "test loss is 0.00011388376496241922\n",
      "Batch: 2900,train loss is: 0.00010657746843534804\n",
      "test loss is 9.959178016170698e-05\n",
      "Batch: 3000,train loss is: 6.370602848570706e-05\n",
      "test loss is 0.00010351628888657494\n",
      "Batch: 3100,train loss is: 6.233817790084911e-05\n",
      "test loss is 0.0001007529209392065\n",
      "Batch: 3200,train loss is: 0.00012464197215507198\n",
      "test loss is 0.00010632148195825183\n",
      "Batch: 3300,train loss is: 8.808880598741834e-05\n",
      "test loss is 0.00010008869902060131\n",
      "Batch: 3400,train loss is: 8.954699680027211e-05\n",
      "test loss is 0.00012348329427205637\n",
      "Batch: 3500,train loss is: 6.415965013909978e-05\n",
      "test loss is 0.00010822538958887192\n",
      "Batch: 3600,train loss is: 5.7117022135793045e-05\n",
      "test loss is 0.00010316298758952028\n",
      "Batch: 3700,train loss is: 5.850016473167155e-05\n",
      "test loss is 9.920156039422456e-05\n",
      "Batch: 3800,train loss is: 5.9290929706096576e-05\n",
      "test loss is 9.888444648009988e-05\n",
      "Batch: 3900,train loss is: 0.00010387879694981048\n",
      "test loss is 0.00010385438699681557\n",
      "Batch: 4000,train loss is: 4.2474914130077326e-05\n",
      "test loss is 0.00010283287767800404\n",
      "Batch: 4100,train loss is: 9.621428495072088e-05\n",
      "test loss is 0.00010803060412123702\n",
      "Batch: 4200,train loss is: 0.00010964141074576339\n",
      "test loss is 0.00011769107595567077\n",
      "Batch: 4300,train loss is: 6.72657679929854e-05\n",
      "test loss is 0.00010118935269817439\n",
      "Batch: 4400,train loss is: 5.763966211692649e-05\n",
      "test loss is 0.0001067384447204494\n",
      "Batch: 4500,train loss is: 6.256115031125854e-05\n",
      "test loss is 9.947946791278461e-05\n",
      "Batch: 4600,train loss is: 7.336346304180444e-05\n",
      "test loss is 0.00010299897397213934\n",
      "Batch: 4700,train loss is: 0.0001354930280570608\n",
      "test loss is 0.00010780127734747823\n",
      "Batch: 4800,train loss is: 0.00022503273984306903\n",
      "test loss is 0.00012162458419599835\n",
      "Batch: 4900,train loss is: 8.788471650974898e-05\n",
      "test loss is 0.0001060201459069761\n",
      "Batch: 5000,train loss is: 5.717921561375978e-05\n",
      "test loss is 0.00010769048526994317\n",
      "Batch: 5100,train loss is: 6.539735954396506e-05\n",
      "test loss is 0.00010058096048538853\n",
      "Batch: 5200,train loss is: 7.707427634168151e-05\n",
      "test loss is 0.00011068583040279492\n",
      "Batch: 5300,train loss is: 8.196622229130896e-05\n",
      "test loss is 0.00010660723441182557\n",
      "Batch: 5400,train loss is: 0.00019060775486992744\n",
      "test loss is 0.00010684152306953098\n",
      "Batch: 5500,train loss is: 7.271473461659193e-05\n",
      "test loss is 0.0001178354438822195\n",
      "Batch: 5600,train loss is: 0.00012839764395425202\n",
      "test loss is 0.00010262442730412725\n",
      "Batch: 5700,train loss is: 0.000124042775414599\n",
      "test loss is 0.0001096744758400212\n",
      "Batch: 5800,train loss is: 0.00013358995336980073\n",
      "test loss is 0.00011169910649943829\n",
      "Batch: 5900,train loss is: 0.0002182502887230875\n",
      "test loss is 0.00010874359384024179\n",
      "Batch: 6000,train loss is: 0.00014602497208167036\n",
      "test loss is 0.00010886723121246176\n",
      "Batch: 6100,train loss is: 6.014806971943911e-05\n",
      "test loss is 0.00010217730969966234\n",
      "Batch: 6200,train loss is: 7.841220533838895e-05\n",
      "test loss is 9.949984686295188e-05\n",
      "Batch: 6300,train loss is: 0.00011203221360920465\n",
      "test loss is 0.00011405562560399426\n",
      "Batch: 6400,train loss is: 5.813965418718447e-05\n",
      "test loss is 0.00010213194293296801\n",
      "Batch: 6500,train loss is: 7.355746590541058e-05\n",
      "test loss is 0.00010301491712484003\n",
      "Batch: 6600,train loss is: 0.00018198282703289352\n",
      "test loss is 0.00010205871080343983\n",
      "Batch: 6700,train loss is: 8.420809892980915e-05\n",
      "test loss is 0.00010275687220368836\n",
      "Batch: 6800,train loss is: 0.00011090714380678885\n",
      "test loss is 0.00010512538191567813\n",
      "Batch: 6900,train loss is: 0.0001154796293292158\n",
      "test loss is 0.00010657863250480201\n",
      "Batch: 7000,train loss is: 0.00010440120614692606\n",
      "test loss is 0.0001015254019463841\n",
      "Batch: 7100,train loss is: 0.00011224069266300247\n",
      "test loss is 0.0001144732545536134\n",
      "Batch: 7200,train loss is: 5.5807509544908474e-05\n",
      "test loss is 9.8885360933963e-05\n",
      "Batch: 7300,train loss is: 0.0001967645223651574\n",
      "test loss is 0.00010919379852390634\n",
      "Batch: 7400,train loss is: 7.64969288051482e-05\n",
      "test loss is 0.00012029947005060268\n",
      "Batch: 7500,train loss is: 8.617045794350078e-05\n",
      "test loss is 0.00010386937998213264\n",
      "Batch: 7600,train loss is: 0.0001563672592794826\n",
      "test loss is 0.00012963659750629548\n",
      "Batch: 7700,train loss is: 0.00016305291969547148\n",
      "test loss is 9.835377138721495e-05\n",
      "Batch: 7800,train loss is: 5.9337587501313255e-05\n",
      "test loss is 0.00010061913080045718\n",
      "Batch: 7900,train loss is: 4.457459147534723e-05\n",
      "test loss is 9.874797964283507e-05\n",
      "Batch: 8000,train loss is: 3.932637918715362e-05\n",
      "test loss is 0.00011252045934843818\n",
      "Batch: 8100,train loss is: 0.00013361379513354812\n",
      "test loss is 0.00010386511696161752\n",
      "Batch: 8200,train loss is: 0.00010288406337957469\n",
      "test loss is 9.946571413351171e-05\n",
      "Batch: 8300,train loss is: 6.333894212840972e-05\n",
      "test loss is 0.00010595074436788912\n",
      "Batch: 8400,train loss is: 0.00012291600169896238\n",
      "test loss is 0.00011302297279345107\n",
      "Batch: 8500,train loss is: 0.0001871714017775531\n",
      "test loss is 0.00011119441001179248\n",
      "Batch: 8600,train loss is: 9.430719321068918e-05\n",
      "test loss is 0.0001233912846649498\n",
      "Batch: 8700,train loss is: 8.873609468338808e-05\n",
      "test loss is 0.00010579334748463263\n",
      "Batch: 8800,train loss is: 8.287723094036123e-05\n",
      "test loss is 0.00010158936855560801\n",
      "Batch: 8900,train loss is: 8.115695905405802e-05\n",
      "test loss is 9.840159470547482e-05\n",
      "Batch: 9000,train loss is: 6.215121769928877e-05\n",
      "test loss is 9.994335242819188e-05\n",
      "Batch: 9100,train loss is: 8.195730921131262e-05\n",
      "test loss is 0.00010670548073309208\n",
      "Batch: 9200,train loss is: 0.00010915891792254331\n",
      "test loss is 0.00010510689914202209\n",
      "Batch: 9300,train loss is: 0.00011992381079489578\n",
      "test loss is 0.00010150886509071905\n",
      "Batch: 9400,train loss is: 9.455336012858598e-05\n",
      "test loss is 0.00010486535944723107\n",
      "Batch: 9500,train loss is: 6.118404547400427e-05\n",
      "test loss is 0.00010718141971921646\n",
      "Batch: 9600,train loss is: 6.95321857851127e-05\n",
      "test loss is 0.00010094295649447802\n",
      "Batch: 9700,train loss is: 7.49175394241367e-05\n",
      "test loss is 0.00010230839677763444\n",
      "Batch: 9800,train loss is: 5.5780316612570096e-05\n",
      "test loss is 9.905643889605127e-05\n",
      "Batch: 9900,train loss is: 5.859673139388908e-05\n",
      "test loss is 9.742461853680377e-05\n",
      "Batch: 10000,train loss is: 5.823060459918765e-05\n",
      "test loss is 0.00010763454281420376\n",
      "Batch: 10100,train loss is: 0.0001849476904194986\n",
      "test loss is 0.00010352070949002553\n",
      "Batch: 10200,train loss is: 0.00024684536524517903\n",
      "test loss is 0.0001024702687260472\n",
      "Batch: 10300,train loss is: 8.182241878293582e-05\n",
      "test loss is 0.00010077951425068378\n",
      "Batch: 10400,train loss is: 4.2896722190441425e-05\n",
      "test loss is 0.00010701148898671357\n",
      "Batch: 10500,train loss is: 9.232970040824132e-05\n",
      "test loss is 0.00010224836500770473\n",
      "Batch: 10600,train loss is: 6.879968866418025e-05\n",
      "test loss is 0.00010204387861327999\n",
      "Batch: 10700,train loss is: 6.80288736332548e-05\n",
      "test loss is 9.806659064390358e-05\n",
      "Batch: 10800,train loss is: 8.763497124495915e-05\n",
      "test loss is 0.00010475308680580354\n",
      "Batch: 10900,train loss is: 8.779495586848259e-05\n",
      "test loss is 0.00010015466465462956\n",
      "Batch: 11000,train loss is: 0.00011028100937392863\n",
      "test loss is 9.772915555232285e-05\n",
      "Batch: 11100,train loss is: 0.00011658139648325512\n",
      "test loss is 0.0001014072191363632\n",
      "Batch: 11200,train loss is: 0.000118846363410156\n",
      "test loss is 9.8339560179337e-05\n",
      "Batch: 11300,train loss is: 6.334524603244414e-05\n",
      "test loss is 0.00010539115425836594\n",
      "Batch: 11400,train loss is: 0.00017525829116268697\n",
      "test loss is 9.966369434788326e-05\n",
      "Batch: 11500,train loss is: 6.144214759146327e-05\n",
      "test loss is 0.00010913609326637541\n",
      "Batch: 11600,train loss is: 0.00010476027412390595\n",
      "test loss is 0.00010015183466112712\n",
      "Batch: 11700,train loss is: 6.509468508457006e-05\n",
      "test loss is 9.816000580686403e-05\n",
      "Batch: 11800,train loss is: 8.836700714808615e-05\n",
      "test loss is 9.970728355515008e-05\n",
      "Batch: 11900,train loss is: 0.00010390930975611789\n",
      "test loss is 0.00010982125900998692\n",
      "Batch: 12000,train loss is: 8.430761738658782e-05\n",
      "test loss is 0.00010259812112309094\n",
      "Batch: 12100,train loss is: 0.00011729800100459407\n",
      "test loss is 9.94077005501432e-05\n",
      "Batch: 12200,train loss is: 6.211358074754716e-05\n",
      "test loss is 0.00010134270763038893\n",
      "Batch: 12300,train loss is: 6.479650533757844e-05\n",
      "test loss is 0.00010300451507567565\n",
      "Batch: 12400,train loss is: 7.272767415053005e-05\n",
      "test loss is 0.00010032393929291113\n",
      "Batch: 12500,train loss is: 0.0001556633768834517\n",
      "test loss is 0.00010165583819473029\n",
      "Batch: 12600,train loss is: 5.694535288584165e-05\n",
      "test loss is 0.00010180628466445593\n",
      "Batch: 12700,train loss is: 7.138715317633342e-05\n",
      "test loss is 0.0001011360439173309\n",
      "Batch: 12800,train loss is: 4.482134724398885e-05\n",
      "test loss is 0.00010019727751329285\n",
      "Batch: 12900,train loss is: 0.00012940502860578313\n",
      "test loss is 9.932120818227051e-05\n",
      "Batch: 13000,train loss is: 8.488592660408099e-05\n",
      "test loss is 9.977566008029123e-05\n",
      "Batch: 13100,train loss is: 0.00017407704077914183\n",
      "test loss is 9.823456675463993e-05\n",
      "Batch: 13200,train loss is: 9.516323969349197e-05\n",
      "test loss is 0.00011233577603412103\n",
      "Batch: 13300,train loss is: 9.857163116005616e-05\n",
      "test loss is 0.0001299855634217447\n",
      "Batch: 13400,train loss is: 7.604940654695305e-05\n",
      "test loss is 9.868526967370396e-05\n",
      "Batch: 13500,train loss is: 5.676950016533671e-05\n",
      "test loss is 0.00010249491347367222\n",
      "Batch: 13600,train loss is: 6.500674194216345e-05\n",
      "test loss is 0.00011337213353593264\n",
      "Batch: 13700,train loss is: 9.845679051615475e-05\n",
      "test loss is 0.00010081913628786988\n",
      "Batch: 13800,train loss is: 6.13078574124871e-05\n",
      "test loss is 0.00010795337808998928\n",
      "Batch: 13900,train loss is: 4.677785205495621e-05\n",
      "test loss is 0.00010156603531831405\n",
      "Batch: 14000,train loss is: 9.508662449692983e-05\n",
      "test loss is 9.986047781868961e-05\n",
      "Batch: 14100,train loss is: 6.369180616241982e-05\n",
      "test loss is 0.00010147168586858045\n",
      "Batch: 14200,train loss is: 6.362303760639212e-05\n",
      "test loss is 0.00010035800080185712\n",
      "Batch: 14300,train loss is: 8.226637435573101e-05\n",
      "test loss is 0.00010198675465292099\n",
      "Batch: 14400,train loss is: 0.00014264857124279508\n",
      "test loss is 9.917741874362059e-05\n",
      "Batch: 14500,train loss is: 6.639634253784007e-05\n",
      "test loss is 9.982219375399815e-05\n",
      "Batch: 14600,train loss is: 0.00011401996055614406\n",
      "test loss is 0.00010805825833763885\n",
      "Batch: 14700,train loss is: 9.742725989584088e-05\n",
      "test loss is 0.00010172092209464102\n",
      "Batch: 14800,train loss is: 0.00015580491602309313\n",
      "test loss is 0.00010471981202704775\n",
      "Batch: 14900,train loss is: 7.508201308691519e-05\n",
      "test loss is 9.941407560984231e-05\n",
      "Batch: 15000,train loss is: 0.00010938028295770043\n",
      "test loss is 0.00010206333129791587\n",
      "Batch: 15100,train loss is: 0.00012039290781840344\n",
      "test loss is 0.0001069575767392939\n",
      "Batch: 15200,train loss is: 0.00011906094335080778\n",
      "test loss is 0.00010677564274368145\n",
      "Batch: 15300,train loss is: 0.00011745970193007883\n",
      "test loss is 9.902770859432029e-05\n",
      "Batch: 15400,train loss is: 5.838606694291428e-05\n",
      "test loss is 9.949731274916789e-05\n",
      "Batch: 15500,train loss is: 7.337410019619004e-05\n",
      "test loss is 0.0001026110814129659\n",
      "Batch: 15600,train loss is: 0.00015205694474926246\n",
      "test loss is 0.00010289791582761421\n",
      "Batch: 15700,train loss is: 7.428932224935955e-05\n",
      "test loss is 0.0001053403121627351\n",
      "Batch: 15800,train loss is: 7.633370525196988e-05\n",
      "test loss is 0.00010766032270612014\n",
      "Batch: 15900,train loss is: 0.00011056412660389126\n",
      "test loss is 0.00010533272814643662\n",
      "Batch: 16000,train loss is: 0.00012850742201665183\n",
      "test loss is 0.00010258979456116852\n",
      "Batch: 16100,train loss is: 0.0001671070274789676\n",
      "test loss is 9.991252274174327e-05\n",
      "Batch: 16200,train loss is: 6.332875620079031e-05\n",
      "test loss is 0.00010465010346119291\n",
      "Batch: 16300,train loss is: 0.00010010947038938192\n",
      "test loss is 0.00010858467331920393\n",
      "Batch: 16400,train loss is: 6.0380840185494766e-05\n",
      "test loss is 0.00010595260561518367\n",
      "Batch: 16500,train loss is: 6.588197564429199e-05\n",
      "test loss is 0.00010011438653354098\n",
      "Batch: 16600,train loss is: 6.2012405593266e-05\n",
      "test loss is 9.891220735509467e-05\n",
      "Batch: 16700,train loss is: 0.00015932859713768\n",
      "test loss is 0.00010428266023734711\n",
      "Batch: 16800,train loss is: 0.00013911987210741392\n",
      "test loss is 0.00011631343873747948\n",
      "Batch: 16900,train loss is: 7.219962901524263e-05\n",
      "test loss is 0.0001068968577828662\n",
      "Batch: 17000,train loss is: 0.00010572202712809823\n",
      "test loss is 0.00011408049199434996\n",
      "Batch: 17100,train loss is: 0.0001025250032791596\n",
      "test loss is 0.00010344216213925773\n",
      "Batch: 17200,train loss is: 5.766828540191498e-05\n",
      "test loss is 0.00010202563124165184\n",
      "Batch: 17300,train loss is: 6.729078447445321e-05\n",
      "test loss is 0.00010600475418688681\n",
      "Batch: 17400,train loss is: 4.5935258645058224e-05\n",
      "test loss is 0.00011175747947387401\n",
      "Batch: 17500,train loss is: 0.00010396481893124035\n",
      "test loss is 9.808821188985949e-05\n",
      "Batch: 17600,train loss is: 5.550589114435473e-05\n",
      "test loss is 0.00010262111582149055\n",
      "Batch: 17700,train loss is: 4.817611779673613e-05\n",
      "test loss is 9.938453634457004e-05\n",
      "Batch: 17800,train loss is: 0.0002137314783894544\n",
      "test loss is 0.00010209980074687037\n",
      "Batch: 17900,train loss is: 4.0722260258180676e-05\n",
      "test loss is 9.841356984809756e-05\n",
      "Batch: 18000,train loss is: 6.865401761823364e-05\n",
      "test loss is 0.0001094271838089778\n",
      "Batch: 18100,train loss is: 0.00015179778309819516\n",
      "test loss is 0.00010273147333994973\n",
      "Batch: 18200,train loss is: 8.934166840292461e-05\n",
      "test loss is 0.00010017547150529777\n",
      "Batch: 18300,train loss is: 0.00010873413315976026\n",
      "test loss is 9.82895342281485e-05\n",
      "Batch: 18400,train loss is: 9.566359503835065e-05\n",
      "test loss is 9.704700762203886e-05\n",
      "Batch: 18500,train loss is: 0.00012327827532034467\n",
      "test loss is 0.00012101777840000722\n",
      "Batch: 18600,train loss is: 7.05902385949961e-05\n",
      "test loss is 0.00012183042817241146\n",
      "Batch: 18700,train loss is: 0.00011946409163011535\n",
      "test loss is 0.00010407182855658323\n",
      "Batch: 18800,train loss is: 8.131548963775612e-05\n",
      "test loss is 0.00010071133589402148\n",
      "Batch: 18900,train loss is: 0.00011830768882495856\n",
      "test loss is 0.00010356117928848952\n",
      "Batch: 19000,train loss is: 4.9758918859974816e-05\n",
      "test loss is 0.0001011938526779494\n",
      "Batch: 19100,train loss is: 0.00012288840326110775\n",
      "test loss is 0.00011422786972244149\n",
      "Batch: 19200,train loss is: 8.373807205006106e-05\n",
      "test loss is 9.814681472155884e-05\n",
      "Batch: 19300,train loss is: 0.00011919294476507198\n",
      "test loss is 0.00010282007357878388\n",
      "Batch: 19400,train loss is: 6.348001059874201e-05\n",
      "test loss is 0.00010105268120735537\n",
      "Batch: 19500,train loss is: 6.0277311733817165e-05\n",
      "test loss is 0.00010910561259084022\n",
      "Batch: 19600,train loss is: 6.901939227188946e-05\n",
      "test loss is 0.00010168747823073276\n",
      "Batch: 19700,train loss is: 0.0001478584803623357\n",
      "test loss is 0.00010048499853088051\n",
      "Batch: 19800,train loss is: 6.260481744877528e-05\n",
      "test loss is 0.00010127604913100895\n",
      "Batch: 19900,train loss is: 6.25167984604033e-05\n",
      "test loss is 0.0001074107266078355\n",
      "Batch: 20000,train loss is: 0.00015235170522043742\n",
      "test loss is 0.00010283362166758558\n",
      "Batch: 20100,train loss is: 0.00010407823290451944\n",
      "test loss is 0.0001137532675308934\n",
      "Batch: 20200,train loss is: 7.33940491652746e-05\n",
      "test loss is 0.00010649618660974676\n",
      "Batch: 20300,train loss is: 4.3014889748189275e-05\n",
      "test loss is 0.00010915550136672926\n",
      "Batch: 20400,train loss is: 9.296231280027014e-05\n",
      "test loss is 0.00011150401108370952\n",
      "Batch: 20500,train loss is: 9.160041565247186e-05\n",
      "test loss is 9.871141193852043e-05\n",
      "Batch: 20600,train loss is: 0.0001757309045650992\n",
      "test loss is 0.00010284525533662958\n",
      "Batch: 20700,train loss is: 0.00010217953368411772\n",
      "test loss is 0.00010614191339341292\n",
      "Batch: 20800,train loss is: 9.25770774960007e-05\n",
      "test loss is 0.00011001859212599237\n",
      "Batch: 20900,train loss is: 7.71488740733764e-05\n",
      "test loss is 0.0001021012340698649\n",
      "Batch: 21000,train loss is: 3.857356172803348e-05\n",
      "test loss is 9.767327206323252e-05\n",
      "Batch: 21100,train loss is: 6.625394307760585e-05\n",
      "test loss is 9.909653794504815e-05\n",
      "Batch: 21200,train loss is: 4.978805769677483e-05\n",
      "test loss is 0.00010136545521661571\n",
      "Batch: 21300,train loss is: 7.684739220811111e-05\n",
      "test loss is 0.00010513766534012095\n",
      "Batch: 21400,train loss is: 0.00010030782200775523\n",
      "test loss is 9.680567014581805e-05\n",
      "Batch: 21500,train loss is: 7.453723495857142e-05\n",
      "test loss is 0.00010014571709005217\n",
      "Batch: 21600,train loss is: 0.00012773345907906368\n",
      "test loss is 0.00011283412944651137\n",
      "Batch: 21700,train loss is: 6.743593434789077e-05\n",
      "test loss is 0.00010565074741288932\n",
      "Batch: 21800,train loss is: 0.00014225645756249161\n",
      "test loss is 9.818906905840303e-05\n",
      "Batch: 21900,train loss is: 6.106725141824753e-05\n",
      "test loss is 0.00010579524735807754\n",
      "Batch: 22000,train loss is: 8.077721291731392e-05\n",
      "test loss is 0.00012002117481632577\n",
      "Batch: 22100,train loss is: 5.752823782128897e-05\n",
      "test loss is 9.75536210952709e-05\n",
      "Batch: 22200,train loss is: 6.059938169793114e-05\n",
      "test loss is 0.00010764260870188664\n",
      "Batch: 22300,train loss is: 8.044653971436738e-05\n",
      "test loss is 0.00011156118562014246\n",
      "Batch: 22400,train loss is: 8.697783228623249e-05\n",
      "test loss is 0.00010428088614529104\n",
      "Batch: 22500,train loss is: 7.404340905520752e-05\n",
      "test loss is 0.00010087378449325658\n",
      "Batch: 22600,train loss is: 8.708502625755533e-05\n",
      "test loss is 0.00010314089211980845\n",
      "Batch: 22700,train loss is: 6.208999545059585e-05\n",
      "test loss is 9.896630524103719e-05\n",
      "Batch: 22800,train loss is: 0.00010794042794922166\n",
      "test loss is 0.00010829553846025081\n",
      "Batch: 22900,train loss is: 4.975778092962875e-05\n",
      "test loss is 0.00010532038715710897\n",
      "Batch: 23000,train loss is: 0.0001305259087669694\n",
      "test loss is 0.00010038110745328413\n",
      "Batch: 23100,train loss is: 7.456280926573802e-05\n",
      "test loss is 9.673580322452461e-05\n",
      "Batch: 23200,train loss is: 6.7794376374413e-05\n",
      "test loss is 9.820266350527867e-05\n",
      "Batch: 23300,train loss is: 4.8998301354395737e-05\n",
      "test loss is 9.86131196688011e-05\n",
      "Batch: 23400,train loss is: 7.357895363984956e-05\n",
      "test loss is 0.00010315258603709105\n",
      "Batch: 23500,train loss is: 0.0001047837249987108\n",
      "test loss is 0.00010183255249755241\n",
      "Batch: 23600,train loss is: 6.448060052459824e-05\n",
      "test loss is 0.0001083578464141993\n",
      "Batch: 23700,train loss is: 0.00013385083410619503\n",
      "test loss is 0.0001018189679890572\n",
      "Batch: 23800,train loss is: 0.00013460656844645482\n",
      "test loss is 0.00010133265072778023\n",
      "Batch: 23900,train loss is: 0.0003053294023778089\n",
      "test loss is 0.00010099722780295652\n",
      "Batch: 24000,train loss is: 7.395127161731254e-05\n",
      "test loss is 0.00011114316419561556\n",
      "Batch: 24100,train loss is: 0.00010057966754907235\n",
      "test loss is 9.857575294503691e-05\n",
      "Batch: 24200,train loss is: 0.0001469572932803508\n",
      "test loss is 0.00010610656350653866\n",
      "Batch: 24300,train loss is: 8.634282848043272e-05\n",
      "test loss is 0.00011287489328385561\n",
      "Batch: 24400,train loss is: 0.00012836999580627536\n",
      "test loss is 0.00010084705032751534\n",
      "Batch: 24500,train loss is: 0.00011514848686927278\n",
      "test loss is 0.00010669733349330774\n",
      "Batch: 24600,train loss is: 9.330584196229853e-05\n",
      "test loss is 0.00011518104437751217\n",
      "Batch: 24700,train loss is: 7.236772958080039e-05\n",
      "test loss is 9.926557031478369e-05\n",
      "Batch: 24800,train loss is: 0.0001291604204880513\n",
      "test loss is 0.00010071715762293347\n",
      "Batch: 24900,train loss is: 4.597074568127976e-05\n",
      "test loss is 9.791011966882757e-05\n",
      "Batch: 25000,train loss is: 8.100747680450048e-05\n",
      "test loss is 0.00010053657647783079\n",
      "Batch: 25100,train loss is: 5.299811937819677e-05\n",
      "test loss is 9.827302429257647e-05\n",
      "Batch: 25200,train loss is: 9.94521440043794e-05\n",
      "test loss is 0.00010201233791752453\n",
      "Batch: 25300,train loss is: 8.321196538659013e-05\n",
      "test loss is 9.778401995671526e-05\n",
      "Batch: 25400,train loss is: 7.784475113023067e-05\n",
      "test loss is 0.00010472159348682341\n",
      "Batch: 25500,train loss is: 6.485513946638855e-05\n",
      "test loss is 9.949156959540012e-05\n",
      "Batch: 25600,train loss is: 0.00010121444167257044\n",
      "test loss is 0.00010573087025223856\n",
      "Batch: 25700,train loss is: 0.00011028884401168946\n",
      "test loss is 9.919657579133542e-05\n",
      "Batch: 25800,train loss is: 7.058496992977595e-05\n",
      "test loss is 0.000100998204090945\n",
      "Batch: 25900,train loss is: 4.318480205035905e-05\n",
      "test loss is 9.937856437925092e-05\n",
      "Batch: 26000,train loss is: 0.00010011191415720648\n",
      "test loss is 0.0001007421804329993\n",
      "Batch: 26100,train loss is: 0.00015549608240021984\n",
      "test loss is 9.787916409579953e-05\n",
      "Batch: 26200,train loss is: 0.0001520920172721681\n",
      "test loss is 0.00010300444953712006\n",
      "Batch: 26300,train loss is: 0.00011724289397847317\n",
      "test loss is 9.865611438202468e-05\n",
      "Batch: 26400,train loss is: 6.764982112103419e-05\n",
      "test loss is 9.678856114193567e-05\n",
      "Batch: 26500,train loss is: 9.985104245733565e-05\n",
      "test loss is 9.808441488206679e-05\n",
      "Batch: 26600,train loss is: 0.00013851403542003364\n",
      "test loss is 9.745097193969361e-05\n",
      "Batch: 26700,train loss is: 5.8474855585689986e-05\n",
      "test loss is 0.00010442950501766573\n",
      "Batch: 26800,train loss is: 0.00014937526446405847\n",
      "test loss is 0.00010032901353244869\n",
      "Batch: 26900,train loss is: 4.593996597556705e-05\n",
      "test loss is 0.00011187538853061336\n",
      "Batch: 27000,train loss is: 8.709501222651153e-05\n",
      "test loss is 0.00010830917957390941\n",
      "Batch: 27100,train loss is: 5.0686454730331686e-05\n",
      "test loss is 0.00011497457878265733\n",
      "Batch: 27200,train loss is: 8.268394656560419e-05\n",
      "test loss is 9.840739656303131e-05\n",
      "Batch: 27300,train loss is: 9.302837790873025e-05\n",
      "test loss is 9.793855717604725e-05\n",
      "Batch: 27400,train loss is: 8.670016166692234e-05\n",
      "test loss is 9.982641650063619e-05\n",
      "Batch: 27500,train loss is: 7.001661508601192e-05\n",
      "test loss is 0.0001101280156479543\n",
      "Batch: 27600,train loss is: 8.767596798047208e-05\n",
      "test loss is 0.00011901296948615034\n",
      "Batch: 27700,train loss is: 0.00027522517553563804\n",
      "test loss is 0.00010982219846146733\n",
      "Batch: 27800,train loss is: 7.668409847345788e-05\n",
      "test loss is 0.00010174832049181014\n",
      "Batch: 27900,train loss is: 0.00012029443836888989\n",
      "test loss is 9.813033492294602e-05\n",
      "Batch: 28000,train loss is: 6.81054236833867e-05\n",
      "test loss is 0.0001081658606945141\n",
      "Batch: 28100,train loss is: 0.00016234323447209358\n",
      "test loss is 0.0001095910338676186\n",
      "Batch: 28200,train loss is: 0.00010778198085345407\n",
      "test loss is 0.00010565439150507421\n",
      "Batch: 28300,train loss is: 0.00027815440765022816\n",
      "test loss is 9.933094365318146e-05\n",
      "Batch: 28400,train loss is: 8.95262591611628e-05\n",
      "test loss is 9.985732595234121e-05\n",
      "Batch: 28500,train loss is: 8.89818856151722e-05\n",
      "test loss is 0.0001060884728213445\n",
      "Batch: 28600,train loss is: 7.040393051895556e-05\n",
      "test loss is 0.00010115258341962345\n",
      "Batch: 28700,train loss is: 8.605351602573862e-05\n",
      "test loss is 9.85069778864797e-05\n",
      "Batch: 28800,train loss is: 0.000155676584860161\n",
      "test loss is 9.702762682950954e-05\n",
      "Batch: 28900,train loss is: 0.00010033814272712766\n",
      "test loss is 0.00010437912163364905\n",
      "Batch: 29000,train loss is: 8.223528384032286e-05\n",
      "test loss is 9.813733322115836e-05\n",
      "Batch: 29100,train loss is: 8.301159505654322e-05\n",
      "test loss is 0.00010579057825218922\n",
      "Batch: 29200,train loss is: 9.757209623455282e-05\n",
      "test loss is 0.00013758961851864998\n",
      "Batch: 29300,train loss is: 9.269015920101019e-05\n",
      "test loss is 0.0001022046745113056\n",
      "Batch: 29400,train loss is: 0.00012243971334406701\n",
      "test loss is 0.00010712343627725476\n",
      "Batch: 29500,train loss is: 5.928935143259491e-05\n",
      "test loss is 9.86844530795454e-05\n",
      "Batch: 29600,train loss is: 0.00018531442416718207\n",
      "test loss is 0.00010696348906913964\n",
      "Batch: 29700,train loss is: 6.209327969164733e-05\n",
      "test loss is 9.717589005200924e-05\n",
      "Batch: 29800,train loss is: 0.00018017817407492296\n",
      "test loss is 9.83418856602528e-05\n",
      "Batch: 29900,train loss is: 0.00023984335239203808\n",
      "test loss is 9.852452845933823e-05\n",
      "Batch: 30000,train loss is: 7.148451578211925e-05\n",
      "test loss is 0.00010058378176974135\n",
      "Batch: 30100,train loss is: 8.185600193775479e-05\n",
      "test loss is 0.00011098700560807954\n",
      "Batch: 30200,train loss is: 6.600675832739948e-05\n",
      "test loss is 9.867442992869879e-05\n",
      "Batch: 30300,train loss is: 8.249827854560693e-05\n",
      "test loss is 9.768930539424012e-05\n",
      "Batch: 30400,train loss is: 7.273963163354474e-05\n",
      "test loss is 0.00010636831432388899\n",
      "Batch: 30500,train loss is: 6.895050399276793e-05\n",
      "test loss is 9.808608702437746e-05\n",
      "Batch: 30600,train loss is: 6.10720942507669e-05\n",
      "test loss is 0.00010003438132486545\n",
      "Batch: 30700,train loss is: 7.396995858084524e-05\n",
      "test loss is 0.00010431680202538605\n",
      "Batch: 30800,train loss is: 6.080945378208044e-05\n",
      "test loss is 0.00010164140491898217\n",
      "Batch: 30900,train loss is: 7.612111427369531e-05\n",
      "test loss is 9.902394672994448e-05\n",
      "Batch: 31000,train loss is: 4.606632711138387e-05\n",
      "test loss is 0.00010215231257389393\n",
      "Batch: 31100,train loss is: 7.935539353534433e-05\n",
      "test loss is 0.0001014362237807555\n",
      "Batch: 31200,train loss is: 7.163738010996589e-05\n",
      "test loss is 0.00010002168344905954\n",
      "Batch: 31300,train loss is: 0.0001181123795644447\n",
      "test loss is 9.79797688716904e-05\n",
      "Batch: 31400,train loss is: 8.417217406769906e-05\n",
      "test loss is 0.00012037638489363472\n",
      "Batch: 31500,train loss is: 5.713060437649822e-05\n",
      "test loss is 0.00010506980155868681\n",
      "Batch: 31600,train loss is: 7.460990650218061e-05\n",
      "test loss is 9.803000448194313e-05\n",
      "Batch: 31700,train loss is: 5.82484085334892e-05\n",
      "test loss is 0.0001020577431634685\n",
      "Batch: 31800,train loss is: 0.00012703340190407567\n",
      "test loss is 9.82139786843178e-05\n",
      "Batch: 31900,train loss is: 0.00010228182255747642\n",
      "test loss is 0.00010479493384607397\n",
      "Batch: 32000,train loss is: 0.00038176538450587376\n",
      "test loss is 0.00010337606414080883\n",
      "Batch: 32100,train loss is: 3.505676592963053e-05\n",
      "test loss is 0.00010567211608061688\n",
      "Batch: 32200,train loss is: 6.419815156273914e-05\n",
      "test loss is 9.841945748044024e-05\n",
      "Batch: 32300,train loss is: 6.527602191688822e-05\n",
      "test loss is 9.990008909943321e-05\n",
      "Batch: 32400,train loss is: 7.252974906297469e-05\n",
      "test loss is 0.0001036957371856118\n",
      "Batch: 32500,train loss is: 0.00011009813658981204\n",
      "test loss is 0.00010291506225231924\n",
      "Batch: 32600,train loss is: 6.48280648995392e-05\n",
      "test loss is 0.00010241339877739083\n",
      "Batch: 32700,train loss is: 5.313742310072938e-05\n",
      "test loss is 9.882190743026817e-05\n",
      "Batch: 32800,train loss is: 7.692642388764744e-05\n",
      "test loss is 0.00011287088801763423\n",
      "Batch: 32900,train loss is: 7.424719185410406e-05\n",
      "test loss is 9.954552307706598e-05\n",
      "Batch: 33000,train loss is: 4.877778755205889e-05\n",
      "test loss is 9.976392090682898e-05\n",
      "Batch: 33100,train loss is: 6.926743843340876e-05\n",
      "test loss is 0.00011194796024022206\n",
      "Batch: 33200,train loss is: 7.838039162595435e-05\n",
      "test loss is 0.00010682115028441184\n",
      "Batch: 33300,train loss is: 7.231049955948726e-05\n",
      "test loss is 0.00010897058162706253\n",
      "Batch: 33400,train loss is: 5.811252681111171e-05\n",
      "test loss is 9.96556661580741e-05\n",
      "Batch: 33500,train loss is: 6.579785883935024e-05\n",
      "test loss is 9.958309960064784e-05\n",
      "Batch: 33600,train loss is: 5.1538558883695996e-05\n",
      "test loss is 0.00010460635917207241\n",
      "Batch: 33700,train loss is: 7.195713458241245e-05\n",
      "test loss is 0.00010909898168982935\n",
      "Batch: 33800,train loss is: 8.863108401377756e-05\n",
      "test loss is 0.0001025579233159585\n",
      "Batch: 33900,train loss is: 7.13736915997729e-05\n",
      "test loss is 0.0001024857143572103\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optim_Adam = torch.optim.Adam(model.parameters(),lr= 0.00001)\n",
    "train_model(loss_MSE,optim_Adam,model,data_loader,train_data,test_data,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'../../Data/YU/rflatBergomi_pointwise88.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "initial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
