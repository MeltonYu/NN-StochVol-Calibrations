{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "f = gzip.GzipFile(r\"../../Data/rBergomiTrainSet.txt.gz\", \"r\")\n",
    "dat=np.load(f)\n",
    "xx=dat[:,:4]\n",
    "yy=dat[:,4:]\n",
    "strikes=np.array([0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4,1.5 ])\n",
    "maturities=np.array([0.1,0.3,0.6,0.9,1.2,1.5,1.8,2.0 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    xx, yy, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_and_expand(a,x,y):\n",
    "    # use choose and where !\n",
    "    n = len(x)*len(y)\n",
    "    a_index = np.arange(len(a))%n\n",
    "    \n",
    "    \n",
    "    x_index = a_index//len(y)\n",
    "    y_index = a_index%len(y)\n",
    "    \n",
    "    x_added = np.choose(x_index,x.reshape(-1,1)).reshape(-1,1)\n",
    "    y_added = np.choose(y_index,y.reshape(-1,1)).reshape(-1,1)\n",
    "    \n",
    "    return np.hstack([a,x_added,y_added])\n",
    "\n",
    "y_train,y_test = y_train.reshape(-1,8,11),y_test.reshape(-1,8,11)\n",
    "x_train,x_test = np.repeat(x_train, 8*11,axis=0),np.repeat(x_test, 8*11,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test=append_and_expand(x_train,maturities,strikes),append_and_expand(x_test,maturities,strikes)\n",
    "y_train,y_test = y_train.reshape(-1).reshape(-1,1), y_test.reshape(-1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "scale_y=  StandardScaler()\n",
    "\n",
    "def ytransform(y_train,y_test):\n",
    "    return [scale_y.fit_transform(y_train),scale_y.transform(y_test)]\n",
    "\n",
    "def yinversetransform(y):\n",
    "    return scale_y.inverse_transform(y)\n",
    "\n",
    "# Upper and lower bounds used in the training set\n",
    "ub=np.array([0.16,4,-0.1,0.5,2.0,1.5])\n",
    "lb=np.array([0.01,0.3,-0.95,0.025,0.1,0.5])\n",
    "\n",
    "def myscale(x):\n",
    "    return (x - (ub+lb)*0.5)*2/(ub-lb)\n",
    "def myinverse(x):\n",
    "    return x*(ub-lb)*0.5+(ub+lb)*0.5\n",
    "\n",
    "x_train_transform = myscale(x_train)\n",
    "x_test_transform = myscale(x_test)\n",
    "[y_train_transform,y_test_transform] = ytransform(y_train,y_test)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"device is {device}\")\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(x_train_transform).to(device=device),\n",
    "                                               torch.from_numpy(y_train_transform).to(device=device))\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.from_numpy(x_test_transform).to(device=device),\n",
    "                                              torch.from_numpy(y_test_transform).to(device=device))\n",
    "\n",
    "\n",
    "train_data = (torch.from_numpy(x_train_transform).to(device=device),torch.from_numpy(y_train_transform).to(device=device))\n",
    "test_data = (torch.from_numpy(x_test_transform).to(device=device),torch.from_numpy(y_test_transform).to(device=device))\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(train_dataset,batch_size =64,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')  # Add the parent directory to the Python path\n",
    "\n",
    "from torch_NN.nn import ResNN_pricing\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "hyperparas = {'input_dim':6,'hidden_dim':32,'hidden_nums':1,'output_dim':1,'block_layer_nums':2}\n",
    "\n",
    "model = ResNN_pricing(hyperparas=hyperparas).to(device=device,dtype=torch.float64)\n",
    "loss_MSE = nn.MSELoss()\n",
    "optim_Adam = torch.optim.Adam(model.parameters(),lr= 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Epoch: 0----------------------------------\n",
      "Batch: 0,train loss is: 1.6513748690296053\n",
      "test loss is 1.1528048996840883\n",
      "Batch: 100,train loss is: 0.3060376832328417\n",
      "test loss is 0.11749520206951293\n",
      "Batch: 200,train loss is: 0.053149343401825797\n",
      "test loss is 0.058098857496964314\n",
      "Batch: 300,train loss is: 0.024291172439173628\n",
      "test loss is 0.041431303530121553\n",
      "Batch: 400,train loss is: 0.02975635642836824\n",
      "test loss is 0.029730337087886454\n",
      "Batch: 500,train loss is: 0.015100169323078835\n",
      "test loss is 0.021997616148664682\n",
      "Batch: 600,train loss is: 0.014547703817935128\n",
      "test loss is 0.01750172555376074\n",
      "Batch: 700,train loss is: 0.013778782157444647\n",
      "test loss is 0.016388299636195473\n",
      "Batch: 800,train loss is: 0.010560610691290415\n",
      "test loss is 0.013724114327087186\n",
      "Batch: 900,train loss is: 0.012058859937922384\n",
      "test loss is 0.012309293934766008\n",
      "Batch: 1000,train loss is: 0.00877761996742453\n",
      "test loss is 0.011296558293648333\n",
      "Batch: 1100,train loss is: 0.012744263946390895\n",
      "test loss is 0.009864436866263752\n",
      "Batch: 1200,train loss is: 0.007398194951180943\n",
      "test loss is 0.009524532418760818\n",
      "Batch: 1300,train loss is: 0.01395328965225921\n",
      "test loss is 0.009858049683488409\n",
      "Batch: 1400,train loss is: 0.006523863765137652\n",
      "test loss is 0.009935533979154972\n",
      "Batch: 1500,train loss is: 0.009986241067740634\n",
      "test loss is 0.00963790812872164\n",
      "Batch: 1600,train loss is: 0.007294982806504297\n",
      "test loss is 0.00792640471116266\n",
      "Batch: 1700,train loss is: 0.017737131824646948\n",
      "test loss is 0.007062769560547058\n",
      "Batch: 1800,train loss is: 0.005023337224470287\n",
      "test loss is 0.007675534776023605\n",
      "Batch: 1900,train loss is: 0.0054808137117895\n",
      "test loss is 0.007236044562191803\n",
      "Batch: 2000,train loss is: 0.00797230634979871\n",
      "test loss is 0.00810829774521166\n",
      "Batch: 2100,train loss is: 0.004974612451032085\n",
      "test loss is 0.006473940402466706\n",
      "Batch: 2200,train loss is: 0.004814476049517291\n",
      "test loss is 0.005796476724441379\n",
      "Batch: 2300,train loss is: 0.0034897256289825093\n",
      "test loss is 0.005700565186242148\n",
      "Batch: 2400,train loss is: 0.009157319868738112\n",
      "test loss is 0.005574331045690117\n",
      "Batch: 2500,train loss is: 0.004463264413351661\n",
      "test loss is 0.005676314651646602\n",
      "Batch: 2600,train loss is: 0.005788473353586015\n",
      "test loss is 0.005523644266631365\n",
      "Batch: 2700,train loss is: 0.004987961475272539\n",
      "test loss is 0.005703967813429599\n",
      "Batch: 2800,train loss is: 0.006939723904088509\n",
      "test loss is 0.007442784056244493\n",
      "Batch: 2900,train loss is: 0.005377504588238264\n",
      "test loss is 0.005287817806336159\n",
      "Batch: 3000,train loss is: 0.0078210869379429\n",
      "test loss is 0.005063423380958012\n",
      "Batch: 3100,train loss is: 0.00607140669327832\n",
      "test loss is 0.004684530232891091\n",
      "Batch: 3200,train loss is: 0.0038366549051738986\n",
      "test loss is 0.005651269123311062\n",
      "Batch: 3300,train loss is: 0.0052766044622006335\n",
      "test loss is 0.005737995790170291\n",
      "Batch: 3400,train loss is: 0.005325405189393555\n",
      "test loss is 0.004108406571991433\n",
      "Batch: 3500,train loss is: 0.004510700459833055\n",
      "test loss is 0.003997747730655276\n",
      "Batch: 3600,train loss is: 0.004962754887694651\n",
      "test loss is 0.005613871690836988\n",
      "Batch: 3700,train loss is: 0.004124013349390854\n",
      "test loss is 0.004470900206936254\n",
      "Batch: 3800,train loss is: 0.005420462052494798\n",
      "test loss is 0.004187662439408953\n",
      "Batch: 3900,train loss is: 0.0030980976842118887\n",
      "test loss is 0.003794220202282284\n",
      "Batch: 4000,train loss is: 0.003105972535552284\n",
      "test loss is 0.004245103601691984\n",
      "Batch: 4100,train loss is: 0.0068257281706446266\n",
      "test loss is 0.008967191048275515\n",
      "Batch: 4200,train loss is: 0.004842796223153206\n",
      "test loss is 0.004776032884531553\n",
      "Batch: 4300,train loss is: 0.005420795800126767\n",
      "test loss is 0.0038889402971490575\n",
      "Batch: 4400,train loss is: 0.00385042102076179\n",
      "test loss is 0.005567612285417746\n",
      "Batch: 4500,train loss is: 0.003370802152193264\n",
      "test loss is 0.0038415465671984153\n",
      "Batch: 4600,train loss is: 0.003357502381822353\n",
      "test loss is 0.004134576148329676\n",
      "Batch: 4700,train loss is: 0.002709240449310658\n",
      "test loss is 0.004835632267534613\n",
      "Batch: 4800,train loss is: 0.003909144994597328\n",
      "test loss is 0.003907684192796986\n",
      "Batch: 4900,train loss is: 0.0024411675294079365\n",
      "test loss is 0.003403017266628927\n",
      "Batch: 5000,train loss is: 0.0022973518586322773\n",
      "test loss is 0.004875742363474623\n",
      "Batch: 5100,train loss is: 0.0023737611507555767\n",
      "test loss is 0.004284469003595036\n",
      "Batch: 5200,train loss is: 0.006552117458935108\n",
      "test loss is 0.006545604652383075\n",
      "Batch: 5300,train loss is: 0.003283648135930617\n",
      "test loss is 0.0043157449635680915\n",
      "Batch: 5400,train loss is: 0.008522836800262392\n",
      "test loss is 0.005056283054446399\n",
      "Batch: 5500,train loss is: 0.004417286063346938\n",
      "test loss is 0.004047517404481379\n",
      "Batch: 5600,train loss is: 0.002912470776012537\n",
      "test loss is 0.003906250075968459\n",
      "Batch: 5700,train loss is: 0.0018900281962835532\n",
      "test loss is 0.003653408456564951\n",
      "Batch: 5800,train loss is: 0.005043635841251646\n",
      "test loss is 0.0045180319005603264\n",
      "Batch: 5900,train loss is: 0.0026064601137409035\n",
      "test loss is 0.003932737489819947\n",
      "Batch: 6000,train loss is: 0.0021102202710760517\n",
      "test loss is 0.0035038802598916104\n",
      "Batch: 6100,train loss is: 0.0034386162886317663\n",
      "test loss is 0.005336172090213551\n",
      "Batch: 6200,train loss is: 0.0029991706011917585\n",
      "test loss is 0.002883957569244591\n",
      "Batch: 6300,train loss is: 0.006745634580957291\n",
      "test loss is 0.00470940445403472\n",
      "Batch: 6400,train loss is: 0.004716933515287816\n",
      "test loss is 0.004185062689130696\n",
      "Batch: 6500,train loss is: 0.002019540542611147\n",
      "test loss is 0.003171489330689747\n",
      "Batch: 6600,train loss is: 0.00476007674105976\n",
      "test loss is 0.004084047641676527\n",
      "Batch: 6700,train loss is: 0.00454822723693466\n",
      "test loss is 0.0038429749527438208\n",
      "Batch: 6800,train loss is: 0.00361700383843498\n",
      "test loss is 0.0035194487376390988\n",
      "Batch: 6900,train loss is: 0.0020773951618499624\n",
      "test loss is 0.002409919523746165\n",
      "Batch: 7000,train loss is: 0.002893845631050818\n",
      "test loss is 0.002602712795777733\n",
      "Batch: 7100,train loss is: 0.0027345086321506154\n",
      "test loss is 0.0028257357492578257\n",
      "Batch: 7200,train loss is: 0.0029239601389375158\n",
      "test loss is 0.0032863787012568767\n",
      "Batch: 7300,train loss is: 0.0041793622586916955\n",
      "test loss is 0.0037405505378093084\n",
      "Batch: 7400,train loss is: 0.001969381985649721\n",
      "test loss is 0.003259296097226919\n",
      "Batch: 7500,train loss is: 0.002136605013048522\n",
      "test loss is 0.0025884138521186377\n",
      "Batch: 7600,train loss is: 0.002012664671098594\n",
      "test loss is 0.003684730288114541\n",
      "Batch: 7700,train loss is: 0.0018225346786687733\n",
      "test loss is 0.0037603828955278303\n",
      "Batch: 7800,train loss is: 0.003709468614023264\n",
      "test loss is 0.003888645494825285\n",
      "Batch: 7900,train loss is: 0.0017430640692987776\n",
      "test loss is 0.003239094499459442\n",
      "Batch: 8000,train loss is: 0.003107280878306229\n",
      "test loss is 0.0024746499865879778\n",
      "Batch: 8100,train loss is: 0.0030599724555583742\n",
      "test loss is 0.002729371659676478\n",
      "Batch: 8200,train loss is: 0.001737896756101133\n",
      "test loss is 0.0034828158801603797\n",
      "Batch: 8300,train loss is: 0.0024769043936214334\n",
      "test loss is 0.002450542059042602\n",
      "Batch: 8400,train loss is: 0.0029260419375116103\n",
      "test loss is 0.003740183128237169\n",
      "Batch: 8500,train loss is: 0.003902600813022345\n",
      "test loss is 0.002945935034936277\n",
      "Batch: 8600,train loss is: 0.0033334083341424064\n",
      "test loss is 0.002622667300257878\n",
      "Batch: 8700,train loss is: 0.0028236288901914244\n",
      "test loss is 0.003380679859445396\n",
      "Batch: 8800,train loss is: 0.0040654532891071845\n",
      "test loss is 0.00363364252284545\n",
      "Batch: 8900,train loss is: 0.0025526776573933383\n",
      "test loss is 0.0024699120386001074\n",
      "Batch: 9000,train loss is: 0.002399991530369464\n",
      "test loss is 0.00266196444694109\n",
      "Batch: 9100,train loss is: 0.00197501336090158\n",
      "test loss is 0.0025982607959108657\n",
      "Batch: 9200,train loss is: 0.007760748910542402\n",
      "test loss is 0.002980760747497461\n",
      "Batch: 9300,train loss is: 0.003663325109395689\n",
      "test loss is 0.0022163265563295907\n",
      "Batch: 9400,train loss is: 0.004644713623909336\n",
      "test loss is 0.0026373620516424827\n",
      "Batch: 9500,train loss is: 0.0022906944483150845\n",
      "test loss is 0.0038748493933062084\n",
      "Batch: 9600,train loss is: 0.004560823739392794\n",
      "test loss is 0.002497383950967692\n",
      "Batch: 9700,train loss is: 0.003665945216085962\n",
      "test loss is 0.002702856093784731\n",
      "Batch: 9800,train loss is: 0.0012880252780462134\n",
      "test loss is 0.0026613573299088082\n",
      "Batch: 9900,train loss is: 0.0025532325456963227\n",
      "test loss is 0.002799174751848249\n",
      "Batch: 10000,train loss is: 0.002517884590030748\n",
      "test loss is 0.0021495789122288607\n",
      "Batch: 10100,train loss is: 0.0033897833176840244\n",
      "test loss is 0.002036116967898522\n",
      "Batch: 10200,train loss is: 0.0021096527863046534\n",
      "test loss is 0.0022343922004939305\n",
      "Batch: 10300,train loss is: 0.0021014258598065216\n",
      "test loss is 0.0027151970205944874\n",
      "Batch: 10400,train loss is: 0.006038720842213491\n",
      "test loss is 0.004535670815454336\n",
      "Batch: 10500,train loss is: 0.0021688641922819785\n",
      "test loss is 0.0023970552722926293\n",
      "Batch: 10600,train loss is: 0.0036278861288690733\n",
      "test loss is 0.0023926615544570496\n",
      "Batch: 10700,train loss is: 0.0025667567652561704\n",
      "test loss is 0.0021181550930792076\n",
      "Batch: 10800,train loss is: 0.0022191722381483146\n",
      "test loss is 0.003034817501320019\n",
      "Batch: 10900,train loss is: 0.0013960054499665515\n",
      "test loss is 0.002229213403771872\n",
      "Batch: 11000,train loss is: 0.002363569371883824\n",
      "test loss is 0.002570540387103002\n",
      "Batch: 11100,train loss is: 0.002490830357290668\n",
      "test loss is 0.0023704432604010244\n",
      "Batch: 11200,train loss is: 0.0034717765346223223\n",
      "test loss is 0.002270853390996151\n",
      "Batch: 11300,train loss is: 0.0023776173086429596\n",
      "test loss is 0.003370306753409746\n",
      "Batch: 11400,train loss is: 0.0027223970591823635\n",
      "test loss is 0.0022324339624360274\n",
      "Batch: 11500,train loss is: 0.005200272060788725\n",
      "test loss is 0.002131613553292189\n",
      "Batch: 11600,train loss is: 0.0033751036578304762\n",
      "test loss is 0.002572380716334562\n",
      "Batch: 11700,train loss is: 0.0016889093110798391\n",
      "test loss is 0.0019146332746456302\n",
      "Batch: 11800,train loss is: 0.0019345956713290931\n",
      "test loss is 0.0020903167268832737\n",
      "Batch: 11900,train loss is: 0.005345860074090528\n",
      "test loss is 0.0039022875238449435\n",
      "Batch: 12000,train loss is: 0.0018444409885654423\n",
      "test loss is 0.0025838192402600295\n",
      "Batch: 12100,train loss is: 0.002025727459098829\n",
      "test loss is 0.0022462092551165363\n",
      "Batch: 12200,train loss is: 0.0025527801839302046\n",
      "test loss is 0.002634293236737494\n",
      "Batch: 12300,train loss is: 0.0020434078673367264\n",
      "test loss is 0.002112932885275211\n",
      "Batch: 12400,train loss is: 0.002766509785805972\n",
      "test loss is 0.0018465605125182827\n",
      "Batch: 12500,train loss is: 0.006898213233819515\n",
      "test loss is 0.005711766316982012\n",
      "Batch: 12600,train loss is: 0.0015901959531176657\n",
      "test loss is 0.0017628395505854969\n",
      "Batch: 12700,train loss is: 0.0021677520060264226\n",
      "test loss is 0.0018563546865246572\n",
      "Batch: 12800,train loss is: 0.0028960595516714957\n",
      "test loss is 0.002958362002701529\n",
      "Batch: 12900,train loss is: 0.0038280011554364104\n",
      "test loss is 0.00372401601410264\n",
      "Batch: 13000,train loss is: 0.001970769867369153\n",
      "test loss is 0.0021745334148815247\n",
      "Batch: 13100,train loss is: 0.003426097696140913\n",
      "test loss is 0.001962652044696726\n",
      "Batch: 13200,train loss is: 0.003689333924229441\n",
      "test loss is 0.002531837804623627\n",
      "Batch: 13300,train loss is: 0.0018218534748171912\n",
      "test loss is 0.0030426663306063594\n",
      "Batch: 13400,train loss is: 0.002303205159919751\n",
      "test loss is 0.0023591454217353823\n",
      "Batch: 13500,train loss is: 0.0023657939200255203\n",
      "test loss is 0.0029043335114715425\n",
      "Batch: 13600,train loss is: 0.0023756470018996816\n",
      "test loss is 0.00198222489408434\n",
      "Batch: 13700,train loss is: 0.0016980497101859587\n",
      "test loss is 0.002171351430635873\n",
      "Batch: 13800,train loss is: 0.0016848762064345432\n",
      "test loss is 0.001747022919016862\n",
      "Batch: 13900,train loss is: 0.0020705924039504404\n",
      "test loss is 0.002780923536872016\n",
      "Batch: 14000,train loss is: 0.0016829471848161383\n",
      "test loss is 0.0017850048987799049\n",
      "Batch: 14100,train loss is: 0.001877728685658001\n",
      "test loss is 0.0016572747861765436\n",
      "Batch: 14200,train loss is: 0.004152429286934189\n",
      "test loss is 0.002194298120785486\n",
      "Batch: 14300,train loss is: 0.0015586566970838215\n",
      "test loss is 0.0025906120161978373\n",
      "Batch: 14400,train loss is: 0.002139148370395424\n",
      "test loss is 0.002423673089634179\n",
      "Batch: 14500,train loss is: 0.0015641282752692615\n",
      "test loss is 0.0017110683245343378\n",
      "Batch: 14600,train loss is: 0.0015418511464780956\n",
      "test loss is 0.002398165142501237\n",
      "Batch: 14700,train loss is: 0.001931629637695387\n",
      "test loss is 0.0016468382509822069\n",
      "Batch: 14800,train loss is: 0.0015060878707581168\n",
      "test loss is 0.0017440868604482209\n",
      "Batch: 14900,train loss is: 0.0015456486622838837\n",
      "test loss is 0.0018375625403526878\n",
      "Batch: 15000,train loss is: 0.00283284540968718\n",
      "test loss is 0.002142626839741769\n",
      "Batch: 15100,train loss is: 0.004058603709781532\n",
      "test loss is 0.002965479191909412\n",
      "Batch: 15200,train loss is: 0.003113650453113303\n",
      "test loss is 0.0035074463204647433\n",
      "Batch: 15300,train loss is: 0.001112525069681531\n",
      "test loss is 0.0019795121137922552\n",
      "Batch: 15400,train loss is: 0.0012521618906519087\n",
      "test loss is 0.002020756467715458\n",
      "Batch: 15500,train loss is: 0.00150875210809941\n",
      "test loss is 0.0019008693690151133\n",
      "Batch: 15600,train loss is: 0.0011280378596861419\n",
      "test loss is 0.0021946231800134687\n",
      "Batch: 15700,train loss is: 0.0017529132598669925\n",
      "test loss is 0.002048432120070523\n",
      "Batch: 15800,train loss is: 0.0027533083226928988\n",
      "test loss is 0.002487021577589332\n",
      "Batch: 15900,train loss is: 0.0030172997143444627\n",
      "test loss is 0.002061686907568986\n",
      "Batch: 16000,train loss is: 0.0038868034354519313\n",
      "test loss is 0.0017810337276764738\n",
      "Batch: 16100,train loss is: 0.0021081512077702805\n",
      "test loss is 0.001654927974321634\n",
      "Batch: 16200,train loss is: 0.0011533385900284576\n",
      "test loss is 0.0017059345424602405\n",
      "Batch: 16300,train loss is: 0.002187151095068775\n",
      "test loss is 0.0023749997252526453\n",
      "Batch: 16400,train loss is: 0.0015924669852437855\n",
      "test loss is 0.0017936529029285975\n",
      "Batch: 16500,train loss is: 0.0023984967923698134\n",
      "test loss is 0.0035062722939789926\n",
      "Batch: 16600,train loss is: 0.0019313077232292332\n",
      "test loss is 0.0029132230544201823\n",
      "Batch: 16700,train loss is: 0.0011960919687690513\n",
      "test loss is 0.0015543018297862745\n",
      "Batch: 16800,train loss is: 0.0013144068382851828\n",
      "test loss is 0.0017997058094048458\n",
      "Batch: 16900,train loss is: 0.0024853655423930808\n",
      "test loss is 0.0017950193344358056\n",
      "Batch: 17000,train loss is: 0.0011693929484370288\n",
      "test loss is 0.0016827751879211358\n",
      "Batch: 17100,train loss is: 0.0012678367909165825\n",
      "test loss is 0.0016692431967251903\n",
      "Batch: 17200,train loss is: 0.003501168688701842\n",
      "test loss is 0.004113630408863057\n",
      "Batch: 17300,train loss is: 0.0019785695569455126\n",
      "test loss is 0.001552614219704535\n",
      "Batch: 17400,train loss is: 0.0017488637085155048\n",
      "test loss is 0.0018489939416118887\n",
      "Batch: 17500,train loss is: 0.0016968884048059922\n",
      "test loss is 0.0025642564556169105\n",
      "Batch: 17600,train loss is: 0.0020082783178951066\n",
      "test loss is 0.002363557979230001\n",
      "Batch: 17700,train loss is: 0.0018594464543545237\n",
      "test loss is 0.001995809574106469\n",
      "Batch: 17800,train loss is: 0.002205019622406418\n",
      "test loss is 0.0017455307322727274\n",
      "Batch: 17900,train loss is: 0.0016314520887437416\n",
      "test loss is 0.0024555251862589315\n",
      "Batch: 18000,train loss is: 0.002102395248968615\n",
      "test loss is 0.0018851008566078615\n",
      "Batch: 18100,train loss is: 0.004933652181787511\n",
      "test loss is 0.0037670932280677383\n",
      "Batch: 18200,train loss is: 0.001436044817153623\n",
      "test loss is 0.0017125451408562071\n",
      "Batch: 18300,train loss is: 0.0012276196179354586\n",
      "test loss is 0.0019972307672197604\n",
      "Batch: 18400,train loss is: 0.0021652372054862725\n",
      "test loss is 0.002158868421336589\n",
      "Batch: 18500,train loss is: 0.0016332276165604322\n",
      "test loss is 0.0028757823195574464\n",
      "Batch: 18600,train loss is: 0.0014785709995409739\n",
      "test loss is 0.0018724508556118046\n",
      "Batch: 18700,train loss is: 0.0011779088990385759\n",
      "test loss is 0.0016123895922342505\n",
      "Batch: 18800,train loss is: 0.0020080842057784093\n",
      "test loss is 0.002000446219002885\n",
      "Batch: 18900,train loss is: 0.0017172246720606496\n",
      "test loss is 0.002053134720095458\n",
      "Batch: 19000,train loss is: 0.0069323423981581\n",
      "test loss is 0.003611154705168131\n",
      "Batch: 19100,train loss is: 0.0014030112444833528\n",
      "test loss is 0.0015261822331303725\n",
      "Batch: 19200,train loss is: 0.0019151076797132634\n",
      "test loss is 0.0014702909946401664\n",
      "Batch: 19300,train loss is: 0.0020161214565873387\n",
      "test loss is 0.0016410120837405963\n",
      "Batch: 19400,train loss is: 0.0015244110756816422\n",
      "test loss is 0.001410049354725902\n",
      "Batch: 19500,train loss is: 0.001224152427361765\n",
      "test loss is 0.0014340650093005833\n",
      "Batch: 19600,train loss is: 0.0018395384056915562\n",
      "test loss is 0.0016879935138468787\n",
      "Batch: 19700,train loss is: 0.0010469859336461088\n",
      "test loss is 0.001858426240608919\n",
      "Batch: 19800,train loss is: 0.0006849962315248399\n",
      "test loss is 0.0014217288942690025\n",
      "Batch: 19900,train loss is: 0.0024272705670714207\n",
      "test loss is 0.0017500457493207636\n",
      "Batch: 20000,train loss is: 0.0013698084549372356\n",
      "test loss is 0.0017564140555533722\n",
      "Batch: 20100,train loss is: 0.0024070962328841595\n",
      "test loss is 0.0019257846904398134\n",
      "Batch: 20200,train loss is: 0.0011648622294779562\n",
      "test loss is 0.0014059309082898057\n",
      "Batch: 20300,train loss is: 0.0027876910392285585\n",
      "test loss is 0.0020339028724977535\n",
      "Batch: 20400,train loss is: 0.001963354644581534\n",
      "test loss is 0.001530855757096521\n",
      "Batch: 20500,train loss is: 0.0012342346999310268\n",
      "test loss is 0.0018625003124312462\n",
      "Batch: 20600,train loss is: 0.001735674267749058\n",
      "test loss is 0.0018671444322954647\n",
      "Batch: 20700,train loss is: 0.0032677628992982834\n",
      "test loss is 0.0027253201973248013\n",
      "Batch: 20800,train loss is: 0.001553459357170016\n",
      "test loss is 0.001584662338956481\n",
      "Batch: 20900,train loss is: 0.0012694779190649767\n",
      "test loss is 0.0016901631830790363\n",
      "Batch: 21000,train loss is: 0.0019078754321113533\n",
      "test loss is 0.0018226965975675388\n",
      "Batch: 21100,train loss is: 0.0014413203061043745\n",
      "test loss is 0.00221284109126218\n",
      "Batch: 21200,train loss is: 0.001994495717043971\n",
      "test loss is 0.0016426099685567042\n",
      "Batch: 21300,train loss is: 0.004374640220947957\n",
      "test loss is 0.003518006559026429\n",
      "Batch: 21400,train loss is: 0.0017459035125401725\n",
      "test loss is 0.002419750870974875\n",
      "Batch: 21500,train loss is: 0.0009520520389685649\n",
      "test loss is 0.0012923191039570128\n",
      "Batch: 21600,train loss is: 0.0015030168090525873\n",
      "test loss is 0.001661572294426953\n",
      "Batch: 21700,train loss is: 0.0021585188414038406\n",
      "test loss is 0.0017546589046790386\n",
      "Batch: 21800,train loss is: 0.0012670093804137334\n",
      "test loss is 0.0016004151591707508\n",
      "Batch: 21900,train loss is: 0.0008218092440378339\n",
      "test loss is 0.0013970024526561405\n",
      "Batch: 22000,train loss is: 0.001976230695121331\n",
      "test loss is 0.0017810525927990997\n",
      "Batch: 22100,train loss is: 0.0012642461550541296\n",
      "test loss is 0.0015317015221609567\n",
      "Batch: 22200,train loss is: 0.0015713140821364121\n",
      "test loss is 0.001548500576227714\n",
      "Batch: 22300,train loss is: 0.0018770839436455392\n",
      "test loss is 0.0017313185814924576\n",
      "Batch: 22400,train loss is: 0.0013370257804943696\n",
      "test loss is 0.001667490058511718\n",
      "Batch: 22500,train loss is: 0.001895541502631692\n",
      "test loss is 0.0016411177903585196\n",
      "Batch: 22600,train loss is: 0.00164044840837022\n",
      "test loss is 0.0014580481678143783\n",
      "Batch: 22700,train loss is: 0.002093407185118434\n",
      "test loss is 0.0021668467485913318\n",
      "Batch: 22800,train loss is: 0.0020549908838329973\n",
      "test loss is 0.0017365194154434103\n",
      "Batch: 22900,train loss is: 0.0011449694440532128\n",
      "test loss is 0.001358388437717468\n",
      "Batch: 23000,train loss is: 0.006141170306095794\n",
      "test loss is 0.003830174581133858\n",
      "Batch: 23100,train loss is: 0.0019006542491225493\n",
      "test loss is 0.001789321515040786\n",
      "Batch: 23200,train loss is: 0.0013259863706935548\n",
      "test loss is 0.002576661170981136\n",
      "Batch: 23300,train loss is: 0.0013754650166778962\n",
      "test loss is 0.00224085197683305\n",
      "Batch: 23400,train loss is: 0.0011739112321479434\n",
      "test loss is 0.0011448814301844314\n",
      "Batch: 23500,train loss is: 0.0020486425728529324\n",
      "test loss is 0.002410795145685095\n",
      "Batch: 23600,train loss is: 0.0014679517174071052\n",
      "test loss is 0.00177686530275647\n",
      "Batch: 23700,train loss is: 0.0018754452280554412\n",
      "test loss is 0.00190576405677124\n",
      "Batch: 23800,train loss is: 0.0011928478449656328\n",
      "test loss is 0.0015632547216901714\n",
      "Batch: 23900,train loss is: 0.0015292166544588661\n",
      "test loss is 0.0017495886490081947\n",
      "Batch: 24000,train loss is: 0.0010672048108086001\n",
      "test loss is 0.0015509378756032326\n",
      "Batch: 24100,train loss is: 0.0012952623026013096\n",
      "test loss is 0.001397515863147709\n",
      "Batch: 24200,train loss is: 0.00132525029340534\n",
      "test loss is 0.0014194005307196062\n",
      "Batch: 24300,train loss is: 0.0012752737035213514\n",
      "test loss is 0.0028903414326792265\n",
      "Batch: 24400,train loss is: 0.0015944385658679644\n",
      "test loss is 0.0014262767947013902\n",
      "Batch: 24500,train loss is: 0.0012987694040788198\n",
      "test loss is 0.0014261277882059499\n",
      "Batch: 24600,train loss is: 0.0006883110615173308\n",
      "test loss is 0.001474906748690632\n",
      "Batch: 24700,train loss is: 0.005520667382460569\n",
      "test loss is 0.0021430195484916483\n",
      "Batch: 24800,train loss is: 0.0011882609956086684\n",
      "test loss is 0.002007157557104338\n",
      "Batch: 24900,train loss is: 0.0015528882102347678\n",
      "test loss is 0.002425827171140841\n",
      "Batch: 25000,train loss is: 0.0015289860186064417\n",
      "test loss is 0.0013941918231035208\n",
      "Batch: 25100,train loss is: 0.0010807368352638382\n",
      "test loss is 0.0013001815933653012\n",
      "Batch: 25200,train loss is: 0.0016541234001962426\n",
      "test loss is 0.001401412699167148\n",
      "Batch: 25300,train loss is: 0.0010563455892810083\n",
      "test loss is 0.0012616797506483032\n",
      "Batch: 25400,train loss is: 0.001689499424347452\n",
      "test loss is 0.0011463267670744552\n",
      "Batch: 25500,train loss is: 0.0010757610395359743\n",
      "test loss is 0.0013921591481151929\n",
      "Batch: 25600,train loss is: 0.0015364203905414283\n",
      "test loss is 0.0023174187602490117\n",
      "Batch: 25700,train loss is: 0.0014116391816955715\n",
      "test loss is 0.00227570445054803\n",
      "Batch: 25800,train loss is: 0.001391904752841581\n",
      "test loss is 0.0017748608700471694\n",
      "Batch: 25900,train loss is: 0.0012173092061993716\n",
      "test loss is 0.0014672404369259235\n",
      "Batch: 26000,train loss is: 0.0013452352111026908\n",
      "test loss is 0.0014051232886397843\n",
      "Batch: 26100,train loss is: 0.0012401059325426566\n",
      "test loss is 0.001784486469499917\n",
      "Batch: 26200,train loss is: 0.002090155345176633\n",
      "test loss is 0.001636903629352079\n",
      "Batch: 26300,train loss is: 0.0014474694574218435\n",
      "test loss is 0.0018318737575433227\n",
      "Batch: 26400,train loss is: 0.0007814763856284226\n",
      "test loss is 0.0012930506264516271\n",
      "Batch: 26500,train loss is: 0.001284057286223034\n",
      "test loss is 0.001391715956412203\n",
      "Batch: 26600,train loss is: 0.0025553100496241276\n",
      "test loss is 0.002546221883264996\n",
      "Batch: 26700,train loss is: 0.0013840044078689663\n",
      "test loss is 0.0014378794721050176\n",
      "Batch: 26800,train loss is: 0.0025111573529328097\n",
      "test loss is 0.0016092032948944127\n",
      "Batch: 26900,train loss is: 0.001422982510257373\n",
      "test loss is 0.0012243197385641699\n",
      "Batch: 27000,train loss is: 0.004285276817905732\n",
      "test loss is 0.0013400198898928315\n",
      "Batch: 27100,train loss is: 0.0010949223659000307\n",
      "test loss is 0.0015497421077449378\n",
      "Batch: 27200,train loss is: 0.0013197919833397489\n",
      "test loss is 0.001168689551207589\n",
      "Batch: 27300,train loss is: 0.001550964188990084\n",
      "test loss is 0.0017236508765627744\n",
      "Batch: 27400,train loss is: 0.002512984159850684\n",
      "test loss is 0.0011680786699325414\n",
      "Batch: 27500,train loss is: 0.001241536340630233\n",
      "test loss is 0.0015256114213838688\n",
      "Batch: 27600,train loss is: 0.001010885474053856\n",
      "test loss is 0.0012626296110192696\n",
      "Batch: 27700,train loss is: 0.001074276872174133\n",
      "test loss is 0.0017542054612874206\n",
      "Batch: 27800,train loss is: 0.002116078161508776\n",
      "test loss is 0.001179442085338066\n",
      "Batch: 27900,train loss is: 0.0007375285780318623\n",
      "test loss is 0.0013047257150985802\n",
      "Batch: 28000,train loss is: 0.001019765507784897\n",
      "test loss is 0.0013474221171492114\n",
      "Batch: 28100,train loss is: 0.0014063799028337241\n",
      "test loss is 0.0011905952834574077\n",
      "Batch: 28200,train loss is: 0.0033066085998652316\n",
      "test loss is 0.00213559090039436\n",
      "Batch: 28300,train loss is: 0.001672180704926186\n",
      "test loss is 0.0015253509103820744\n",
      "Batch: 28400,train loss is: 0.0011529434421264524\n",
      "test loss is 0.0012563213343044092\n",
      "Batch: 28500,train loss is: 0.0015610501396418615\n",
      "test loss is 0.001340668639064546\n",
      "Batch: 28600,train loss is: 0.0032191244724285636\n",
      "test loss is 0.0020086206478048397\n",
      "Batch: 28700,train loss is: 0.0006821840529138516\n",
      "test loss is 0.0010680813929391986\n",
      "Batch: 28800,train loss is: 0.0018108961485545968\n",
      "test loss is 0.0013364162291303772\n",
      "Batch: 28900,train loss is: 0.0016489599913261169\n",
      "test loss is 0.0013505196850598714\n",
      "Batch: 29000,train loss is: 0.0014987431285015315\n",
      "test loss is 0.0012148144685315602\n",
      "Batch: 29100,train loss is: 0.0023614924245102765\n",
      "test loss is 0.0016274389268302982\n",
      "Batch: 29200,train loss is: 0.0012950696969839404\n",
      "test loss is 0.001518519701759766\n",
      "Batch: 29300,train loss is: 0.0019509778716228557\n",
      "test loss is 0.0013177335573292628\n",
      "Batch: 29400,train loss is: 0.00132110428991753\n",
      "test loss is 0.0012384520036715647\n",
      "Batch: 29500,train loss is: 0.0015040485550178132\n",
      "test loss is 0.0014682458259435132\n",
      "Batch: 29600,train loss is: 0.000564200339419846\n",
      "test loss is 0.001192893237246036\n",
      "Batch: 29700,train loss is: 0.001129157495523505\n",
      "test loss is 0.0012614207227756706\n",
      "Batch: 29800,train loss is: 0.0012187922081509312\n",
      "test loss is 0.0016389421298168203\n",
      "Batch: 29900,train loss is: 0.001149996997136544\n",
      "test loss is 0.001437459705273731\n",
      "Batch: 30000,train loss is: 0.0010911948209725115\n",
      "test loss is 0.0015760221115994756\n",
      "Batch: 30100,train loss is: 0.0016857398176313336\n",
      "test loss is 0.0016059969826217083\n",
      "Batch: 30200,train loss is: 0.0006861767556466706\n",
      "test loss is 0.0014831420335981544\n",
      "Batch: 30300,train loss is: 0.0025216270979900516\n",
      "test loss is 0.0014514025000558156\n",
      "Batch: 30400,train loss is: 0.001105549675400387\n",
      "test loss is 0.0010772630494981612\n",
      "Batch: 30500,train loss is: 0.0012517430543019085\n",
      "test loss is 0.0014164791029820923\n",
      "Batch: 30600,train loss is: 0.001011049476153195\n",
      "test loss is 0.0015305689689931554\n",
      "Batch: 30700,train loss is: 0.0023982212592804923\n",
      "test loss is 0.0013651238500210917\n",
      "Batch: 30800,train loss is: 0.0016359949240425018\n",
      "test loss is 0.0018374380096427792\n",
      "Batch: 30900,train loss is: 0.0022714951188293627\n",
      "test loss is 0.0012297581856288678\n",
      "Batch: 31000,train loss is: 0.0009830077345154078\n",
      "test loss is 0.00129093808575353\n",
      "Batch: 31100,train loss is: 0.0010787834757868852\n",
      "test loss is 0.001229712728684289\n",
      "Batch: 31200,train loss is: 0.0013874589065929347\n",
      "test loss is 0.001340542001308576\n",
      "Batch: 31300,train loss is: 0.0007827952386585588\n",
      "test loss is 0.001218068681437057\n",
      "Batch: 31400,train loss is: 0.0013587717824452982\n",
      "test loss is 0.0012596801223355406\n",
      "Batch: 31500,train loss is: 0.001212852451774099\n",
      "test loss is 0.0018235166095288593\n",
      "Batch: 31600,train loss is: 0.0016698144691619972\n",
      "test loss is 0.0018363894491754983\n",
      "Batch: 31700,train loss is: 0.0008706935129417322\n",
      "test loss is 0.0012990648179304885\n",
      "Batch: 31800,train loss is: 0.0008988209289760921\n",
      "test loss is 0.001212560840655796\n",
      "Batch: 31900,train loss is: 0.001087810889607011\n",
      "test loss is 0.0013379666773420764\n",
      "Batch: 32000,train loss is: 0.0013887760315400544\n",
      "test loss is 0.001272701404975922\n",
      "Batch: 32100,train loss is: 0.0009947196271793627\n",
      "test loss is 0.0011610079620548853\n",
      "Batch: 32200,train loss is: 0.0008732262134651312\n",
      "test loss is 0.0013731747570739842\n",
      "Batch: 32300,train loss is: 0.0007182066996425352\n",
      "test loss is 0.001051049590460489\n",
      "Batch: 32400,train loss is: 0.0015406051316814663\n",
      "test loss is 0.0018670568471783045\n",
      "Batch: 32500,train loss is: 0.0011871282368207127\n",
      "test loss is 0.0009745992901495268\n",
      "Batch: 32600,train loss is: 0.0014317964530168616\n",
      "test loss is 0.0011806078539874085\n",
      "Batch: 32700,train loss is: 0.0007620365401907834\n",
      "test loss is 0.0014183948521154506\n",
      "Batch: 32800,train loss is: 0.002343318504310997\n",
      "test loss is 0.0016397538534992596\n",
      "Batch: 32900,train loss is: 0.0012787035607916214\n",
      "test loss is 0.0012495916004653792\n",
      "Batch: 33000,train loss is: 0.0028052684682469105\n",
      "test loss is 0.0013374438086267882\n",
      "Batch: 33100,train loss is: 0.00145366841654648\n",
      "test loss is 0.0013603228658081294\n",
      "Batch: 33200,train loss is: 0.001880871194826618\n",
      "test loss is 0.001153195273073181\n",
      "Batch: 33300,train loss is: 0.0008679209778460093\n",
      "test loss is 0.0010135830815037588\n",
      "Batch: 33400,train loss is: 0.0008432490565003934\n",
      "test loss is 0.0010739035158472054\n",
      "Batch: 33500,train loss is: 0.0010559451108678674\n",
      "test loss is 0.0012217798411018397\n",
      "Batch: 33600,train loss is: 0.0010266843771059827\n",
      "test loss is 0.0015810820212189543\n",
      "Batch: 33700,train loss is: 0.001413742551943483\n",
      "test loss is 0.001085954657581\n",
      "Batch: 33800,train loss is: 0.0011737895896763348\n",
      "test loss is 0.0017233823431250384\n",
      "Batch: 33900,train loss is: 0.0010339740413003629\n",
      "test loss is 0.0015914398485393833\n",
      "Batch: 34000,train loss is: 0.0009104136984533353\n",
      "test loss is 0.0015910510337450434\n",
      "Batch: 34100,train loss is: 0.0029839803590838336\n",
      "test loss is 0.0014104586079616193\n",
      "Batch: 34200,train loss is: 0.0014300095046911345\n",
      "test loss is 0.0011083451701635243\n",
      "Batch: 34300,train loss is: 0.0044988224564990464\n",
      "test loss is 0.0018936592234220088\n",
      "Batch: 34400,train loss is: 0.0009412503330314671\n",
      "test loss is 0.0012560286477994555\n",
      "Batch: 34500,train loss is: 0.0008786974639286901\n",
      "test loss is 0.0009785444827336211\n",
      "Batch: 34600,train loss is: 0.0012142657520745495\n",
      "test loss is 0.0010714582809944343\n",
      "Batch: 34700,train loss is: 0.0025280768991689804\n",
      "test loss is 0.0011616289641132242\n",
      "Batch: 34800,train loss is: 0.0010061277778826835\n",
      "test loss is 0.001010853929409678\n",
      "Batch: 34900,train loss is: 0.0008862338490795406\n",
      "test loss is 0.0011017801151731639\n",
      "Batch: 35000,train loss is: 0.0010852206481969412\n",
      "test loss is 0.0010887393408136865\n",
      "Batch: 35100,train loss is: 0.0009448339816507778\n",
      "test loss is 0.0013683133966841403\n",
      "Batch: 35200,train loss is: 0.0010536847332664127\n",
      "test loss is 0.0011757893369354262\n",
      "Batch: 35300,train loss is: 0.0007748532347337223\n",
      "test loss is 0.0010759061775854498\n",
      "Batch: 35400,train loss is: 0.0007874025852495014\n",
      "test loss is 0.0009856912481355522\n",
      "Batch: 35500,train loss is: 0.001174398302960374\n",
      "test loss is 0.0012181632020356675\n",
      "Batch: 35600,train loss is: 0.0009104510841279132\n",
      "test loss is 0.001189414184832363\n",
      "Batch: 35700,train loss is: 0.0015302649550682926\n",
      "test loss is 0.0018844912060890096\n",
      "Batch: 35800,train loss is: 0.00171227265037156\n",
      "test loss is 0.0016091805336371468\n",
      "Batch: 35900,train loss is: 0.0012090204080901354\n",
      "test loss is 0.0011848426927031387\n",
      "Batch: 36000,train loss is: 0.0011614503674225415\n",
      "test loss is 0.0010073032379242465\n",
      "Batch: 36100,train loss is: 0.0016558796941961097\n",
      "test loss is 0.0021787590533545306\n",
      "Batch: 36200,train loss is: 0.0011317321253152416\n",
      "test loss is 0.0013437437946039988\n",
      "Batch: 36300,train loss is: 0.001467811454858782\n",
      "test loss is 0.0013817947783693835\n",
      "Batch: 36400,train loss is: 0.0008384038946476487\n",
      "test loss is 0.0014168215453935146\n",
      "Batch: 36500,train loss is: 0.0008188344645724153\n",
      "test loss is 0.001044076549611305\n",
      "Batch: 36600,train loss is: 0.0017597223124346936\n",
      "test loss is 0.0015991734882492684\n",
      "Batch: 36700,train loss is: 0.0006704139267309356\n",
      "test loss is 0.0010595125332979658\n",
      "Batch: 36800,train loss is: 0.0011611557094007691\n",
      "test loss is 0.0011466738822848014\n",
      "Batch: 36900,train loss is: 0.0008586994856023765\n",
      "test loss is 0.001287996478700775\n",
      "Batch: 37000,train loss is: 0.0010718562575528282\n",
      "test loss is 0.0017572405966594755\n",
      "Batch: 37100,train loss is: 0.0009211396655911925\n",
      "test loss is 0.0012853052968837273\n",
      "Batch: 37200,train loss is: 0.0012597711990974879\n",
      "test loss is 0.0009429778992223564\n",
      "Batch: 37300,train loss is: 0.0012776696392168957\n",
      "test loss is 0.0016848191292375735\n",
      "Batch: 37400,train loss is: 0.0012014576773706279\n",
      "test loss is 0.001641482637299505\n",
      "Batch: 37500,train loss is: 0.0014555856564296642\n",
      "test loss is 0.0012355679611968395\n",
      "Batch: 37600,train loss is: 0.000816127045779701\n",
      "test loss is 0.001115799496351146\n",
      "Batch: 37700,train loss is: 0.0015174960308378983\n",
      "test loss is 0.0013468698600727618\n",
      "Batch: 37800,train loss is: 0.0009052313130956599\n",
      "test loss is 0.0011866708707784623\n",
      "Batch: 37900,train loss is: 0.0008876779220934707\n",
      "test loss is 0.0015928690901732586\n",
      "Batch: 38000,train loss is: 0.0021503893508996712\n",
      "test loss is 0.0026086392829513874\n",
      "Batch: 38100,train loss is: 0.001179080413824732\n",
      "test loss is 0.001325828499197426\n",
      "Batch: 38200,train loss is: 0.0016595701825933419\n",
      "test loss is 0.0013613693912135657\n",
      "Batch: 38300,train loss is: 0.0011610881141669834\n",
      "test loss is 0.0013242287207332595\n",
      "Batch: 38400,train loss is: 0.0007474287921747728\n",
      "test loss is 0.0009848560817267771\n",
      "Batch: 38500,train loss is: 0.002021769751854834\n",
      "test loss is 0.0009279039230240084\n",
      "Batch: 38600,train loss is: 0.0005285371996681308\n",
      "test loss is 0.0012013489340176847\n",
      "Batch: 38700,train loss is: 0.0009097169785705436\n",
      "test loss is 0.0009802256388010534\n",
      "Batch: 38800,train loss is: 0.0011403918851471365\n",
      "test loss is 0.00123950526658499\n",
      "Batch: 38900,train loss is: 0.0007162856465882236\n",
      "test loss is 0.0011952328278261753\n",
      "Batch: 39000,train loss is: 0.0009456644998561326\n",
      "test loss is 0.0011361347518478709\n",
      "Batch: 39100,train loss is: 0.001187136144956565\n",
      "test loss is 0.0012456349808073928\n",
      "Batch: 39200,train loss is: 0.0009686384526341959\n",
      "test loss is 0.0010874941474320868\n",
      "Batch: 39300,train loss is: 0.0011493449596163125\n",
      "test loss is 0.0015581368355871855\n",
      "Batch: 39400,train loss is: 0.0017468664090437578\n",
      "test loss is 0.0013938116043213193\n",
      "Batch: 39500,train loss is: 0.001341557233976737\n",
      "test loss is 0.0015550958254687823\n",
      "Batch: 39600,train loss is: 0.001521740523616446\n",
      "test loss is 0.0015463452788333964\n",
      "Batch: 39700,train loss is: 0.0011071151838773579\n",
      "test loss is 0.0011137034203621593\n",
      "Batch: 39800,train loss is: 0.001164145641398663\n",
      "test loss is 0.0011830404983571255\n",
      "Batch: 39900,train loss is: 0.002356856948509791\n",
      "test loss is 0.0021052873783017614\n",
      "Batch: 40000,train loss is: 0.0007818953442492776\n",
      "test loss is 0.0013849174421952627\n",
      "Batch: 40100,train loss is: 0.0014739548647315784\n",
      "test loss is 0.0015146381101083964\n",
      "Batch: 40200,train loss is: 0.0018227179279817086\n",
      "test loss is 0.0013352613135267323\n",
      "Batch: 40300,train loss is: 0.0007233041409926563\n",
      "test loss is 0.001245108791170632\n",
      "Batch: 40400,train loss is: 0.0006161090406535375\n",
      "test loss is 0.0009455494601931492\n",
      "Batch: 40500,train loss is: 0.0008689648384500324\n",
      "test loss is 0.0010457268401390372\n",
      "Batch: 40600,train loss is: 0.002078241952823415\n",
      "test loss is 0.0010909940438995195\n",
      "Batch: 40700,train loss is: 0.001118062273680688\n",
      "test loss is 0.0011974673521208908\n",
      "Batch: 40800,train loss is: 0.0010059203451730928\n",
      "test loss is 0.0011672188059682598\n",
      "Batch: 40900,train loss is: 0.0009400955956308059\n",
      "test loss is 0.0010328568239458668\n",
      "Batch: 41000,train loss is: 0.0016136033853119377\n",
      "test loss is 0.0011579902249982997\n",
      "Batch: 41100,train loss is: 0.0009681701361544861\n",
      "test loss is 0.0015363946943136027\n",
      "Batch: 41200,train loss is: 0.0009191497686006964\n",
      "test loss is 0.001620689348331837\n",
      "Batch: 41300,train loss is: 0.0007713715807027865\n",
      "test loss is 0.0011345511737933802\n",
      "Batch: 41400,train loss is: 0.0011649145510917273\n",
      "test loss is 0.0009338424352436754\n",
      "Batch: 41500,train loss is: 0.0025957096480822813\n",
      "test loss is 0.0023113002764826845\n",
      "Batch: 41600,train loss is: 0.001718590930840396\n",
      "test loss is 0.0013315942161798427\n",
      "Batch: 41700,train loss is: 0.0007661367098452776\n",
      "test loss is 0.0012985005105366548\n",
      "Batch: 41800,train loss is: 0.0007901949395052457\n",
      "test loss is 0.001307229499052581\n",
      "Batch: 41900,train loss is: 0.0013926734947544168\n",
      "test loss is 0.0015662787688691172\n",
      "Batch: 42000,train loss is: 0.002580872378615367\n",
      "test loss is 0.0013460201414595215\n",
      "Batch: 42100,train loss is: 0.0016660355518136824\n",
      "test loss is 0.001197323037387518\n",
      "Batch: 42200,train loss is: 0.0008298003889053319\n",
      "test loss is 0.001251322823493863\n",
      "Batch: 42300,train loss is: 0.0018155334932206085\n",
      "test loss is 0.0011437538146989164\n",
      "Batch: 42400,train loss is: 0.0007941944358832568\n",
      "test loss is 0.0011155107711353087\n",
      "Batch: 42500,train loss is: 0.0008399660322812295\n",
      "test loss is 0.0012870231530461044\n",
      "Batch: 42600,train loss is: 0.0007188784925219894\n",
      "test loss is 0.0008613888990710747\n",
      "Batch: 42700,train loss is: 0.0017457466246138028\n",
      "test loss is 0.0010603098366018348\n",
      "Batch: 42800,train loss is: 0.0006772583692448937\n",
      "test loss is 0.0011125876575721289\n",
      "Batch: 42900,train loss is: 0.0008687852867949602\n",
      "test loss is 0.001405088905312617\n",
      "Batch: 43000,train loss is: 0.0011400555611539801\n",
      "test loss is 0.001014838529083393\n",
      "Batch: 43100,train loss is: 0.0006742302886465964\n",
      "test loss is 0.0010184553544041527\n",
      "Batch: 43200,train loss is: 0.0010035970927763115\n",
      "test loss is 0.0010753317407871422\n",
      "Batch: 43300,train loss is: 0.0011466456491385371\n",
      "test loss is 0.0015410818000270894\n",
      "Batch: 43400,train loss is: 0.0008750852399815195\n",
      "test loss is 0.0013416832747699212\n",
      "Batch: 43500,train loss is: 0.0018095702497565608\n",
      "test loss is 0.0011711237091563817\n",
      "Batch: 43600,train loss is: 0.0013997340168915146\n",
      "test loss is 0.00109697205214559\n",
      "Batch: 43700,train loss is: 0.0010521635836578884\n",
      "test loss is 0.0014959214945507743\n",
      "Batch: 43800,train loss is: 0.0009814380018276937\n",
      "test loss is 0.000987608052360917\n",
      "Batch: 43900,train loss is: 0.0009954074027669546\n",
      "test loss is 0.001166530571205323\n",
      "Batch: 44000,train loss is: 0.0009626809080924415\n",
      "test loss is 0.0009969369214990367\n",
      "Batch: 44100,train loss is: 0.0007000283502671677\n",
      "test loss is 0.0012370068224886901\n",
      "Batch: 44200,train loss is: 0.0008571550139791445\n",
      "test loss is 0.0009163247246507301\n",
      "Batch: 44300,train loss is: 0.0010393422096538149\n",
      "test loss is 0.0009788599983423568\n",
      "Batch: 44400,train loss is: 0.0013004345792444272\n",
      "test loss is 0.0014636926009456005\n",
      "Batch: 44500,train loss is: 0.0011467320073708992\n",
      "test loss is 0.0014058292459475973\n",
      "Batch: 44600,train loss is: 0.0010813608310911234\n",
      "test loss is 0.0008956155310698736\n",
      "Batch: 44700,train loss is: 0.0024128665436227463\n",
      "test loss is 0.0018277952324328957\n",
      "Batch: 44800,train loss is: 0.0022815126471723117\n",
      "test loss is 0.0010360460187047269\n",
      "Batch: 44900,train loss is: 0.0014892063619310102\n",
      "test loss is 0.0013890977961151033\n",
      "Batch: 45000,train loss is: 0.0010126578239732377\n",
      "test loss is 0.0017286490629538627\n",
      "Batch: 45100,train loss is: 0.0005679991535521526\n",
      "test loss is 0.00109954570179574\n",
      "Batch: 45200,train loss is: 0.0010808216636308547\n",
      "test loss is 0.0009581620494701925\n",
      "Batch: 45300,train loss is: 0.0009094419702575416\n",
      "test loss is 0.0009554766732148269\n",
      "Batch: 45400,train loss is: 0.0010847014584396445\n",
      "test loss is 0.000995815457316389\n",
      "Batch: 45500,train loss is: 0.0011101405990619328\n",
      "test loss is 0.0009685979508088944\n",
      "Batch: 45600,train loss is: 0.0009639047342728092\n",
      "test loss is 0.0009284335001882664\n",
      "Batch: 45700,train loss is: 0.0017434976424375022\n",
      "test loss is 0.0011384420868836497\n",
      "Batch: 45800,train loss is: 0.001143651144150802\n",
      "test loss is 0.0013741778183664164\n",
      "Batch: 45900,train loss is: 0.000861650771342433\n",
      "test loss is 0.0009249186724736322\n",
      "Batch: 46000,train loss is: 0.000808854833899689\n",
      "test loss is 0.0015620402727063538\n",
      "Batch: 46100,train loss is: 0.0016114512539881134\n",
      "test loss is 0.0009834495030513768\n",
      "Batch: 46200,train loss is: 0.0007645158225773293\n",
      "test loss is 0.001126226696779132\n",
      "Batch: 46300,train loss is: 0.0009682553849567192\n",
      "test loss is 0.001443779633509253\n",
      "Batch: 46400,train loss is: 0.0027874466772812535\n",
      "test loss is 0.0012337456376842124\n",
      "Batch: 46500,train loss is: 0.002310607264930759\n",
      "test loss is 0.0015317040947938328\n",
      "Batch: 46600,train loss is: 0.0011477638655546319\n",
      "test loss is 0.0010222863277291102\n",
      "Batch: 46700,train loss is: 0.0007151411673423061\n",
      "test loss is 0.0010063515073267199\n",
      "-----------------------Epoch: 1----------------------------------\n",
      "Batch: 0,train loss is: 0.0007185666959589627\n",
      "test loss is 0.0009620699530073838\n",
      "Batch: 100,train loss is: 0.0010181792914104328\n",
      "test loss is 0.0013624473254387143\n",
      "Batch: 200,train loss is: 0.0007835102977868139\n",
      "test loss is 0.0010466097137034848\n",
      "Batch: 300,train loss is: 0.0008175444137647959\n",
      "test loss is 0.0010560881761139526\n",
      "Batch: 400,train loss is: 0.0010307038380056247\n",
      "test loss is 0.0009851558283124526\n",
      "Batch: 500,train loss is: 0.0011574043307477633\n",
      "test loss is 0.0010402495168491972\n",
      "Batch: 600,train loss is: 0.0008373431044691956\n",
      "test loss is 0.0009562738704730589\n",
      "Batch: 700,train loss is: 0.0011448022779969851\n",
      "test loss is 0.000971397584596245\n",
      "Batch: 800,train loss is: 0.002097684571649885\n",
      "test loss is 0.0012229187032790287\n",
      "Batch: 900,train loss is: 0.0008355258980596672\n",
      "test loss is 0.0009926563348987172\n",
      "Batch: 1000,train loss is: 0.0011989733379472838\n",
      "test loss is 0.0009201294643827742\n",
      "Batch: 1100,train loss is: 0.0007032889915984531\n",
      "test loss is 0.000936203154404823\n",
      "Batch: 1200,train loss is: 0.0008823359760767376\n",
      "test loss is 0.0011547265478365429\n",
      "Batch: 1300,train loss is: 0.000940717175011726\n",
      "test loss is 0.0008698077544035148\n",
      "Batch: 1400,train loss is: 0.0012663800681086163\n",
      "test loss is 0.001062440591269411\n",
      "Batch: 1500,train loss is: 0.0007874924758172557\n",
      "test loss is 0.0011180885174225943\n",
      "Batch: 1600,train loss is: 0.0006199598151259598\n",
      "test loss is 0.0008565985995370563\n",
      "Batch: 1700,train loss is: 0.001646272164700484\n",
      "test loss is 0.0017166280300082814\n",
      "Batch: 1800,train loss is: 0.0011212823160761914\n",
      "test loss is 0.001104673085883224\n",
      "Batch: 1900,train loss is: 0.0008748771596048506\n",
      "test loss is 0.0008802588256893717\n",
      "Batch: 2000,train loss is: 0.0013017299394211652\n",
      "test loss is 0.0015261031698127919\n",
      "Batch: 2100,train loss is: 0.0008790197598794239\n",
      "test loss is 0.0009906302258714716\n",
      "Batch: 2200,train loss is: 0.0010998721093714594\n",
      "test loss is 0.0008927617525573087\n",
      "Batch: 2300,train loss is: 0.0011202933580294428\n",
      "test loss is 0.0009981056032862257\n",
      "Batch: 2400,train loss is: 0.0015028141696012726\n",
      "test loss is 0.0009128528629156917\n",
      "Batch: 2500,train loss is: 0.001110959308244374\n",
      "test loss is 0.0009028845965229074\n",
      "Batch: 2600,train loss is: 0.0016059502998031884\n",
      "test loss is 0.0009113317970250726\n",
      "Batch: 2700,train loss is: 0.0010251155580185713\n",
      "test loss is 0.0013812493252331416\n",
      "Batch: 2800,train loss is: 0.001338379096369653\n",
      "test loss is 0.0010921270289066154\n",
      "Batch: 2900,train loss is: 0.001023856004732033\n",
      "test loss is 0.0010206145419556108\n",
      "Batch: 3000,train loss is: 0.002017683470957136\n",
      "test loss is 0.0015976678456840697\n",
      "Batch: 3100,train loss is: 0.0005895391942291022\n",
      "test loss is 0.0011480763975967284\n",
      "Batch: 3200,train loss is: 0.0007980739276132215\n",
      "test loss is 0.0011546380566660578\n",
      "Batch: 3300,train loss is: 0.0010191132966151598\n",
      "test loss is 0.0009498428349080744\n",
      "Batch: 3400,train loss is: 0.0007925193696927587\n",
      "test loss is 0.0009613730629919457\n",
      "Batch: 3500,train loss is: 0.0005925814989043546\n",
      "test loss is 0.0010195831628635119\n",
      "Batch: 3600,train loss is: 0.0010155018833315384\n",
      "test loss is 0.0009269592954911549\n",
      "Batch: 3700,train loss is: 0.002037255904693988\n",
      "test loss is 0.0010379612221693185\n",
      "Batch: 3800,train loss is: 0.0006455942712122497\n",
      "test loss is 0.0009310047936152323\n",
      "Batch: 3900,train loss is: 0.0021668956399695256\n",
      "test loss is 0.0011361663019209887\n",
      "Batch: 4000,train loss is: 0.0013198217860751964\n",
      "test loss is 0.0010708168652705263\n",
      "Batch: 4100,train loss is: 0.0013911156025681781\n",
      "test loss is 0.0009761820240252839\n",
      "Batch: 4200,train loss is: 0.0010466480057729679\n",
      "test loss is 0.0011987116306426448\n",
      "Batch: 4300,train loss is: 0.002404655756229046\n",
      "test loss is 0.0026948323459994334\n",
      "Batch: 4400,train loss is: 0.0010665989923734397\n",
      "test loss is 0.0009064253633288153\n",
      "Batch: 4500,train loss is: 0.0007715861547263775\n",
      "test loss is 0.0009519054568797079\n",
      "Batch: 4600,train loss is: 0.000788281244554169\n",
      "test loss is 0.0009679098056544525\n",
      "Batch: 4700,train loss is: 0.0007651458388553322\n",
      "test loss is 0.0008119980385016382\n",
      "Batch: 4800,train loss is: 0.0006994584566339327\n",
      "test loss is 0.000989849761035752\n",
      "Batch: 4900,train loss is: 0.0010086796499932204\n",
      "test loss is 0.0009749941519583394\n",
      "Batch: 5000,train loss is: 0.0010039384335893801\n",
      "test loss is 0.0009387776126859547\n",
      "Batch: 5100,train loss is: 0.0005846224812954672\n",
      "test loss is 0.0008639895213973042\n",
      "Batch: 5200,train loss is: 0.0010231630957766977\n",
      "test loss is 0.0008957961569209923\n",
      "Batch: 5300,train loss is: 0.0005079232290698424\n",
      "test loss is 0.0010502531203361058\n",
      "Batch: 5400,train loss is: 0.001452826218233187\n",
      "test loss is 0.0010655554315511415\n",
      "Batch: 5500,train loss is: 0.0012204483646438419\n",
      "test loss is 0.0010999595679947727\n",
      "Batch: 5600,train loss is: 0.0009522946623827237\n",
      "test loss is 0.0009819581520443843\n",
      "Batch: 5700,train loss is: 0.0010128829677197937\n",
      "test loss is 0.0013187166231492522\n",
      "Batch: 5800,train loss is: 0.0015302315425317235\n",
      "test loss is 0.0009685967503203878\n",
      "Batch: 5900,train loss is: 0.0008501828778184278\n",
      "test loss is 0.0010450224067092\n",
      "Batch: 6000,train loss is: 0.0010596353805451143\n",
      "test loss is 0.0015737171503807468\n",
      "Batch: 6100,train loss is: 0.0007467321645205065\n",
      "test loss is 0.000811688417022735\n",
      "Batch: 6200,train loss is: 0.0006518569816149992\n",
      "test loss is 0.001067537292335308\n",
      "Batch: 6300,train loss is: 0.0007997844482223306\n",
      "test loss is 0.0008513148621303376\n",
      "Batch: 6400,train loss is: 0.0005274568181585475\n",
      "test loss is 0.0010626466237945308\n",
      "Batch: 6500,train loss is: 0.0007217303960851756\n",
      "test loss is 0.0009587092154782114\n",
      "Batch: 6600,train loss is: 0.0008200583453183213\n",
      "test loss is 0.0011796555429564434\n",
      "Batch: 6700,train loss is: 0.0016984894315062108\n",
      "test loss is 0.001091217007115909\n",
      "Batch: 6800,train loss is: 0.0006396780929435223\n",
      "test loss is 0.0007950276065013116\n",
      "Batch: 6900,train loss is: 0.0006176961966204155\n",
      "test loss is 0.0010569418164283076\n",
      "Batch: 7000,train loss is: 0.0007013202275438703\n",
      "test loss is 0.0009783640723763773\n",
      "Batch: 7100,train loss is: 0.0008505478208828155\n",
      "test loss is 0.0009040274216132819\n",
      "Batch: 7200,train loss is: 0.0006302138804127817\n",
      "test loss is 0.0007789002478379126\n",
      "Batch: 7300,train loss is: 0.0013947664996457236\n",
      "test loss is 0.0009494099614223901\n",
      "Batch: 7400,train loss is: 0.001117479816712393\n",
      "test loss is 0.0016384824515693942\n",
      "Batch: 7500,train loss is: 0.0012041402006589437\n",
      "test loss is 0.0008225307152129562\n",
      "Batch: 7600,train loss is: 0.001077417497955479\n",
      "test loss is 0.0011230546713721207\n",
      "Batch: 7700,train loss is: 0.0007815982046542469\n",
      "test loss is 0.0010553670779984046\n",
      "Batch: 7800,train loss is: 0.00127941845183767\n",
      "test loss is 0.0011433501529367915\n",
      "Batch: 7900,train loss is: 0.0014369326482636807\n",
      "test loss is 0.0008327934475458789\n",
      "Batch: 8000,train loss is: 0.0010244260757078186\n",
      "test loss is 0.001107335481985469\n",
      "Batch: 8100,train loss is: 0.0007956254207513583\n",
      "test loss is 0.00117494033582116\n",
      "Batch: 8200,train loss is: 0.0006757394958315026\n",
      "test loss is 0.0008533872864293033\n",
      "Batch: 8300,train loss is: 0.0010401104945141366\n",
      "test loss is 0.0011781718550004133\n",
      "Batch: 8400,train loss is: 0.0007929546592423995\n",
      "test loss is 0.0008937151743040797\n",
      "Batch: 8500,train loss is: 0.0027893004800635763\n",
      "test loss is 0.0022509947775645974\n",
      "Batch: 8600,train loss is: 0.0008736640120559323\n",
      "test loss is 0.0008225722708498407\n",
      "Batch: 8700,train loss is: 0.0006357263151019196\n",
      "test loss is 0.0008900139145754807\n",
      "Batch: 8800,train loss is: 0.0011649951070696058\n",
      "test loss is 0.0009934535430548839\n",
      "Batch: 8900,train loss is: 0.0008260770745248556\n",
      "test loss is 0.0008596830324384675\n",
      "Batch: 9000,train loss is: 0.0006861490067874734\n",
      "test loss is 0.0010910231775048327\n",
      "Batch: 9100,train loss is: 0.000954609568649526\n",
      "test loss is 0.0011059820077612035\n",
      "Batch: 9200,train loss is: 0.004267283991098957\n",
      "test loss is 0.0010867338517023717\n",
      "Batch: 9300,train loss is: 0.0011089323059164332\n",
      "test loss is 0.0016356283037524569\n",
      "Batch: 9400,train loss is: 0.0005937026004151717\n",
      "test loss is 0.0008807203058866978\n",
      "Batch: 9500,train loss is: 0.0006719589355923121\n",
      "test loss is 0.0010699557041901064\n",
      "Batch: 9600,train loss is: 0.0009706037344649696\n",
      "test loss is 0.0010527870349293993\n",
      "Batch: 9700,train loss is: 0.0006545721761970376\n",
      "test loss is 0.001160929572029707\n",
      "Batch: 9800,train loss is: 0.0014321098739191783\n",
      "test loss is 0.0015566403362128514\n",
      "Batch: 9900,train loss is: 0.0011632110946911715\n",
      "test loss is 0.0009484908222771655\n",
      "Batch: 10000,train loss is: 0.0006723001121080666\n",
      "test loss is 0.0010222222103811096\n",
      "Batch: 10100,train loss is: 0.0012166462044421547\n",
      "test loss is 0.0010203098961258828\n",
      "Batch: 10200,train loss is: 0.0006979560178121051\n",
      "test loss is 0.0009341994592905269\n",
      "Batch: 10300,train loss is: 0.001563346017909698\n",
      "test loss is 0.0014294180190002005\n",
      "Batch: 10400,train loss is: 0.0007448605773260979\n",
      "test loss is 0.0008422232836731532\n",
      "Batch: 10500,train loss is: 0.0011329430708774664\n",
      "test loss is 0.0008906583799865634\n",
      "Batch: 10600,train loss is: 0.0011792327095049055\n",
      "test loss is 0.0011810851607903395\n",
      "Batch: 10700,train loss is: 0.0009336608993513035\n",
      "test loss is 0.0010302095594672493\n",
      "Batch: 10800,train loss is: 0.0023005263639980685\n",
      "test loss is 0.0009444414379407376\n",
      "Batch: 10900,train loss is: 0.0006970619983534017\n",
      "test loss is 0.001242597933467815\n",
      "Batch: 11000,train loss is: 0.0008417525147766103\n",
      "test loss is 0.0009262239437641262\n",
      "Batch: 11100,train loss is: 0.0023059456249130183\n",
      "test loss is 0.0009025416808782636\n",
      "Batch: 11200,train loss is: 0.0011204354875067267\n",
      "test loss is 0.0008140017604359501\n",
      "Batch: 11300,train loss is: 0.0009495452649826827\n",
      "test loss is 0.0010097141916692847\n",
      "Batch: 11400,train loss is: 0.0017218317085694538\n",
      "test loss is 0.0009923119695754675\n",
      "Batch: 11500,train loss is: 0.0009136237254622223\n",
      "test loss is 0.0009809853630889757\n",
      "Batch: 11600,train loss is: 0.0006538810716539892\n",
      "test loss is 0.0008692783770707026\n",
      "Batch: 11700,train loss is: 0.0017141428921047192\n",
      "test loss is 0.0008116439008766068\n",
      "Batch: 11800,train loss is: 0.0009324821176928234\n",
      "test loss is 0.0015876819903350842\n",
      "Batch: 11900,train loss is: 0.0008844540563493392\n",
      "test loss is 0.0008365549771734551\n",
      "Batch: 12000,train loss is: 0.0009696151496532715\n",
      "test loss is 0.000985834636315295\n",
      "Batch: 12100,train loss is: 0.0009972720096522473\n",
      "test loss is 0.0014375158636057274\n",
      "Batch: 12200,train loss is: 0.0009832201986798151\n",
      "test loss is 0.0007602489771947666\n",
      "Batch: 12300,train loss is: 0.0010213709174220094\n",
      "test loss is 0.0011437654156935698\n",
      "Batch: 12400,train loss is: 0.001868949058261998\n",
      "test loss is 0.0009118517372745301\n",
      "Batch: 12500,train loss is: 0.0010394325249981303\n",
      "test loss is 0.001078557601406808\n",
      "Batch: 12600,train loss is: 0.0019695710638021127\n",
      "test loss is 0.0010146614347470867\n",
      "Batch: 12700,train loss is: 0.0008846738857266539\n",
      "test loss is 0.0008858552188002131\n",
      "Batch: 12800,train loss is: 0.0005637722193261535\n",
      "test loss is 0.0010653222568141234\n",
      "Batch: 12900,train loss is: 0.0009314974681380189\n",
      "test loss is 0.0010110975442374644\n",
      "Batch: 13000,train loss is: 0.00046783282478433187\n",
      "test loss is 0.0009659684000183085\n",
      "Batch: 13100,train loss is: 0.0025650375513635065\n",
      "test loss is 0.0014203326503698566\n",
      "Batch: 13200,train loss is: 0.0008598423367512866\n",
      "test loss is 0.001027000427167056\n",
      "Batch: 13300,train loss is: 0.0006933914211410676\n",
      "test loss is 0.0008828621851871456\n",
      "Batch: 13400,train loss is: 0.0007120278541385297\n",
      "test loss is 0.0007867076983727422\n",
      "Batch: 13500,train loss is: 0.004215874399444366\n",
      "test loss is 0.0019063958860665673\n",
      "Batch: 13600,train loss is: 0.0008110645151692387\n",
      "test loss is 0.0007886441547440518\n",
      "Batch: 13700,train loss is: 0.000893256016369815\n",
      "test loss is 0.0010438061579270649\n",
      "Batch: 13800,train loss is: 0.0012493871887243378\n",
      "test loss is 0.0010331507858648242\n",
      "Batch: 13900,train loss is: 0.0016873427386160495\n",
      "test loss is 0.0010035777217061593\n",
      "Batch: 14000,train loss is: 0.0014709223790522977\n",
      "test loss is 0.0010467426198387752\n",
      "Batch: 14100,train loss is: 0.001119925181519125\n",
      "test loss is 0.0010239987940784856\n",
      "Batch: 14200,train loss is: 0.0007267494859637532\n",
      "test loss is 0.0008917431343566019\n",
      "Batch: 14300,train loss is: 0.001254169771480718\n",
      "test loss is 0.001048641032215078\n",
      "Batch: 14400,train loss is: 0.0004917863952313726\n",
      "test loss is 0.000999447226011525\n",
      "Batch: 14500,train loss is: 0.0006969187609833612\n",
      "test loss is 0.0013480624935820267\n",
      "Batch: 14600,train loss is: 0.0007193103633450022\n",
      "test loss is 0.0010363929824025884\n",
      "Batch: 14700,train loss is: 0.0007587384863746192\n",
      "test loss is 0.0009318612050394858\n",
      "Batch: 14800,train loss is: 0.0009640780388671711\n",
      "test loss is 0.0010732309038868786\n",
      "Batch: 14900,train loss is: 0.0007464600540682916\n",
      "test loss is 0.000816067868341214\n",
      "Batch: 15000,train loss is: 0.0009670456773967331\n",
      "test loss is 0.0009511975317248225\n",
      "Batch: 15100,train loss is: 0.000662249709018223\n",
      "test loss is 0.0008303952541150433\n",
      "Batch: 15200,train loss is: 0.000766954195833183\n",
      "test loss is 0.0009279966692484105\n",
      "Batch: 15300,train loss is: 0.001492869294675853\n",
      "test loss is 0.001074910014132195\n",
      "Batch: 15400,train loss is: 0.001104058982331733\n",
      "test loss is 0.0007962176942638565\n",
      "Batch: 15500,train loss is: 0.0005080801936734814\n",
      "test loss is 0.0008720053999934513\n",
      "Batch: 15600,train loss is: 0.00039102848276444243\n",
      "test loss is 0.0007859351099725165\n",
      "Batch: 15700,train loss is: 0.0013951767830617866\n",
      "test loss is 0.0008518340917330286\n",
      "Batch: 15800,train loss is: 0.0005118208725510484\n",
      "test loss is 0.0011250979128922512\n",
      "Batch: 15900,train loss is: 0.0007728980791107208\n",
      "test loss is 0.0010027061562983314\n",
      "Batch: 16000,train loss is: 0.0008272052865888801\n",
      "test loss is 0.0006933856885256929\n",
      "Batch: 16100,train loss is: 0.0013299084370925504\n",
      "test loss is 0.00119132700182079\n",
      "Batch: 16200,train loss is: 0.0006639426260645462\n",
      "test loss is 0.0007613486131901992\n",
      "Batch: 16300,train loss is: 0.0009792907417074826\n",
      "test loss is 0.0008824123039045416\n",
      "Batch: 16400,train loss is: 0.0007224363468757448\n",
      "test loss is 0.0009752467049172055\n",
      "Batch: 16500,train loss is: 0.0009246832298494138\n",
      "test loss is 0.0007854390713939423\n",
      "Batch: 16600,train loss is: 0.0005036281311268434\n",
      "test loss is 0.0008693845190449464\n",
      "Batch: 16700,train loss is: 0.0011746332283204997\n",
      "test loss is 0.0013472587149513256\n",
      "Batch: 16800,train loss is: 0.0012077884870595208\n",
      "test loss is 0.0007740186316077304\n",
      "Batch: 16900,train loss is: 0.0008482353047340429\n",
      "test loss is 0.0007786637230603594\n",
      "Batch: 17000,train loss is: 0.0007242483852353818\n",
      "test loss is 0.0009436854324661278\n",
      "Batch: 17100,train loss is: 0.0014213465326282171\n",
      "test loss is 0.0012118473429746362\n",
      "Batch: 17200,train loss is: 0.0008522553084459139\n",
      "test loss is 0.0010964124367062228\n",
      "Batch: 17300,train loss is: 0.0008280391776801013\n",
      "test loss is 0.0009709479263071282\n",
      "Batch: 17400,train loss is: 0.0008303556763674222\n",
      "test loss is 0.0009137208796151496\n",
      "Batch: 17500,train loss is: 0.0008743010926428235\n",
      "test loss is 0.0008960541802018814\n",
      "Batch: 17600,train loss is: 0.0004606095648070523\n",
      "test loss is 0.0009737947109167964\n",
      "Batch: 17700,train loss is: 0.0005049725435807603\n",
      "test loss is 0.0008841526533895908\n",
      "Batch: 17800,train loss is: 0.00110272780527804\n",
      "test loss is 0.0008914417198724729\n",
      "Batch: 17900,train loss is: 0.0004989095592296672\n",
      "test loss is 0.0008148245268122565\n",
      "Batch: 18000,train loss is: 0.0007066231932294238\n",
      "test loss is 0.0008923609307637654\n",
      "Batch: 18100,train loss is: 0.0010809835252377064\n",
      "test loss is 0.002152973028609458\n",
      "Batch: 18200,train loss is: 0.0009816125410629498\n",
      "test loss is 0.0008647910219885608\n",
      "Batch: 18300,train loss is: 0.0004861018641342776\n",
      "test loss is 0.0008408483051123538\n",
      "Batch: 18400,train loss is: 0.0011991453850458834\n",
      "test loss is 0.0011479572312451218\n",
      "Batch: 18500,train loss is: 0.00045812176698324127\n",
      "test loss is 0.0009140385534260974\n",
      "Batch: 18600,train loss is: 0.002848373285234332\n",
      "test loss is 0.0013173750406176537\n",
      "Batch: 18700,train loss is: 0.0006478969874722354\n",
      "test loss is 0.0007076338630837444\n",
      "Batch: 18800,train loss is: 0.0006674106141915702\n",
      "test loss is 0.001201085836121983\n",
      "Batch: 18900,train loss is: 0.0015714693155269382\n",
      "test loss is 0.0012825457029081286\n",
      "Batch: 19000,train loss is: 0.001161303069832193\n",
      "test loss is 0.0010433737691652888\n",
      "Batch: 19100,train loss is: 0.0008491434939960957\n",
      "test loss is 0.0011372091814094408\n",
      "Batch: 19200,train loss is: 0.0006843928052791677\n",
      "test loss is 0.0007125880350247393\n",
      "Batch: 19300,train loss is: 0.0009367686717456881\n",
      "test loss is 0.0008001894013497464\n",
      "Batch: 19400,train loss is: 0.001046452591743267\n",
      "test loss is 0.0007184456850166361\n",
      "Batch: 19500,train loss is: 0.0010171854384755862\n",
      "test loss is 0.0008179521958006935\n",
      "Batch: 19600,train loss is: 0.00128697687646931\n",
      "test loss is 0.0013885809600980852\n",
      "Batch: 19700,train loss is: 0.0007476759040427371\n",
      "test loss is 0.0006812512694445262\n",
      "Batch: 19800,train loss is: 0.0019964629755910594\n",
      "test loss is 0.0009418537991776398\n",
      "Batch: 19900,train loss is: 0.0007165037056392807\n",
      "test loss is 0.0008301056926425238\n",
      "Batch: 20000,train loss is: 0.0013420847595647\n",
      "test loss is 0.0008176895093636428\n",
      "Batch: 20100,train loss is: 0.0016416159047326919\n",
      "test loss is 0.0011121678030273166\n",
      "Batch: 20200,train loss is: 0.0006242255289523554\n",
      "test loss is 0.0007918530255527495\n",
      "Batch: 20300,train loss is: 0.000617290292188849\n",
      "test loss is 0.0010630900022981589\n",
      "Batch: 20400,train loss is: 0.0008419491592629423\n",
      "test loss is 0.001440490174764512\n",
      "Batch: 20500,train loss is: 0.0006751853767794776\n",
      "test loss is 0.0007649416985109434\n",
      "Batch: 20600,train loss is: 0.0011702923681271892\n",
      "test loss is 0.0008248560462699946\n",
      "Batch: 20700,train loss is: 0.0009750848542040818\n",
      "test loss is 0.0008162999780825539\n",
      "Batch: 20800,train loss is: 0.0010373602693817586\n",
      "test loss is 0.000814773212022474\n",
      "Batch: 20900,train loss is: 0.0004262172398182121\n",
      "test loss is 0.0006825231541756967\n",
      "Batch: 21000,train loss is: 0.0007876413707948375\n",
      "test loss is 0.0007203667059994099\n",
      "Batch: 21100,train loss is: 0.0012070605233743055\n",
      "test loss is 0.0012838758044532927\n",
      "Batch: 21200,train loss is: 0.0006879539585060268\n",
      "test loss is 0.000984920831582389\n",
      "Batch: 21300,train loss is: 0.0011398316638626667\n",
      "test loss is 0.0012194008793556348\n",
      "Batch: 21400,train loss is: 0.0008518294831734901\n",
      "test loss is 0.001063028118116652\n",
      "Batch: 21500,train loss is: 0.0008830551234848784\n",
      "test loss is 0.001496537993014148\n",
      "Batch: 21600,train loss is: 0.000438370089811011\n",
      "test loss is 0.0008240034859329191\n",
      "Batch: 21700,train loss is: 0.0006514292936550986\n",
      "test loss is 0.0010068695252013309\n",
      "Batch: 21800,train loss is: 0.002323112184092263\n",
      "test loss is 0.0018658619346418616\n",
      "Batch: 21900,train loss is: 0.0009024677897402539\n",
      "test loss is 0.0012466082195827244\n",
      "Batch: 22000,train loss is: 0.0005240325417643933\n",
      "test loss is 0.000965012072273589\n",
      "Batch: 22100,train loss is: 0.0007796614930201514\n",
      "test loss is 0.0006787034186041097\n",
      "Batch: 22200,train loss is: 0.0006604216458651739\n",
      "test loss is 0.0008344774675651793\n",
      "Batch: 22300,train loss is: 0.0007990381938988694\n",
      "test loss is 0.0011744539478164393\n",
      "Batch: 22400,train loss is: 0.0005342223641968762\n",
      "test loss is 0.0007577232983317614\n",
      "Batch: 22500,train loss is: 0.0008942554663215346\n",
      "test loss is 0.0009740221396724493\n",
      "Batch: 22600,train loss is: 0.00256484705768578\n",
      "test loss is 0.0012707794713846929\n",
      "Batch: 22700,train loss is: 0.0006808205529079187\n",
      "test loss is 0.0008666113635710854\n",
      "Batch: 22800,train loss is: 0.0004670377798740212\n",
      "test loss is 0.0008047417591754694\n",
      "Batch: 22900,train loss is: 0.0006898333435506347\n",
      "test loss is 0.0008289874536194278\n",
      "Batch: 23000,train loss is: 0.00156647507921288\n",
      "test loss is 0.000846508466853622\n",
      "Batch: 23100,train loss is: 0.0006097475658021677\n",
      "test loss is 0.0011248157458144887\n",
      "Batch: 23200,train loss is: 0.0010402396537465032\n",
      "test loss is 0.0008683262040089715\n",
      "Batch: 23300,train loss is: 0.0006033129942135704\n",
      "test loss is 0.0007550278386224616\n",
      "Batch: 23400,train loss is: 0.0013524821047245015\n",
      "test loss is 0.000894390283787566\n",
      "Batch: 23500,train loss is: 0.0005644132568528731\n",
      "test loss is 0.0006789136467411275\n",
      "Batch: 23600,train loss is: 0.0007278519045590029\n",
      "test loss is 0.0007204270014197155\n",
      "Batch: 23700,train loss is: 0.0006070257367100878\n",
      "test loss is 0.0006765132816785762\n",
      "Batch: 23800,train loss is: 0.0008371580161878505\n",
      "test loss is 0.0012523394459537012\n",
      "Batch: 23900,train loss is: 0.0009485825983405\n",
      "test loss is 0.0011309409420068681\n",
      "Batch: 24000,train loss is: 0.001307649850095298\n",
      "test loss is 0.0008143510668883086\n",
      "Batch: 24100,train loss is: 0.0008842440628611624\n",
      "test loss is 0.0010237659401708004\n",
      "Batch: 24200,train loss is: 0.0005518513967995948\n",
      "test loss is 0.001097284619281318\n",
      "Batch: 24300,train loss is: 0.000488643073693013\n",
      "test loss is 0.000814273188619316\n",
      "Batch: 24400,train loss is: 0.0008256711660892264\n",
      "test loss is 0.0008846844178124902\n",
      "Batch: 24500,train loss is: 0.0013457783723741872\n",
      "test loss is 0.0010352385581467458\n",
      "Batch: 24600,train loss is: 0.0010014331462893998\n",
      "test loss is 0.0007789550004031673\n",
      "Batch: 24700,train loss is: 0.0011823111237249998\n",
      "test loss is 0.001159041942327606\n",
      "Batch: 24800,train loss is: 0.0005817288236133908\n",
      "test loss is 0.0006954028393707845\n",
      "Batch: 24900,train loss is: 0.0008053028362215164\n",
      "test loss is 0.0007789002405312414\n",
      "Batch: 25000,train loss is: 0.001020079139347331\n",
      "test loss is 0.0007576336703083614\n",
      "Batch: 25100,train loss is: 0.0005248899387427406\n",
      "test loss is 0.0009467435954644563\n",
      "Batch: 25200,train loss is: 0.0011648853735615027\n",
      "test loss is 0.0010094372629274053\n",
      "Batch: 25300,train loss is: 0.0005335322547873072\n",
      "test loss is 0.0007895836117657978\n",
      "Batch: 25400,train loss is: 0.000522044439515418\n",
      "test loss is 0.0007759364005374698\n",
      "Batch: 25500,train loss is: 0.0005512187491661679\n",
      "test loss is 0.0009346487877235347\n",
      "Batch: 25600,train loss is: 0.0005270916767374553\n",
      "test loss is 0.0010021770184238075\n",
      "Batch: 25700,train loss is: 0.0011402528425772458\n",
      "test loss is 0.0011414272616192002\n",
      "Batch: 25800,train loss is: 0.0003284001840934221\n",
      "test loss is 0.000922863622994587\n",
      "Batch: 25900,train loss is: 0.0005721693688344354\n",
      "test loss is 0.0008126618820682461\n",
      "Batch: 26000,train loss is: 0.0008171004306116034\n",
      "test loss is 0.0007179727526760352\n",
      "Batch: 26100,train loss is: 0.0005047432121154527\n",
      "test loss is 0.0010525789553280478\n",
      "Batch: 26200,train loss is: 0.0005532681111927267\n",
      "test loss is 0.001159707222121516\n",
      "Batch: 26300,train loss is: 0.0010800699980338217\n",
      "test loss is 0.0013565481748258924\n",
      "Batch: 26400,train loss is: 0.0008268583059696259\n",
      "test loss is 0.000843895084201601\n",
      "Batch: 26500,train loss is: 0.0006277051045767267\n",
      "test loss is 0.0014078012796564098\n",
      "Batch: 26600,train loss is: 0.00046506118722722804\n",
      "test loss is 0.0012509772114337892\n",
      "Batch: 26700,train loss is: 0.000885384841096103\n",
      "test loss is 0.0008985220862593854\n",
      "Batch: 26800,train loss is: 0.0005948229368359988\n",
      "test loss is 0.0008607674778911946\n",
      "Batch: 26900,train loss is: 0.0007309296979309285\n",
      "test loss is 0.0008158041380216865\n",
      "Batch: 27000,train loss is: 0.0010404569003456508\n",
      "test loss is 0.000889515785915531\n",
      "Batch: 27100,train loss is: 0.0007952322837882649\n",
      "test loss is 0.0008521963551133909\n",
      "Batch: 27200,train loss is: 0.000840912657993804\n",
      "test loss is 0.000998053907705434\n",
      "Batch: 27300,train loss is: 0.0007016020761177908\n",
      "test loss is 0.0008957892733848082\n",
      "Batch: 27400,train loss is: 0.0005044418613639786\n",
      "test loss is 0.0009655304251192648\n",
      "Batch: 27500,train loss is: 0.0009418333944457612\n",
      "test loss is 0.0009015715016648382\n",
      "Batch: 27600,train loss is: 0.0008315629470326753\n",
      "test loss is 0.0009669703241353746\n",
      "Batch: 27700,train loss is: 0.0007948647911570379\n",
      "test loss is 0.0006583647073463367\n",
      "Batch: 27800,train loss is: 0.0005399664457011212\n",
      "test loss is 0.0006313503919740907\n",
      "Batch: 27900,train loss is: 0.0009092027507725559\n",
      "test loss is 0.0008801759507989144\n",
      "Batch: 28000,train loss is: 0.0012747631794309635\n",
      "test loss is 0.0012524029494637794\n",
      "Batch: 28100,train loss is: 0.0005613331280806284\n",
      "test loss is 0.0006759168709657807\n",
      "Batch: 28200,train loss is: 0.0007247428365974111\n",
      "test loss is 0.0008080319933487844\n",
      "Batch: 28300,train loss is: 0.0008962465049489604\n",
      "test loss is 0.0008112065389713754\n",
      "Batch: 28400,train loss is: 0.000810907014355792\n",
      "test loss is 0.000762166860832255\n",
      "Batch: 28500,train loss is: 0.0009362436947827833\n",
      "test loss is 0.0007394055019732657\n",
      "Batch: 28600,train loss is: 0.0005721479986430472\n",
      "test loss is 0.0009843389495392029\n",
      "Batch: 28700,train loss is: 0.0009894496125716465\n",
      "test loss is 0.0008923095620657556\n",
      "Batch: 28800,train loss is: 0.0008768595510015194\n",
      "test loss is 0.0007825247159605051\n",
      "Batch: 28900,train loss is: 0.0006131615934628809\n",
      "test loss is 0.000776444209522106\n",
      "Batch: 29000,train loss is: 0.0005657496452572837\n",
      "test loss is 0.0009305396826358814\n",
      "Batch: 29100,train loss is: 0.0013579159756812835\n",
      "test loss is 0.0015895871770408225\n",
      "Batch: 29200,train loss is: 0.0009487983103238548\n",
      "test loss is 0.0011831000522032982\n",
      "Batch: 29300,train loss is: 0.0005353140583961895\n",
      "test loss is 0.0011418533441298149\n",
      "Batch: 29400,train loss is: 0.0007641499392906937\n",
      "test loss is 0.0012412350070170107\n",
      "Batch: 29500,train loss is: 0.00043009683979490927\n",
      "test loss is 0.000741983539302651\n",
      "Batch: 29600,train loss is: 0.0003707964819392218\n",
      "test loss is 0.0007328135859777501\n",
      "Batch: 29700,train loss is: 0.0009124744978986301\n",
      "test loss is 0.0006370481675438685\n",
      "Batch: 29800,train loss is: 0.0008088115110854483\n",
      "test loss is 0.0009702579693249045\n",
      "Batch: 29900,train loss is: 0.0004809289769862091\n",
      "test loss is 0.0009175961308448736\n",
      "Batch: 30000,train loss is: 0.0005744553950284897\n",
      "test loss is 0.0008041323474860483\n",
      "Batch: 30100,train loss is: 0.0010600372902920823\n",
      "test loss is 0.0008961116437869503\n",
      "Batch: 30200,train loss is: 0.0011289762928486883\n",
      "test loss is 0.000767293033200843\n",
      "Batch: 30300,train loss is: 0.0006669364785788299\n",
      "test loss is 0.0007162554006575497\n",
      "Batch: 30400,train loss is: 0.0007385156072685843\n",
      "test loss is 0.0007183381693556124\n",
      "Batch: 30500,train loss is: 0.0009219083602046054\n",
      "test loss is 0.0008795467160933069\n",
      "Batch: 30600,train loss is: 0.0006044238580606595\n",
      "test loss is 0.0008427262193876761\n",
      "Batch: 30700,train loss is: 0.0007156634297234866\n",
      "test loss is 0.00091458230562635\n",
      "Batch: 30800,train loss is: 0.000605613601731317\n",
      "test loss is 0.0007049457944566962\n",
      "Batch: 30900,train loss is: 0.0009139564708622116\n",
      "test loss is 0.0007860774951403595\n",
      "Batch: 31000,train loss is: 0.0007024672435750729\n",
      "test loss is 0.0007056016314994258\n",
      "Batch: 31100,train loss is: 0.0008634282968247208\n",
      "test loss is 0.0007082171300756805\n",
      "Batch: 31200,train loss is: 0.0013265926367659118\n",
      "test loss is 0.0014439683309641507\n",
      "Batch: 31300,train loss is: 0.0008777095719233161\n",
      "test loss is 0.001389085366620356\n",
      "Batch: 31400,train loss is: 0.000933678362756963\n",
      "test loss is 0.0006971294213807022\n",
      "Batch: 31500,train loss is: 0.0005560855505160934\n",
      "test loss is 0.0007378514904225843\n",
      "Batch: 31600,train loss is: 0.0010343304965582958\n",
      "test loss is 0.0006536003649474802\n",
      "Batch: 31700,train loss is: 0.0009643748382850997\n",
      "test loss is 0.0010196545483106825\n",
      "Batch: 31800,train loss is: 0.0010305365567450315\n",
      "test loss is 0.0007261773445433063\n",
      "Batch: 31900,train loss is: 0.0006102669864096637\n",
      "test loss is 0.000742326873775695\n",
      "Batch: 32000,train loss is: 0.0005471636693611821\n",
      "test loss is 0.0010070320260840739\n",
      "Batch: 32100,train loss is: 0.001221523590229576\n",
      "test loss is 0.0010255733845339228\n",
      "Batch: 32200,train loss is: 0.000799978030254787\n",
      "test loss is 0.0008857123191062492\n",
      "Batch: 32300,train loss is: 0.0008524539034098327\n",
      "test loss is 0.0008202311400967888\n",
      "Batch: 32400,train loss is: 0.0006125775045220591\n",
      "test loss is 0.0007603694060703771\n",
      "Batch: 32500,train loss is: 0.0005482938391124679\n",
      "test loss is 0.0010219797067034493\n",
      "Batch: 32600,train loss is: 0.0004848715784940123\n",
      "test loss is 0.0008958614097282454\n",
      "Batch: 32700,train loss is: 0.0007331216409390358\n",
      "test loss is 0.0008560772581474899\n",
      "Batch: 32800,train loss is: 0.0009081945511419095\n",
      "test loss is 0.0009085853017086041\n",
      "Batch: 32900,train loss is: 0.0006093780823510269\n",
      "test loss is 0.000972659520966456\n",
      "Batch: 33000,train loss is: 0.0006974170622966772\n",
      "test loss is 0.0007991021011899519\n",
      "Batch: 33100,train loss is: 0.001153231318176492\n",
      "test loss is 0.0011900039769755283\n",
      "Batch: 33200,train loss is: 0.0015240688318672068\n",
      "test loss is 0.0008984653942156314\n",
      "Batch: 33300,train loss is: 0.0007683070391682537\n",
      "test loss is 0.0006845716244957806\n",
      "Batch: 33400,train loss is: 0.002092715409540598\n",
      "test loss is 0.0016742226355026654\n",
      "Batch: 33500,train loss is: 0.0007042353200395254\n",
      "test loss is 0.0009745553932052488\n",
      "Batch: 33600,train loss is: 0.0007158676430074929\n",
      "test loss is 0.0008930865516515706\n",
      "Batch: 33700,train loss is: 0.0009505553196556989\n",
      "test loss is 0.0009547896155394359\n",
      "Batch: 33800,train loss is: 0.0006557670753188391\n",
      "test loss is 0.0007476642636836556\n",
      "Batch: 33900,train loss is: 0.000634040118144852\n",
      "test loss is 0.0005864403600067845\n",
      "Batch: 34000,train loss is: 0.0004083102239094813\n",
      "test loss is 0.0006310818656441113\n",
      "Batch: 34100,train loss is: 0.001218876981770139\n",
      "test loss is 0.0008116750111388522\n",
      "Batch: 34200,train loss is: 0.0005109602157158418\n",
      "test loss is 0.0006775100544789827\n",
      "Batch: 34300,train loss is: 0.0014425496206820322\n",
      "test loss is 0.0008122539723296658\n",
      "Batch: 34400,train loss is: 0.0007220544887921785\n",
      "test loss is 0.0007750253464388495\n",
      "Batch: 34500,train loss is: 0.0004423063464395005\n",
      "test loss is 0.0007298003125227587\n",
      "Batch: 34600,train loss is: 0.0011934464841918376\n",
      "test loss is 0.0009522373132503431\n",
      "Batch: 34700,train loss is: 0.0008286994512440211\n",
      "test loss is 0.0007693358374396566\n",
      "Batch: 34800,train loss is: 0.001001975390847204\n",
      "test loss is 0.000720693667960783\n",
      "Batch: 34900,train loss is: 0.0007102909928286209\n",
      "test loss is 0.0006464350395058727\n",
      "Batch: 35000,train loss is: 0.00034559587844175177\n",
      "test loss is 0.0008294316959981431\n",
      "Batch: 35100,train loss is: 0.002085743175190253\n",
      "test loss is 0.0009266179187797924\n",
      "Batch: 35200,train loss is: 0.0008299900878073882\n",
      "test loss is 0.001203791776359252\n",
      "Batch: 35300,train loss is: 0.0006478715900743318\n",
      "test loss is 0.000917026377800714\n",
      "Batch: 35400,train loss is: 0.0010616685292905877\n",
      "test loss is 0.0008805928864253627\n",
      "Batch: 35500,train loss is: 0.0007783199225516585\n",
      "test loss is 0.0014066133861756743\n",
      "Batch: 35600,train loss is: 0.0006063433399234232\n",
      "test loss is 0.0006670105183773985\n",
      "Batch: 35700,train loss is: 0.0009501548064062974\n",
      "test loss is 0.0006913463760595293\n",
      "Batch: 35800,train loss is: 0.001164269277204732\n",
      "test loss is 0.0007157464692245065\n",
      "Batch: 35900,train loss is: 0.0009091139283494615\n",
      "test loss is 0.0006420650924305966\n",
      "Batch: 36000,train loss is: 0.0006813232546429972\n",
      "test loss is 0.00129219675759343\n",
      "Batch: 36100,train loss is: 0.0020948184752314107\n",
      "test loss is 0.002026217735504258\n",
      "Batch: 36200,train loss is: 0.000536354189469038\n",
      "test loss is 0.0008007460772841032\n",
      "Batch: 36300,train loss is: 0.000910096566599506\n",
      "test loss is 0.000791168469500759\n",
      "Batch: 36400,train loss is: 0.0009865170482037423\n",
      "test loss is 0.0013308493138332498\n",
      "Batch: 36500,train loss is: 0.0010784290845757161\n",
      "test loss is 0.0009349286257729144\n",
      "Batch: 36600,train loss is: 0.0010226919422797196\n",
      "test loss is 0.0010005922648575805\n",
      "Batch: 36700,train loss is: 0.0007351132538948807\n",
      "test loss is 0.000914255228604281\n",
      "Batch: 36800,train loss is: 0.0005687703739499407\n",
      "test loss is 0.0006416070657019937\n",
      "Batch: 36900,train loss is: 0.0006868542673975405\n",
      "test loss is 0.001045267163961206\n",
      "Batch: 37000,train loss is: 0.001435689961022895\n",
      "test loss is 0.0009489118835646302\n",
      "Batch: 37100,train loss is: 0.001031855077271195\n",
      "test loss is 0.0009196466825304928\n",
      "Batch: 37200,train loss is: 0.0007770969174532205\n",
      "test loss is 0.0008647680733895272\n",
      "Batch: 37300,train loss is: 0.000743015161987078\n",
      "test loss is 0.0007849498263606989\n",
      "Batch: 37400,train loss is: 0.0005625196911530765\n",
      "test loss is 0.0009577052492397228\n",
      "Batch: 37500,train loss is: 0.0007496782653720129\n",
      "test loss is 0.0007488829670398983\n",
      "Batch: 37600,train loss is: 0.0006036462175535889\n",
      "test loss is 0.0006764973698138439\n",
      "Batch: 37700,train loss is: 0.0011425995947281387\n",
      "test loss is 0.0007649595527087005\n",
      "Batch: 37800,train loss is: 0.00046917871888369487\n",
      "test loss is 0.0007549461896723175\n",
      "Batch: 37900,train loss is: 0.0008978786424351354\n",
      "test loss is 0.0008234700839350622\n",
      "Batch: 38000,train loss is: 0.0011951009732659888\n",
      "test loss is 0.0008836680860725513\n",
      "Batch: 38100,train loss is: 0.0004999237991812472\n",
      "test loss is 0.000802757606991488\n",
      "Batch: 38200,train loss is: 0.0009234216511556613\n",
      "test loss is 0.0006823333653986402\n",
      "Batch: 38300,train loss is: 0.0005395030290558389\n",
      "test loss is 0.0008338832309303287\n",
      "Batch: 38400,train loss is: 0.0007209421067340913\n",
      "test loss is 0.000774595232033292\n",
      "Batch: 38500,train loss is: 0.000782267108993491\n",
      "test loss is 0.0008562768180071191\n",
      "Batch: 38600,train loss is: 0.0009164989805118652\n",
      "test loss is 0.0008667098172010862\n",
      "Batch: 38700,train loss is: 0.000976568398655645\n",
      "test loss is 0.000780086243304844\n",
      "Batch: 38800,train loss is: 0.0011787429449740233\n",
      "test loss is 0.0010741737221910941\n",
      "Batch: 38900,train loss is: 0.0010960859952014975\n",
      "test loss is 0.0007157200929219246\n",
      "Batch: 39000,train loss is: 0.0005511203614007736\n",
      "test loss is 0.0006823105376854904\n",
      "Batch: 39100,train loss is: 0.000634635196368516\n",
      "test loss is 0.000692139410202678\n",
      "Batch: 39200,train loss is: 0.0011510653645940773\n",
      "test loss is 0.0009287977056394926\n",
      "Batch: 39300,train loss is: 0.0006044769812662095\n",
      "test loss is 0.0006977048911784187\n",
      "Batch: 39400,train loss is: 0.00040434002186367265\n",
      "test loss is 0.0006160592951096642\n",
      "Batch: 39500,train loss is: 0.0009329558998177541\n",
      "test loss is 0.0006976854460374719\n",
      "Batch: 39600,train loss is: 0.0007272913423043188\n",
      "test loss is 0.0008283686601637761\n",
      "Batch: 39700,train loss is: 0.0005756915281000662\n",
      "test loss is 0.0007529076069177483\n",
      "Batch: 39800,train loss is: 0.0008639363467184497\n",
      "test loss is 0.0011523262527551056\n",
      "Batch: 39900,train loss is: 0.00043829243257594133\n",
      "test loss is 0.0007142539286489821\n",
      "Batch: 40000,train loss is: 0.0006057086830976407\n",
      "test loss is 0.0011967358803987316\n",
      "Batch: 40100,train loss is: 0.0005620081962199695\n",
      "test loss is 0.001044449756413507\n",
      "Batch: 40200,train loss is: 0.0006427466148360855\n",
      "test loss is 0.0006172093801947141\n",
      "Batch: 40300,train loss is: 0.000387240385408778\n",
      "test loss is 0.0007804755413591231\n",
      "Batch: 40400,train loss is: 0.0009216906467899818\n",
      "test loss is 0.0008183386692175791\n",
      "Batch: 40500,train loss is: 0.0005420100215082493\n",
      "test loss is 0.0006192353774237112\n",
      "Batch: 40600,train loss is: 0.0008530251972481968\n",
      "test loss is 0.0011060283026282452\n",
      "Batch: 40700,train loss is: 0.000730280024066146\n",
      "test loss is 0.0008605725188504463\n",
      "Batch: 40800,train loss is: 0.0012609033994720428\n",
      "test loss is 0.0006998618126805473\n",
      "Batch: 40900,train loss is: 0.0008676070338658275\n",
      "test loss is 0.0009250076749472937\n",
      "Batch: 41000,train loss is: 0.0007859092826497748\n",
      "test loss is 0.0006732165331167432\n",
      "Batch: 41100,train loss is: 0.0007187109173146615\n",
      "test loss is 0.0007582044844743704\n",
      "Batch: 41200,train loss is: 0.0005277751620990552\n",
      "test loss is 0.0007354143489788444\n",
      "Batch: 41300,train loss is: 0.0011638751700152683\n",
      "test loss is 0.0006550931904717321\n",
      "Batch: 41400,train loss is: 0.0008599928463412333\n",
      "test loss is 0.0007502852232667517\n",
      "Batch: 41500,train loss is: 0.0017981520342922502\n",
      "test loss is 0.0012223069024243224\n",
      "Batch: 41600,train loss is: 0.0008825489296713372\n",
      "test loss is 0.0006364795897017778\n",
      "Batch: 41700,train loss is: 0.0007734689605321714\n",
      "test loss is 0.0010015483686490114\n",
      "Batch: 41800,train loss is: 0.0008680034533266531\n",
      "test loss is 0.0006867661512575896\n",
      "Batch: 41900,train loss is: 0.0015526884040241314\n",
      "test loss is 0.0006870250975733316\n",
      "Batch: 42000,train loss is: 0.0007437325111758065\n",
      "test loss is 0.0006886374085819512\n",
      "Batch: 42100,train loss is: 0.0009052774361721336\n",
      "test loss is 0.0013400382509052655\n",
      "Batch: 42200,train loss is: 0.0007456290562598685\n",
      "test loss is 0.0007246970277290511\n",
      "Batch: 42300,train loss is: 0.0005347146216492371\n",
      "test loss is 0.0006317255648326043\n",
      "Batch: 42400,train loss is: 0.0007871135979163814\n",
      "test loss is 0.0008119966843022847\n",
      "Batch: 42500,train loss is: 0.0017190627941920432\n",
      "test loss is 0.0009079180908681869\n",
      "Batch: 42600,train loss is: 0.001484069649894385\n",
      "test loss is 0.0013451924702971817\n",
      "Batch: 42700,train loss is: 0.0010143715858944983\n",
      "test loss is 0.0008011495084898079\n",
      "Batch: 42800,train loss is: 0.000810242425578728\n",
      "test loss is 0.0007139771563507688\n",
      "Batch: 42900,train loss is: 0.000428624665333657\n",
      "test loss is 0.0007689368784440197\n",
      "Batch: 43000,train loss is: 0.0010200064004820108\n",
      "test loss is 0.0008289231140047872\n",
      "Batch: 43100,train loss is: 0.000611937462265346\n",
      "test loss is 0.001001276559276117\n",
      "Batch: 43200,train loss is: 0.00037192799772658167\n",
      "test loss is 0.0006585290813796421\n",
      "Batch: 43300,train loss is: 0.0007774465751368275\n",
      "test loss is 0.0008059837104740726\n",
      "Batch: 43400,train loss is: 0.0012197672150671176\n",
      "test loss is 0.0007066037990118347\n",
      "Batch: 43500,train loss is: 0.0017277684416348248\n",
      "test loss is 0.0006417381343437271\n",
      "Batch: 43600,train loss is: 0.0009786793839068465\n",
      "test loss is 0.000719852469896481\n",
      "Batch: 43700,train loss is: 0.00038592713803139794\n",
      "test loss is 0.000730080565947068\n",
      "Batch: 43800,train loss is: 0.000911056398763498\n",
      "test loss is 0.0009053429423281832\n",
      "Batch: 43900,train loss is: 0.0013055668084885012\n",
      "test loss is 0.0006659481617045833\n",
      "Batch: 44000,train loss is: 0.0008780830295978988\n",
      "test loss is 0.0006686135968768233\n",
      "Batch: 44100,train loss is: 0.001085492250791337\n",
      "test loss is 0.0010017246465077595\n",
      "Batch: 44200,train loss is: 0.0005895615200337979\n",
      "test loss is 0.0006847705658180232\n",
      "Batch: 44300,train loss is: 0.0004188607568991309\n",
      "test loss is 0.0007250567466496261\n",
      "Batch: 44400,train loss is: 0.0007583808472796802\n",
      "test loss is 0.0008321441332669196\n",
      "Batch: 44500,train loss is: 0.0007581966506581887\n",
      "test loss is 0.0011022180178094229\n",
      "Batch: 44600,train loss is: 0.0006121352138417533\n",
      "test loss is 0.0005947599665533754\n",
      "Batch: 44700,train loss is: 0.0006246172247934928\n",
      "test loss is 0.0006754490395415868\n",
      "Batch: 44800,train loss is: 0.0008109072154529923\n",
      "test loss is 0.0006810651407280624\n",
      "Batch: 44900,train loss is: 0.000576831551747448\n",
      "test loss is 0.0007555370760222997\n",
      "Batch: 45000,train loss is: 0.0007255655285267119\n",
      "test loss is 0.0006277442282506495\n",
      "Batch: 45100,train loss is: 0.0004715563469843251\n",
      "test loss is 0.0006928279242536371\n",
      "Batch: 45200,train loss is: 0.0009484501928180087\n",
      "test loss is 0.0006886825078117591\n",
      "Batch: 45300,train loss is: 0.0005253841496782891\n",
      "test loss is 0.0006786705359580453\n",
      "Batch: 45400,train loss is: 0.0011751303488798986\n",
      "test loss is 0.0008706486002491822\n",
      "Batch: 45500,train loss is: 0.00040959089758261255\n",
      "test loss is 0.000668278699450759\n",
      "Batch: 45600,train loss is: 0.0004289995926695153\n",
      "test loss is 0.0007501071273557951\n",
      "Batch: 45700,train loss is: 0.0017656220730642258\n",
      "test loss is 0.001295776176453934\n",
      "Batch: 45800,train loss is: 0.0007735542064586359\n",
      "test loss is 0.0009443815060121436\n",
      "Batch: 45900,train loss is: 0.0009835597146724754\n",
      "test loss is 0.001121251841564396\n",
      "Batch: 46000,train loss is: 0.0006645480760038892\n",
      "test loss is 0.00060251550377751\n",
      "Batch: 46100,train loss is: 0.00046438687478962063\n",
      "test loss is 0.0007273443363859534\n",
      "Batch: 46200,train loss is: 0.00055334466995374\n",
      "test loss is 0.0006335146421760354\n",
      "Batch: 46300,train loss is: 0.000847111003453921\n",
      "test loss is 0.0006343497274468556\n",
      "Batch: 46400,train loss is: 0.0009186314473091454\n",
      "test loss is 0.0006249276694260445\n",
      "Batch: 46500,train loss is: 0.0006309249455794187\n",
      "test loss is 0.0008160074989139728\n",
      "Batch: 46600,train loss is: 0.0010447944851195874\n",
      "test loss is 0.0006713883119377262\n",
      "Batch: 46700,train loss is: 0.0007022969825115381\n",
      "test loss is 0.0006995271334540764\n",
      "-----------------------Epoch: 2----------------------------------\n",
      "Batch: 0,train loss is: 0.0004860999631699913\n",
      "test loss is 0.0006058829380999436\n",
      "Batch: 100,train loss is: 0.00928605724242675\n",
      "test loss is 0.007491983017689009\n",
      "Batch: 200,train loss is: 0.0006631611483071491\n",
      "test loss is 0.0007962042319075453\n",
      "Batch: 300,train loss is: 0.0009774652167884043\n",
      "test loss is 0.0007287737313095625\n",
      "Batch: 400,train loss is: 0.0007151854464957617\n",
      "test loss is 0.0007529619104728568\n",
      "Batch: 500,train loss is: 0.0006853475424674434\n",
      "test loss is 0.0007175061978141115\n",
      "Batch: 600,train loss is: 0.0007999272226822784\n",
      "test loss is 0.0010382620114573174\n",
      "Batch: 700,train loss is: 0.0005072457884630655\n",
      "test loss is 0.0008796998535189338\n",
      "Batch: 800,train loss is: 0.0006011740648762432\n",
      "test loss is 0.0007527369846991695\n",
      "Batch: 900,train loss is: 0.0005940209638937662\n",
      "test loss is 0.0007318419525276863\n",
      "Batch: 1000,train loss is: 0.0005233250097401178\n",
      "test loss is 0.000761842701660171\n",
      "Batch: 1100,train loss is: 0.0006795001673770751\n",
      "test loss is 0.0007692199896523491\n",
      "Batch: 1200,train loss is: 0.0013218892410467096\n",
      "test loss is 0.0006515696713110902\n",
      "Batch: 1300,train loss is: 0.0010399519593638847\n",
      "test loss is 0.0011113317325431636\n",
      "Batch: 1400,train loss is: 0.000651503882587071\n",
      "test loss is 0.0008764648353168673\n",
      "Batch: 1500,train loss is: 0.0007419294518546131\n",
      "test loss is 0.000638924409781885\n",
      "Batch: 1600,train loss is: 0.0009184948090055982\n",
      "test loss is 0.0006841280363431722\n",
      "Batch: 1700,train loss is: 0.0006767188687216634\n",
      "test loss is 0.0006773684118082043\n",
      "Batch: 1800,train loss is: 0.0013594995856664868\n",
      "test loss is 0.0017749823916167798\n",
      "Batch: 1900,train loss is: 0.0006616915980742848\n",
      "test loss is 0.0006714313404700762\n",
      "Batch: 2000,train loss is: 0.0006331394176513712\n",
      "test loss is 0.0006247995724815218\n",
      "Batch: 2100,train loss is: 0.0014530661407570291\n",
      "test loss is 0.0005979783281653934\n",
      "Batch: 2200,train loss is: 0.0007691813535525112\n",
      "test loss is 0.0007714692297590495\n",
      "Batch: 2300,train loss is: 0.0006766817391195882\n",
      "test loss is 0.0006711489575285576\n",
      "Batch: 2400,train loss is: 0.0032890406153656916\n",
      "test loss is 0.0015247197388636065\n",
      "Batch: 2500,train loss is: 0.0006062530899335544\n",
      "test loss is 0.0006183556095741053\n",
      "Batch: 2600,train loss is: 0.0019080273515735314\n",
      "test loss is 0.001248484442438584\n",
      "Batch: 2700,train loss is: 0.0020773109458476083\n",
      "test loss is 0.0008948070513890545\n",
      "Batch: 2800,train loss is: 0.000667741663941116\n",
      "test loss is 0.000638521067147482\n",
      "Batch: 2900,train loss is: 0.000613121549896415\n",
      "test loss is 0.000673182023858832\n",
      "Batch: 3000,train loss is: 0.0007004966575907387\n",
      "test loss is 0.0005747846711885215\n",
      "Batch: 3100,train loss is: 0.0008300519911381225\n",
      "test loss is 0.0008329442800688823\n",
      "Batch: 3200,train loss is: 0.0012332297096663072\n",
      "test loss is 0.0009805641585296893\n",
      "Batch: 3300,train loss is: 0.0006045368415352826\n",
      "test loss is 0.0007036924946190291\n",
      "Batch: 3400,train loss is: 0.0009058527225086964\n",
      "test loss is 0.0006714761786154463\n",
      "Batch: 3500,train loss is: 0.0010801584588655563\n",
      "test loss is 0.000990832772689083\n",
      "Batch: 3600,train loss is: 0.0005205474497989565\n",
      "test loss is 0.0008383633126485581\n",
      "Batch: 3700,train loss is: 0.000806981151860046\n",
      "test loss is 0.0009886792954571784\n",
      "Batch: 3800,train loss is: 0.000354726779106714\n",
      "test loss is 0.0006077054995913766\n",
      "Batch: 3900,train loss is: 0.0018223548106116464\n",
      "test loss is 0.0011749018296190183\n",
      "Batch: 4000,train loss is: 0.0003597926659646844\n",
      "test loss is 0.0005992192217106973\n",
      "Batch: 4100,train loss is: 0.0012469190224066147\n",
      "test loss is 0.0007918331862375281\n",
      "Batch: 4200,train loss is: 0.0005900948941617728\n",
      "test loss is 0.0007167517313319032\n",
      "Batch: 4300,train loss is: 0.0006565799988438704\n",
      "test loss is 0.0008964404622857629\n",
      "Batch: 4400,train loss is: 0.0004859333606405443\n",
      "test loss is 0.0005460683231954933\n",
      "Batch: 4500,train loss is: 0.0003910045704879687\n",
      "test loss is 0.0007657311058996557\n",
      "Batch: 4600,train loss is: 0.000410225753709442\n",
      "test loss is 0.0006191031563603066\n",
      "Batch: 4700,train loss is: 0.00046401318227131267\n",
      "test loss is 0.0006052031548613621\n",
      "Batch: 4800,train loss is: 0.001911802749997053\n",
      "test loss is 0.0009759947971744584\n",
      "Batch: 4900,train loss is: 0.0008677385212424037\n",
      "test loss is 0.0007867351377045059\n",
      "Batch: 5000,train loss is: 0.0009182428528616966\n",
      "test loss is 0.0008083507765081075\n",
      "Batch: 5100,train loss is: 0.0006986214205918417\n",
      "test loss is 0.0006553678648678625\n",
      "Batch: 5200,train loss is: 0.0014837163671242934\n",
      "test loss is 0.0009572430075482775\n",
      "Batch: 5300,train loss is: 0.0007905788159023442\n",
      "test loss is 0.0008333992596184372\n",
      "Batch: 5400,train loss is: 0.0004992923447904493\n",
      "test loss is 0.0006947539152110707\n",
      "Batch: 5500,train loss is: 0.0007266507055677396\n",
      "test loss is 0.0007229008506235455\n",
      "Batch: 5600,train loss is: 0.0006769693070206777\n",
      "test loss is 0.0008109716209116089\n",
      "Batch: 5700,train loss is: 0.001210266305433104\n",
      "test loss is 0.0011205757328446528\n",
      "Batch: 5800,train loss is: 0.0007039644073366121\n",
      "test loss is 0.0010192022341114974\n",
      "Batch: 5900,train loss is: 0.0009797582163921254\n",
      "test loss is 0.0006613099937840351\n",
      "Batch: 6000,train loss is: 0.0007473636057851354\n",
      "test loss is 0.000559887588250362\n",
      "Batch: 6100,train loss is: 0.0007401919663121491\n",
      "test loss is 0.0006433000031966968\n",
      "Batch: 6200,train loss is: 0.0008841823904530408\n",
      "test loss is 0.0008603761484118117\n",
      "Batch: 6300,train loss is: 0.0004239704985947207\n",
      "test loss is 0.0005696875739906345\n",
      "Batch: 6400,train loss is: 0.0008750573072708997\n",
      "test loss is 0.000913449021498942\n",
      "Batch: 6500,train loss is: 0.0007023930792916031\n",
      "test loss is 0.0008169631356679883\n",
      "Batch: 6600,train loss is: 0.0005524659980861886\n",
      "test loss is 0.000783972259890418\n",
      "Batch: 6700,train loss is: 0.0006385526554090056\n",
      "test loss is 0.0005928972948483322\n",
      "Batch: 6800,train loss is: 0.0009802918072889717\n",
      "test loss is 0.0010232532792854457\n",
      "Batch: 6900,train loss is: 0.001257227073669362\n",
      "test loss is 0.0007362177482349333\n",
      "Batch: 7000,train loss is: 0.0008415234031168786\n",
      "test loss is 0.000691300878945577\n",
      "Batch: 7100,train loss is: 0.0019354906384831025\n",
      "test loss is 0.0012993181482277537\n",
      "Batch: 7200,train loss is: 0.00042499784856122604\n",
      "test loss is 0.0005247413889404419\n",
      "Batch: 7300,train loss is: 0.00043277241589751694\n",
      "test loss is 0.0008721521100908991\n",
      "Batch: 7400,train loss is: 0.000563847249121215\n",
      "test loss is 0.0006246502852959218\n",
      "Batch: 7500,train loss is: 0.0011220221190926223\n",
      "test loss is 0.0007559822780800103\n",
      "Batch: 7600,train loss is: 0.0008745894361390784\n",
      "test loss is 0.0007798724561556573\n",
      "Batch: 7700,train loss is: 0.0005638118786317886\n",
      "test loss is 0.0006919459460969355\n",
      "Batch: 7800,train loss is: 0.000968002216285295\n",
      "test loss is 0.0006514560844672026\n",
      "Batch: 7900,train loss is: 0.0006914437932973908\n",
      "test loss is 0.0007457455344157529\n",
      "Batch: 8000,train loss is: 0.00048581429819606885\n",
      "test loss is 0.0006394374142314381\n",
      "Batch: 8100,train loss is: 0.0011540878866811342\n",
      "test loss is 0.0007259108358658584\n",
      "Batch: 8200,train loss is: 0.0006791060795916649\n",
      "test loss is 0.0008129133664714162\n",
      "Batch: 8300,train loss is: 0.0019158412595005388\n",
      "test loss is 0.0005982343843789864\n",
      "Batch: 8400,train loss is: 0.000922380477344837\n",
      "test loss is 0.0007102558134652788\n",
      "Batch: 8500,train loss is: 0.0013174403463750669\n",
      "test loss is 0.0009121809719387383\n",
      "Batch: 8600,train loss is: 0.0009278780743626906\n",
      "test loss is 0.0006729599128436956\n",
      "Batch: 8700,train loss is: 0.0007182087443936869\n",
      "test loss is 0.0007301731376845383\n",
      "Batch: 8800,train loss is: 0.0005892736232075474\n",
      "test loss is 0.0008429475209849891\n",
      "Batch: 8900,train loss is: 0.0017918115417831044\n",
      "test loss is 0.0029907359101527253\n",
      "Batch: 9000,train loss is: 0.000584476532742398\n",
      "test loss is 0.0005878283350792106\n",
      "Batch: 9100,train loss is: 0.000581675393090937\n",
      "test loss is 0.0007521186444909185\n",
      "Batch: 9200,train loss is: 0.0006293529131201226\n",
      "test loss is 0.0008532788813822428\n",
      "Batch: 9300,train loss is: 0.000980968222585012\n",
      "test loss is 0.0006506055857674754\n",
      "Batch: 9400,train loss is: 0.0006346030846355854\n",
      "test loss is 0.0006409282789305605\n",
      "Batch: 9500,train loss is: 0.000529127845412886\n",
      "test loss is 0.000634347881774168\n",
      "Batch: 9600,train loss is: 0.0008710037512391477\n",
      "test loss is 0.0006748487808676659\n",
      "Batch: 9700,train loss is: 0.0005856944325885377\n",
      "test loss is 0.0007123584750411306\n",
      "Batch: 9800,train loss is: 0.0008033364225593532\n",
      "test loss is 0.0005927778332103485\n",
      "Batch: 9900,train loss is: 0.0005641325230051272\n",
      "test loss is 0.0008838704981412407\n",
      "Batch: 10000,train loss is: 0.0006636884213006429\n",
      "test loss is 0.0009737871673377396\n",
      "Batch: 10100,train loss is: 0.0007203195552633713\n",
      "test loss is 0.0010028349603806513\n",
      "Batch: 10200,train loss is: 0.0007662741365901208\n",
      "test loss is 0.0007862700982977712\n",
      "Batch: 10300,train loss is: 0.0008576646613406146\n",
      "test loss is 0.00067094540201954\n",
      "Batch: 10400,train loss is: 0.0006089353079423218\n",
      "test loss is 0.0006190941484203348\n",
      "Batch: 10500,train loss is: 0.000697937698961275\n",
      "test loss is 0.000672927171739842\n",
      "Batch: 10600,train loss is: 0.0010347733751364695\n",
      "test loss is 0.0011072398461116576\n",
      "Batch: 10700,train loss is: 0.0004066034278131821\n",
      "test loss is 0.0006731564918721323\n",
      "Batch: 10800,train loss is: 0.0006877431948700635\n",
      "test loss is 0.0005648855776175407\n",
      "Batch: 10900,train loss is: 0.0009533205521301885\n",
      "test loss is 0.0009728667938506875\n",
      "Batch: 11000,train loss is: 0.0008363434412857055\n",
      "test loss is 0.000894162557526922\n",
      "Batch: 11100,train loss is: 0.0005790199413359406\n",
      "test loss is 0.0007690063654075709\n",
      "Batch: 11200,train loss is: 0.00039720033981523916\n",
      "test loss is 0.0007995834383026118\n",
      "Batch: 11300,train loss is: 0.0006896919473495085\n",
      "test loss is 0.000576478971922661\n",
      "Batch: 11400,train loss is: 0.0014965208774315474\n",
      "test loss is 0.0006848960898172756\n",
      "Batch: 11500,train loss is: 0.0007395909163536797\n",
      "test loss is 0.0008501541473343247\n",
      "Batch: 11600,train loss is: 0.0009402020992298946\n",
      "test loss is 0.0007300998694464317\n",
      "Batch: 11700,train loss is: 0.0002980047410212822\n",
      "test loss is 0.0005918879866512373\n",
      "Batch: 11800,train loss is: 0.0006894807759036793\n",
      "test loss is 0.0006729511864859573\n",
      "Batch: 11900,train loss is: 0.0006718412777134929\n",
      "test loss is 0.0005602960543402468\n",
      "Batch: 12000,train loss is: 0.0014208767946353902\n",
      "test loss is 0.0008730940763690802\n",
      "Batch: 12100,train loss is: 0.0008065723322488726\n",
      "test loss is 0.0006628165503443827\n",
      "Batch: 12200,train loss is: 0.0006181532364208979\n",
      "test loss is 0.0006595887351990566\n",
      "Batch: 12300,train loss is: 0.0004933672950539016\n",
      "test loss is 0.000580703986920997\n",
      "Batch: 12400,train loss is: 0.00042845386162801516\n",
      "test loss is 0.0006440712373143269\n",
      "Batch: 12500,train loss is: 0.0005747252910252539\n",
      "test loss is 0.0007787665514108434\n",
      "Batch: 12600,train loss is: 0.0010376406783265619\n",
      "test loss is 0.0012343458201610476\n",
      "Batch: 12700,train loss is: 0.0015541049143932002\n",
      "test loss is 0.000732722712875591\n",
      "Batch: 12800,train loss is: 0.0007220553788821976\n",
      "test loss is 0.0006961366549047272\n",
      "Batch: 12900,train loss is: 0.00041396074729888055\n",
      "test loss is 0.0006492519766710228\n",
      "Batch: 13000,train loss is: 0.0008640132064844975\n",
      "test loss is 0.0006966018863638024\n",
      "Batch: 13100,train loss is: 0.0005745462672068159\n",
      "test loss is 0.000644251597742999\n",
      "Batch: 13200,train loss is: 0.0006092639250516085\n",
      "test loss is 0.0006867534202472264\n",
      "Batch: 13300,train loss is: 0.0008857014840043915\n",
      "test loss is 0.0006654171610237235\n",
      "Batch: 13400,train loss is: 0.000703267272764549\n",
      "test loss is 0.000595355043224409\n",
      "Batch: 13500,train loss is: 0.0005522867223501954\n",
      "test loss is 0.0006786053354980535\n",
      "Batch: 13600,train loss is: 0.0009077400167225272\n",
      "test loss is 0.0008923459350076664\n",
      "Batch: 13700,train loss is: 0.0005333384285193929\n",
      "test loss is 0.0010068994928594038\n",
      "Batch: 13800,train loss is: 0.0006075426553302653\n",
      "test loss is 0.0005740695468137408\n",
      "Batch: 13900,train loss is: 0.0005546484228874077\n",
      "test loss is 0.0006054425175492199\n",
      "Batch: 14000,train loss is: 0.00038829604523645434\n",
      "test loss is 0.0005863577146990847\n",
      "Batch: 14100,train loss is: 0.0008470340717150972\n",
      "test loss is 0.0008152430822352914\n",
      "Batch: 14200,train loss is: 0.000990283162644382\n",
      "test loss is 0.0013349463190411425\n",
      "Batch: 14300,train loss is: 0.00042414492849682055\n",
      "test loss is 0.0006817017985466683\n",
      "Batch: 14400,train loss is: 0.0005730964617324002\n",
      "test loss is 0.0007213608323681292\n",
      "Batch: 14500,train loss is: 0.001177449092801679\n",
      "test loss is 0.0017637814583704066\n",
      "Batch: 14600,train loss is: 0.0020183339782326535\n",
      "test loss is 0.001025615690245053\n",
      "Batch: 14700,train loss is: 0.0004575377962063766\n",
      "test loss is 0.0007147146943742109\n",
      "Batch: 14800,train loss is: 0.0005421040435145766\n",
      "test loss is 0.00085182738948159\n",
      "Batch: 14900,train loss is: 0.0010562820691532588\n",
      "test loss is 0.0007021718076773524\n",
      "Batch: 15000,train loss is: 0.000988784287824941\n",
      "test loss is 0.001512180293201663\n",
      "Batch: 15100,train loss is: 0.0009575819227546111\n",
      "test loss is 0.0007411833234071268\n",
      "Batch: 15200,train loss is: 0.0006057542970706001\n",
      "test loss is 0.0006696718539987658\n",
      "Batch: 15300,train loss is: 0.0008347750777371149\n",
      "test loss is 0.0008499641035334331\n",
      "Batch: 15400,train loss is: 0.0007798205035089161\n",
      "test loss is 0.0005809383039805462\n",
      "Batch: 15500,train loss is: 0.0007525105675657727\n",
      "test loss is 0.0006065091483816809\n",
      "Batch: 15600,train loss is: 0.0005686468928257254\n",
      "test loss is 0.0007260694753851116\n",
      "Batch: 15700,train loss is: 0.000842266863742521\n",
      "test loss is 0.0007255963335922204\n",
      "Batch: 15800,train loss is: 0.00043981314524131127\n",
      "test loss is 0.0006513380617553686\n",
      "Batch: 15900,train loss is: 0.0005613760220418397\n",
      "test loss is 0.0006096382106266288\n",
      "Batch: 16000,train loss is: 0.0014187614296231276\n",
      "test loss is 0.0011650141097808897\n",
      "Batch: 16100,train loss is: 0.0005729831250428699\n",
      "test loss is 0.0006731808078288381\n",
      "Batch: 16200,train loss is: 0.0008743259699628114\n",
      "test loss is 0.0007326572985175621\n",
      "Batch: 16300,train loss is: 0.0009779353491684091\n",
      "test loss is 0.0009489089294976455\n",
      "Batch: 16400,train loss is: 0.0006774316429670813\n",
      "test loss is 0.0007305398299897685\n",
      "Batch: 16500,train loss is: 0.0005857410954205107\n",
      "test loss is 0.0007365262133398122\n",
      "Batch: 16600,train loss is: 0.0004531377761524026\n",
      "test loss is 0.0006385758978027748\n",
      "Batch: 16700,train loss is: 0.0009238759953084054\n",
      "test loss is 0.0005724358379901345\n",
      "Batch: 16800,train loss is: 0.0004524308142143281\n",
      "test loss is 0.0005789251516563494\n",
      "Batch: 16900,train loss is: 0.0007684503917919914\n",
      "test loss is 0.0008783036540990092\n",
      "Batch: 17000,train loss is: 0.0009262288546705281\n",
      "test loss is 0.0006187277206892315\n",
      "Batch: 17100,train loss is: 0.0011066042587559056\n",
      "test loss is 0.0007476986083788245\n",
      "Batch: 17200,train loss is: 0.0005466877392014995\n",
      "test loss is 0.0006058269282420163\n",
      "Batch: 17300,train loss is: 0.0006064593594620286\n",
      "test loss is 0.0007241560761959434\n",
      "Batch: 17400,train loss is: 0.0005216295791070371\n",
      "test loss is 0.0009160570400025631\n",
      "Batch: 17500,train loss is: 0.000492867171715822\n",
      "test loss is 0.0009204675719126312\n",
      "Batch: 17600,train loss is: 0.0005484292440440808\n",
      "test loss is 0.0005746194676235152\n",
      "Batch: 17700,train loss is: 0.0006057736973557708\n",
      "test loss is 0.0006568456032259406\n",
      "Batch: 17800,train loss is: 0.0012339557389108385\n",
      "test loss is 0.0006563958121551508\n",
      "Batch: 17900,train loss is: 0.0005686756696564861\n",
      "test loss is 0.0007166355889499128\n",
      "Batch: 18000,train loss is: 0.000769680821348919\n",
      "test loss is 0.0006820045963499588\n",
      "Batch: 18100,train loss is: 0.0004448219770157607\n",
      "test loss is 0.000610520445174042\n",
      "Batch: 18200,train loss is: 0.0013982787790731744\n",
      "test loss is 0.0025315392395857173\n",
      "Batch: 18300,train loss is: 0.0007208702713222806\n",
      "test loss is 0.0007656648909153362\n",
      "Batch: 18400,train loss is: 0.000570485704597351\n",
      "test loss is 0.000631381017437857\n",
      "Batch: 18500,train loss is: 0.0004557189065826379\n",
      "test loss is 0.0005833390756297623\n",
      "Batch: 18600,train loss is: 0.00045595036299285826\n",
      "test loss is 0.0006439863450482458\n",
      "Batch: 18700,train loss is: 0.0005890403859125206\n",
      "test loss is 0.0006658679342287216\n",
      "Batch: 18800,train loss is: 0.0012021049976853621\n",
      "test loss is 0.0010264937112912467\n",
      "Batch: 18900,train loss is: 0.0004927366171538852\n",
      "test loss is 0.0006122995736993704\n",
      "Batch: 19000,train loss is: 0.0004353490881826249\n",
      "test loss is 0.0012996132982311585\n",
      "Batch: 19100,train loss is: 0.0005832433181041437\n",
      "test loss is 0.0007355262590660386\n",
      "Batch: 19200,train loss is: 0.0004984885629167369\n",
      "test loss is 0.0006008498994274662\n",
      "Batch: 19300,train loss is: 0.0008179139827783379\n",
      "test loss is 0.0008503906781654318\n",
      "Batch: 19400,train loss is: 0.0005958156378518825\n",
      "test loss is 0.0006195374309298951\n",
      "Batch: 19500,train loss is: 0.000498919814654872\n",
      "test loss is 0.0006125015939468458\n",
      "Batch: 19600,train loss is: 0.00043608686518278015\n",
      "test loss is 0.0005909193061982734\n",
      "Batch: 19700,train loss is: 0.00033845581654414316\n",
      "test loss is 0.0006337679949196614\n",
      "Batch: 19800,train loss is: 0.0004756658647698551\n",
      "test loss is 0.0007356953207083497\n",
      "Batch: 19900,train loss is: 0.0006297953771765849\n",
      "test loss is 0.0005862481390048819\n",
      "Batch: 20000,train loss is: 0.0006704940876265778\n",
      "test loss is 0.0006028325159447561\n",
      "Batch: 20100,train loss is: 0.0010498648547212592\n",
      "test loss is 0.0009332635474443409\n",
      "Batch: 20200,train loss is: 0.0008961001378619418\n",
      "test loss is 0.0006314941874004672\n",
      "Batch: 20300,train loss is: 0.0002705048633815744\n",
      "test loss is 0.0010826126687614435\n",
      "Batch: 20400,train loss is: 0.000648161540869603\n",
      "test loss is 0.0008702654604168862\n",
      "Batch: 20500,train loss is: 0.0006985531519158718\n",
      "test loss is 0.0007377261737345517\n",
      "Batch: 20600,train loss is: 0.0005045026213769754\n",
      "test loss is 0.0006246071232392775\n",
      "Batch: 20700,train loss is: 0.0007948049834862116\n",
      "test loss is 0.0007166652557725899\n",
      "Batch: 20800,train loss is: 0.001175441402500046\n",
      "test loss is 0.0007738826848915789\n",
      "Batch: 20900,train loss is: 0.00047792196979538546\n",
      "test loss is 0.0007831870390781308\n",
      "Batch: 21000,train loss is: 0.0005896260607076089\n",
      "test loss is 0.0007836049766216052\n",
      "Batch: 21100,train loss is: 0.0008445008952806233\n",
      "test loss is 0.000573278773383041\n",
      "Batch: 21200,train loss is: 0.0005997574184506028\n",
      "test loss is 0.0007766454628216246\n",
      "Batch: 21300,train loss is: 0.00044769704668384586\n",
      "test loss is 0.000620271005006199\n",
      "Batch: 21400,train loss is: 0.0005512792151917943\n",
      "test loss is 0.0010191601014940602\n",
      "Batch: 21500,train loss is: 0.0004733578998481555\n",
      "test loss is 0.0007660942384302135\n",
      "Batch: 21600,train loss is: 0.0006711380808404194\n",
      "test loss is 0.0006169951836368818\n",
      "Batch: 21700,train loss is: 0.0009357004943131373\n",
      "test loss is 0.0009757241273306017\n",
      "Batch: 21800,train loss is: 0.0004995364607335628\n",
      "test loss is 0.0007017601389568464\n",
      "Batch: 21900,train loss is: 0.0007469072927687218\n",
      "test loss is 0.0007227957156109064\n",
      "Batch: 22000,train loss is: 0.0004917316190499757\n",
      "test loss is 0.0006751402001427594\n",
      "Batch: 22100,train loss is: 0.0005608983136051127\n",
      "test loss is 0.0006063756435744746\n",
      "Batch: 22200,train loss is: 0.0005388238193892822\n",
      "test loss is 0.000592342450820095\n",
      "Batch: 22300,train loss is: 0.000439582297653969\n",
      "test loss is 0.0005480079558722388\n",
      "Batch: 22400,train loss is: 0.0007060615702439591\n",
      "test loss is 0.0010536367930982725\n",
      "Batch: 22500,train loss is: 0.0008222913371914379\n",
      "test loss is 0.0009128322374165925\n",
      "Batch: 22600,train loss is: 0.000682454396598271\n",
      "test loss is 0.0006096070939549399\n",
      "Batch: 22700,train loss is: 0.0015487866486712605\n",
      "test loss is 0.0005211273295737732\n",
      "Batch: 22800,train loss is: 0.0006062799769728715\n",
      "test loss is 0.0005166071648888151\n",
      "Batch: 22900,train loss is: 0.0010788401931339868\n",
      "test loss is 0.000796640916174961\n",
      "Batch: 23000,train loss is: 0.0005106656753477817\n",
      "test loss is 0.0007198058899145965\n",
      "Batch: 23100,train loss is: 0.00034418265434356176\n",
      "test loss is 0.0005593945434071284\n",
      "Batch: 23200,train loss is: 0.0002980161820536307\n",
      "test loss is 0.0006732104070636608\n",
      "Batch: 23300,train loss is: 0.0006531338791705324\n",
      "test loss is 0.000561066561994479\n",
      "Batch: 23400,train loss is: 0.0006362100253648694\n",
      "test loss is 0.0006396008982348941\n",
      "Batch: 23500,train loss is: 0.0016335210107946269\n",
      "test loss is 0.0006621889084859338\n",
      "Batch: 23600,train loss is: 0.0005040801015344378\n",
      "test loss is 0.0005760253154464119\n",
      "Batch: 23700,train loss is: 0.0006031517421962843\n",
      "test loss is 0.0006518503719506328\n",
      "Batch: 23800,train loss is: 0.000624918796360301\n",
      "test loss is 0.0008148942932771282\n",
      "Batch: 23900,train loss is: 0.0010184158582596452\n",
      "test loss is 0.0011038980471052367\n",
      "Batch: 24000,train loss is: 0.00047702474829184303\n",
      "test loss is 0.0008922261224732767\n",
      "Batch: 24100,train loss is: 0.0007899492060075521\n",
      "test loss is 0.000790829074089592\n",
      "Batch: 24200,train loss is: 0.0006366296081451691\n",
      "test loss is 0.0005591412141463751\n",
      "Batch: 24300,train loss is: 0.0008090346912834922\n",
      "test loss is 0.0005738050230879337\n",
      "Batch: 24400,train loss is: 0.0010337830747574038\n",
      "test loss is 0.0006652998957858322\n",
      "Batch: 24500,train loss is: 0.0007487701463148942\n",
      "test loss is 0.0005522184413265938\n",
      "Batch: 24600,train loss is: 0.0005344530838430865\n",
      "test loss is 0.0005335601504369022\n",
      "Batch: 24700,train loss is: 0.0009256764847414048\n",
      "test loss is 0.0008665870027512611\n",
      "Batch: 24800,train loss is: 0.0003359203906503039\n",
      "test loss is 0.0005918692785943792\n",
      "Batch: 24900,train loss is: 0.00035849635693479696\n",
      "test loss is 0.0007398123738636727\n",
      "Batch: 25000,train loss is: 0.0008083850919819544\n",
      "test loss is 0.000542691474502681\n",
      "Batch: 25100,train loss is: 0.0006783325591069937\n",
      "test loss is 0.0005731390099771836\n",
      "Batch: 25200,train loss is: 0.0003958016927233174\n",
      "test loss is 0.0005172913108290419\n",
      "Batch: 25300,train loss is: 0.0007014603621125987\n",
      "test loss is 0.0007009075396447449\n",
      "Batch: 25400,train loss is: 0.0003131706490309093\n",
      "test loss is 0.0006961061836478178\n",
      "Batch: 25500,train loss is: 0.0006027958039908508\n",
      "test loss is 0.0006847916273650174\n",
      "Batch: 25600,train loss is: 0.0003610209993577372\n",
      "test loss is 0.0006612052712269579\n",
      "Batch: 25700,train loss is: 0.0007958720809063714\n",
      "test loss is 0.000861992821195688\n",
      "Batch: 25800,train loss is: 0.00038058172058003445\n",
      "test loss is 0.0007640857454880934\n",
      "Batch: 25900,train loss is: 0.00038633288884909467\n",
      "test loss is 0.0006515029082529589\n",
      "Batch: 26000,train loss is: 0.0007353779215929213\n",
      "test loss is 0.0005790165825786931\n",
      "Batch: 26100,train loss is: 0.0003880069237554407\n",
      "test loss is 0.0005804213883353708\n",
      "Batch: 26200,train loss is: 0.0007437187079092979\n",
      "test loss is 0.0008876919567052358\n",
      "Batch: 26300,train loss is: 0.0019005409309652177\n",
      "test loss is 0.0009061840545397722\n",
      "Batch: 26400,train loss is: 0.0006550861463436733\n",
      "test loss is 0.0009094711922086562\n",
      "Batch: 26500,train loss is: 0.0010873931770138776\n",
      "test loss is 0.0007697933539726647\n",
      "Batch: 26600,train loss is: 0.0007055444495991114\n",
      "test loss is 0.0007139393868227256\n",
      "Batch: 26700,train loss is: 0.0008037932730406215\n",
      "test loss is 0.0005700222456038808\n",
      "Batch: 26800,train loss is: 0.000852887365467942\n",
      "test loss is 0.0006215862494350706\n",
      "Batch: 26900,train loss is: 0.0003981658012121209\n",
      "test loss is 0.0006769084582070611\n",
      "Batch: 27000,train loss is: 0.002104711330749167\n",
      "test loss is 0.0007551826251475208\n",
      "Batch: 27100,train loss is: 0.0005193951648543512\n",
      "test loss is 0.0005614479055310182\n",
      "Batch: 27200,train loss is: 0.0011371894079192954\n",
      "test loss is 0.0008444830176745461\n",
      "Batch: 27300,train loss is: 0.0006526111128504491\n",
      "test loss is 0.0005845287237290355\n",
      "Batch: 27400,train loss is: 0.0006594573860021476\n",
      "test loss is 0.0005474747292951213\n",
      "Batch: 27500,train loss is: 0.00036939278314581444\n",
      "test loss is 0.0006828522671107506\n",
      "Batch: 27600,train loss is: 0.0005259293394974504\n",
      "test loss is 0.0006767157631643646\n",
      "Batch: 27700,train loss is: 0.0004559282326934896\n",
      "test loss is 0.0007692192277457523\n",
      "Batch: 27800,train loss is: 0.0010116336359234923\n",
      "test loss is 0.0006753608363730578\n",
      "Batch: 27900,train loss is: 0.000577488275732771\n",
      "test loss is 0.000595120527439596\n",
      "Batch: 28000,train loss is: 0.0006505565242515676\n",
      "test loss is 0.0007175574781444271\n",
      "Batch: 28100,train loss is: 0.0004674967656267216\n",
      "test loss is 0.0006257895516927496\n",
      "Batch: 28200,train loss is: 0.0017351044868474837\n",
      "test loss is 0.0005341359299189023\n",
      "Batch: 28300,train loss is: 0.0007813548726970118\n",
      "test loss is 0.0005870104195880811\n",
      "Batch: 28400,train loss is: 0.0007403463229452662\n",
      "test loss is 0.0006602526448954756\n",
      "Batch: 28500,train loss is: 0.0004950606197548259\n",
      "test loss is 0.0005709056569539066\n",
      "Batch: 28600,train loss is: 0.0005432415096873807\n",
      "test loss is 0.0006602694187358642\n",
      "Batch: 28700,train loss is: 0.0013037438702298205\n",
      "test loss is 0.001591551413710309\n",
      "Batch: 28800,train loss is: 0.0005037638624389341\n",
      "test loss is 0.0006269774127573348\n",
      "Batch: 28900,train loss is: 0.0005501204768932398\n",
      "test loss is 0.000640396090562392\n",
      "Batch: 29000,train loss is: 0.0004756808250476754\n",
      "test loss is 0.0006241010925537139\n",
      "Batch: 29100,train loss is: 0.0005759881788070245\n",
      "test loss is 0.0006354908432301076\n",
      "Batch: 29200,train loss is: 0.0007981795575602284\n",
      "test loss is 0.0007833742176675666\n",
      "Batch: 29300,train loss is: 0.0003257241847972934\n",
      "test loss is 0.0006108677671348195\n",
      "Batch: 29400,train loss is: 0.0009716276644112777\n",
      "test loss is 0.0005767664146250989\n",
      "Batch: 29500,train loss is: 0.0006224834947686469\n",
      "test loss is 0.0007440758484328157\n",
      "Batch: 29600,train loss is: 0.0005116279277146884\n",
      "test loss is 0.0007269847121372065\n",
      "Batch: 29700,train loss is: 0.0008187453713355826\n",
      "test loss is 0.0009224619596567731\n",
      "Batch: 29800,train loss is: 0.00048408924432371755\n",
      "test loss is 0.0006679058147814496\n",
      "Batch: 29900,train loss is: 0.0005016724734270215\n",
      "test loss is 0.0005706621011997744\n",
      "Batch: 30000,train loss is: 0.0006102426346169965\n",
      "test loss is 0.0005513129278217194\n",
      "Batch: 30100,train loss is: 0.0007475708928982045\n",
      "test loss is 0.000568414677768335\n",
      "Batch: 30200,train loss is: 0.000438973587348556\n",
      "test loss is 0.0006770213092580651\n",
      "Batch: 30300,train loss is: 0.00038027106391576597\n",
      "test loss is 0.0005091511354866371\n",
      "Batch: 30400,train loss is: 0.0004726184635019657\n",
      "test loss is 0.0005416042949883352\n",
      "Batch: 30500,train loss is: 0.0005175024488816683\n",
      "test loss is 0.0006205716344544675\n",
      "Batch: 30600,train loss is: 0.0007162829516461416\n",
      "test loss is 0.0005640424745166742\n",
      "Batch: 30700,train loss is: 0.0004817568893782711\n",
      "test loss is 0.0005354807314510482\n",
      "Batch: 30800,train loss is: 0.0004835573034745106\n",
      "test loss is 0.0006317493647162405\n",
      "Batch: 30900,train loss is: 0.0006864746737336013\n",
      "test loss is 0.0007585590808516371\n",
      "Batch: 31000,train loss is: 0.0005716084578553133\n",
      "test loss is 0.0007787806224593942\n",
      "Batch: 31100,train loss is: 0.0008054338870370598\n",
      "test loss is 0.0006387930240002934\n",
      "Batch: 31200,train loss is: 0.0012041733831335017\n",
      "test loss is 0.0008009377005143374\n",
      "Batch: 31300,train loss is: 0.00122173383800747\n",
      "test loss is 0.0009751500063868617\n",
      "Batch: 31400,train loss is: 0.0006091197418293555\n",
      "test loss is 0.0005508841552799362\n",
      "Batch: 31500,train loss is: 0.000605272671419599\n",
      "test loss is 0.0007284088541568796\n",
      "Batch: 31600,train loss is: 0.0005773179926670841\n",
      "test loss is 0.0008421483695877144\n",
      "Batch: 31700,train loss is: 0.0004674924722049157\n",
      "test loss is 0.0005424984500532368\n",
      "Batch: 31800,train loss is: 0.0005471730451162482\n",
      "test loss is 0.000526949662754191\n",
      "Batch: 31900,train loss is: 0.0005565261814277344\n",
      "test loss is 0.0005825608487438695\n",
      "Batch: 32000,train loss is: 0.0003708536549933725\n",
      "test loss is 0.000545521123918749\n",
      "Batch: 32100,train loss is: 0.0009096504430630276\n",
      "test loss is 0.000807971501466081\n",
      "Batch: 32200,train loss is: 0.0008863023506798324\n",
      "test loss is 0.00065265419210974\n",
      "Batch: 32300,train loss is: 0.0005926560685925804\n",
      "test loss is 0.0008004194126801306\n",
      "Batch: 32400,train loss is: 0.0007342113482567897\n",
      "test loss is 0.0008867088471082674\n",
      "Batch: 32500,train loss is: 0.0012363952020089332\n",
      "test loss is 0.0007554832770873557\n",
      "Batch: 32600,train loss is: 0.0006973609236815071\n",
      "test loss is 0.0007163015329891691\n",
      "Batch: 32700,train loss is: 0.0002862683031111238\n",
      "test loss is 0.0005602724298329841\n",
      "Batch: 32800,train loss is: 0.0008337139453467171\n",
      "test loss is 0.0006555980505945273\n",
      "Batch: 32900,train loss is: 0.00036228375050220577\n",
      "test loss is 0.000551977359982708\n",
      "Batch: 33000,train loss is: 0.00042145226663327433\n",
      "test loss is 0.0005135589905055061\n",
      "Batch: 33100,train loss is: 0.00040005640762927526\n",
      "test loss is 0.000678555136322911\n",
      "Batch: 33200,train loss is: 0.0003982534015287058\n",
      "test loss is 0.0005592469958147042\n",
      "Batch: 33300,train loss is: 0.00072764797283223\n",
      "test loss is 0.0005306258106638271\n",
      "Batch: 33400,train loss is: 0.0003583299586679652\n",
      "test loss is 0.0005897238419701301\n",
      "Batch: 33500,train loss is: 0.0013478669640074627\n",
      "test loss is 0.0006653785095583474\n",
      "Batch: 33600,train loss is: 0.0007975256591487051\n",
      "test loss is 0.0007785843650628087\n",
      "Batch: 33700,train loss is: 0.0009289194014495132\n",
      "test loss is 0.0008529662404610686\n",
      "Batch: 33800,train loss is: 0.0007400436290254934\n",
      "test loss is 0.0008586296368794244\n",
      "Batch: 33900,train loss is: 0.0007296795143547722\n",
      "test loss is 0.0009098976978577599\n",
      "Batch: 34000,train loss is: 0.00047046483074547255\n",
      "test loss is 0.000554076850771821\n",
      "Batch: 34100,train loss is: 0.0005035953056165834\n",
      "test loss is 0.0007275755104546895\n",
      "Batch: 34200,train loss is: 0.0008850996230995905\n",
      "test loss is 0.0006733103435302988\n",
      "Batch: 34300,train loss is: 0.0010132980558295993\n",
      "test loss is 0.0006480047383900433\n",
      "Batch: 34400,train loss is: 0.0008742095591624925\n",
      "test loss is 0.0008993390573939394\n",
      "Batch: 34500,train loss is: 0.0005971174903312854\n",
      "test loss is 0.0008917186110508522\n",
      "Batch: 34600,train loss is: 0.0016992339508236768\n",
      "test loss is 0.0008889901938066151\n",
      "Batch: 34700,train loss is: 0.0005394748887903533\n",
      "test loss is 0.0009270274633119976\n",
      "Batch: 34800,train loss is: 0.000524298113477936\n",
      "test loss is 0.0005046705436982408\n",
      "Batch: 34900,train loss is: 0.00041749372485794034\n",
      "test loss is 0.0005707833566898291\n",
      "Batch: 35000,train loss is: 0.0005861084596179938\n",
      "test loss is 0.0009244828604130404\n",
      "Batch: 35100,train loss is: 0.0006408715268546442\n",
      "test loss is 0.0006209767176005813\n",
      "Batch: 35200,train loss is: 0.0005069776351764908\n",
      "test loss is 0.0008985408426103965\n",
      "Batch: 35300,train loss is: 0.000616668743900042\n",
      "test loss is 0.0005078167996050677\n",
      "Batch: 35400,train loss is: 0.0008451965312387871\n",
      "test loss is 0.0008177299245895844\n",
      "Batch: 35500,train loss is: 0.00056394060936689\n",
      "test loss is 0.0005271254903622581\n",
      "Batch: 35600,train loss is: 0.0005986300767111938\n",
      "test loss is 0.0010308016373587827\n",
      "Batch: 35700,train loss is: 0.001043623599388254\n",
      "test loss is 0.0010423691617094423\n",
      "Batch: 35800,train loss is: 0.0006172813045975846\n",
      "test loss is 0.0004912685083373089\n",
      "Batch: 35900,train loss is: 0.0006310411492640457\n",
      "test loss is 0.0005764302464783702\n",
      "Batch: 36000,train loss is: 0.00042696889955072797\n",
      "test loss is 0.00045500338922734295\n",
      "Batch: 36100,train loss is: 0.0007631561814419182\n",
      "test loss is 0.0008930695527305789\n",
      "Batch: 36200,train loss is: 0.0005347351190830537\n",
      "test loss is 0.0006505059389710456\n",
      "Batch: 36300,train loss is: 0.00084103829397057\n",
      "test loss is 0.0006338997093268922\n",
      "Batch: 36400,train loss is: 0.0008058874758783628\n",
      "test loss is 0.0005160247741429225\n",
      "Batch: 36500,train loss is: 0.0003285646202799981\n",
      "test loss is 0.0005904661798579591\n",
      "Batch: 36600,train loss is: 0.0005842575304150172\n",
      "test loss is 0.0005896294664253156\n",
      "Batch: 36700,train loss is: 0.00040523567691711437\n",
      "test loss is 0.0008125627332969467\n",
      "Batch: 36800,train loss is: 0.000561507627328813\n",
      "test loss is 0.0007056596168999559\n",
      "Batch: 36900,train loss is: 0.0004546148225093953\n",
      "test loss is 0.0006683576761286661\n",
      "Batch: 37000,train loss is: 0.0003249297198240949\n",
      "test loss is 0.0005606052593087551\n",
      "Batch: 37100,train loss is: 0.000560435276187364\n",
      "test loss is 0.0004914823363697255\n",
      "Batch: 37200,train loss is: 0.00036067949645926067\n",
      "test loss is 0.0007911187689487536\n",
      "Batch: 37300,train loss is: 0.00041309231479210855\n",
      "test loss is 0.0004973472526477202\n",
      "Batch: 37400,train loss is: 0.001071893197884928\n",
      "test loss is 0.0005930734197943254\n",
      "Batch: 37500,train loss is: 0.0005727188677991147\n",
      "test loss is 0.0005786214114689895\n",
      "Batch: 37600,train loss is: 0.001052722606580547\n",
      "test loss is 0.0007252834625500697\n",
      "Batch: 37700,train loss is: 0.0005490784059641851\n",
      "test loss is 0.0006011969967117055\n",
      "Batch: 37800,train loss is: 0.0013118266363679357\n",
      "test loss is 0.0011631043156412192\n",
      "Batch: 37900,train loss is: 0.00040914342484475285\n",
      "test loss is 0.0006992362596806029\n",
      "Batch: 38000,train loss is: 0.0007108990506098584\n",
      "test loss is 0.0007876157949930304\n",
      "Batch: 38100,train loss is: 0.0006137216777871629\n",
      "test loss is 0.0005573595109013941\n",
      "Batch: 38200,train loss is: 0.00037367380147033236\n",
      "test loss is 0.0006154585086436204\n",
      "Batch: 38300,train loss is: 0.0004959548096155977\n",
      "test loss is 0.0007058504974925123\n",
      "Batch: 38400,train loss is: 0.0007124950486663656\n",
      "test loss is 0.0005384838837645555\n",
      "Batch: 38500,train loss is: 0.0009115208761837422\n",
      "test loss is 0.001384708455137684\n",
      "Batch: 38600,train loss is: 0.000701172160361238\n",
      "test loss is 0.0007165980061654009\n",
      "Batch: 38700,train loss is: 0.0006445811034096635\n",
      "test loss is 0.0006862574590007736\n",
      "Batch: 38800,train loss is: 0.0004357794090991507\n",
      "test loss is 0.0007039327223020724\n",
      "Batch: 38900,train loss is: 0.0008292403833089425\n",
      "test loss is 0.0006350864311387962\n",
      "Batch: 39000,train loss is: 0.0005182999433107735\n",
      "test loss is 0.0005416207230988545\n",
      "Batch: 39100,train loss is: 0.0006946454441308179\n",
      "test loss is 0.0012024452947467217\n",
      "Batch: 39200,train loss is: 0.0004612279520172769\n",
      "test loss is 0.0006476783760340852\n",
      "Batch: 39300,train loss is: 0.0004594086886159831\n",
      "test loss is 0.0005471405272180061\n",
      "Batch: 39400,train loss is: 0.0008420412290693813\n",
      "test loss is 0.0007376303691318438\n",
      "Batch: 39500,train loss is: 0.0005618838743113616\n",
      "test loss is 0.0005199688409929764\n",
      "Batch: 39600,train loss is: 0.00046281412366710507\n",
      "test loss is 0.0005168686382100553\n",
      "Batch: 39700,train loss is: 0.0003710321355933885\n",
      "test loss is 0.000562452739532628\n",
      "Batch: 39800,train loss is: 0.0007163731560029245\n",
      "test loss is 0.0005719315505220766\n",
      "Batch: 39900,train loss is: 0.00032803245842222484\n",
      "test loss is 0.0006135422916093353\n",
      "Batch: 40000,train loss is: 0.0007332408257310016\n",
      "test loss is 0.0005196445040306415\n",
      "Batch: 40100,train loss is: 0.0003622796721235957\n",
      "test loss is 0.000510758718698537\n",
      "Batch: 40200,train loss is: 0.0005495574532248628\n",
      "test loss is 0.0006021366755308409\n",
      "Batch: 40300,train loss is: 0.00048465554565343207\n",
      "test loss is 0.000508984008929256\n",
      "Batch: 40400,train loss is: 0.0005057397652618879\n",
      "test loss is 0.0005956926166402098\n",
      "Batch: 40500,train loss is: 0.0004697633858272653\n",
      "test loss is 0.0006175859742928046\n",
      "Batch: 40600,train loss is: 0.00041404485195202826\n",
      "test loss is 0.0007450682924598021\n",
      "Batch: 40700,train loss is: 0.0010317622040138317\n",
      "test loss is 0.0006654664926946382\n",
      "Batch: 40800,train loss is: 0.00041103740208949067\n",
      "test loss is 0.0007071609602874561\n",
      "Batch: 40900,train loss is: 0.0005886636969376695\n",
      "test loss is 0.0006548290170989455\n",
      "Batch: 41000,train loss is: 0.00041818078729101085\n",
      "test loss is 0.0005527536262721799\n",
      "Batch: 41100,train loss is: 0.00046138531428038777\n",
      "test loss is 0.0005083912136963696\n",
      "Batch: 41200,train loss is: 0.0005641380832687027\n",
      "test loss is 0.0006129706494301534\n",
      "Batch: 41300,train loss is: 0.0004971001661283218\n",
      "test loss is 0.0005720566525004425\n",
      "Batch: 41400,train loss is: 0.0005283754373684767\n",
      "test loss is 0.0005702367103100367\n",
      "Batch: 41500,train loss is: 0.000747823648285375\n",
      "test loss is 0.0006491575694252575\n",
      "Batch: 41600,train loss is: 0.0005428534439413672\n",
      "test loss is 0.0006988207589667786\n",
      "Batch: 41700,train loss is: 0.0009246196738856151\n",
      "test loss is 0.0005991221324843569\n",
      "Batch: 41800,train loss is: 0.00044654266864722425\n",
      "test loss is 0.000670894096306031\n",
      "Batch: 41900,train loss is: 0.0003706400411975086\n",
      "test loss is 0.0007828137677541924\n",
      "Batch: 42000,train loss is: 0.0005396363624073455\n",
      "test loss is 0.0006772900460310694\n",
      "Batch: 42100,train loss is: 0.0004865335800646309\n",
      "test loss is 0.0007665188967192822\n",
      "Batch: 42200,train loss is: 0.0008380888947837889\n",
      "test loss is 0.0006332266747992519\n",
      "Batch: 42300,train loss is: 0.0006050086320208385\n",
      "test loss is 0.0007568527863792775\n",
      "Batch: 42400,train loss is: 0.0008313341198251485\n",
      "test loss is 0.000598793784804709\n",
      "Batch: 42500,train loss is: 0.00060363403117101\n",
      "test loss is 0.0006663080076380316\n",
      "Batch: 42600,train loss is: 0.0008552363489673446\n",
      "test loss is 0.0008274645734006191\n",
      "Batch: 42700,train loss is: 0.0004898521719968751\n",
      "test loss is 0.0005567429383364069\n",
      "Batch: 42800,train loss is: 0.00106781784803479\n",
      "test loss is 0.0008578972145924634\n",
      "Batch: 42900,train loss is: 0.0007203766763689993\n",
      "test loss is 0.0005953896946711765\n",
      "Batch: 43000,train loss is: 0.0006394086682182877\n",
      "test loss is 0.0005037948995228802\n",
      "Batch: 43100,train loss is: 0.000436781275110659\n",
      "test loss is 0.0006630112039102986\n",
      "Batch: 43200,train loss is: 0.0003063263801298561\n",
      "test loss is 0.0005334528649029626\n",
      "Batch: 43300,train loss is: 0.0005273448788389134\n",
      "test loss is 0.0006214525129576177\n",
      "Batch: 43400,train loss is: 0.00038812965250116353\n",
      "test loss is 0.0005121496655549651\n",
      "Batch: 43500,train loss is: 0.00038051622046595876\n",
      "test loss is 0.0006195887083338242\n",
      "Batch: 43600,train loss is: 0.000615427928658305\n",
      "test loss is 0.0005054542872707543\n",
      "Batch: 43700,train loss is: 0.00048598083498667573\n",
      "test loss is 0.0005970486729699932\n",
      "Batch: 43800,train loss is: 0.0004599227726558968\n",
      "test loss is 0.0006820936720347405\n",
      "Batch: 43900,train loss is: 0.00043627407233339617\n",
      "test loss is 0.0007319871166102413\n",
      "Batch: 44000,train loss is: 0.0007861406729931517\n",
      "test loss is 0.0006996878882230979\n",
      "Batch: 44100,train loss is: 0.0008327706506415177\n",
      "test loss is 0.0005819405934621758\n",
      "Batch: 44200,train loss is: 0.0007179982186920729\n",
      "test loss is 0.0005797531104932085\n",
      "Batch: 44300,train loss is: 0.0007568377567618698\n",
      "test loss is 0.0009044331844417046\n",
      "Batch: 44400,train loss is: 0.0005097579075665033\n",
      "test loss is 0.0006403931681837954\n",
      "Batch: 44500,train loss is: 0.000983137945032536\n",
      "test loss is 0.0019882355639524066\n",
      "Batch: 44600,train loss is: 0.0005373714594599454\n",
      "test loss is 0.00047537385811498306\n",
      "Batch: 44700,train loss is: 0.00041000005699707854\n",
      "test loss is 0.0005125802260050825\n",
      "Batch: 44800,train loss is: 0.0003341834865881327\n",
      "test loss is 0.0006360412128771904\n",
      "Batch: 44900,train loss is: 0.00042563726801624763\n",
      "test loss is 0.0005108935052837714\n",
      "Batch: 45000,train loss is: 0.0004885497727315028\n",
      "test loss is 0.0005329179115472523\n",
      "Batch: 45100,train loss is: 0.00042721243866395336\n",
      "test loss is 0.0005204051840399293\n",
      "Batch: 45200,train loss is: 0.0006505012005407508\n",
      "test loss is 0.0006018651042929941\n",
      "Batch: 45300,train loss is: 0.0006476510789351934\n",
      "test loss is 0.0007681277403021119\n",
      "Batch: 45400,train loss is: 0.0009179390349374045\n",
      "test loss is 0.0005441570447450241\n",
      "Batch: 45500,train loss is: 0.00036369159209891246\n",
      "test loss is 0.0005963742617580902\n",
      "Batch: 45600,train loss is: 0.0005143187257272997\n",
      "test loss is 0.0005668758494454242\n",
      "Batch: 45700,train loss is: 0.0002459069985182321\n",
      "test loss is 0.0006070918631738011\n",
      "Batch: 45800,train loss is: 0.000839868329324396\n",
      "test loss is 0.0006215469072578977\n",
      "Batch: 45900,train loss is: 0.0012435318556535521\n",
      "test loss is 0.0011342535366443416\n",
      "Batch: 46000,train loss is: 0.0011248890768737131\n",
      "test loss is 0.000726306626740639\n",
      "Batch: 46100,train loss is: 0.0008758666960898613\n",
      "test loss is 0.0011780361919228822\n",
      "Batch: 46200,train loss is: 0.001389352140343096\n",
      "test loss is 0.0005274193277199899\n",
      "Batch: 46300,train loss is: 0.0004935623976276443\n",
      "test loss is 0.0005780620841569773\n",
      "Batch: 46400,train loss is: 0.0007723391009696594\n",
      "test loss is 0.0004962644537549071\n",
      "Batch: 46500,train loss is: 0.00037036006144225875\n",
      "test loss is 0.0005314514706155075\n",
      "Batch: 46600,train loss is: 0.0004973859058843889\n",
      "test loss is 0.0006505715717499269\n",
      "Batch: 46700,train loss is: 0.0007310499945342316\n",
      "test loss is 0.000662204657363379\n",
      "-----------------------Epoch: 3----------------------------------\n",
      "Batch: 0,train loss is: 0.00037321689899647123\n",
      "test loss is 0.0006241442242437212\n",
      "Batch: 100,train loss is: 0.0009408624854352428\n",
      "test loss is 0.0006333066214077801\n",
      "Batch: 200,train loss is: 0.0007690635887901065\n",
      "test loss is 0.0006966790861139219\n",
      "Batch: 300,train loss is: 0.0005468736862275634\n",
      "test loss is 0.0006238663267292229\n",
      "Batch: 400,train loss is: 0.0008323810529462695\n",
      "test loss is 0.0007065588902902107\n",
      "Batch: 500,train loss is: 0.0006224308984042995\n",
      "test loss is 0.000690828372325003\n",
      "Batch: 600,train loss is: 0.0004345091572395215\n",
      "test loss is 0.0009404217123134402\n",
      "Batch: 700,train loss is: 0.0007182214554418119\n",
      "test loss is 0.0006860854931463019\n",
      "Batch: 800,train loss is: 0.0009977127450997738\n",
      "test loss is 0.0009193266655190974\n",
      "Batch: 900,train loss is: 0.0008006785594252292\n",
      "test loss is 0.0007785189770447828\n",
      "Batch: 1000,train loss is: 0.00043786622259656355\n",
      "test loss is 0.0005072848499516658\n",
      "Batch: 1100,train loss is: 0.0003665813430510041\n",
      "test loss is 0.00048063687473103755\n",
      "Batch: 1200,train loss is: 0.0006399115111675462\n",
      "test loss is 0.0007596538493823175\n",
      "Batch: 1300,train loss is: 0.000585608852003333\n",
      "test loss is 0.0006635597629509643\n",
      "Batch: 1400,train loss is: 0.001193543576717185\n",
      "test loss is 0.000537085865443794\n",
      "Batch: 1500,train loss is: 0.0011724505812177044\n",
      "test loss is 0.0015106016164340953\n",
      "Batch: 1600,train loss is: 0.0006337638687375984\n",
      "test loss is 0.0006892756567256549\n",
      "Batch: 1700,train loss is: 0.0006052828516671941\n",
      "test loss is 0.0007209884716361171\n",
      "Batch: 1800,train loss is: 0.0007074537332919571\n",
      "test loss is 0.0006164444937304984\n",
      "Batch: 1900,train loss is: 0.00076502653205703\n",
      "test loss is 0.0006298063553195294\n",
      "Batch: 2000,train loss is: 0.0007636700562926196\n",
      "test loss is 0.0008684162963077549\n",
      "Batch: 2100,train loss is: 0.0005187582318508469\n",
      "test loss is 0.0005501868056189714\n",
      "Batch: 2200,train loss is: 0.00042477645807151776\n",
      "test loss is 0.000577842236245616\n",
      "Batch: 2300,train loss is: 0.0004211269496253998\n",
      "test loss is 0.0005199146472882517\n",
      "Batch: 2400,train loss is: 0.00044682605562097527\n",
      "test loss is 0.000508300872823252\n",
      "Batch: 2500,train loss is: 0.0003585366060967854\n",
      "test loss is 0.0005082574898881253\n",
      "Batch: 2600,train loss is: 0.0003611213720915172\n",
      "test loss is 0.0005929441968910342\n",
      "Batch: 2700,train loss is: 0.0008441631209484665\n",
      "test loss is 0.0005854816500520446\n",
      "Batch: 2800,train loss is: 0.0005672992859955715\n",
      "test loss is 0.0005558782893401472\n",
      "Batch: 2900,train loss is: 0.00046937282864332027\n",
      "test loss is 0.0005718211562089898\n",
      "Batch: 3000,train loss is: 0.0011801408186534698\n",
      "test loss is 0.0006382203661584235\n",
      "Batch: 3100,train loss is: 0.0009362726622328731\n",
      "test loss is 0.0006853388555485957\n",
      "Batch: 3200,train loss is: 0.0005534107967148818\n",
      "test loss is 0.0005549447776705083\n",
      "Batch: 3300,train loss is: 0.000606825768554888\n",
      "test loss is 0.000515420324039343\n",
      "Batch: 3400,train loss is: 0.0007120018408531645\n",
      "test loss is 0.0008838025985656144\n",
      "Batch: 3500,train loss is: 0.0003882590221509524\n",
      "test loss is 0.0004723073452862782\n",
      "Batch: 3600,train loss is: 0.00030586152331597\n",
      "test loss is 0.0005834004827191189\n",
      "Batch: 3700,train loss is: 0.0006116506436464193\n",
      "test loss is 0.0005454860439786992\n",
      "Batch: 3800,train loss is: 0.0014080862518552973\n",
      "test loss is 0.0006732780505873207\n",
      "Batch: 3900,train loss is: 0.0005646082915670228\n",
      "test loss is 0.0005052912064765202\n",
      "Batch: 4000,train loss is: 0.0006661458867747701\n",
      "test loss is 0.0005446399138197489\n",
      "Batch: 4100,train loss is: 0.0010990897383468792\n",
      "test loss is 0.0005848697853616942\n",
      "Batch: 4200,train loss is: 0.000595212832984082\n",
      "test loss is 0.0006862108966061302\n",
      "Batch: 4300,train loss is: 0.0005671774275175061\n",
      "test loss is 0.0005287751592340446\n",
      "Batch: 4400,train loss is: 0.0003654895978936812\n",
      "test loss is 0.0006380538695036997\n",
      "Batch: 4500,train loss is: 0.0007462994021317078\n",
      "test loss is 0.0006997532086063049\n",
      "Batch: 4600,train loss is: 0.000797956747895704\n",
      "test loss is 0.000565485774761998\n",
      "Batch: 4700,train loss is: 0.000489771956837377\n",
      "test loss is 0.0005301447066550925\n",
      "Batch: 4800,train loss is: 0.0004619736423552961\n",
      "test loss is 0.0004427180559250792\n",
      "Batch: 4900,train loss is: 0.0006655134187469938\n",
      "test loss is 0.0008026794055740619\n",
      "Batch: 5000,train loss is: 0.00029156450453690283\n",
      "test loss is 0.00046570661552493693\n",
      "Batch: 5100,train loss is: 0.0005596809197944308\n",
      "test loss is 0.0006365096275118864\n",
      "Batch: 5200,train loss is: 0.0006596797602689745\n",
      "test loss is 0.0006288500257941242\n",
      "Batch: 5300,train loss is: 0.0008875170770991423\n",
      "test loss is 0.0007001980994417242\n",
      "Batch: 5400,train loss is: 0.00047680171606552264\n",
      "test loss is 0.0006270358182909131\n",
      "Batch: 5500,train loss is: 0.0005119652121655742\n",
      "test loss is 0.0005302892752844975\n",
      "Batch: 5600,train loss is: 0.0006626008728305694\n",
      "test loss is 0.0004600517320412827\n",
      "Batch: 5700,train loss is: 0.000455878636669679\n",
      "test loss is 0.0009455334459657954\n",
      "Batch: 5800,train loss is: 0.0005350392063760085\n",
      "test loss is 0.0006223372487403554\n",
      "Batch: 5900,train loss is: 0.0010328636797684728\n",
      "test loss is 0.000602413882815688\n",
      "Batch: 6000,train loss is: 0.0006651546693709088\n",
      "test loss is 0.00067025743927938\n",
      "Batch: 6100,train loss is: 0.0007558057930896293\n",
      "test loss is 0.0007112125643264655\n",
      "Batch: 6200,train loss is: 0.0009006974898613443\n",
      "test loss is 0.0006479474708143412\n",
      "Batch: 6300,train loss is: 0.0007089425930176264\n",
      "test loss is 0.0005271650402375912\n",
      "Batch: 6400,train loss is: 0.0005543967641314354\n",
      "test loss is 0.0005039754782577711\n",
      "Batch: 6500,train loss is: 0.0008008975488660679\n",
      "test loss is 0.0005705522728813748\n",
      "Batch: 6600,train loss is: 0.0005367093460846037\n",
      "test loss is 0.0005126972281735586\n",
      "Batch: 6700,train loss is: 0.0009125396989882932\n",
      "test loss is 0.0006570720416215139\n",
      "Batch: 6800,train loss is: 0.000902108326186651\n",
      "test loss is 0.0006557311364356984\n",
      "Batch: 6900,train loss is: 0.00034792493160990716\n",
      "test loss is 0.000612999419948965\n",
      "Batch: 7000,train loss is: 0.0006823159800508052\n",
      "test loss is 0.0006024822814079788\n",
      "Batch: 7100,train loss is: 0.000571578632569684\n",
      "test loss is 0.0006310106884487681\n",
      "Batch: 7200,train loss is: 0.0008296625146043434\n",
      "test loss is 0.0008146157325218564\n",
      "Batch: 7300,train loss is: 0.0005096970184212462\n",
      "test loss is 0.00047116153491171876\n",
      "Batch: 7400,train loss is: 0.0004887817632613723\n",
      "test loss is 0.0006425148637734574\n",
      "Batch: 7500,train loss is: 0.0007163902287251007\n",
      "test loss is 0.0006109336381354114\n",
      "Batch: 7600,train loss is: 0.0003687088426029509\n",
      "test loss is 0.0006745200210588419\n",
      "Batch: 7700,train loss is: 0.0004687648585962956\n",
      "test loss is 0.0006053576664906233\n",
      "Batch: 7800,train loss is: 0.0005890159120725801\n",
      "test loss is 0.0005006661012834404\n",
      "Batch: 7900,train loss is: 0.00047245564582755744\n",
      "test loss is 0.0004763838112791018\n",
      "Batch: 8000,train loss is: 0.0005570860141992438\n",
      "test loss is 0.0007979142301074713\n",
      "Batch: 8100,train loss is: 0.0006507390362666351\n",
      "test loss is 0.0006024236356796152\n",
      "Batch: 8200,train loss is: 0.00036579687083759826\n",
      "test loss is 0.0007229261132157405\n",
      "Batch: 8300,train loss is: 0.0006046919740803137\n",
      "test loss is 0.0006308142162790853\n",
      "Batch: 8400,train loss is: 0.000373257753980169\n",
      "test loss is 0.0005323727693401647\n",
      "Batch: 8500,train loss is: 0.0005811590656895991\n",
      "test loss is 0.0005952838383402214\n",
      "Batch: 8600,train loss is: 0.0003903794619696426\n",
      "test loss is 0.0005578528961255796\n",
      "Batch: 8700,train loss is: 0.0011736077329683515\n",
      "test loss is 0.0007034263116600738\n",
      "Batch: 8800,train loss is: 0.0007811277526642345\n",
      "test loss is 0.0008413956983858848\n",
      "Batch: 8900,train loss is: 0.0002481460251882727\n",
      "test loss is 0.0007307942803230609\n",
      "Batch: 9000,train loss is: 0.00033912621139971087\n",
      "test loss is 0.0005236058562419311\n",
      "Batch: 9100,train loss is: 0.0003811995001951443\n",
      "test loss is 0.0006151130119511788\n",
      "Batch: 9200,train loss is: 0.0005986207946145487\n",
      "test loss is 0.0006011393063007247\n",
      "Batch: 9300,train loss is: 0.0007318802159442805\n",
      "test loss is 0.000596252719870903\n",
      "Batch: 9400,train loss is: 0.0006835916485284412\n",
      "test loss is 0.0005973514390807702\n",
      "Batch: 9500,train loss is: 0.0006870361814002873\n",
      "test loss is 0.000582454535798601\n",
      "Batch: 9600,train loss is: 0.0004164055354562129\n",
      "test loss is 0.0007954593030617569\n",
      "Batch: 9700,train loss is: 0.0007898547248655179\n",
      "test loss is 0.0006158735260987951\n",
      "Batch: 9800,train loss is: 0.00048757102028456766\n",
      "test loss is 0.0005490357931339654\n",
      "Batch: 9900,train loss is: 0.0004292188517056838\n",
      "test loss is 0.0008142304600052978\n",
      "Batch: 10000,train loss is: 0.0005499757197137061\n",
      "test loss is 0.0006088165843439413\n",
      "Batch: 10100,train loss is: 0.0005569144052460119\n",
      "test loss is 0.000556788501516599\n",
      "Batch: 10200,train loss is: 0.00043979564300656085\n",
      "test loss is 0.0005457805743729795\n",
      "Batch: 10300,train loss is: 0.0007215895638915219\n",
      "test loss is 0.0007454628680321823\n",
      "Batch: 10400,train loss is: 0.000462059314472457\n",
      "test loss is 0.000487559200259913\n",
      "Batch: 10500,train loss is: 0.000613505601523133\n",
      "test loss is 0.0008687977093049669\n",
      "Batch: 10600,train loss is: 0.0005075619186893378\n",
      "test loss is 0.0005457745839900908\n",
      "Batch: 10700,train loss is: 0.0006368669971196622\n",
      "test loss is 0.0009683077645921458\n",
      "Batch: 10800,train loss is: 0.0006086275619673636\n",
      "test loss is 0.0004898564258787717\n",
      "Batch: 10900,train loss is: 0.0005090680672082305\n",
      "test loss is 0.0005887280037054419\n",
      "Batch: 11000,train loss is: 0.0004951025068868959\n",
      "test loss is 0.0006592813804936553\n",
      "Batch: 11100,train loss is: 0.0006000418295603863\n",
      "test loss is 0.0007283134627162982\n",
      "Batch: 11200,train loss is: 0.0007188494768824148\n",
      "test loss is 0.0005655144575584301\n",
      "Batch: 11300,train loss is: 0.000421105318255726\n",
      "test loss is 0.000535264427058578\n",
      "Batch: 11400,train loss is: 0.0004293848153793701\n",
      "test loss is 0.0005540987219512825\n",
      "Batch: 11500,train loss is: 0.00042519528153948276\n",
      "test loss is 0.0004903235378635447\n",
      "Batch: 11600,train loss is: 0.0011975484564909766\n",
      "test loss is 0.0012188575358304395\n",
      "Batch: 11700,train loss is: 0.0004322539091709874\n",
      "test loss is 0.0004281976245943366\n",
      "Batch: 11800,train loss is: 0.000631187969837983\n",
      "test loss is 0.0006720952415672953\n",
      "Batch: 11900,train loss is: 0.0011133312119580045\n",
      "test loss is 0.0005621360967901006\n",
      "Batch: 12000,train loss is: 0.00029425021599440934\n",
      "test loss is 0.0005918708601887809\n",
      "Batch: 12100,train loss is: 0.0005503070206312662\n",
      "test loss is 0.0010788736626860605\n",
      "Batch: 12200,train loss is: 0.0004411112550838782\n",
      "test loss is 0.0005193598769275047\n",
      "Batch: 12300,train loss is: 0.0007625666281204071\n",
      "test loss is 0.00045175788282385455\n",
      "Batch: 12400,train loss is: 0.000414023429295895\n",
      "test loss is 0.0005361777941871614\n",
      "Batch: 12500,train loss is: 0.0047335182261729995\n",
      "test loss is 0.000672179958825917\n",
      "Batch: 12600,train loss is: 0.000675558958833478\n",
      "test loss is 0.0005859302158351278\n",
      "Batch: 12700,train loss is: 0.00040773082423729975\n",
      "test loss is 0.0005385712659978943\n",
      "Batch: 12800,train loss is: 0.0003813174130436749\n",
      "test loss is 0.0005052076678943201\n",
      "Batch: 12900,train loss is: 0.000599209818784054\n",
      "test loss is 0.0005003335188381264\n",
      "Batch: 13000,train loss is: 0.0007729252637467889\n",
      "test loss is 0.0006224734020358472\n",
      "Batch: 13100,train loss is: 0.0007186417734412378\n",
      "test loss is 0.0005458315920707083\n",
      "Batch: 13200,train loss is: 0.0006359036453683741\n",
      "test loss is 0.0009350801135376211\n",
      "Batch: 13300,train loss is: 0.000419633515532342\n",
      "test loss is 0.00048684684920789675\n",
      "Batch: 13400,train loss is: 0.0008987711590595785\n",
      "test loss is 0.0008953458598568245\n",
      "Batch: 13500,train loss is: 0.0012388764230805054\n",
      "test loss is 0.0011543559465217986\n",
      "Batch: 13600,train loss is: 0.0005001302110005955\n",
      "test loss is 0.0005200405089816374\n",
      "Batch: 13700,train loss is: 0.0005035075778451815\n",
      "test loss is 0.00047298099896015734\n",
      "Batch: 13800,train loss is: 0.0004473231863162001\n",
      "test loss is 0.0006079409811779262\n",
      "Batch: 13900,train loss is: 0.0005449587442438685\n",
      "test loss is 0.0005273212472143239\n",
      "Batch: 14000,train loss is: 0.0002733484602117618\n",
      "test loss is 0.0006207773647391081\n",
      "Batch: 14100,train loss is: 0.00048513879345906026\n",
      "test loss is 0.0006246817467639706\n",
      "Batch: 14200,train loss is: 0.0005446162170650764\n",
      "test loss is 0.0007761306659332622\n",
      "Batch: 14300,train loss is: 0.0004913051303000544\n",
      "test loss is 0.0008685378844896236\n",
      "Batch: 14400,train loss is: 0.0003995682702627812\n",
      "test loss is 0.00048244269954454943\n",
      "Batch: 14500,train loss is: 0.0004077247914959268\n",
      "test loss is 0.0004455482050623477\n",
      "Batch: 14600,train loss is: 0.0008483408371552826\n",
      "test loss is 0.0007188058053088449\n",
      "Batch: 14700,train loss is: 0.0007086373100577154\n",
      "test loss is 0.000809773281542184\n",
      "Batch: 14800,train loss is: 0.000715799466098705\n",
      "test loss is 0.0005352389337919016\n",
      "Batch: 14900,train loss is: 0.00043089532610793486\n",
      "test loss is 0.0005398217514863775\n",
      "Batch: 15000,train loss is: 0.0006040490810916779\n",
      "test loss is 0.0007004404417381979\n",
      "Batch: 15100,train loss is: 0.00039821485693995474\n",
      "test loss is 0.0005045462935766766\n",
      "Batch: 15200,train loss is: 0.00029930270689802437\n",
      "test loss is 0.0004691549398342444\n",
      "Batch: 15300,train loss is: 0.0007165614308130869\n",
      "test loss is 0.0007882335037380052\n",
      "Batch: 15400,train loss is: 0.0007908570642365909\n",
      "test loss is 0.0005032082367445276\n",
      "Batch: 15500,train loss is: 0.0038151978690106904\n",
      "test loss is 0.0006844070583699013\n",
      "Batch: 15600,train loss is: 0.0010905427780261942\n",
      "test loss is 0.0007382722681754673\n",
      "Batch: 15700,train loss is: 0.0005386748035611099\n",
      "test loss is 0.0007762058146930941\n",
      "Batch: 15800,train loss is: 0.00054076650637788\n",
      "test loss is 0.0005770703480515247\n",
      "Batch: 15900,train loss is: 0.0007440235995086818\n",
      "test loss is 0.0005129639769408396\n",
      "Batch: 16000,train loss is: 0.000359438706460242\n",
      "test loss is 0.0006058577144010735\n",
      "Batch: 16100,train loss is: 0.0008165179628495636\n",
      "test loss is 0.0007645604959589959\n",
      "Batch: 16200,train loss is: 0.0005659337764226606\n",
      "test loss is 0.0005085416074301488\n",
      "Batch: 16300,train loss is: 0.0006329821076717177\n",
      "test loss is 0.0005135056577824242\n",
      "Batch: 16400,train loss is: 0.0002717963397634375\n",
      "test loss is 0.0006525965460246239\n",
      "Batch: 16500,train loss is: 0.0004980017834722639\n",
      "test loss is 0.0005735748571136565\n",
      "Batch: 16600,train loss is: 0.0004581836652130653\n",
      "test loss is 0.0005834173858581081\n",
      "Batch: 16700,train loss is: 0.0008525408946823169\n",
      "test loss is 0.0008667019067581187\n",
      "Batch: 16800,train loss is: 0.0005017506322276832\n",
      "test loss is 0.0004426278870741532\n",
      "Batch: 16900,train loss is: 0.0003690935152065539\n",
      "test loss is 0.0005088618729492498\n",
      "Batch: 17000,train loss is: 0.0004749365810886446\n",
      "test loss is 0.0006422574433806276\n",
      "Batch: 17100,train loss is: 0.0005290802160770161\n",
      "test loss is 0.0006221120322116007\n",
      "Batch: 17200,train loss is: 0.0004665867715332346\n",
      "test loss is 0.0004742378812206686\n",
      "Batch: 17300,train loss is: 0.000499508468077464\n",
      "test loss is 0.0005602103747017888\n",
      "Batch: 17400,train loss is: 0.0006565454277519686\n",
      "test loss is 0.0005647878530078359\n",
      "Batch: 17500,train loss is: 0.0005311442331609273\n",
      "test loss is 0.0005993914018421141\n",
      "Batch: 17600,train loss is: 0.0006415987847735351\n",
      "test loss is 0.0005963763864184047\n",
      "Batch: 17700,train loss is: 0.0005630631887530348\n",
      "test loss is 0.0005813085567743274\n",
      "Batch: 17800,train loss is: 0.000547631116968747\n",
      "test loss is 0.0005278442306910556\n",
      "Batch: 17900,train loss is: 0.0003285281871031261\n",
      "test loss is 0.0006097084647770318\n",
      "Batch: 18000,train loss is: 0.0007286713770241775\n",
      "test loss is 0.0005171218524047511\n",
      "Batch: 18100,train loss is: 0.0009439825825144604\n",
      "test loss is 0.00047814803946956544\n",
      "Batch: 18200,train loss is: 0.0004598505146989977\n",
      "test loss is 0.0005688021383162673\n",
      "Batch: 18300,train loss is: 0.00042793515704975466\n",
      "test loss is 0.0006485564136077568\n",
      "Batch: 18400,train loss is: 0.0012857969738937246\n",
      "test loss is 0.0006759211509316708\n",
      "Batch: 18500,train loss is: 0.00040600524119920235\n",
      "test loss is 0.0004825049070478869\n",
      "Batch: 18600,train loss is: 0.0006248822642352786\n",
      "test loss is 0.0005279730141130103\n",
      "Batch: 18700,train loss is: 0.0004261480173973582\n",
      "test loss is 0.00048318022302366664\n",
      "Batch: 18800,train loss is: 0.0007936700592038243\n",
      "test loss is 0.0007077718135936868\n",
      "Batch: 18900,train loss is: 0.0004393240305317964\n",
      "test loss is 0.0006025886877243349\n",
      "Batch: 19000,train loss is: 0.0006196909408495686\n",
      "test loss is 0.0004287751176962831\n",
      "Batch: 19100,train loss is: 0.0010074230182895586\n",
      "test loss is 0.000610265591326356\n",
      "Batch: 19200,train loss is: 0.0014496048525509996\n",
      "test loss is 0.0009887125281742004\n",
      "Batch: 19300,train loss is: 0.0007355898443562035\n",
      "test loss is 0.0006142961995557159\n",
      "Batch: 19400,train loss is: 0.0004851998886201021\n",
      "test loss is 0.0005467998470742709\n",
      "Batch: 19500,train loss is: 0.00046479411317912057\n",
      "test loss is 0.0004844890738124387\n",
      "Batch: 19600,train loss is: 0.00042308970237475154\n",
      "test loss is 0.0004992108173580308\n",
      "Batch: 19700,train loss is: 0.0007801027209116634\n",
      "test loss is 0.0005326953052172986\n",
      "Batch: 19800,train loss is: 0.000623857745769899\n",
      "test loss is 0.000592283603930054\n",
      "Batch: 19900,train loss is: 0.0006775769372415413\n",
      "test loss is 0.0005039811444339882\n",
      "Batch: 20000,train loss is: 0.0006080141308940106\n",
      "test loss is 0.0006680692013074734\n",
      "Batch: 20100,train loss is: 0.0006851880933828573\n",
      "test loss is 0.0005827530322368865\n",
      "Batch: 20200,train loss is: 0.0006684170957432892\n",
      "test loss is 0.0004845921327579396\n",
      "Batch: 20300,train loss is: 0.0006032774765100785\n",
      "test loss is 0.000554389390401169\n",
      "Batch: 20400,train loss is: 0.0004056242244087538\n",
      "test loss is 0.0006009399910151363\n",
      "Batch: 20500,train loss is: 0.0005262550505489402\n",
      "test loss is 0.0006140427029257032\n",
      "Batch: 20600,train loss is: 0.00037556818838322396\n",
      "test loss is 0.000532842612090586\n",
      "Batch: 20700,train loss is: 0.000513477073725187\n",
      "test loss is 0.00047013391777644286\n",
      "Batch: 20800,train loss is: 0.0004008965314903079\n",
      "test loss is 0.000589575061493205\n",
      "Batch: 20900,train loss is: 0.00112699732520135\n",
      "test loss is 0.0004914290403852262\n",
      "Batch: 21000,train loss is: 0.0008855653801426348\n",
      "test loss is 0.0007391173483276073\n",
      "Batch: 21100,train loss is: 0.0005088653698082327\n",
      "test loss is 0.0005717279862663873\n",
      "Batch: 21200,train loss is: 0.000970854400837456\n",
      "test loss is 0.0006700108792661705\n",
      "Batch: 21300,train loss is: 0.0006342459241557342\n",
      "test loss is 0.0005523583926657511\n",
      "Batch: 21400,train loss is: 0.0008100324040030523\n",
      "test loss is 0.0005498864356819699\n",
      "Batch: 21500,train loss is: 0.0007998951483147074\n",
      "test loss is 0.0004894370475348438\n",
      "Batch: 21600,train loss is: 0.0005103557315137032\n",
      "test loss is 0.0006504131572956419\n",
      "Batch: 21700,train loss is: 0.000503010145907187\n",
      "test loss is 0.0004931798056454404\n",
      "Batch: 21800,train loss is: 0.0009173053773737776\n",
      "test loss is 0.0006062203597495255\n",
      "Batch: 21900,train loss is: 0.0003887613265820596\n",
      "test loss is 0.0005420216460163175\n",
      "Batch: 22000,train loss is: 0.0004195177257070869\n",
      "test loss is 0.0005663897772392127\n",
      "Batch: 22100,train loss is: 0.0005202881023553887\n",
      "test loss is 0.0005526099025943701\n",
      "Batch: 22200,train loss is: 0.000710108971794052\n",
      "test loss is 0.0006273682444611424\n",
      "Batch: 22300,train loss is: 0.0005428787905968054\n",
      "test loss is 0.0005835642724209197\n",
      "Batch: 22400,train loss is: 0.0003295319846073467\n",
      "test loss is 0.0005305551520571418\n",
      "Batch: 22500,train loss is: 0.0005982463702577114\n",
      "test loss is 0.00045870680907788663\n",
      "Batch: 22600,train loss is: 0.0012233927486146542\n",
      "test loss is 0.0005291480517054339\n",
      "Batch: 22700,train loss is: 0.0010247175262884428\n",
      "test loss is 0.0006467937705492064\n",
      "Batch: 22800,train loss is: 0.0005292870225036366\n",
      "test loss is 0.000565468404971741\n",
      "Batch: 22900,train loss is: 0.0003669851923858075\n",
      "test loss is 0.0005692182575551553\n",
      "Batch: 23000,train loss is: 0.0004925838949280249\n",
      "test loss is 0.00046170835126068137\n",
      "Batch: 23100,train loss is: 0.0006124144796650485\n",
      "test loss is 0.0007173165901068869\n",
      "Batch: 23200,train loss is: 0.0007767939460063368\n",
      "test loss is 0.0008499631676654478\n",
      "Batch: 23300,train loss is: 0.0003387956339350901\n",
      "test loss is 0.0005745437139823029\n",
      "Batch: 23400,train loss is: 0.0005861302988214518\n",
      "test loss is 0.0004668680006024012\n",
      "Batch: 23500,train loss is: 0.0004973125933942525\n",
      "test loss is 0.0005160923193591163\n",
      "Batch: 23600,train loss is: 0.0006287929554602674\n",
      "test loss is 0.0006835863650997766\n",
      "Batch: 23700,train loss is: 0.0004851479314872416\n",
      "test loss is 0.0008484942981771065\n",
      "Batch: 23800,train loss is: 0.00029967327503529654\n",
      "test loss is 0.0004665957401038453\n",
      "Batch: 23900,train loss is: 0.0003270263716377285\n",
      "test loss is 0.0005199389199735204\n",
      "Batch: 24000,train loss is: 0.000651132824069455\n",
      "test loss is 0.0005052729165027208\n",
      "Batch: 24100,train loss is: 0.002112723848927681\n",
      "test loss is 0.0011241065684664783\n",
      "Batch: 24200,train loss is: 0.0007510481769326446\n",
      "test loss is 0.0006456986539896094\n",
      "Batch: 24300,train loss is: 0.00029506870902639283\n",
      "test loss is 0.0004489042599540269\n",
      "Batch: 24400,train loss is: 0.0008734339696654744\n",
      "test loss is 0.0008559550169870804\n",
      "Batch: 24500,train loss is: 0.0005333500587279725\n",
      "test loss is 0.0005580803995608259\n",
      "Batch: 24600,train loss is: 0.0005543660650758423\n",
      "test loss is 0.0007242350780808542\n",
      "Batch: 24700,train loss is: 0.0009074903824194538\n",
      "test loss is 0.0006361185795138725\n",
      "Batch: 24800,train loss is: 0.0008856334692977243\n",
      "test loss is 0.0009067440360258302\n",
      "Batch: 24900,train loss is: 0.0006640525926083791\n",
      "test loss is 0.0005011093581142675\n",
      "Batch: 25000,train loss is: 0.0005666018177876422\n",
      "test loss is 0.0007955278309390607\n",
      "Batch: 25100,train loss is: 0.0010839467738521656\n",
      "test loss is 0.0005492913643507627\n",
      "Batch: 25200,train loss is: 0.00046871060552793\n",
      "test loss is 0.00048580486339138004\n",
      "Batch: 25300,train loss is: 0.0004473983028469693\n",
      "test loss is 0.0006218612977658154\n",
      "Batch: 25400,train loss is: 0.0025444309563403223\n",
      "test loss is 0.000718809068341036\n",
      "Batch: 25500,train loss is: 0.0006026494048093302\n",
      "test loss is 0.0004716600385051401\n",
      "Batch: 25600,train loss is: 0.00037356388502207737\n",
      "test loss is 0.000442647048845266\n",
      "Batch: 25700,train loss is: 0.0004148646282656375\n",
      "test loss is 0.000495803254260094\n",
      "Batch: 25800,train loss is: 0.000820765161230861\n",
      "test loss is 0.0005347942446783964\n",
      "Batch: 25900,train loss is: 0.0002906241350356331\n",
      "test loss is 0.0005694486715141173\n",
      "Batch: 26000,train loss is: 0.0005614685790956192\n",
      "test loss is 0.0005456030545497387\n",
      "Batch: 26100,train loss is: 0.002604815709282631\n",
      "test loss is 0.0005644411812865593\n",
      "Batch: 26200,train loss is: 0.00037628659052760976\n",
      "test loss is 0.00044855040335834497\n",
      "Batch: 26300,train loss is: 0.00043648620185510027\n",
      "test loss is 0.0005345361021725824\n",
      "Batch: 26400,train loss is: 0.0006342779235742526\n",
      "test loss is 0.0006336953791590096\n",
      "Batch: 26500,train loss is: 0.00043631655801622585\n",
      "test loss is 0.0005968214811347528\n",
      "Batch: 26600,train loss is: 0.0003239167522320252\n",
      "test loss is 0.0007226446627404798\n",
      "Batch: 26700,train loss is: 0.0006977373073997689\n",
      "test loss is 0.0006696803291126943\n",
      "Batch: 26800,train loss is: 0.0010772176653929108\n",
      "test loss is 0.0007376377618245229\n",
      "Batch: 26900,train loss is: 0.0007600349364585738\n",
      "test loss is 0.0006021297379422793\n",
      "Batch: 27000,train loss is: 0.0003163497170764394\n",
      "test loss is 0.00046616118949659705\n",
      "Batch: 27100,train loss is: 0.0005828546493855425\n",
      "test loss is 0.0005645010239335042\n",
      "Batch: 27200,train loss is: 0.00047303979489965193\n",
      "test loss is 0.0004755272283198081\n",
      "Batch: 27300,train loss is: 0.0005461387224512119\n",
      "test loss is 0.0007377660846126641\n",
      "Batch: 27400,train loss is: 0.00032475982007408344\n",
      "test loss is 0.00047954896340905923\n",
      "Batch: 27500,train loss is: 0.0006047509670676304\n",
      "test loss is 0.0005163898179734534\n",
      "Batch: 27600,train loss is: 0.0003361526772498555\n",
      "test loss is 0.0005177944202142741\n",
      "Batch: 27700,train loss is: 0.00034195549395148536\n",
      "test loss is 0.0006094055155468111\n",
      "Batch: 27800,train loss is: 0.0005588485831054291\n",
      "test loss is 0.0006757752550707343\n",
      "Batch: 27900,train loss is: 0.0005511147453878835\n",
      "test loss is 0.0005226298454614393\n",
      "Batch: 28000,train loss is: 0.000642137249908254\n",
      "test loss is 0.00046506696375522036\n",
      "Batch: 28100,train loss is: 0.0008616371644072277\n",
      "test loss is 0.000533159713585932\n",
      "Batch: 28200,train loss is: 0.0004077583200864153\n",
      "test loss is 0.0005692574867324464\n",
      "Batch: 28300,train loss is: 0.0006306245661807698\n",
      "test loss is 0.0006692625372095689\n",
      "Batch: 28400,train loss is: 0.0004171478006618031\n",
      "test loss is 0.0005693737877298519\n",
      "Batch: 28500,train loss is: 0.0005101412140914819\n",
      "test loss is 0.00044588572406785925\n",
      "Batch: 28600,train loss is: 0.0007946487777828161\n",
      "test loss is 0.0007119060538748475\n",
      "Batch: 28700,train loss is: 0.00032033115557108095\n",
      "test loss is 0.0005124807785875348\n",
      "Batch: 28800,train loss is: 0.00041748097286437914\n",
      "test loss is 0.0005290022763965752\n",
      "Batch: 28900,train loss is: 0.0006291742832521218\n",
      "test loss is 0.0006176070681725702\n",
      "Batch: 29000,train loss is: 0.0006542912545557932\n",
      "test loss is 0.0005594918504368511\n",
      "Batch: 29100,train loss is: 0.0009448673984955496\n",
      "test loss is 0.0006103633057044327\n",
      "Batch: 29200,train loss is: 0.000516372356369028\n",
      "test loss is 0.0007795805675261327\n",
      "Batch: 29300,train loss is: 0.000576356182714594\n",
      "test loss is 0.0005457086762344827\n",
      "Batch: 29400,train loss is: 0.0007692984281937936\n",
      "test loss is 0.0007243520401793651\n",
      "Batch: 29500,train loss is: 0.0005026251402023075\n",
      "test loss is 0.0004556171145325274\n",
      "Batch: 29600,train loss is: 0.0007178835799850852\n",
      "test loss is 0.0005792511408681142\n",
      "Batch: 29700,train loss is: 0.0005949909035581941\n",
      "test loss is 0.00059801415074978\n",
      "Batch: 29800,train loss is: 0.0004713975460881057\n",
      "test loss is 0.00047588473393062443\n",
      "Batch: 29900,train loss is: 0.0005552285553887665\n",
      "test loss is 0.0004504497689365968\n",
      "Batch: 30000,train loss is: 0.0005715854326962989\n",
      "test loss is 0.0005550869351985328\n",
      "Batch: 30100,train loss is: 0.0004740768239658041\n",
      "test loss is 0.00042663745977372837\n",
      "Batch: 30200,train loss is: 0.000934711717979156\n",
      "test loss is 0.0008159963647824129\n",
      "Batch: 30300,train loss is: 0.0007213961244346696\n",
      "test loss is 0.0005655317223916613\n",
      "Batch: 30400,train loss is: 0.00035300749519527933\n",
      "test loss is 0.00046626581357178497\n",
      "Batch: 30500,train loss is: 0.0004424392421750564\n",
      "test loss is 0.0006385815127246613\n",
      "Batch: 30600,train loss is: 0.0007388630388914046\n",
      "test loss is 0.0005961879046220273\n",
      "Batch: 30700,train loss is: 0.0003922919427683469\n",
      "test loss is 0.0008557918696484828\n",
      "Batch: 30800,train loss is: 0.00038392335457749745\n",
      "test loss is 0.0004900342511080687\n",
      "Batch: 30900,train loss is: 0.0005561882964902175\n",
      "test loss is 0.0007128278313269256\n",
      "Batch: 31000,train loss is: 0.000906202079637296\n",
      "test loss is 0.0008163984616977112\n",
      "Batch: 31100,train loss is: 0.0005454154542033608\n",
      "test loss is 0.0006593650479511848\n",
      "Batch: 31200,train loss is: 0.0005365892557297465\n",
      "test loss is 0.00041549906295971584\n",
      "Batch: 31300,train loss is: 0.0006416390465059823\n",
      "test loss is 0.00046148783974669233\n",
      "Batch: 31400,train loss is: 0.0004906313046426083\n",
      "test loss is 0.00045561981857973377\n",
      "Batch: 31500,train loss is: 0.0006019193493428216\n",
      "test loss is 0.0005674791240019922\n",
      "Batch: 31600,train loss is: 0.0006474515436098903\n",
      "test loss is 0.0007093177258137753\n",
      "Batch: 31700,train loss is: 0.00044030040854212485\n",
      "test loss is 0.0005705361815543442\n",
      "Batch: 31800,train loss is: 0.0008736929608667958\n",
      "test loss is 0.0008129076507720453\n",
      "Batch: 31900,train loss is: 0.0005541971300176056\n",
      "test loss is 0.0005647640414891574\n",
      "Batch: 32000,train loss is: 0.0005504577374508728\n",
      "test loss is 0.0005442938464004182\n",
      "Batch: 32100,train loss is: 0.0004514571297179195\n",
      "test loss is 0.0007245462156486997\n",
      "Batch: 32200,train loss is: 0.0004114334112297809\n",
      "test loss is 0.000491871750457989\n",
      "Batch: 32300,train loss is: 0.0004937622917800789\n",
      "test loss is 0.00046773078760359706\n",
      "Batch: 32400,train loss is: 0.00062543368853148\n",
      "test loss is 0.0006977640451178903\n",
      "Batch: 32500,train loss is: 0.0027012982055427924\n",
      "test loss is 0.000589730380221081\n",
      "Batch: 32600,train loss is: 0.0005552589800481545\n",
      "test loss is 0.0006211980049380611\n",
      "Batch: 32700,train loss is: 0.00042402140110600866\n",
      "test loss is 0.0004946787948922612\n",
      "Batch: 32800,train loss is: 0.0004594792860758398\n",
      "test loss is 0.0007135561147152303\n",
      "Batch: 32900,train loss is: 0.0007664088440802591\n",
      "test loss is 0.0007876970325447169\n",
      "Batch: 33000,train loss is: 0.0004141393484622148\n",
      "test loss is 0.0005280295296103605\n",
      "Batch: 33100,train loss is: 0.0004513875707370245\n",
      "test loss is 0.0007367407620823555\n",
      "Batch: 33200,train loss is: 0.0003320885395081597\n",
      "test loss is 0.0006083590908723738\n",
      "Batch: 33300,train loss is: 0.0003854634979668405\n",
      "test loss is 0.0007144802000572523\n",
      "Batch: 33400,train loss is: 0.00032832490882155416\n",
      "test loss is 0.0005526003018344027\n",
      "Batch: 33500,train loss is: 0.0005583021425332765\n",
      "test loss is 0.00048259877530865396\n",
      "Batch: 33600,train loss is: 0.0006137572566558609\n",
      "test loss is 0.0004834512973336495\n",
      "Batch: 33700,train loss is: 0.00034696615355707145\n",
      "test loss is 0.0005618003872557968\n",
      "Batch: 33800,train loss is: 0.000848634744886081\n",
      "test loss is 0.0006168939919571932\n",
      "Batch: 33900,train loss is: 0.0007183103202466291\n",
      "test loss is 0.0008156912767171812\n",
      "Batch: 34000,train loss is: 0.0003814734136309496\n",
      "test loss is 0.0005872369881280786\n",
      "Batch: 34100,train loss is: 0.0006251491100898255\n",
      "test loss is 0.0005139323007886758\n",
      "Batch: 34200,train loss is: 0.0008663219982004621\n",
      "test loss is 0.000516002452748921\n",
      "Batch: 34300,train loss is: 0.00058298133348353\n",
      "test loss is 0.0005431229818823615\n",
      "Batch: 34400,train loss is: 0.0005951773323795199\n",
      "test loss is 0.0005882651907452472\n",
      "Batch: 34500,train loss is: 0.0005782509170621799\n",
      "test loss is 0.0006992256835117726\n",
      "Batch: 34600,train loss is: 0.0008407868770246551\n",
      "test loss is 0.0012013859066708238\n",
      "Batch: 34700,train loss is: 0.00046487076941628317\n",
      "test loss is 0.00047352506820991024\n",
      "Batch: 34800,train loss is: 0.0005880648725583056\n",
      "test loss is 0.0005854627324527877\n",
      "Batch: 34900,train loss is: 0.000560320230216075\n",
      "test loss is 0.0005072821855770807\n",
      "Batch: 35000,train loss is: 0.0005238930733595703\n",
      "test loss is 0.0004922604928233733\n",
      "Batch: 35100,train loss is: 0.0004256686343464305\n",
      "test loss is 0.0005545443640168729\n",
      "Batch: 35200,train loss is: 0.0005113168032965592\n",
      "test loss is 0.0005923557877160937\n",
      "Batch: 35300,train loss is: 0.0002910098983813491\n",
      "test loss is 0.0006020225612725479\n",
      "Batch: 35400,train loss is: 0.0004411140596091599\n",
      "test loss is 0.0004941978071198857\n",
      "Batch: 35500,train loss is: 0.0004905173485103833\n",
      "test loss is 0.0007276942635467684\n",
      "Batch: 35600,train loss is: 0.0006503495179318876\n",
      "test loss is 0.000567132728832107\n",
      "Batch: 35700,train loss is: 0.00045635706168845687\n",
      "test loss is 0.0005671629605480474\n",
      "Batch: 35800,train loss is: 0.00038910820386684706\n",
      "test loss is 0.0005935857748465062\n",
      "Batch: 35900,train loss is: 0.0006848098701553102\n",
      "test loss is 0.0004648987541595951\n",
      "Batch: 36000,train loss is: 0.0005492916083109794\n",
      "test loss is 0.0005807685654623262\n",
      "Batch: 36100,train loss is: 0.0008232154318522687\n",
      "test loss is 0.0005500427396687779\n",
      "Batch: 36200,train loss is: 0.0009265330071964686\n",
      "test loss is 0.0004640580015891166\n",
      "Batch: 36300,train loss is: 0.000792177703313851\n",
      "test loss is 0.0009459218241365194\n",
      "Batch: 36400,train loss is: 0.0004436025447600772\n",
      "test loss is 0.0004519363767732485\n",
      "Batch: 36500,train loss is: 0.000571872959246033\n",
      "test loss is 0.0004960186653400133\n",
      "Batch: 36600,train loss is: 0.0007159564654895961\n",
      "test loss is 0.0006207712249789004\n",
      "Batch: 36700,train loss is: 0.0002869327688011858\n",
      "test loss is 0.0004894989194051882\n",
      "Batch: 36800,train loss is: 0.00031646879578454447\n",
      "test loss is 0.0004984281003043505\n",
      "Batch: 36900,train loss is: 0.0006120834471028616\n",
      "test loss is 0.0008211831733317471\n",
      "Batch: 37000,train loss is: 0.0011502265824696024\n",
      "test loss is 0.0013563549254200313\n",
      "Batch: 37100,train loss is: 0.0005654938371918084\n",
      "test loss is 0.0005493202862063806\n",
      "Batch: 37200,train loss is: 0.00031309259349039355\n",
      "test loss is 0.0004565524526203648\n",
      "Batch: 37300,train loss is: 0.0007163014677390751\n",
      "test loss is 0.0005832575019979136\n",
      "Batch: 37400,train loss is: 0.0004311609600646497\n",
      "test loss is 0.0007869961547201682\n",
      "Batch: 37500,train loss is: 0.00032834723578222604\n",
      "test loss is 0.0005538296973947342\n",
      "Batch: 37600,train loss is: 0.0004163762849763588\n",
      "test loss is 0.0005823776224990845\n",
      "Batch: 37700,train loss is: 0.0004156284032074326\n",
      "test loss is 0.00046935239372620943\n",
      "Batch: 37800,train loss is: 0.0003087748683750553\n",
      "test loss is 0.0006398500404381411\n",
      "Batch: 37900,train loss is: 0.0004930769210842871\n",
      "test loss is 0.0006088987720926692\n",
      "Batch: 38000,train loss is: 0.0005999870249674827\n",
      "test loss is 0.0005387684644800146\n",
      "Batch: 38100,train loss is: 0.0007433475363266359\n",
      "test loss is 0.0005318938896074497\n",
      "Batch: 38200,train loss is: 0.0003056661188885245\n",
      "test loss is 0.00047822125778908763\n",
      "Batch: 38300,train loss is: 0.0005100313212899857\n",
      "test loss is 0.000605014820293012\n",
      "Batch: 38400,train loss is: 0.00047454400771170316\n",
      "test loss is 0.0006627786125706717\n",
      "Batch: 38500,train loss is: 0.0007794007206885206\n",
      "test loss is 0.0007600493398039085\n",
      "Batch: 38600,train loss is: 0.0011479736042691838\n",
      "test loss is 0.0006258283154215692\n",
      "Batch: 38700,train loss is: 0.0003594896363482761\n",
      "test loss is 0.00048181117937630397\n",
      "Batch: 38800,train loss is: 0.000636329834887633\n",
      "test loss is 0.0008402005272513718\n",
      "Batch: 38900,train loss is: 0.0005271271686258701\n",
      "test loss is 0.0006181150667673788\n",
      "Batch: 39000,train loss is: 0.0010513920708394276\n",
      "test loss is 0.0012382656274629851\n",
      "Batch: 39100,train loss is: 0.000794480607734892\n",
      "test loss is 0.0005318493057953622\n",
      "Batch: 39200,train loss is: 0.0007056818605025733\n",
      "test loss is 0.0009201759343097692\n",
      "Batch: 39300,train loss is: 0.0010674595635894834\n",
      "test loss is 0.0009039282389794527\n",
      "Batch: 39400,train loss is: 0.00036560882978444684\n",
      "test loss is 0.00046241255121743963\n",
      "Batch: 39500,train loss is: 0.0007543607394633017\n",
      "test loss is 0.0005092283507487955\n",
      "Batch: 39600,train loss is: 0.00044267182364860456\n",
      "test loss is 0.0006985340775873713\n",
      "Batch: 39700,train loss is: 0.0005234652861891895\n",
      "test loss is 0.0005176938086665116\n",
      "Batch: 39800,train loss is: 0.0002635588393334738\n",
      "test loss is 0.00043373021675289684\n",
      "Batch: 39900,train loss is: 0.0008299088065814487\n",
      "test loss is 0.00048555645464200536\n",
      "Batch: 40000,train loss is: 0.0005743821739772045\n",
      "test loss is 0.000712660324785299\n",
      "Batch: 40100,train loss is: 0.00033837084398441914\n",
      "test loss is 0.0006292241702336523\n",
      "Batch: 40200,train loss is: 0.0004079637110513042\n",
      "test loss is 0.00045707553211881297\n",
      "Batch: 40300,train loss is: 0.00027607525613762985\n",
      "test loss is 0.0004652908181303401\n",
      "Batch: 40400,train loss is: 0.0004507673694006867\n",
      "test loss is 0.0005569180556694742\n",
      "Batch: 40500,train loss is: 0.00024089597300240069\n",
      "test loss is 0.0005109633380124751\n",
      "Batch: 40600,train loss is: 0.0004989234386842717\n",
      "test loss is 0.00045670069629626103\n",
      "Batch: 40700,train loss is: 0.0008683435708769605\n",
      "test loss is 0.0007442416858798728\n",
      "Batch: 40800,train loss is: 0.000686094863899989\n",
      "test loss is 0.0006295384227213057\n",
      "Batch: 40900,train loss is: 0.00032022118155316756\n",
      "test loss is 0.0006093506998661745\n",
      "Batch: 41000,train loss is: 0.00036097335775323345\n",
      "test loss is 0.0005280627506089048\n",
      "Batch: 41100,train loss is: 0.0005692029838470209\n",
      "test loss is 0.0005708010624522621\n",
      "Batch: 41200,train loss is: 0.0005713103245309199\n",
      "test loss is 0.0008557902158682202\n",
      "Batch: 41300,train loss is: 0.0003988840217467939\n",
      "test loss is 0.0004596276933819632\n",
      "Batch: 41400,train loss is: 0.0005372592501785279\n",
      "test loss is 0.0005572069766926484\n",
      "Batch: 41500,train loss is: 0.0008426643494496486\n",
      "test loss is 0.0005983466671632527\n",
      "Batch: 41600,train loss is: 0.000519990128574012\n",
      "test loss is 0.0006507031700581946\n",
      "Batch: 41700,train loss is: 0.0005309297772341842\n",
      "test loss is 0.0008425362187715784\n",
      "Batch: 41800,train loss is: 0.000547934695289372\n",
      "test loss is 0.0004631443453511159\n",
      "Batch: 41900,train loss is: 0.00029468916124585044\n",
      "test loss is 0.0006297525263011525\n",
      "Batch: 42000,train loss is: 0.0004244743909158116\n",
      "test loss is 0.0006699770417000617\n",
      "Batch: 42100,train loss is: 0.0004376886948871659\n",
      "test loss is 0.0006221260959858347\n",
      "Batch: 42200,train loss is: 0.00030364757040098967\n",
      "test loss is 0.0004902811794490237\n",
      "Batch: 42300,train loss is: 0.0006874685466040938\n",
      "test loss is 0.0007429038873910306\n",
      "Batch: 42400,train loss is: 0.0005256361202470139\n",
      "test loss is 0.0006018104561899703\n",
      "Batch: 42500,train loss is: 0.0006907032038098471\n",
      "test loss is 0.0008151217083641932\n",
      "Batch: 42600,train loss is: 0.0007734907358284378\n",
      "test loss is 0.0004928849573467511\n",
      "Batch: 42700,train loss is: 0.0007091258542116475\n",
      "test loss is 0.0006168663459701669\n",
      "Batch: 42800,train loss is: 0.0004622808344452343\n",
      "test loss is 0.00047527155316523333\n",
      "Batch: 42900,train loss is: 0.0005344982505602371\n",
      "test loss is 0.0007572584638693107\n",
      "Batch: 43000,train loss is: 0.00041285109384437487\n",
      "test loss is 0.0006210437577840499\n",
      "Batch: 43100,train loss is: 0.0007500348157364529\n",
      "test loss is 0.0005659390588205471\n",
      "Batch: 43200,train loss is: 0.0005043053024356417\n",
      "test loss is 0.0005320609344663034\n",
      "Batch: 43300,train loss is: 0.0009682935204376964\n",
      "test loss is 0.0006334897287452052\n",
      "Batch: 43400,train loss is: 0.0005424938336132733\n",
      "test loss is 0.000559650165695354\n",
      "Batch: 43500,train loss is: 0.00033929355263501246\n",
      "test loss is 0.0005270838765783984\n",
      "Batch: 43600,train loss is: 0.0011227387611435857\n",
      "test loss is 0.0006139141087750097\n",
      "Batch: 43700,train loss is: 0.0008829635076840736\n",
      "test loss is 0.0005750262570116156\n",
      "Batch: 43800,train loss is: 0.00033385439445025476\n",
      "test loss is 0.0005706921264068991\n",
      "Batch: 43900,train loss is: 0.0006379236594963395\n",
      "test loss is 0.0006850680379310231\n",
      "Batch: 44000,train loss is: 0.0006382270840167084\n",
      "test loss is 0.00058724459385377\n",
      "Batch: 44100,train loss is: 0.00045432911503691804\n",
      "test loss is 0.0005275321784810908\n",
      "Batch: 44200,train loss is: 0.0009364706348903213\n",
      "test loss is 0.0005894920959343352\n",
      "Batch: 44300,train loss is: 0.0007763613234685936\n",
      "test loss is 0.0006901273308290974\n",
      "Batch: 44400,train loss is: 0.000932028415878103\n",
      "test loss is 0.0008150602206832804\n",
      "Batch: 44500,train loss is: 0.0006009486734123786\n",
      "test loss is 0.0006099796877665891\n",
      "Batch: 44600,train loss is: 0.0004466708647449802\n",
      "test loss is 0.0005848516091359548\n",
      "Batch: 44700,train loss is: 0.00041427928383399476\n",
      "test loss is 0.0004925835800676582\n",
      "Batch: 44800,train loss is: 0.0005740430205503143\n",
      "test loss is 0.0006646277814121243\n",
      "Batch: 44900,train loss is: 0.0004701747792336794\n",
      "test loss is 0.00047492768447246134\n",
      "Batch: 45000,train loss is: 0.0004830595191666955\n",
      "test loss is 0.0004870934566222847\n",
      "Batch: 45100,train loss is: 0.0005476152874072278\n",
      "test loss is 0.0006519277409150772\n",
      "Batch: 45200,train loss is: 0.00027263948772377665\n",
      "test loss is 0.0004143557527919644\n",
      "Batch: 45300,train loss is: 0.0006819586838567005\n",
      "test loss is 0.0006475058529117185\n",
      "Batch: 45400,train loss is: 0.0003806102939308071\n",
      "test loss is 0.0005859263050956129\n",
      "Batch: 45500,train loss is: 0.0003850243875367012\n",
      "test loss is 0.0007685378663609694\n",
      "Batch: 45600,train loss is: 0.0009002608838436563\n",
      "test loss is 0.0012409256948552815\n",
      "Batch: 45700,train loss is: 0.00036671509658449424\n",
      "test loss is 0.0005805395251640773\n",
      "Batch: 45800,train loss is: 0.0004704127388013467\n",
      "test loss is 0.0005524868512794597\n",
      "Batch: 45900,train loss is: 0.0007513654229437765\n",
      "test loss is 0.0009339227044065088\n",
      "Batch: 46000,train loss is: 0.0003356601743237687\n",
      "test loss is 0.0004785034430703313\n",
      "Batch: 46100,train loss is: 0.0004204599206854296\n",
      "test loss is 0.00046639766778929664\n",
      "Batch: 46200,train loss is: 0.0004519603631601157\n",
      "test loss is 0.0004999971079108363\n",
      "Batch: 46300,train loss is: 0.0006496012141700056\n",
      "test loss is 0.00046066809411889775\n",
      "Batch: 46400,train loss is: 0.0005094954623261443\n",
      "test loss is 0.0008554101233709955\n",
      "Batch: 46500,train loss is: 0.000341631595842646\n",
      "test loss is 0.0005154048326743671\n",
      "Batch: 46600,train loss is: 0.0005553176390087808\n",
      "test loss is 0.0004609276168766765\n",
      "Batch: 46700,train loss is: 0.0004921079359632805\n",
      "test loss is 0.0004603280444229521\n",
      "-----------------------Epoch: 4----------------------------------\n",
      "Batch: 0,train loss is: 0.00038771979381199326\n",
      "test loss is 0.0004462601538100927\n",
      "Batch: 100,train loss is: 0.00040091501421854806\n",
      "test loss is 0.0005771373389364066\n",
      "Batch: 200,train loss is: 0.0004555375109710737\n",
      "test loss is 0.0005010581806408125\n",
      "Batch: 300,train loss is: 0.0005152946587314189\n",
      "test loss is 0.0004985997529936538\n",
      "Batch: 400,train loss is: 0.0003465029940277407\n",
      "test loss is 0.0005415957906405161\n",
      "Batch: 500,train loss is: 0.0005323554605085668\n",
      "test loss is 0.0005238204713000405\n",
      "Batch: 600,train loss is: 0.0009648563209048069\n",
      "test loss is 0.0008376245081622965\n",
      "Batch: 700,train loss is: 0.0007299170785216918\n",
      "test loss is 0.0005137781182807866\n",
      "Batch: 800,train loss is: 0.00043952925581982027\n",
      "test loss is 0.0005513744765260486\n",
      "Batch: 900,train loss is: 0.0004108509453252782\n",
      "test loss is 0.00045999433856974464\n",
      "Batch: 1000,train loss is: 0.0004285929404764967\n",
      "test loss is 0.0005973373619297074\n",
      "Batch: 1100,train loss is: 0.0004071262128933674\n",
      "test loss is 0.0005200777891645589\n",
      "Batch: 1200,train loss is: 0.0003476200452210604\n",
      "test loss is 0.000432134513322262\n",
      "Batch: 1300,train loss is: 0.0006342468767067003\n",
      "test loss is 0.0006254037791669449\n",
      "Batch: 1400,train loss is: 0.0005592701009906131\n",
      "test loss is 0.00045835163740301564\n",
      "Batch: 1500,train loss is: 0.0005492456182557823\n",
      "test loss is 0.0006206349571858447\n",
      "Batch: 1600,train loss is: 0.0005831616037112054\n",
      "test loss is 0.0006295096579945193\n",
      "Batch: 1700,train loss is: 0.0009397905059148936\n",
      "test loss is 0.0004152018473011323\n",
      "Batch: 1800,train loss is: 0.0011753400882224922\n",
      "test loss is 0.0005280561543716605\n",
      "Batch: 1900,train loss is: 0.0005890281856635383\n",
      "test loss is 0.0006480512729266475\n",
      "Batch: 2000,train loss is: 0.0006124624412696514\n",
      "test loss is 0.0005074355534106082\n",
      "Batch: 2100,train loss is: 0.000587268905766464\n",
      "test loss is 0.0006812450295962689\n",
      "Batch: 2200,train loss is: 0.0003166288903864057\n",
      "test loss is 0.0004573150992756128\n",
      "Batch: 2300,train loss is: 0.0003370902244562219\n",
      "test loss is 0.0006057141820685755\n",
      "Batch: 2400,train loss is: 0.0006505650791740664\n",
      "test loss is 0.0006053511237420955\n",
      "Batch: 2500,train loss is: 0.0005253272765863095\n",
      "test loss is 0.0005666678443164531\n",
      "Batch: 2600,train loss is: 0.0005410294275040015\n",
      "test loss is 0.0005180561572626177\n",
      "Batch: 2700,train loss is: 0.00040665663538264117\n",
      "test loss is 0.0005206251564434707\n",
      "Batch: 2800,train loss is: 0.0003636470245820413\n",
      "test loss is 0.00048150034331840354\n",
      "Batch: 2900,train loss is: 0.0003574499885629352\n",
      "test loss is 0.0004325127494652749\n",
      "Batch: 3000,train loss is: 0.00032758151210675454\n",
      "test loss is 0.0004578459900592052\n",
      "Batch: 3100,train loss is: 0.00036607440734576024\n",
      "test loss is 0.0004790078943259362\n",
      "Batch: 3200,train loss is: 0.000521044835129319\n",
      "test loss is 0.0004929828073063098\n",
      "Batch: 3300,train loss is: 0.0017222506978069184\n",
      "test loss is 0.0007467691347647043\n",
      "Batch: 3400,train loss is: 0.0005127524553577342\n",
      "test loss is 0.0006110316479822199\n",
      "Batch: 3500,train loss is: 0.0002843151149599009\n",
      "test loss is 0.000505834622503403\n",
      "Batch: 3600,train loss is: 0.0002719721611231686\n",
      "test loss is 0.0005198507430979844\n",
      "Batch: 3700,train loss is: 0.0007686381819470803\n",
      "test loss is 0.0005518862257126598\n",
      "Batch: 3800,train loss is: 0.00042688257820064547\n",
      "test loss is 0.0011388196520150259\n",
      "Batch: 3900,train loss is: 0.000386133403024148\n",
      "test loss is 0.0004083399644591315\n",
      "Batch: 4000,train loss is: 0.0014369079186015343\n",
      "test loss is 0.0005840279385944341\n",
      "Batch: 4100,train loss is: 0.000467136564145589\n",
      "test loss is 0.00045744605299021207\n",
      "Batch: 4200,train loss is: 0.0003808478750665991\n",
      "test loss is 0.0004574898317854629\n",
      "Batch: 4300,train loss is: 0.0005205016178304895\n",
      "test loss is 0.0005058024797730835\n",
      "Batch: 4400,train loss is: 0.00032278178172776863\n",
      "test loss is 0.000422789866771678\n",
      "Batch: 4500,train loss is: 0.000509180248684204\n",
      "test loss is 0.00061962463483318\n",
      "Batch: 4600,train loss is: 0.00029983020595826537\n",
      "test loss is 0.0005691910515353161\n",
      "Batch: 4700,train loss is: 0.00065492635375188\n",
      "test loss is 0.00046025782861968783\n",
      "Batch: 4800,train loss is: 0.0003116552023157711\n",
      "test loss is 0.0004628842732955272\n",
      "Batch: 4900,train loss is: 0.0006038449101995583\n",
      "test loss is 0.0006587486837255185\n",
      "Batch: 5000,train loss is: 0.0007369075172251579\n",
      "test loss is 0.0006057195220732817\n",
      "Batch: 5100,train loss is: 0.0007747138383379728\n",
      "test loss is 0.0005299513024532866\n",
      "Batch: 5200,train loss is: 0.00031476414035431794\n",
      "test loss is 0.0005247683495025293\n",
      "Batch: 5300,train loss is: 0.0004380749168411791\n",
      "test loss is 0.0006637653909677891\n",
      "Batch: 5400,train loss is: 0.0003944215238202173\n",
      "test loss is 0.0005026116069215222\n",
      "Batch: 5500,train loss is: 0.0003654826589135193\n",
      "test loss is 0.0006265822654875681\n",
      "Batch: 5600,train loss is: 0.00044231591112437374\n",
      "test loss is 0.0006762191125540388\n",
      "Batch: 5700,train loss is: 0.00041012785954217066\n",
      "test loss is 0.0007135460821904058\n",
      "Batch: 5800,train loss is: 0.00035502499854094156\n",
      "test loss is 0.0006021893056504051\n",
      "Batch: 5900,train loss is: 0.0005603524008334546\n",
      "test loss is 0.000450783275286512\n",
      "Batch: 6000,train loss is: 0.0005119116248077823\n",
      "test loss is 0.0008884439376239004\n",
      "Batch: 6100,train loss is: 0.00048490757592096155\n",
      "test loss is 0.0004511642350099394\n",
      "Batch: 6200,train loss is: 0.00040503041838753216\n",
      "test loss is 0.0006334224641324713\n",
      "Batch: 6300,train loss is: 0.0003801208619606053\n",
      "test loss is 0.0005955551091980006\n",
      "Batch: 6400,train loss is: 0.00034004155716250886\n",
      "test loss is 0.00048583546870192195\n",
      "Batch: 6500,train loss is: 0.0016508628304859633\n",
      "test loss is 0.0010068991873544554\n",
      "Batch: 6600,train loss is: 0.0003469733528368424\n",
      "test loss is 0.0005651992723000405\n",
      "Batch: 6700,train loss is: 0.0003480328820258595\n",
      "test loss is 0.0005346412202170635\n",
      "Batch: 6800,train loss is: 0.0008076125433518389\n",
      "test loss is 0.0005852751824188055\n",
      "Batch: 6900,train loss is: 0.00025204681135265576\n",
      "test loss is 0.0004919279714393234\n",
      "Batch: 7000,train loss is: 0.0005931684663853062\n",
      "test loss is 0.0005739145297037293\n",
      "Batch: 7100,train loss is: 0.0014464926221914396\n",
      "test loss is 0.0006755924033264053\n",
      "Batch: 7200,train loss is: 0.0013534007337585592\n",
      "test loss is 0.0007401278265039364\n",
      "Batch: 7300,train loss is: 0.0005150083641082984\n",
      "test loss is 0.0004152921750168547\n",
      "Batch: 7400,train loss is: 0.0007325520220880061\n",
      "test loss is 0.0005383654004442783\n",
      "Batch: 7500,train loss is: 0.00034859936723578345\n",
      "test loss is 0.0006142556183044158\n",
      "Batch: 7600,train loss is: 0.0003826288568419279\n",
      "test loss is 0.000617671530411214\n",
      "Batch: 7700,train loss is: 0.0006680707288191476\n",
      "test loss is 0.00043631303838906745\n",
      "Batch: 7800,train loss is: 0.00038450475753371775\n",
      "test loss is 0.00044950910099153865\n",
      "Batch: 7900,train loss is: 0.0004594431112404968\n",
      "test loss is 0.0010380799884349057\n",
      "Batch: 8000,train loss is: 0.0003818805505217159\n",
      "test loss is 0.0008610421208476885\n",
      "Batch: 8100,train loss is: 0.0002925016566577424\n",
      "test loss is 0.0004555001745314063\n",
      "Batch: 8200,train loss is: 0.0003874991624482707\n",
      "test loss is 0.000422929799532557\n",
      "Batch: 8300,train loss is: 0.0005337199374378073\n",
      "test loss is 0.0005110392116747191\n",
      "Batch: 8400,train loss is: 0.00029337271540064554\n",
      "test loss is 0.0006401198354056035\n",
      "Batch: 8500,train loss is: 0.00042831732880414905\n",
      "test loss is 0.0005679542726501317\n",
      "Batch: 8600,train loss is: 0.0004981175080566253\n",
      "test loss is 0.0009397357460174352\n",
      "Batch: 8700,train loss is: 0.00036617957971415353\n",
      "test loss is 0.0006339475981590233\n",
      "Batch: 8800,train loss is: 0.0009483775433107187\n",
      "test loss is 0.000746967685574056\n",
      "Batch: 8900,train loss is: 0.0005419740794403773\n",
      "test loss is 0.0004986013336936636\n",
      "Batch: 9000,train loss is: 0.0005323192595925693\n",
      "test loss is 0.0006607463201678347\n",
      "Batch: 9100,train loss is: 0.0004745449208599502\n",
      "test loss is 0.0004429592688808759\n",
      "Batch: 9200,train loss is: 0.0003078715573858376\n",
      "test loss is 0.0006027598481218987\n",
      "Batch: 9300,train loss is: 0.00037332703670364827\n",
      "test loss is 0.0004411523879948113\n",
      "Batch: 9400,train loss is: 0.0009430374763297923\n",
      "test loss is 0.0007425523118260328\n",
      "Batch: 9500,train loss is: 0.0009120056065405317\n",
      "test loss is 0.0004882602396674761\n",
      "Batch: 9600,train loss is: 0.0004707805929270339\n",
      "test loss is 0.0004365552709177993\n",
      "Batch: 9700,train loss is: 0.000533011303400236\n",
      "test loss is 0.00044598153750234105\n",
      "Batch: 9800,train loss is: 0.0007637141361542346\n",
      "test loss is 0.0005129922022281226\n",
      "Batch: 9900,train loss is: 0.0005545375977665835\n",
      "test loss is 0.0006860957950234033\n",
      "Batch: 10000,train loss is: 0.00042243896835372384\n",
      "test loss is 0.00048796456113164193\n",
      "Batch: 10100,train loss is: 0.00028095670663591266\n",
      "test loss is 0.0004091719054074908\n",
      "Batch: 10200,train loss is: 0.0005432718794302425\n",
      "test loss is 0.0006708677024810086\n",
      "Batch: 10300,train loss is: 0.0008176553188241466\n",
      "test loss is 0.0006790068983727455\n",
      "Batch: 10400,train loss is: 0.00046147361129148206\n",
      "test loss is 0.0005802303675123316\n",
      "Batch: 10500,train loss is: 0.0004351748238745024\n",
      "test loss is 0.0005508925951274527\n",
      "Batch: 10600,train loss is: 0.0004515548897422968\n",
      "test loss is 0.0005051222057995497\n",
      "Batch: 10700,train loss is: 0.00040100478667496773\n",
      "test loss is 0.00047688841483913886\n",
      "Batch: 10800,train loss is: 0.0004761569878699094\n",
      "test loss is 0.0005302502788052492\n",
      "Batch: 10900,train loss is: 0.00031564284674661037\n",
      "test loss is 0.0004731852378677899\n",
      "Batch: 11000,train loss is: 0.00033427039881292594\n",
      "test loss is 0.0006766939688747821\n",
      "Batch: 11100,train loss is: 0.000508094494112805\n",
      "test loss is 0.00045515052992202757\n",
      "Batch: 11200,train loss is: 0.0007769914355201654\n",
      "test loss is 0.000422404564063633\n",
      "Batch: 11300,train loss is: 0.0005638120956463581\n",
      "test loss is 0.00045891614853600356\n",
      "Batch: 11400,train loss is: 0.0004681550175263472\n",
      "test loss is 0.0005388030631395223\n",
      "Batch: 11500,train loss is: 0.0004239091915848388\n",
      "test loss is 0.000594826800934535\n",
      "Batch: 11600,train loss is: 0.00037548545047533287\n",
      "test loss is 0.0009392548052697552\n",
      "Batch: 11700,train loss is: 0.00039003727591543387\n",
      "test loss is 0.0004175678708707918\n",
      "Batch: 11800,train loss is: 0.000398078413812548\n",
      "test loss is 0.0007318746368734654\n",
      "Batch: 11900,train loss is: 0.0005347682766296857\n",
      "test loss is 0.0005110078795300884\n",
      "Batch: 12000,train loss is: 0.0005281363878840516\n",
      "test loss is 0.00042532032292880894\n",
      "Batch: 12100,train loss is: 0.0004921893770286782\n",
      "test loss is 0.0005540026884446069\n",
      "Batch: 12200,train loss is: 0.00035919772154639995\n",
      "test loss is 0.0004376304312266688\n",
      "Batch: 12300,train loss is: 0.0006136696049432998\n",
      "test loss is 0.000549527655513788\n",
      "Batch: 12400,train loss is: 0.0004885454194435855\n",
      "test loss is 0.0005189628170890118\n",
      "Batch: 12500,train loss is: 0.0004354239905686179\n",
      "test loss is 0.00041651160880162814\n",
      "Batch: 12600,train loss is: 0.00040155625048218\n",
      "test loss is 0.0005318692657422417\n",
      "Batch: 12700,train loss is: 0.0005766247798515133\n",
      "test loss is 0.0008187376158753212\n",
      "Batch: 12800,train loss is: 0.00046768114188268253\n",
      "test loss is 0.0005541953668806186\n",
      "Batch: 12900,train loss is: 0.0003961229113676641\n",
      "test loss is 0.0004153171943593686\n",
      "Batch: 13000,train loss is: 0.0004254801893626954\n",
      "test loss is 0.0005133208056361754\n",
      "Batch: 13100,train loss is: 0.00039198824115161047\n",
      "test loss is 0.0004638869686319808\n",
      "Batch: 13200,train loss is: 0.0008115968867669403\n",
      "test loss is 0.0009982532391762668\n",
      "Batch: 13300,train loss is: 0.0002948773868573228\n",
      "test loss is 0.00045199247507014074\n",
      "Batch: 13400,train loss is: 0.00027772536141634687\n",
      "test loss is 0.0004701541862945577\n",
      "Batch: 13500,train loss is: 0.0005550670869132284\n",
      "test loss is 0.0004593449015432429\n",
      "Batch: 13600,train loss is: 0.0005511160190682911\n",
      "test loss is 0.0004664959880286083\n",
      "Batch: 13700,train loss is: 0.0008143655803473221\n",
      "test loss is 0.0005157099116111181\n",
      "Batch: 13800,train loss is: 0.00036873285657514673\n",
      "test loss is 0.0010123456832001954\n",
      "Batch: 13900,train loss is: 0.00039621880201241323\n",
      "test loss is 0.0004356398347069848\n",
      "Batch: 14000,train loss is: 0.0003541977601920554\n",
      "test loss is 0.0004937067218780697\n",
      "Batch: 14100,train loss is: 0.0005555437906608369\n",
      "test loss is 0.0006234252169987105\n",
      "Batch: 14200,train loss is: 0.000641471092850021\n",
      "test loss is 0.0005428514494838677\n",
      "Batch: 14300,train loss is: 0.00027992367742695715\n",
      "test loss is 0.00046928942286265105\n",
      "Batch: 14400,train loss is: 0.0003765625834494747\n",
      "test loss is 0.0008397039396750267\n",
      "Batch: 14500,train loss is: 0.0004543806884032489\n",
      "test loss is 0.0006233443986031491\n",
      "Batch: 14600,train loss is: 0.00039813979502270494\n",
      "test loss is 0.0006609615084783055\n",
      "Batch: 14700,train loss is: 0.000477494367967025\n",
      "test loss is 0.0004921513555146002\n",
      "Batch: 14800,train loss is: 0.00040811194461258277\n",
      "test loss is 0.0005519634417563\n",
      "Batch: 14900,train loss is: 0.0004937276346639748\n",
      "test loss is 0.0004184114612928614\n",
      "Batch: 15000,train loss is: 0.00043579514561961147\n",
      "test loss is 0.0005093030591293629\n",
      "Batch: 15100,train loss is: 0.0006029598884089019\n",
      "test loss is 0.0005315050087333963\n",
      "Batch: 15200,train loss is: 0.00035430657086119575\n",
      "test loss is 0.0004851854631137808\n",
      "Batch: 15300,train loss is: 0.0007990912386429309\n",
      "test loss is 0.000799470715933261\n",
      "Batch: 15400,train loss is: 0.0007309418492381019\n",
      "test loss is 0.0007619162194225136\n",
      "Batch: 15500,train loss is: 0.0003852327175842111\n",
      "test loss is 0.0004098627717524034\n",
      "Batch: 15600,train loss is: 0.0005822896356319413\n",
      "test loss is 0.0005683088428044427\n",
      "Batch: 15700,train loss is: 0.0005230647719951674\n",
      "test loss is 0.0005208374384839587\n",
      "Batch: 15800,train loss is: 0.0005190907078508283\n",
      "test loss is 0.0005107702825514257\n",
      "Batch: 15900,train loss is: 0.0006736557684144654\n",
      "test loss is 0.0007026054361335149\n",
      "Batch: 16000,train loss is: 0.0005060450967555434\n",
      "test loss is 0.0004298052197443229\n",
      "Batch: 16100,train loss is: 0.0007370192430207234\n",
      "test loss is 0.0005104905794448305\n",
      "Batch: 16200,train loss is: 0.00046928363657551097\n",
      "test loss is 0.0004626625255964735\n",
      "Batch: 16300,train loss is: 0.0012497524012931782\n",
      "test loss is 0.0005253165837141013\n",
      "Batch: 16400,train loss is: 0.00047097920847255644\n",
      "test loss is 0.0005031769163647918\n",
      "Batch: 16500,train loss is: 0.0003958077168147321\n",
      "test loss is 0.0004321611529569331\n",
      "Batch: 16600,train loss is: 0.0005794765172800344\n",
      "test loss is 0.0005129709522905008\n",
      "Batch: 16700,train loss is: 0.0004662542423109759\n",
      "test loss is 0.00045134696708233537\n",
      "Batch: 16800,train loss is: 0.0010047390389891053\n",
      "test loss is 0.0005177927339032514\n",
      "Batch: 16900,train loss is: 0.0005153071375295738\n",
      "test loss is 0.00044364424180959647\n",
      "Batch: 17000,train loss is: 0.0006375835654994747\n",
      "test loss is 0.0005605027916262677\n",
      "Batch: 17100,train loss is: 0.00036375440459444414\n",
      "test loss is 0.0004761716175397464\n",
      "Batch: 17200,train loss is: 0.0006332238293917933\n",
      "test loss is 0.0004959571992392105\n",
      "Batch: 17300,train loss is: 0.0005839090934649486\n",
      "test loss is 0.0004523977203384724\n",
      "Batch: 17400,train loss is: 0.00048739055956155744\n",
      "test loss is 0.0006029207263094111\n",
      "Batch: 17500,train loss is: 0.0006634412246493155\n",
      "test loss is 0.0004992955146154577\n",
      "Batch: 17600,train loss is: 0.00043921774508768376\n",
      "test loss is 0.0006354108963647166\n",
      "Batch: 17700,train loss is: 0.0006862762469076416\n",
      "test loss is 0.0006411351852055381\n",
      "Batch: 17800,train loss is: 0.0004944906586717356\n",
      "test loss is 0.0007336342293797328\n",
      "Batch: 17900,train loss is: 0.000198229628442815\n",
      "test loss is 0.00043710739986898666\n",
      "Batch: 18000,train loss is: 0.00046801953507995947\n",
      "test loss is 0.00043205320463262536\n",
      "Batch: 18100,train loss is: 0.00040907554835867955\n",
      "test loss is 0.0006079434844448869\n",
      "Batch: 18200,train loss is: 0.0007103295892143553\n",
      "test loss is 0.0007763627729405033\n",
      "Batch: 18300,train loss is: 0.0003455237473541237\n",
      "test loss is 0.0004100861933649956\n",
      "Batch: 18400,train loss is: 0.00036556963341467823\n",
      "test loss is 0.0004509098465925594\n",
      "Batch: 18500,train loss is: 0.0006926814677923019\n",
      "test loss is 0.000556118784552395\n",
      "Batch: 18600,train loss is: 0.000551629917167806\n",
      "test loss is 0.0005269279076218061\n",
      "Batch: 18700,train loss is: 0.0002819809059820452\n",
      "test loss is 0.0005052248333790582\n",
      "Batch: 18800,train loss is: 0.0007996979723644358\n",
      "test loss is 0.0005080822969588455\n",
      "Batch: 18900,train loss is: 0.0003398638522267236\n",
      "test loss is 0.0005004616501765107\n",
      "Batch: 19000,train loss is: 0.0007034919979779968\n",
      "test loss is 0.0005426591162196042\n",
      "Batch: 19100,train loss is: 0.0016822736462603822\n",
      "test loss is 0.0006832813359242788\n",
      "Batch: 19200,train loss is: 0.00030295621292524986\n",
      "test loss is 0.0005374891733850195\n",
      "Batch: 19300,train loss is: 0.0006575976401047771\n",
      "test loss is 0.0005857552745739269\n",
      "Batch: 19400,train loss is: 0.0015203604502121411\n",
      "test loss is 0.0009527356239556166\n",
      "Batch: 19500,train loss is: 0.0010360213031253194\n",
      "test loss is 0.0006535676777600792\n",
      "Batch: 19600,train loss is: 0.0005134347841948044\n",
      "test loss is 0.00041269328947124304\n",
      "Batch: 19700,train loss is: 0.0006990323484763617\n",
      "test loss is 0.0006667055770390701\n",
      "Batch: 19800,train loss is: 0.00039368424892558397\n",
      "test loss is 0.00048791732420769025\n",
      "Batch: 19900,train loss is: 0.0003238981274627441\n",
      "test loss is 0.0006276667338916835\n",
      "Batch: 20000,train loss is: 0.00041443166088494377\n",
      "test loss is 0.00043001782580922063\n",
      "Batch: 20100,train loss is: 0.0009156165813293761\n",
      "test loss is 0.0004520500820722928\n",
      "Batch: 20200,train loss is: 0.00041087513773416294\n",
      "test loss is 0.00043978836618638914\n",
      "Batch: 20300,train loss is: 0.0004237319314903882\n",
      "test loss is 0.0004610409133240998\n",
      "Batch: 20400,train loss is: 0.0004159544614176308\n",
      "test loss is 0.00046536741618914234\n",
      "Batch: 20500,train loss is: 0.0004191986854010041\n",
      "test loss is 0.0006257829567900044\n",
      "Batch: 20600,train loss is: 0.0003806005391616382\n",
      "test loss is 0.0005912731853193262\n",
      "Batch: 20700,train loss is: 0.0006850669242480234\n",
      "test loss is 0.0003910456923689922\n",
      "Batch: 20800,train loss is: 0.0005371492049144916\n",
      "test loss is 0.0005281182920118669\n",
      "Batch: 20900,train loss is: 0.0006077204256124877\n",
      "test loss is 0.0006755315652958808\n",
      "Batch: 21000,train loss is: 0.00045712538027428576\n",
      "test loss is 0.000443553407595411\n",
      "Batch: 21100,train loss is: 0.0006033378688702113\n",
      "test loss is 0.0006697313864462233\n",
      "Batch: 21200,train loss is: 0.0003479978314210806\n",
      "test loss is 0.000461959868515289\n",
      "Batch: 21300,train loss is: 0.0005643463253553972\n",
      "test loss is 0.00046971693230962974\n",
      "Batch: 21400,train loss is: 0.0005169869263505409\n",
      "test loss is 0.0007019880486543882\n",
      "Batch: 21500,train loss is: 0.0006005504346515737\n",
      "test loss is 0.0005018565672298852\n",
      "Batch: 21600,train loss is: 0.0005848527742531467\n",
      "test loss is 0.0006031314970087834\n",
      "Batch: 21700,train loss is: 0.000519232743632926\n",
      "test loss is 0.0006741167537189395\n",
      "Batch: 21800,train loss is: 0.0005319027859169921\n",
      "test loss is 0.0008290627878761519\n",
      "Batch: 21900,train loss is: 0.0009405717953733263\n",
      "test loss is 0.0005797692424452971\n",
      "Batch: 22000,train loss is: 0.0006434921205636697\n",
      "test loss is 0.0005007750185626772\n",
      "Batch: 22100,train loss is: 0.00033268873796327614\n",
      "test loss is 0.0003841824441003492\n",
      "Batch: 22200,train loss is: 0.0005993796137926316\n",
      "test loss is 0.0006530514300257045\n",
      "Batch: 22300,train loss is: 0.0005146651782131435\n",
      "test loss is 0.0007128949573755673\n",
      "Batch: 22400,train loss is: 0.0003618490101942227\n",
      "test loss is 0.00044196586751296316\n",
      "Batch: 22500,train loss is: 0.00032541354442154434\n",
      "test loss is 0.0004912977641879099\n",
      "Batch: 22600,train loss is: 0.0006227954120078958\n",
      "test loss is 0.0006172147093469488\n",
      "Batch: 22700,train loss is: 0.00032460615399744594\n",
      "test loss is 0.0005400065101904303\n",
      "Batch: 22800,train loss is: 0.0003775618430973414\n",
      "test loss is 0.00048265544327387536\n",
      "Batch: 22900,train loss is: 0.0003039954783066862\n",
      "test loss is 0.0005439886235821977\n",
      "Batch: 23000,train loss is: 0.0004474951570357901\n",
      "test loss is 0.0005132264929254875\n",
      "Batch: 23100,train loss is: 0.0005733388113086438\n",
      "test loss is 0.0005928096397565432\n",
      "Batch: 23200,train loss is: 0.00036013388862661804\n",
      "test loss is 0.000533530223694078\n",
      "Batch: 23300,train loss is: 0.0006467205410630871\n",
      "test loss is 0.0005387310906649643\n",
      "Batch: 23400,train loss is: 0.0003712439193731485\n",
      "test loss is 0.0004863781762090087\n",
      "Batch: 23500,train loss is: 0.0002984731811671164\n",
      "test loss is 0.0004993043235385841\n",
      "Batch: 23600,train loss is: 0.00036621757855755956\n",
      "test loss is 0.00044663275308872536\n",
      "Batch: 23700,train loss is: 0.000654695167996281\n",
      "test loss is 0.0005472257630846053\n",
      "Batch: 23800,train loss is: 0.0004518851309572398\n",
      "test loss is 0.000493600034014711\n",
      "Batch: 23900,train loss is: 0.0003436786955471091\n",
      "test loss is 0.0004666362031387553\n",
      "Batch: 24000,train loss is: 0.0008509841461045609\n",
      "test loss is 0.0005573007017051013\n",
      "Batch: 24100,train loss is: 0.0006602181889611689\n",
      "test loss is 0.00048388049510535684\n",
      "Batch: 24200,train loss is: 0.0002546769703123616\n",
      "test loss is 0.00043054774027425355\n",
      "Batch: 24300,train loss is: 0.0007054870710262856\n",
      "test loss is 0.0005951816696112878\n",
      "Batch: 24400,train loss is: 0.0008181600246217325\n",
      "test loss is 0.0005717473156366283\n",
      "Batch: 24500,train loss is: 0.0007190185297554915\n",
      "test loss is 0.00042570925483882097\n",
      "Batch: 24600,train loss is: 0.000542474008726584\n",
      "test loss is 0.0004944747683278696\n",
      "Batch: 24700,train loss is: 0.0003855534029391228\n",
      "test loss is 0.0004312433371166259\n",
      "Batch: 24800,train loss is: 0.0008747462464465603\n",
      "test loss is 0.000531299348924549\n",
      "Batch: 24900,train loss is: 0.00023354721827797373\n",
      "test loss is 0.00046632568174313305\n",
      "Batch: 25000,train loss is: 0.000352692921760411\n",
      "test loss is 0.0004305764858423569\n",
      "Batch: 25100,train loss is: 0.0005908498786805843\n",
      "test loss is 0.00045981569606086316\n",
      "Batch: 25200,train loss is: 0.0005143660922662416\n",
      "test loss is 0.00054582620033791\n",
      "Batch: 25300,train loss is: 0.00035811000236457774\n",
      "test loss is 0.0004177764401097131\n",
      "Batch: 25400,train loss is: 0.0005607520149633919\n",
      "test loss is 0.0006811115077437809\n",
      "Batch: 25500,train loss is: 0.0004093342963422113\n",
      "test loss is 0.00046721543652286776\n",
      "Batch: 25600,train loss is: 0.0011929305582196775\n",
      "test loss is 0.0007302797538688211\n",
      "Batch: 25700,train loss is: 0.0006072433755132823\n",
      "test loss is 0.0004935497742144528\n",
      "Batch: 25800,train loss is: 0.00038642484906727993\n",
      "test loss is 0.00045348034032661376\n",
      "Batch: 25900,train loss is: 0.0003601801721201946\n",
      "test loss is 0.0005288836964138384\n",
      "Batch: 26000,train loss is: 0.0002622769225173844\n",
      "test loss is 0.0005303809190587342\n",
      "Batch: 26100,train loss is: 0.0002898612251009002\n",
      "test loss is 0.000493221939112555\n",
      "Batch: 26200,train loss is: 0.0005158456587675837\n",
      "test loss is 0.0005285881478318238\n",
      "Batch: 26300,train loss is: 0.0006697584967959192\n",
      "test loss is 0.0005263965514623199\n",
      "Batch: 26400,train loss is: 0.000528719206282071\n",
      "test loss is 0.0005888969116343707\n",
      "Batch: 26500,train loss is: 0.00037585511826415094\n",
      "test loss is 0.0004240943163340521\n",
      "Batch: 26600,train loss is: 0.00040878564158950467\n",
      "test loss is 0.0007138003936515573\n",
      "Batch: 26700,train loss is: 0.00032717637766678943\n",
      "test loss is 0.00044528691833409847\n",
      "Batch: 26800,train loss is: 0.000763847710925851\n",
      "test loss is 0.0004719391003319088\n",
      "Batch: 26900,train loss is: 0.0004137630342936967\n",
      "test loss is 0.0006040906895677545\n",
      "Batch: 27000,train loss is: 0.00048052241129970083\n",
      "test loss is 0.00044458210685321893\n",
      "Batch: 27100,train loss is: 0.0003378261564985821\n",
      "test loss is 0.0004385718002741393\n",
      "Batch: 27200,train loss is: 0.0004053277218281064\n",
      "test loss is 0.00043682711235391695\n",
      "Batch: 27300,train loss is: 0.00037352592494706126\n",
      "test loss is 0.0004721498050370486\n",
      "Batch: 27400,train loss is: 0.0002822959337323295\n",
      "test loss is 0.00040799888298987194\n",
      "Batch: 27500,train loss is: 0.0003809618952004059\n",
      "test loss is 0.0010288622251870556\n",
      "Batch: 27600,train loss is: 0.00028725180895072415\n",
      "test loss is 0.00044007056693246623\n",
      "Batch: 27700,train loss is: 0.00044178575931568106\n",
      "test loss is 0.0005440004524867858\n",
      "Batch: 27800,train loss is: 0.0006492144452786503\n",
      "test loss is 0.0004517333974478815\n",
      "Batch: 27900,train loss is: 0.0008596719904185167\n",
      "test loss is 0.0007934824277900149\n",
      "Batch: 28000,train loss is: 0.00035983905514379614\n",
      "test loss is 0.0004048003714912547\n",
      "Batch: 28100,train loss is: 0.00044182216744473\n",
      "test loss is 0.000520831269988664\n",
      "Batch: 28200,train loss is: 0.0011275646892308306\n",
      "test loss is 0.0005671432963169614\n",
      "Batch: 28300,train loss is: 0.0006539737864367577\n",
      "test loss is 0.0005003445762260454\n",
      "Batch: 28400,train loss is: 0.0003188630409099991\n",
      "test loss is 0.00039386331454597834\n",
      "Batch: 28500,train loss is: 0.00045079034110661817\n",
      "test loss is 0.0005600675410156223\n",
      "Batch: 28600,train loss is: 0.0004094554337854714\n",
      "test loss is 0.00043447642405091205\n",
      "Batch: 28700,train loss is: 0.0008811830103699749\n",
      "test loss is 0.0005281857394671614\n",
      "Batch: 28800,train loss is: 0.0005189363553772758\n",
      "test loss is 0.0004765554388438878\n",
      "Batch: 28900,train loss is: 0.0008400869324192854\n",
      "test loss is 0.0005141086694168375\n",
      "Batch: 29000,train loss is: 0.000438817927407532\n",
      "test loss is 0.0006124605651219193\n",
      "Batch: 29100,train loss is: 0.00023368101770778753\n",
      "test loss is 0.0005467304161569451\n",
      "Batch: 29200,train loss is: 0.0011125379113481127\n",
      "test loss is 0.0006201741888405265\n",
      "Batch: 29300,train loss is: 0.0006305032488725156\n",
      "test loss is 0.000556948978157174\n",
      "Batch: 29400,train loss is: 0.0005143013639537959\n",
      "test loss is 0.0005488668920487623\n",
      "Batch: 29500,train loss is: 0.00047080068218131947\n",
      "test loss is 0.00046119276691695456\n",
      "Batch: 29600,train loss is: 0.00040879103660859434\n",
      "test loss is 0.0004345525208961884\n",
      "Batch: 29700,train loss is: 0.0004601672147853586\n",
      "test loss is 0.0005073127058992424\n",
      "Batch: 29800,train loss is: 0.0004297140089320788\n",
      "test loss is 0.0005487099544688648\n",
      "Batch: 29900,train loss is: 0.000528011008317603\n",
      "test loss is 0.0008619867231874074\n",
      "Batch: 30000,train loss is: 0.0007080626052797039\n",
      "test loss is 0.00048455280877260625\n",
      "Batch: 30100,train loss is: 0.0006223357277140192\n",
      "test loss is 0.0006355464719672981\n",
      "Batch: 30200,train loss is: 0.0008329266869309333\n",
      "test loss is 0.0005331524967289999\n",
      "Batch: 30300,train loss is: 0.0003405143703431738\n",
      "test loss is 0.0005382759344915323\n",
      "Batch: 30400,train loss is: 0.0006139668086948503\n",
      "test loss is 0.0004652582918181926\n",
      "Batch: 30500,train loss is: 0.0003978232466555701\n",
      "test loss is 0.00039349730904908746\n",
      "Batch: 30600,train loss is: 0.0009307458115615649\n",
      "test loss is 0.0005763294158164953\n",
      "Batch: 30700,train loss is: 0.0008661641823102268\n",
      "test loss is 0.0006229712567122844\n",
      "Batch: 30800,train loss is: 0.000339641925370297\n",
      "test loss is 0.000574428575332076\n",
      "Batch: 30900,train loss is: 0.0008994628253660879\n",
      "test loss is 0.000583645958294572\n",
      "Batch: 31000,train loss is: 0.0010399980020420134\n",
      "test loss is 0.00041925884883045145\n",
      "Batch: 31100,train loss is: 0.0005862680877182216\n",
      "test loss is 0.0004391044928060429\n",
      "Batch: 31200,train loss is: 0.0007142796470135589\n",
      "test loss is 0.0005033410048324352\n",
      "Batch: 31300,train loss is: 0.0006162821505389451\n",
      "test loss is 0.0006680764617435489\n",
      "Batch: 31400,train loss is: 0.0005322956437289146\n",
      "test loss is 0.0004495757415572458\n",
      "Batch: 31500,train loss is: 0.0004329494069428996\n",
      "test loss is 0.00047324951871316766\n",
      "Batch: 31600,train loss is: 0.00038406175301050524\n",
      "test loss is 0.00042887146589204884\n",
      "Batch: 31700,train loss is: 0.0006706379099864203\n",
      "test loss is 0.0004915912878857867\n",
      "Batch: 31800,train loss is: 0.00030264087317094686\n",
      "test loss is 0.0005268510401185543\n",
      "Batch: 31900,train loss is: 0.0007841138842485705\n",
      "test loss is 0.0007321932196959791\n",
      "Batch: 32000,train loss is: 0.0006768352813898608\n",
      "test loss is 0.000502796646828673\n",
      "Batch: 32100,train loss is: 0.0002513056010570523\n",
      "test loss is 0.0004944182234305243\n",
      "Batch: 32200,train loss is: 0.0005952221562579515\n",
      "test loss is 0.0005425858281699317\n",
      "Batch: 32300,train loss is: 0.00042957898906611564\n",
      "test loss is 0.0005380040883178835\n",
      "Batch: 32400,train loss is: 0.00046236367097303606\n",
      "test loss is 0.0004097122682988038\n",
      "Batch: 32500,train loss is: 0.0006709011606921639\n",
      "test loss is 0.0005848737633825849\n",
      "Batch: 32600,train loss is: 0.0009430687251282419\n",
      "test loss is 0.0006386774587724996\n",
      "Batch: 32700,train loss is: 0.0006652317396379304\n",
      "test loss is 0.00042604119407162254\n",
      "Batch: 32800,train loss is: 0.0004809651302230999\n",
      "test loss is 0.0005886010017699039\n",
      "Batch: 32900,train loss is: 0.0002767981892119881\n",
      "test loss is 0.00045042993957224327\n",
      "Batch: 33000,train loss is: 0.0007688791826075799\n",
      "test loss is 0.00043846848608280646\n",
      "Batch: 33100,train loss is: 0.0003725178134421982\n",
      "test loss is 0.0005168845659706031\n",
      "Batch: 33200,train loss is: 0.0004510323980166439\n",
      "test loss is 0.00041180906960815454\n",
      "Batch: 33300,train loss is: 0.0005075894933688133\n",
      "test loss is 0.0008005725650439719\n",
      "Batch: 33400,train loss is: 0.0005402614284783328\n",
      "test loss is 0.0006548798573466261\n",
      "Batch: 33500,train loss is: 0.00039546364636062104\n",
      "test loss is 0.0004996431307225443\n",
      "Batch: 33600,train loss is: 0.00029982621715443343\n",
      "test loss is 0.0004229168936513844\n",
      "Batch: 33700,train loss is: 0.000757929299060237\n",
      "test loss is 0.0006721527229222256\n",
      "Batch: 33800,train loss is: 0.0005567978721535448\n",
      "test loss is 0.0004243830126310605\n",
      "Batch: 33900,train loss is: 0.0005571568422440552\n",
      "test loss is 0.0004493447564438864\n",
      "Batch: 34000,train loss is: 0.0004667706154799794\n",
      "test loss is 0.0004323155630682394\n",
      "Batch: 34100,train loss is: 0.0005257870973651121\n",
      "test loss is 0.0004248938995559943\n",
      "Batch: 34200,train loss is: 0.00041655545353511\n",
      "test loss is 0.00044626047005924096\n",
      "Batch: 34300,train loss is: 0.000365394184051204\n",
      "test loss is 0.0007434067049270756\n",
      "Batch: 34400,train loss is: 0.00042818987352646876\n",
      "test loss is 0.0004894205707472891\n",
      "Batch: 34500,train loss is: 0.00030890300191211144\n",
      "test loss is 0.0006728072544399561\n",
      "Batch: 34600,train loss is: 0.0004355075875053005\n",
      "test loss is 0.00042125005067369296\n",
      "Batch: 34700,train loss is: 0.0010857945455317387\n",
      "test loss is 0.000979295749471674\n",
      "Batch: 34800,train loss is: 0.00027725742792036136\n",
      "test loss is 0.0004509240792752583\n",
      "Batch: 34900,train loss is: 0.0003900841396827109\n",
      "test loss is 0.00039502767120106434\n",
      "Batch: 35000,train loss is: 0.0004520420644129713\n",
      "test loss is 0.0004005015218965659\n",
      "Batch: 35100,train loss is: 0.0006356695786421522\n",
      "test loss is 0.0005677622210013638\n",
      "Batch: 35200,train loss is: 0.0005396791630848797\n",
      "test loss is 0.0004254499110295087\n",
      "Batch: 35300,train loss is: 0.0006803618252618378\n",
      "test loss is 0.00047767850363753623\n",
      "Batch: 35400,train loss is: 0.00042677252472770395\n",
      "test loss is 0.0006238111273939407\n",
      "Batch: 35500,train loss is: 0.0006252139670521223\n",
      "test loss is 0.0006730106559479979\n",
      "Batch: 35600,train loss is: 0.0002642154831574453\n",
      "test loss is 0.0004929441515233915\n",
      "Batch: 35700,train loss is: 0.0003726158648810884\n",
      "test loss is 0.0004243215650091767\n",
      "Batch: 35800,train loss is: 0.0003975177888919933\n",
      "test loss is 0.0003749864010930974\n",
      "Batch: 35900,train loss is: 0.0007241017067805794\n",
      "test loss is 0.0005165768415327886\n",
      "Batch: 36000,train loss is: 0.00044368503331413854\n",
      "test loss is 0.000456014236034787\n",
      "Batch: 36100,train loss is: 0.0004493530667897259\n",
      "test loss is 0.0005258808176887944\n",
      "Batch: 36200,train loss is: 0.0003602389715286195\n",
      "test loss is 0.0005978601729449541\n",
      "Batch: 36300,train loss is: 0.0002467626490154769\n",
      "test loss is 0.0004114732869929083\n",
      "Batch: 36400,train loss is: 0.0002546188734380176\n",
      "test loss is 0.0004916594442537901\n",
      "Batch: 36500,train loss is: 0.0007466432775121645\n",
      "test loss is 0.000624398000172463\n",
      "Batch: 36600,train loss is: 0.0004628356340484519\n",
      "test loss is 0.000442981860239542\n",
      "Batch: 36700,train loss is: 0.00043210908479098957\n",
      "test loss is 0.00040434326826094323\n",
      "Batch: 36800,train loss is: 0.000344582405951116\n",
      "test loss is 0.0003907814143604976\n",
      "Batch: 36900,train loss is: 0.00035151593018752027\n",
      "test loss is 0.0005201325718152226\n",
      "Batch: 37000,train loss is: 0.0006324878568287861\n",
      "test loss is 0.0007339752298982684\n",
      "Batch: 37100,train loss is: 0.0002616517401318038\n",
      "test loss is 0.00043106325800602765\n",
      "Batch: 37200,train loss is: 0.0006349388797887365\n",
      "test loss is 0.0006327341596472711\n",
      "Batch: 37300,train loss is: 0.0007160056251205903\n",
      "test loss is 0.00047534944772887553\n",
      "Batch: 37400,train loss is: 0.0006920492466964119\n",
      "test loss is 0.0005515155799281514\n",
      "Batch: 37500,train loss is: 0.0006699609798560895\n",
      "test loss is 0.0005149485961087953\n",
      "Batch: 37600,train loss is: 0.00091053334353117\n",
      "test loss is 0.000509009197076759\n",
      "Batch: 37700,train loss is: 0.00045068560013141606\n",
      "test loss is 0.0004444926152974429\n",
      "Batch: 37800,train loss is: 0.0002939448103568487\n",
      "test loss is 0.00043486682362107185\n",
      "Batch: 37900,train loss is: 0.0003101833278261599\n",
      "test loss is 0.0006311421423029596\n",
      "Batch: 38000,train loss is: 0.0008041788659283193\n",
      "test loss is 0.0006026730859513883\n",
      "Batch: 38100,train loss is: 0.00031999324401438777\n",
      "test loss is 0.0005690345421375857\n",
      "Batch: 38200,train loss is: 0.0004440828881922054\n",
      "test loss is 0.0004240663795559276\n",
      "Batch: 38300,train loss is: 0.000328448701805193\n",
      "test loss is 0.0005302907064533585\n",
      "Batch: 38400,train loss is: 0.00037445109135242675\n",
      "test loss is 0.00041408973380288464\n",
      "Batch: 38500,train loss is: 0.0005345414682037323\n",
      "test loss is 0.0004101935437379336\n",
      "Batch: 38600,train loss is: 0.0005088348650996366\n",
      "test loss is 0.0004330202416899465\n",
      "Batch: 38700,train loss is: 0.0005156138840530389\n",
      "test loss is 0.00042713586151865176\n",
      "Batch: 38800,train loss is: 0.00036772239406116376\n",
      "test loss is 0.00039351413226440963\n",
      "Batch: 38900,train loss is: 0.001040249920428889\n",
      "test loss is 0.00040175083334415186\n",
      "Batch: 39000,train loss is: 0.00034877168926014576\n",
      "test loss is 0.0004365932684066549\n",
      "Batch: 39100,train loss is: 0.00021065468628556018\n",
      "test loss is 0.0005181751676883485\n",
      "Batch: 39200,train loss is: 0.00033191736885815256\n",
      "test loss is 0.00046285645837139833\n",
      "Batch: 39300,train loss is: 0.0007206964652710528\n",
      "test loss is 0.00043472086388900464\n",
      "Batch: 39400,train loss is: 0.0004193045411202153\n",
      "test loss is 0.0005195845161176264\n",
      "Batch: 39500,train loss is: 0.000330180180822003\n",
      "test loss is 0.00045199559750494544\n",
      "Batch: 39600,train loss is: 0.0007755115009083163\n",
      "test loss is 0.0006783576885445407\n",
      "Batch: 39700,train loss is: 0.0005038283923784647\n",
      "test loss is 0.0006224653421230056\n",
      "Batch: 39800,train loss is: 0.0004997328655072237\n",
      "test loss is 0.0005160951051518202\n",
      "Batch: 39900,train loss is: 0.0005011129566336031\n",
      "test loss is 0.0005628545644219739\n",
      "Batch: 40000,train loss is: 0.00038124643289001833\n",
      "test loss is 0.0005569812865111381\n",
      "Batch: 40100,train loss is: 0.0002806128942518289\n",
      "test loss is 0.000369914960179871\n",
      "Batch: 40200,train loss is: 0.00040110930877183115\n",
      "test loss is 0.0005031066290699322\n",
      "Batch: 40300,train loss is: 0.0003075919453840373\n",
      "test loss is 0.000476006506004809\n",
      "Batch: 40400,train loss is: 0.0003199663841064261\n",
      "test loss is 0.000562545423443742\n",
      "Batch: 40500,train loss is: 0.0007047469639769608\n",
      "test loss is 0.0005275320392061781\n",
      "Batch: 40600,train loss is: 0.0006262839244814478\n",
      "test loss is 0.0006575038542534151\n",
      "Batch: 40700,train loss is: 0.0004558270896767397\n",
      "test loss is 0.00045901760291037347\n",
      "Batch: 40800,train loss is: 0.00038125533796976084\n",
      "test loss is 0.00041477024583446223\n",
      "Batch: 40900,train loss is: 0.0006074218583809997\n",
      "test loss is 0.0005146925335459714\n",
      "Batch: 41000,train loss is: 0.000715223140153224\n",
      "test loss is 0.0005370389175962318\n",
      "Batch: 41100,train loss is: 0.0003938805916421014\n",
      "test loss is 0.00038515050926429275\n",
      "Batch: 41200,train loss is: 0.00041219163449071833\n",
      "test loss is 0.00039858936349972167\n",
      "Batch: 41300,train loss is: 0.00040092098174769126\n",
      "test loss is 0.0005348443378794068\n",
      "Batch: 41400,train loss is: 0.0005887371601430185\n",
      "test loss is 0.000434691398817504\n",
      "Batch: 41500,train loss is: 0.0002342038958754415\n",
      "test loss is 0.0004028891251364955\n",
      "Batch: 41600,train loss is: 0.0004857040996680373\n",
      "test loss is 0.0007125190653128466\n",
      "Batch: 41700,train loss is: 0.00029530631422814633\n",
      "test loss is 0.00043418442379446345\n",
      "Batch: 41800,train loss is: 0.0005120161186265386\n",
      "test loss is 0.0006350467533042419\n",
      "Batch: 41900,train loss is: 0.0006037455460600205\n",
      "test loss is 0.0005916674519990436\n",
      "Batch: 42000,train loss is: 0.0005967414155870852\n",
      "test loss is 0.0005532153638023754\n",
      "Batch: 42100,train loss is: 0.0008996607808580027\n",
      "test loss is 0.00045969159571166394\n",
      "Batch: 42200,train loss is: 0.0003175647558273194\n",
      "test loss is 0.0005728491605457494\n",
      "Batch: 42300,train loss is: 0.00047216756302305006\n",
      "test loss is 0.0004890438618216405\n",
      "Batch: 42400,train loss is: 0.0005040652249918394\n",
      "test loss is 0.0008098910088679815\n",
      "Batch: 42500,train loss is: 0.0009780119767703068\n",
      "test loss is 0.0006640979300260848\n",
      "Batch: 42600,train loss is: 0.00027802007710891975\n",
      "test loss is 0.00046177591716820736\n",
      "Batch: 42700,train loss is: 0.0005678443124474406\n",
      "test loss is 0.0005313313812748295\n",
      "Batch: 42800,train loss is: 0.000275633560225508\n",
      "test loss is 0.0004193439585698037\n",
      "Batch: 42900,train loss is: 0.0008560342800881884\n",
      "test loss is 0.0004196014815378783\n",
      "Batch: 43000,train loss is: 0.000311183653870274\n",
      "test loss is 0.00044183637048668314\n",
      "Batch: 43100,train loss is: 0.002180767905515512\n",
      "test loss is 0.00048500084916285854\n",
      "Batch: 43200,train loss is: 0.00046127786715603286\n",
      "test loss is 0.0006262673938406949\n",
      "Batch: 43300,train loss is: 0.00042780520615229675\n",
      "test loss is 0.0004192311136437518\n",
      "Batch: 43400,train loss is: 0.0006416785347966442\n",
      "test loss is 0.0007327791114010799\n",
      "Batch: 43500,train loss is: 0.00041721874777698607\n",
      "test loss is 0.00046898351834758357\n",
      "Batch: 43600,train loss is: 0.000327534115267435\n",
      "test loss is 0.0004390472968468091\n",
      "Batch: 43700,train loss is: 0.0003546306156668265\n",
      "test loss is 0.0004938620330473091\n",
      "Batch: 43800,train loss is: 0.00042718061414793453\n",
      "test loss is 0.0004657300766670001\n",
      "Batch: 43900,train loss is: 0.000354558466565648\n",
      "test loss is 0.00045352867756500234\n",
      "Batch: 44000,train loss is: 0.0006947025583453249\n",
      "test loss is 0.0006128186316992492\n",
      "Batch: 44100,train loss is: 0.000326591563305581\n",
      "test loss is 0.00044952888355501413\n",
      "Batch: 44200,train loss is: 0.0022584852865453215\n",
      "test loss is 0.0005777869217165097\n",
      "Batch: 44300,train loss is: 0.0003566267924727217\n",
      "test loss is 0.00036842913968791383\n",
      "Batch: 44400,train loss is: 0.0006345066069101719\n",
      "test loss is 0.0005376952398867639\n",
      "Batch: 44500,train loss is: 0.0003215858902228119\n",
      "test loss is 0.00046077078064556034\n",
      "Batch: 44600,train loss is: 0.00045894381280991693\n",
      "test loss is 0.0004474943629171582\n",
      "Batch: 44700,train loss is: 0.000279154598457079\n",
      "test loss is 0.00041667907568049446\n",
      "Batch: 44800,train loss is: 0.0004653525324592475\n",
      "test loss is 0.00043217474289439906\n",
      "Batch: 44900,train loss is: 0.0007567503791912187\n",
      "test loss is 0.0006728639321652129\n",
      "Batch: 45000,train loss is: 0.00047783199263040557\n",
      "test loss is 0.0004114078003303519\n",
      "Batch: 45100,train loss is: 0.0011025655359135872\n",
      "test loss is 0.00042558849628636706\n",
      "Batch: 45200,train loss is: 0.0005358825390603355\n",
      "test loss is 0.0006538118588518275\n",
      "Batch: 45300,train loss is: 0.00038588805120757467\n",
      "test loss is 0.00045704139630395064\n",
      "Batch: 45400,train loss is: 0.0007011438127975597\n",
      "test loss is 0.0004941500995498206\n",
      "Batch: 45500,train loss is: 0.000915527101554472\n",
      "test loss is 0.0005881877863735633\n",
      "Batch: 45600,train loss is: 0.0007668354396732787\n",
      "test loss is 0.0005068349538118035\n",
      "Batch: 45700,train loss is: 0.0005712770283181667\n",
      "test loss is 0.0005473105858156237\n",
      "Batch: 45800,train loss is: 0.00045927631889548944\n",
      "test loss is 0.0005202713157289277\n",
      "Batch: 45900,train loss is: 0.00038148920960718875\n",
      "test loss is 0.0005294984000693094\n",
      "Batch: 46000,train loss is: 0.0003331970023592016\n",
      "test loss is 0.0004907036858932631\n",
      "Batch: 46100,train loss is: 0.0002585825134865378\n",
      "test loss is 0.00043808422567149065\n",
      "Batch: 46200,train loss is: 0.0006126850115372213\n",
      "test loss is 0.0004195071914214402\n",
      "Batch: 46300,train loss is: 0.0003306584235393108\n",
      "test loss is 0.0005027627241394371\n",
      "Batch: 46400,train loss is: 0.00037576663884336464\n",
      "test loss is 0.00046869778869646297\n",
      "Batch: 46500,train loss is: 0.0004259730982341706\n",
      "test loss is 0.00048226857147562366\n",
      "Batch: 46600,train loss is: 0.0003755542204900644\n",
      "test loss is 0.00040038943162418915\n",
      "Batch: 46700,train loss is: 0.0002910677325776916\n",
      "test loss is 0.0004581539150868509\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch_NN.train import train_model\n",
    "\n",
    "train_model(loss_MSE,optim_Adam,model,data_loader,train_data,test_data,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Epoch: 0----------------------------------\n",
      "Batch: 0,train loss is: 0.000416533605339905\n",
      "test loss is 0.000539631635882777\n",
      "Batch: 100,train loss is: 0.00029509391312849417\n",
      "test loss is 0.00034515467262665833\n",
      "Batch: 200,train loss is: 0.0002884921779783853\n",
      "test loss is 0.0003914351748201286\n",
      "Batch: 300,train loss is: 0.00035480139074867297\n",
      "test loss is 0.00034635170000821237\n",
      "Batch: 400,train loss is: 0.0003139895513359857\n",
      "test loss is 0.00037880163476459257\n",
      "Batch: 500,train loss is: 0.00035226626437377185\n",
      "test loss is 0.0003202001837569722\n",
      "Batch: 600,train loss is: 0.0002548638136054359\n",
      "test loss is 0.00036513834817953494\n",
      "Batch: 700,train loss is: 0.00045073571400820354\n",
      "test loss is 0.00037254092209726577\n",
      "Batch: 800,train loss is: 0.00033667481108098853\n",
      "test loss is 0.00032598377237169537\n",
      "Batch: 900,train loss is: 0.0004992500216306002\n",
      "test loss is 0.0003572503749401134\n",
      "Batch: 1000,train loss is: 0.0003134262775674148\n",
      "test loss is 0.0003323792859690178\n",
      "Batch: 1100,train loss is: 0.00033741852881540664\n",
      "test loss is 0.0003652131023159228\n",
      "Batch: 1200,train loss is: 0.0006240955806752726\n",
      "test loss is 0.00044748822969483895\n",
      "Batch: 1300,train loss is: 0.00032443222760977905\n",
      "test loss is 0.00031129159172174734\n",
      "Batch: 1400,train loss is: 0.00046956276145131956\n",
      "test loss is 0.00033776270956397466\n",
      "Batch: 1500,train loss is: 0.0005448407569332962\n",
      "test loss is 0.00035704837578384594\n",
      "Batch: 1600,train loss is: 0.0004185486904656489\n",
      "test loss is 0.0003727082618842743\n",
      "Batch: 1700,train loss is: 0.00021803700873950855\n",
      "test loss is 0.00032328785176901666\n",
      "Batch: 1800,train loss is: 0.00031504088545479476\n",
      "test loss is 0.00033330902451446604\n",
      "Batch: 1900,train loss is: 0.0002511322470097311\n",
      "test loss is 0.00033375573229984216\n",
      "Batch: 2000,train loss is: 0.00026392425160472915\n",
      "test loss is 0.0003812856764260455\n",
      "Batch: 2100,train loss is: 0.0003917698237835859\n",
      "test loss is 0.00035510141385771446\n",
      "Batch: 2200,train loss is: 0.0004269199651184525\n",
      "test loss is 0.00032067621215437067\n",
      "Batch: 2300,train loss is: 0.00033976608471537905\n",
      "test loss is 0.0003353547128803311\n",
      "Batch: 2400,train loss is: 0.00017087680208553724\n",
      "test loss is 0.00032851141934648557\n",
      "Batch: 2500,train loss is: 0.0002881902709896906\n",
      "test loss is 0.00033578152638780856\n",
      "Batch: 2600,train loss is: 0.00030723133668777563\n",
      "test loss is 0.0003196073440947424\n",
      "Batch: 2700,train loss is: 0.0002277316839506028\n",
      "test loss is 0.0003569027429714458\n",
      "Batch: 2800,train loss is: 0.00027572917347317425\n",
      "test loss is 0.0003311401775694818\n",
      "Batch: 2900,train loss is: 0.00034008459630619647\n",
      "test loss is 0.0003079074703062137\n",
      "Batch: 3000,train loss is: 0.00025186450696383644\n",
      "test loss is 0.00035598380914312597\n",
      "Batch: 3100,train loss is: 0.0009242314553760376\n",
      "test loss is 0.00032985029433416064\n",
      "Batch: 3200,train loss is: 0.00024135595369514674\n",
      "test loss is 0.000329668287797049\n",
      "Batch: 3300,train loss is: 0.0004320370708813771\n",
      "test loss is 0.00037276362396649335\n",
      "Batch: 3400,train loss is: 0.0003624586290861114\n",
      "test loss is 0.0003406258702554917\n",
      "Batch: 3500,train loss is: 0.0003287194204642908\n",
      "test loss is 0.0003356578521448941\n",
      "Batch: 3600,train loss is: 0.00039187675449730967\n",
      "test loss is 0.0003971493957816782\n",
      "Batch: 3700,train loss is: 0.000449322185942954\n",
      "test loss is 0.00035923474403492503\n",
      "Batch: 3800,train loss is: 0.00019321339769757972\n",
      "test loss is 0.0003254831525741685\n",
      "Batch: 3900,train loss is: 0.00021560744574016663\n",
      "test loss is 0.0003373660237079894\n",
      "Batch: 4000,train loss is: 0.00029674116915668936\n",
      "test loss is 0.00036824810790262005\n",
      "Batch: 4100,train loss is: 0.00045758425187383085\n",
      "test loss is 0.0003211950838980763\n",
      "Batch: 4200,train loss is: 0.0004353136616894042\n",
      "test loss is 0.00038380463787106577\n",
      "Batch: 4300,train loss is: 0.00025575958419924994\n",
      "test loss is 0.00032128020365847074\n",
      "Batch: 4400,train loss is: 0.00022520006079568034\n",
      "test loss is 0.00031732768028089217\n",
      "Batch: 4500,train loss is: 0.00025645756910809255\n",
      "test loss is 0.0004648098592509451\n",
      "Batch: 4600,train loss is: 0.00021025814317460437\n",
      "test loss is 0.00036903009265423607\n",
      "Batch: 4700,train loss is: 0.00040412365969230376\n",
      "test loss is 0.0003443936837067524\n",
      "Batch: 4800,train loss is: 0.0003790021993865278\n",
      "test loss is 0.0003390768143861585\n",
      "Batch: 4900,train loss is: 0.0004277235007356611\n",
      "test loss is 0.0003633703651180016\n",
      "Batch: 5000,train loss is: 0.0002813426041508274\n",
      "test loss is 0.00033710716215714086\n",
      "Batch: 5100,train loss is: 0.00019901811607230335\n",
      "test loss is 0.0004388948089586896\n",
      "Batch: 5200,train loss is: 0.0002988392525610042\n",
      "test loss is 0.00031695917177127793\n",
      "Batch: 5300,train loss is: 0.0003657093907343894\n",
      "test loss is 0.00032651599385773514\n",
      "Batch: 5400,train loss is: 0.0003148701925148517\n",
      "test loss is 0.00036823433535890915\n",
      "Batch: 5500,train loss is: 0.0002351400133682092\n",
      "test loss is 0.00039582299736791405\n",
      "Batch: 5600,train loss is: 0.00029467643163624433\n",
      "test loss is 0.0003951431498803656\n",
      "Batch: 5700,train loss is: 0.0006173787000493585\n",
      "test loss is 0.0004136883245348153\n",
      "Batch: 5800,train loss is: 0.0003639322313944408\n",
      "test loss is 0.0003273431588148788\n",
      "Batch: 5900,train loss is: 0.00039610923225823463\n",
      "test loss is 0.0004359631144383688\n",
      "Batch: 6000,train loss is: 0.00022978523608554292\n",
      "test loss is 0.00033049457329008383\n",
      "Batch: 6100,train loss is: 0.0002521406464853497\n",
      "test loss is 0.00031896114991142457\n",
      "Batch: 6200,train loss is: 0.0005627808800760692\n",
      "test loss is 0.0003841126661922965\n",
      "Batch: 6300,train loss is: 0.0004613925951732707\n",
      "test loss is 0.00036721346633106114\n",
      "Batch: 6400,train loss is: 0.0002873308070123119\n",
      "test loss is 0.0003298060939219036\n",
      "Batch: 6500,train loss is: 0.0002567498829827415\n",
      "test loss is 0.00032336375291221246\n",
      "Batch: 6600,train loss is: 0.0003430044425160591\n",
      "test loss is 0.0004111642605448485\n",
      "Batch: 6700,train loss is: 0.0002902522062592294\n",
      "test loss is 0.0003307060948167716\n",
      "Batch: 6800,train loss is: 0.00025142469396544233\n",
      "test loss is 0.000331219276402344\n",
      "Batch: 6900,train loss is: 0.0004211015197766033\n",
      "test loss is 0.0003797791234396028\n",
      "Batch: 7000,train loss is: 0.00035712581620307363\n",
      "test loss is 0.00036071707386711195\n",
      "Batch: 7100,train loss is: 0.0004966642489691716\n",
      "test loss is 0.0004375062224519411\n",
      "Batch: 7200,train loss is: 0.00027536462328718917\n",
      "test loss is 0.00031158594359860975\n",
      "Batch: 7300,train loss is: 0.0003389848163703071\n",
      "test loss is 0.00038009941282592276\n",
      "Batch: 7400,train loss is: 0.00027605404877640953\n",
      "test loss is 0.00035606095083057546\n",
      "Batch: 7500,train loss is: 0.00022428734908285495\n",
      "test loss is 0.0003510483475334117\n",
      "Batch: 7600,train loss is: 0.0004228958408109229\n",
      "test loss is 0.0004121232241963618\n",
      "Batch: 7700,train loss is: 0.0002587147879453054\n",
      "test loss is 0.00035992742773761075\n",
      "Batch: 7800,train loss is: 0.00041830183926211623\n",
      "test loss is 0.00036657320847007023\n",
      "Batch: 7900,train loss is: 0.0003104614331522939\n",
      "test loss is 0.0003279015322225639\n",
      "Batch: 8000,train loss is: 0.00038909774015819876\n",
      "test loss is 0.0003761895930831579\n",
      "Batch: 8100,train loss is: 0.000382752626787136\n",
      "test loss is 0.0003289457548152316\n",
      "Batch: 8200,train loss is: 0.00024505776789599556\n",
      "test loss is 0.0003164710570605831\n",
      "Batch: 8300,train loss is: 0.0002982684172969399\n",
      "test loss is 0.0003132534910591002\n",
      "Batch: 8400,train loss is: 0.0003603409688718099\n",
      "test loss is 0.0003498310480384311\n",
      "Batch: 8500,train loss is: 0.00027742831384929695\n",
      "test loss is 0.00034434361442516905\n",
      "Batch: 8600,train loss is: 0.00027931052591221163\n",
      "test loss is 0.00032921553444647465\n",
      "Batch: 8700,train loss is: 0.000537902965399398\n",
      "test loss is 0.00034295495522591906\n",
      "Batch: 8800,train loss is: 0.0002464193182627861\n",
      "test loss is 0.0003452800033862958\n",
      "Batch: 8900,train loss is: 0.0003936992516254676\n",
      "test loss is 0.0003557920940852811\n",
      "Batch: 9000,train loss is: 0.0003127372667555248\n",
      "test loss is 0.0004269039026053772\n",
      "Batch: 9100,train loss is: 0.0002862662669575265\n",
      "test loss is 0.00036907548778448794\n",
      "Batch: 9200,train loss is: 0.00034612892209482814\n",
      "test loss is 0.00033773451523899943\n",
      "Batch: 9300,train loss is: 0.0003542333016459306\n",
      "test loss is 0.0003109475407055899\n",
      "Batch: 9400,train loss is: 0.00029469101482601853\n",
      "test loss is 0.0003401975020542294\n",
      "Batch: 9500,train loss is: 0.0004573372392981798\n",
      "test loss is 0.0003380716377018265\n",
      "Batch: 9600,train loss is: 0.0002685847985288266\n",
      "test loss is 0.00034316697611435975\n",
      "Batch: 9700,train loss is: 0.000279462636706845\n",
      "test loss is 0.0003307138642579308\n",
      "Batch: 9800,train loss is: 0.000296782200505693\n",
      "test loss is 0.00035838597219630444\n",
      "Batch: 9900,train loss is: 0.0002708450627999855\n",
      "test loss is 0.00031782737259335506\n",
      "Batch: 10000,train loss is: 0.00024656890593127863\n",
      "test loss is 0.00032711191121407353\n",
      "Batch: 10100,train loss is: 0.0003487553985081114\n",
      "test loss is 0.0004082234634237979\n",
      "Batch: 10200,train loss is: 0.00022203249724677977\n",
      "test loss is 0.0003350274912908171\n",
      "Batch: 10300,train loss is: 0.0002826757612786197\n",
      "test loss is 0.0003300726115312253\n",
      "Batch: 10400,train loss is: 0.00028621095403717317\n",
      "test loss is 0.0003093819823685716\n",
      "Batch: 10500,train loss is: 0.0003150547629640081\n",
      "test loss is 0.00037790205164668893\n",
      "Batch: 10600,train loss is: 0.00034679414075583016\n",
      "test loss is 0.00032300326059233155\n",
      "Batch: 10700,train loss is: 0.00033808526998612966\n",
      "test loss is 0.0003245123857589031\n",
      "Batch: 10800,train loss is: 0.00029755943115640287\n",
      "test loss is 0.00031452683444322836\n",
      "Batch: 10900,train loss is: 0.0003351601836003269\n",
      "test loss is 0.0003532714984497083\n",
      "Batch: 11000,train loss is: 0.00029148242575731333\n",
      "test loss is 0.0003560936381226981\n",
      "Batch: 11100,train loss is: 0.000327140891633135\n",
      "test loss is 0.00037931071418453074\n",
      "Batch: 11200,train loss is: 0.00021317583290614007\n",
      "test loss is 0.00034042991148925344\n",
      "Batch: 11300,train loss is: 0.0002903985468178526\n",
      "test loss is 0.00035018490056982576\n",
      "Batch: 11400,train loss is: 0.00025448761976746187\n",
      "test loss is 0.0003291615228704463\n",
      "Batch: 11500,train loss is: 0.00045667007111272124\n",
      "test loss is 0.00032035981897890356\n",
      "Batch: 11600,train loss is: 0.0002172898075510276\n",
      "test loss is 0.00034400028008435964\n",
      "Batch: 11700,train loss is: 0.00026207926793994494\n",
      "test loss is 0.0004008250955403036\n",
      "Batch: 11800,train loss is: 0.0002032420430206558\n",
      "test loss is 0.00031874930015714\n",
      "Batch: 11900,train loss is: 0.000309034133252902\n",
      "test loss is 0.00035448572689535953\n",
      "Batch: 12000,train loss is: 0.0002431800202650453\n",
      "test loss is 0.00033883434871858286\n",
      "Batch: 12100,train loss is: 0.0004290520901042805\n",
      "test loss is 0.0003202433717709531\n",
      "Batch: 12200,train loss is: 0.0003663938679279921\n",
      "test loss is 0.00033518074418251425\n",
      "Batch: 12300,train loss is: 0.00024961844354122063\n",
      "test loss is 0.00035652133008016583\n",
      "Batch: 12400,train loss is: 0.00033663689872247217\n",
      "test loss is 0.00032258275613735376\n",
      "Batch: 12500,train loss is: 0.00035836681420968117\n",
      "test loss is 0.00032650034338378047\n",
      "Batch: 12600,train loss is: 0.000448950010304197\n",
      "test loss is 0.0003473368242417921\n",
      "Batch: 12700,train loss is: 0.0004442554524917909\n",
      "test loss is 0.00034363607850260863\n",
      "Batch: 12800,train loss is: 0.00036502371317892403\n",
      "test loss is 0.00032332147464725277\n",
      "Batch: 12900,train loss is: 0.00019075959733994667\n",
      "test loss is 0.00037535630132905435\n",
      "Batch: 13000,train loss is: 0.000480068178350741\n",
      "test loss is 0.0003184332846909806\n",
      "Batch: 13100,train loss is: 0.00029584755361640507\n",
      "test loss is 0.0003640585087618027\n",
      "Batch: 13200,train loss is: 0.0003720570938552239\n",
      "test loss is 0.0004059396075496026\n",
      "Batch: 13300,train loss is: 0.00026765048287778087\n",
      "test loss is 0.00032691963587018366\n",
      "Batch: 13400,train loss is: 0.0004804248533165614\n",
      "test loss is 0.0003377574883609728\n",
      "Batch: 13500,train loss is: 0.0002597532383225975\n",
      "test loss is 0.0003506573734436387\n",
      "Batch: 13600,train loss is: 0.00021178668079311117\n",
      "test loss is 0.00033530511116625306\n",
      "Batch: 13700,train loss is: 0.00019164673181409453\n",
      "test loss is 0.00034079492729534105\n",
      "Batch: 13800,train loss is: 0.00017693695376596772\n",
      "test loss is 0.0003357336370233946\n",
      "Batch: 13900,train loss is: 0.00039818040853773374\n",
      "test loss is 0.00035411946955540727\n",
      "Batch: 14000,train loss is: 0.0006736233127265685\n",
      "test loss is 0.0003827422140832208\n",
      "Batch: 14100,train loss is: 0.00046911450596026497\n",
      "test loss is 0.0003578730808038453\n",
      "Batch: 14200,train loss is: 0.000517927158283634\n",
      "test loss is 0.0003021763045727577\n",
      "Batch: 14300,train loss is: 0.0003052127614770869\n",
      "test loss is 0.00032066210526643987\n",
      "Batch: 14400,train loss is: 0.00023951074064785188\n",
      "test loss is 0.000321358986362745\n",
      "Batch: 14500,train loss is: 0.0005215638650625943\n",
      "test loss is 0.00030937984341338057\n",
      "Batch: 14600,train loss is: 0.0003080799926988534\n",
      "test loss is 0.00033358674516326033\n",
      "Batch: 14700,train loss is: 0.0006282983137377394\n",
      "test loss is 0.00035907275547250296\n",
      "Batch: 14800,train loss is: 0.0002733348212652601\n",
      "test loss is 0.0003349143853608764\n",
      "Batch: 14900,train loss is: 0.0001852580014520246\n",
      "test loss is 0.0003756310708852332\n",
      "Batch: 15000,train loss is: 0.0004314048311299691\n",
      "test loss is 0.00031387027226857536\n",
      "Batch: 15100,train loss is: 0.00025255423370012283\n",
      "test loss is 0.00035975611217536197\n",
      "Batch: 15200,train loss is: 0.0002519567501477421\n",
      "test loss is 0.0003185393682575707\n",
      "Batch: 15300,train loss is: 0.0002728010304221082\n",
      "test loss is 0.00034395308025242664\n",
      "Batch: 15400,train loss is: 0.0002921521610686473\n",
      "test loss is 0.00031612727901915905\n",
      "Batch: 15500,train loss is: 0.0005810371027393454\n",
      "test loss is 0.0003365544629844638\n",
      "Batch: 15600,train loss is: 0.0003145322074975573\n",
      "test loss is 0.0003372090137007638\n",
      "Batch: 15700,train loss is: 0.0001998164518347138\n",
      "test loss is 0.0004011806823248136\n",
      "Batch: 15800,train loss is: 0.0006551719003082575\n",
      "test loss is 0.0003084709963073356\n",
      "Batch: 15900,train loss is: 0.0006038264071564011\n",
      "test loss is 0.00035059396545653737\n",
      "Batch: 16000,train loss is: 0.0004323648334026164\n",
      "test loss is 0.0004445887664265886\n",
      "Batch: 16100,train loss is: 0.00020133912590439222\n",
      "test loss is 0.00029935708601021673\n",
      "Batch: 16200,train loss is: 0.00034537101509948614\n",
      "test loss is 0.0004074550831939222\n",
      "Batch: 16300,train loss is: 0.0003523974535744648\n",
      "test loss is 0.000334950030269049\n",
      "Batch: 16400,train loss is: 0.0006476428655857694\n",
      "test loss is 0.0003704343302410649\n",
      "Batch: 16500,train loss is: 0.0002288491388468042\n",
      "test loss is 0.0003216897423760851\n",
      "Batch: 16600,train loss is: 0.00032106436215895335\n",
      "test loss is 0.00030490179630899433\n",
      "Batch: 16700,train loss is: 0.00017913720327993683\n",
      "test loss is 0.000336123068475926\n",
      "Batch: 16800,train loss is: 0.00021114946171570074\n",
      "test loss is 0.00038458079518987004\n",
      "Batch: 16900,train loss is: 0.00028356099801024923\n",
      "test loss is 0.00034404470156799126\n",
      "Batch: 17000,train loss is: 0.00046098178522532477\n",
      "test loss is 0.00033342346438042394\n",
      "Batch: 17100,train loss is: 0.0004992983248289586\n",
      "test loss is 0.0003463210088720033\n",
      "Batch: 17200,train loss is: 0.00025331974783299866\n",
      "test loss is 0.0003078920353033297\n",
      "Batch: 17300,train loss is: 0.00041032509871703385\n",
      "test loss is 0.0003716742106532791\n",
      "Batch: 17400,train loss is: 0.0004645635683809182\n",
      "test loss is 0.0003389718002417898\n",
      "Batch: 17500,train loss is: 0.0002934275399124039\n",
      "test loss is 0.00032177940323548787\n",
      "Batch: 17600,train loss is: 0.0002589867811459511\n",
      "test loss is 0.00034870714813272816\n",
      "Batch: 17700,train loss is: 0.0003500971157784005\n",
      "test loss is 0.00034392580784198647\n",
      "Batch: 17800,train loss is: 0.00026299130699448036\n",
      "test loss is 0.00033312606912775717\n",
      "Batch: 17900,train loss is: 0.0002178356046949219\n",
      "test loss is 0.0003085522232651258\n",
      "Batch: 18000,train loss is: 0.0004512834654735948\n",
      "test loss is 0.0003373263630726121\n",
      "Batch: 18100,train loss is: 0.0007779877532596771\n",
      "test loss is 0.0004539221511625398\n",
      "Batch: 18200,train loss is: 0.0005250224768528864\n",
      "test loss is 0.0003288746278487807\n",
      "Batch: 18300,train loss is: 0.0003147603582765733\n",
      "test loss is 0.00033514942880325503\n",
      "Batch: 18400,train loss is: 0.00025692308922945953\n",
      "test loss is 0.00031511847149876253\n",
      "Batch: 18500,train loss is: 0.0003733161290826026\n",
      "test loss is 0.00035918847055361244\n",
      "Batch: 18600,train loss is: 0.0002357582132801515\n",
      "test loss is 0.00032309805322225306\n",
      "Batch: 18700,train loss is: 0.00027967159699011554\n",
      "test loss is 0.0003640754773113228\n",
      "Batch: 18800,train loss is: 0.0005275478165224113\n",
      "test loss is 0.0003391288510468063\n",
      "Batch: 18900,train loss is: 0.0002348724544165041\n",
      "test loss is 0.0003526279205027083\n",
      "Batch: 19000,train loss is: 0.00023431874500239665\n",
      "test loss is 0.00030368937852396255\n",
      "Batch: 19100,train loss is: 0.0003088091207308391\n",
      "test loss is 0.00033974723330395314\n",
      "Batch: 19200,train loss is: 0.00024902897174709895\n",
      "test loss is 0.00035971935517657564\n",
      "Batch: 19300,train loss is: 0.00023634716740102388\n",
      "test loss is 0.00033773427539016446\n",
      "Batch: 19400,train loss is: 0.0003181485043393124\n",
      "test loss is 0.00040220608123051055\n",
      "Batch: 19500,train loss is: 0.0003385311613078738\n",
      "test loss is 0.0004136457429591737\n",
      "Batch: 19600,train loss is: 0.000301850915455736\n",
      "test loss is 0.000342673522627807\n",
      "Batch: 19700,train loss is: 0.0013323619405183587\n",
      "test loss is 0.0003710677274669729\n",
      "Batch: 19800,train loss is: 0.0005608542362445109\n",
      "test loss is 0.00036441301098093335\n",
      "Batch: 19900,train loss is: 0.00028125656286416014\n",
      "test loss is 0.00030921708329019335\n",
      "Batch: 20000,train loss is: 0.00028697064627828187\n",
      "test loss is 0.00041538016297015443\n",
      "Batch: 20100,train loss is: 0.0003687113157636678\n",
      "test loss is 0.0003135524296792896\n",
      "Batch: 20200,train loss is: 0.00029623732860043444\n",
      "test loss is 0.0003505306409049853\n",
      "Batch: 20300,train loss is: 0.00024795821683712215\n",
      "test loss is 0.0003388952774500257\n",
      "Batch: 20400,train loss is: 0.0003569148355243694\n",
      "test loss is 0.00032475063855426893\n",
      "Batch: 20500,train loss is: 0.0002205646772704755\n",
      "test loss is 0.00031505088332345625\n",
      "Batch: 20600,train loss is: 0.00029583607532073387\n",
      "test loss is 0.00033010137192293973\n",
      "Batch: 20700,train loss is: 0.0002927965779071631\n",
      "test loss is 0.00033955972802968674\n",
      "Batch: 20800,train loss is: 0.0003056174604806873\n",
      "test loss is 0.0003570020017014879\n",
      "Batch: 20900,train loss is: 0.0003536341823332609\n",
      "test loss is 0.0003344807808254282\n",
      "Batch: 21000,train loss is: 0.0003676990781960252\n",
      "test loss is 0.0003241028200222755\n",
      "Batch: 21100,train loss is: 0.00019424640189490513\n",
      "test loss is 0.00032323813746258184\n",
      "Batch: 21200,train loss is: 0.00034867626571222243\n",
      "test loss is 0.00037716241835508425\n",
      "Batch: 21300,train loss is: 0.0003764303203028333\n",
      "test loss is 0.000338948737893451\n",
      "Batch: 21400,train loss is: 0.0003942025430578297\n",
      "test loss is 0.0004543870337078716\n",
      "Batch: 21500,train loss is: 0.0003667504131691506\n",
      "test loss is 0.0003201310392220119\n",
      "Batch: 21600,train loss is: 0.00046957042362826874\n",
      "test loss is 0.00038196979679802596\n",
      "Batch: 21700,train loss is: 0.00029505894911936205\n",
      "test loss is 0.00032383439053763115\n",
      "Batch: 21800,train loss is: 0.0004490564812100957\n",
      "test loss is 0.0003068125488006232\n",
      "Batch: 21900,train loss is: 0.00030254173523028836\n",
      "test loss is 0.00031522664168952204\n",
      "Batch: 22000,train loss is: 0.00039168187455583363\n",
      "test loss is 0.0003328403359937995\n",
      "Batch: 22100,train loss is: 0.00029907785651694035\n",
      "test loss is 0.00033282542624970244\n",
      "Batch: 22200,train loss is: 0.0003160073986081066\n",
      "test loss is 0.00031463783630629797\n",
      "Batch: 22300,train loss is: 0.0005196846774009481\n",
      "test loss is 0.0003873704889348557\n",
      "Batch: 22400,train loss is: 0.00038652570533861686\n",
      "test loss is 0.00037235618659243894\n",
      "Batch: 22500,train loss is: 0.00031447781849113616\n",
      "test loss is 0.00038532162291658853\n",
      "Batch: 22600,train loss is: 0.0002574285777988632\n",
      "test loss is 0.0003387629288525954\n",
      "Batch: 22700,train loss is: 0.00040324964860711176\n",
      "test loss is 0.0003256518460376493\n",
      "Batch: 22800,train loss is: 0.00029415157071605426\n",
      "test loss is 0.00032563283616034595\n",
      "Batch: 22900,train loss is: 0.0002291433810516585\n",
      "test loss is 0.0003414379348385885\n",
      "Batch: 23000,train loss is: 0.0003255153751289897\n",
      "test loss is 0.0003319813171733718\n",
      "Batch: 23100,train loss is: 0.0004166331043148851\n",
      "test loss is 0.0003190842776632924\n",
      "Batch: 23200,train loss is: 0.00022364924017354026\n",
      "test loss is 0.0003214892339778599\n",
      "Batch: 23300,train loss is: 0.0002525173619092269\n",
      "test loss is 0.0003208430392801856\n",
      "Batch: 23400,train loss is: 0.00048375639558093786\n",
      "test loss is 0.0003770005324190248\n",
      "Batch: 23500,train loss is: 0.0004271799167262009\n",
      "test loss is 0.0003265872725525409\n",
      "Batch: 23600,train loss is: 0.00026933313243868097\n",
      "test loss is 0.0003360496201011003\n",
      "Batch: 23700,train loss is: 0.0003500962146315473\n",
      "test loss is 0.00038713636712743913\n",
      "Batch: 23800,train loss is: 0.0003880257356858709\n",
      "test loss is 0.00036299852510624696\n",
      "Batch: 23900,train loss is: 0.00031054583237298907\n",
      "test loss is 0.0003271223324339305\n",
      "Batch: 24000,train loss is: 0.0002018981420817795\n",
      "test loss is 0.0003275861737957825\n",
      "Batch: 24100,train loss is: 0.00021509591752588762\n",
      "test loss is 0.0003317099128697169\n",
      "Batch: 24200,train loss is: 0.0002885568571385571\n",
      "test loss is 0.00031095740700167655\n",
      "Batch: 24300,train loss is: 0.0003532936130973058\n",
      "test loss is 0.0003170798532162585\n",
      "Batch: 24400,train loss is: 0.0002328476702640416\n",
      "test loss is 0.0003356012282804106\n",
      "Batch: 24500,train loss is: 0.00020124076561637086\n",
      "test loss is 0.0003834201083395377\n",
      "Batch: 24600,train loss is: 0.00020021597780648108\n",
      "test loss is 0.0003239368031500881\n",
      "Batch: 24700,train loss is: 0.00020256299576338463\n",
      "test loss is 0.00032750041303073585\n",
      "Batch: 24800,train loss is: 0.0002499445504110335\n",
      "test loss is 0.00035221868489067\n",
      "Batch: 24900,train loss is: 0.0002793517374740672\n",
      "test loss is 0.00032262370967329476\n",
      "Batch: 25000,train loss is: 0.0003001989395577284\n",
      "test loss is 0.00033637294849697387\n",
      "Batch: 25100,train loss is: 0.00045937781538195815\n",
      "test loss is 0.0004103954208868233\n",
      "Batch: 25200,train loss is: 0.0004897599578249543\n",
      "test loss is 0.00033198577684180277\n",
      "Batch: 25300,train loss is: 0.0002504216569450586\n",
      "test loss is 0.00035095183131447636\n",
      "Batch: 25400,train loss is: 0.00029881433894204316\n",
      "test loss is 0.0003525320573059759\n",
      "Batch: 25500,train loss is: 0.0004804269573444474\n",
      "test loss is 0.0003168927489767227\n",
      "Batch: 25600,train loss is: 0.00017201288459813688\n",
      "test loss is 0.00033543632461247\n",
      "Batch: 25700,train loss is: 0.00037559322717458884\n",
      "test loss is 0.0003219132019056254\n",
      "Batch: 25800,train loss is: 0.00029463751829378126\n",
      "test loss is 0.00031965407387638965\n",
      "Batch: 25900,train loss is: 0.00041631851783545604\n",
      "test loss is 0.00033412923691558905\n",
      "Batch: 26000,train loss is: 0.0002738801575595973\n",
      "test loss is 0.0003447890956844164\n",
      "Batch: 26100,train loss is: 0.00022628162962210654\n",
      "test loss is 0.0003072931534273071\n",
      "Batch: 26200,train loss is: 0.0002560890353196691\n",
      "test loss is 0.0003300279176246688\n",
      "Batch: 26300,train loss is: 0.0006190933669573443\n",
      "test loss is 0.0003329927540435585\n",
      "Batch: 26400,train loss is: 0.00037027042872571104\n",
      "test loss is 0.0003645779383698923\n",
      "Batch: 26500,train loss is: 0.000342810269235524\n",
      "test loss is 0.00033313375359271064\n",
      "Batch: 26600,train loss is: 0.00025055783229311056\n",
      "test loss is 0.0003248270128414907\n",
      "Batch: 26700,train loss is: 0.0003350937209841446\n",
      "test loss is 0.0003222713928789172\n",
      "Batch: 26800,train loss is: 0.00025312980512315044\n",
      "test loss is 0.00035195909397673595\n",
      "Batch: 26900,train loss is: 0.000374259668844168\n",
      "test loss is 0.0003282884965954151\n",
      "Batch: 27000,train loss is: 0.0002865103986999082\n",
      "test loss is 0.0003270116331109964\n",
      "Batch: 27100,train loss is: 0.00032562625832944077\n",
      "test loss is 0.0003626012313734604\n",
      "Batch: 27200,train loss is: 0.00030958624532036286\n",
      "test loss is 0.0002986407156641841\n",
      "Batch: 27300,train loss is: 0.00042753871840700726\n",
      "test loss is 0.00038075386554701894\n",
      "Batch: 27400,train loss is: 0.0004714507597765595\n",
      "test loss is 0.0003468291414150017\n",
      "Batch: 27500,train loss is: 0.0002878618676834929\n",
      "test loss is 0.00033403964963841094\n",
      "Batch: 27600,train loss is: 0.0002280925785560411\n",
      "test loss is 0.000371092924529438\n",
      "Batch: 27700,train loss is: 0.0005056154068387964\n",
      "test loss is 0.00039259442154159516\n",
      "Batch: 27800,train loss is: 0.0002358136632898753\n",
      "test loss is 0.0002925484361576979\n",
      "Batch: 27900,train loss is: 0.00022193690134167182\n",
      "test loss is 0.00031184937109335174\n",
      "Batch: 28000,train loss is: 0.00031290250599263147\n",
      "test loss is 0.00033448176914277895\n",
      "Batch: 28100,train loss is: 0.0002366726338485873\n",
      "test loss is 0.0003296040554171366\n",
      "Batch: 28200,train loss is: 0.00024064487956494226\n",
      "test loss is 0.00034612186871237565\n",
      "Batch: 28300,train loss is: 0.0005236650457305318\n",
      "test loss is 0.0003680745450902383\n",
      "Batch: 28400,train loss is: 0.00028957568463520994\n",
      "test loss is 0.0003817994044693603\n",
      "Batch: 28500,train loss is: 0.00026706953808525657\n",
      "test loss is 0.0003463511250994576\n",
      "Batch: 28600,train loss is: 0.00019058393150792096\n",
      "test loss is 0.000340910067002385\n",
      "Batch: 28700,train loss is: 0.0003429935846047845\n",
      "test loss is 0.00035235450478139326\n",
      "Batch: 28800,train loss is: 0.00038685181836310895\n",
      "test loss is 0.0004395314430595427\n",
      "Batch: 28900,train loss is: 0.00024723059207125804\n",
      "test loss is 0.00033937643344829875\n",
      "Batch: 29000,train loss is: 0.00022502047456510034\n",
      "test loss is 0.0003173243225153592\n",
      "Batch: 29100,train loss is: 0.0002670612003014023\n",
      "test loss is 0.0003332508230479319\n",
      "Batch: 29200,train loss is: 0.0003860435725754369\n",
      "test loss is 0.0003733339273643051\n",
      "Batch: 29300,train loss is: 0.00045092466465158877\n",
      "test loss is 0.0003308883508753622\n",
      "Batch: 29400,train loss is: 0.00027211094155698266\n",
      "test loss is 0.0003394423877848751\n",
      "Batch: 29500,train loss is: 0.0004572722239583086\n",
      "test loss is 0.0004126132239091489\n",
      "Batch: 29600,train loss is: 0.0002035608877429624\n",
      "test loss is 0.00034253039159783203\n",
      "Batch: 29700,train loss is: 0.0003032974521521212\n",
      "test loss is 0.00034839535698349966\n",
      "Batch: 29800,train loss is: 0.00037841488429737555\n",
      "test loss is 0.0003532161079794689\n",
      "Batch: 29900,train loss is: 0.00044046415462261154\n",
      "test loss is 0.0003438003737829219\n",
      "Batch: 30000,train loss is: 0.00033922492921357366\n",
      "test loss is 0.00038323965117704025\n",
      "Batch: 30100,train loss is: 0.0001832159236961999\n",
      "test loss is 0.0003401131582638267\n",
      "Batch: 30200,train loss is: 0.00036607513164428455\n",
      "test loss is 0.0003124715381502789\n",
      "Batch: 30300,train loss is: 0.00027628005263642956\n",
      "test loss is 0.0003269567293681315\n",
      "Batch: 30400,train loss is: 0.0003333740162781613\n",
      "test loss is 0.0003590885748768\n",
      "Batch: 30500,train loss is: 0.00043650313488220185\n",
      "test loss is 0.0003493157265159331\n",
      "Batch: 30600,train loss is: 0.00018674740784545594\n",
      "test loss is 0.00040023135868007097\n",
      "Batch: 30700,train loss is: 0.00044561796148954796\n",
      "test loss is 0.0003382145327804875\n",
      "Batch: 30800,train loss is: 0.00035879486942657203\n",
      "test loss is 0.000370544625165528\n",
      "Batch: 30900,train loss is: 0.00021242767967147525\n",
      "test loss is 0.0003325130978945857\n",
      "Batch: 31000,train loss is: 0.00024000558268221523\n",
      "test loss is 0.0003024822275132712\n",
      "Batch: 31100,train loss is: 0.00020366861469238494\n",
      "test loss is 0.00030252623478392983\n",
      "Batch: 31200,train loss is: 0.00028954838448396305\n",
      "test loss is 0.0003130924508599528\n",
      "Batch: 31300,train loss is: 0.000578048711264757\n",
      "test loss is 0.00044791084193689394\n",
      "Batch: 31400,train loss is: 0.0003531590785632233\n",
      "test loss is 0.0003507653471967783\n",
      "Batch: 31500,train loss is: 0.00032406669926684324\n",
      "test loss is 0.00038101974728288154\n",
      "Batch: 31600,train loss is: 0.00032664800192040336\n",
      "test loss is 0.0003337506005358146\n",
      "Batch: 31700,train loss is: 0.00029627521977282817\n",
      "test loss is 0.00032279778476290114\n",
      "Batch: 31800,train loss is: 0.0004502231482364477\n",
      "test loss is 0.0004632646821474535\n",
      "Batch: 31900,train loss is: 0.00035733059412955864\n",
      "test loss is 0.0003157736711999123\n",
      "Batch: 32000,train loss is: 0.00026389986138868136\n",
      "test loss is 0.0003986184361539467\n",
      "Batch: 32100,train loss is: 0.00027015461875372717\n",
      "test loss is 0.00033742604731957593\n",
      "Batch: 32200,train loss is: 0.0004209954301386906\n",
      "test loss is 0.0004956300751612337\n",
      "Batch: 32300,train loss is: 0.00037831425966499625\n",
      "test loss is 0.0003429233559472224\n",
      "Batch: 32400,train loss is: 0.00024475424629416705\n",
      "test loss is 0.0003354423079104757\n",
      "Batch: 32500,train loss is: 0.000314639746456653\n",
      "test loss is 0.00031773811583201675\n",
      "Batch: 32600,train loss is: 0.0003187703643117894\n",
      "test loss is 0.00031116910592894723\n",
      "Batch: 32700,train loss is: 0.00031509992419136997\n",
      "test loss is 0.000331628742123066\n",
      "Batch: 32800,train loss is: 0.0005851151537796425\n",
      "test loss is 0.0003966299696165137\n",
      "Batch: 32900,train loss is: 0.00023320427015099469\n",
      "test loss is 0.00035423639882023586\n",
      "Batch: 33000,train loss is: 0.0002900652283624177\n",
      "test loss is 0.0003295856482587919\n",
      "Batch: 33100,train loss is: 0.0002371087494388614\n",
      "test loss is 0.0003600432385229811\n",
      "Batch: 33200,train loss is: 0.0003771068596880449\n",
      "test loss is 0.00039413300078434385\n",
      "Batch: 33300,train loss is: 0.0003781922002660498\n",
      "test loss is 0.0003683387008578454\n",
      "Batch: 33400,train loss is: 0.00032011463846780736\n",
      "test loss is 0.000303222716293918\n",
      "Batch: 33500,train loss is: 0.0002881589482174782\n",
      "test loss is 0.00031669215484541864\n",
      "Batch: 33600,train loss is: 0.00022262528846686945\n",
      "test loss is 0.0003629703139418722\n",
      "Batch: 33700,train loss is: 0.0002891854228572964\n",
      "test loss is 0.00032150893955663684\n",
      "Batch: 33800,train loss is: 0.00033513246418378194\n",
      "test loss is 0.0003134631275444147\n",
      "Batch: 33900,train loss is: 0.00022132547405738045\n",
      "test loss is 0.00035395473955331086\n",
      "Batch: 34000,train loss is: 0.00031504920529683246\n",
      "test loss is 0.00033532050661091235\n",
      "Batch: 34100,train loss is: 0.00031749879036022783\n",
      "test loss is 0.0003095783573721867\n",
      "Batch: 34200,train loss is: 0.0002730796564217031\n",
      "test loss is 0.00032602752651993876\n",
      "Batch: 34300,train loss is: 0.00025885419679406486\n",
      "test loss is 0.000320873910387184\n",
      "Batch: 34400,train loss is: 0.00023458080382303962\n",
      "test loss is 0.000486123294717499\n",
      "Batch: 34500,train loss is: 0.00028838082316792063\n",
      "test loss is 0.00030276830474399195\n",
      "Batch: 34600,train loss is: 0.00031242280485859603\n",
      "test loss is 0.00033628574401395393\n",
      "Batch: 34700,train loss is: 0.00016970747019045396\n",
      "test loss is 0.00035028779626327056\n",
      "Batch: 34800,train loss is: 0.00034165711291449236\n",
      "test loss is 0.00037709290767152257\n",
      "Batch: 34900,train loss is: 0.0005092237900884895\n",
      "test loss is 0.00031979668247522955\n",
      "Batch: 35000,train loss is: 0.0005271213402308331\n",
      "test loss is 0.00041616183042455607\n",
      "Batch: 35100,train loss is: 0.00034408030345187466\n",
      "test loss is 0.000339293623562746\n",
      "Batch: 35200,train loss is: 0.00032180200307753567\n",
      "test loss is 0.0003072697788871439\n",
      "Batch: 35300,train loss is: 0.0002783594118137563\n",
      "test loss is 0.0003418376701406346\n",
      "Batch: 35400,train loss is: 0.0003941499327883835\n",
      "test loss is 0.0003684685378892904\n",
      "Batch: 35500,train loss is: 0.0003612440126561871\n",
      "test loss is 0.000304877863472455\n",
      "Batch: 35600,train loss is: 0.0003037019562705212\n",
      "test loss is 0.0003576406957148925\n",
      "Batch: 35700,train loss is: 0.00030267900548002164\n",
      "test loss is 0.00036574330286851973\n",
      "Batch: 35800,train loss is: 0.00032272849392260767\n",
      "test loss is 0.00032882182534179683\n",
      "Batch: 35900,train loss is: 0.0002936197624319978\n",
      "test loss is 0.00032014838784046024\n",
      "Batch: 36000,train loss is: 0.0002820091480005423\n",
      "test loss is 0.00032992655962735463\n",
      "Batch: 36100,train loss is: 0.0002463824324836441\n",
      "test loss is 0.00030562430156398613\n",
      "Batch: 36200,train loss is: 0.000327243823432819\n",
      "test loss is 0.0003101580387339998\n",
      "Batch: 36300,train loss is: 0.0004258349870595855\n",
      "test loss is 0.0003185394555488697\n",
      "Batch: 36400,train loss is: 0.00043774525014833983\n",
      "test loss is 0.000335163052428286\n",
      "Batch: 36500,train loss is: 0.00032634834239751037\n",
      "test loss is 0.0003341477847635972\n",
      "Batch: 36600,train loss is: 0.00045004445118988247\n",
      "test loss is 0.0003123047468442904\n",
      "Batch: 36700,train loss is: 0.0003569518308636924\n",
      "test loss is 0.0003794692394740107\n",
      "Batch: 36800,train loss is: 0.0003645760759600541\n",
      "test loss is 0.0003161155754622484\n",
      "Batch: 36900,train loss is: 0.00030704468590419913\n",
      "test loss is 0.00038963623553784954\n",
      "Batch: 37000,train loss is: 0.00025065260811307603\n",
      "test loss is 0.00031457343339905445\n",
      "Batch: 37100,train loss is: 0.00023649814932388754\n",
      "test loss is 0.0003334218528211058\n",
      "Batch: 37200,train loss is: 0.00020465294756639153\n",
      "test loss is 0.0003284362529872766\n",
      "Batch: 37300,train loss is: 0.00034995310084223014\n",
      "test loss is 0.0003274186260261475\n",
      "Batch: 37400,train loss is: 0.0003604990241685714\n",
      "test loss is 0.0003206461395722294\n",
      "Batch: 37500,train loss is: 0.00024999810727174555\n",
      "test loss is 0.00032221627319390815\n",
      "Batch: 37600,train loss is: 0.00048002941391827106\n",
      "test loss is 0.0003361648958641011\n",
      "Batch: 37700,train loss is: 0.0001533113470986685\n",
      "test loss is 0.0004072878438689721\n",
      "Batch: 37800,train loss is: 0.0003258856104595947\n",
      "test loss is 0.00032768357026972673\n",
      "Batch: 37900,train loss is: 0.0002700863328348735\n",
      "test loss is 0.00032815556773127297\n",
      "Batch: 38000,train loss is: 0.0003031242892154484\n",
      "test loss is 0.0003398693883176425\n",
      "Batch: 38100,train loss is: 0.0002885773925375034\n",
      "test loss is 0.0003943905832660868\n",
      "Batch: 38200,train loss is: 0.0003004496689749612\n",
      "test loss is 0.00033384861752569375\n",
      "Batch: 38300,train loss is: 0.0002845604786840442\n",
      "test loss is 0.0003175725968915438\n",
      "Batch: 38400,train loss is: 0.000309190035959862\n",
      "test loss is 0.00033839269843178086\n",
      "Batch: 38500,train loss is: 0.0002504716028087462\n",
      "test loss is 0.0003338138469145355\n",
      "Batch: 38600,train loss is: 0.00031122817937512915\n",
      "test loss is 0.000330359110031667\n",
      "Batch: 38700,train loss is: 0.00024988927422820296\n",
      "test loss is 0.00033065877429313666\n",
      "Batch: 38800,train loss is: 0.0005284388218851058\n",
      "test loss is 0.00033843998999124426\n",
      "Batch: 38900,train loss is: 0.00036960429707872343\n",
      "test loss is 0.0003215829456355386\n",
      "Batch: 39000,train loss is: 0.00031462603683073256\n",
      "test loss is 0.00032568717700282306\n",
      "Batch: 39100,train loss is: 0.0001861257597839915\n",
      "test loss is 0.0003352762756476126\n",
      "Batch: 39200,train loss is: 0.00029362121716507736\n",
      "test loss is 0.000330293734522152\n",
      "Batch: 39300,train loss is: 0.00033249297840451294\n",
      "test loss is 0.0003927236878653735\n",
      "Batch: 39400,train loss is: 0.00027830093200894245\n",
      "test loss is 0.0003193671804340471\n",
      "Batch: 39500,train loss is: 0.0002666778978999599\n",
      "test loss is 0.0003246984859830773\n",
      "Batch: 39600,train loss is: 0.00040469271168571726\n",
      "test loss is 0.00037595033952176655\n",
      "Batch: 39700,train loss is: 0.0002138998304274495\n",
      "test loss is 0.000311634535800114\n",
      "Batch: 39800,train loss is: 0.000548694967954237\n",
      "test loss is 0.00042255659275587204\n",
      "Batch: 39900,train loss is: 0.0002369189009558796\n",
      "test loss is 0.0003017241801689897\n",
      "Batch: 40000,train loss is: 0.00022159395362288098\n",
      "test loss is 0.00036462627754133936\n",
      "Batch: 40100,train loss is: 0.0002283149470856711\n",
      "test loss is 0.00033593318750825915\n",
      "Batch: 40200,train loss is: 0.00032686323709444633\n",
      "test loss is 0.0003171933900278061\n",
      "Batch: 40300,train loss is: 0.00044185743586078186\n",
      "test loss is 0.00032527609506740235\n",
      "Batch: 40400,train loss is: 0.0003345560677856437\n",
      "test loss is 0.0003246673418610257\n",
      "Batch: 40500,train loss is: 0.00021401302225192002\n",
      "test loss is 0.000333997995743696\n",
      "Batch: 40600,train loss is: 0.00032101975165114624\n",
      "test loss is 0.00031639245681103835\n",
      "Batch: 40700,train loss is: 0.00032639743962823234\n",
      "test loss is 0.0003106087733704543\n",
      "Batch: 40800,train loss is: 0.0002788085710874247\n",
      "test loss is 0.00029963107252079977\n",
      "Batch: 40900,train loss is: 0.00018845162152706616\n",
      "test loss is 0.00029779363480229377\n",
      "Batch: 41000,train loss is: 0.0002490097890950094\n",
      "test loss is 0.00031691136876219777\n",
      "Batch: 41100,train loss is: 0.000420314326775611\n",
      "test loss is 0.00033406697110201076\n",
      "Batch: 41200,train loss is: 0.00022448323642093018\n",
      "test loss is 0.0003336694097304301\n",
      "Batch: 41300,train loss is: 0.00035777282435989763\n",
      "test loss is 0.00036506959443337246\n",
      "Batch: 41400,train loss is: 0.0002920628022051459\n",
      "test loss is 0.00031322500413980496\n",
      "Batch: 41500,train loss is: 0.00035810630168908123\n",
      "test loss is 0.00031802754197557544\n",
      "Batch: 41600,train loss is: 0.00022267128151487432\n",
      "test loss is 0.00031677338297910395\n",
      "Batch: 41700,train loss is: 0.0004139140901698242\n",
      "test loss is 0.0003323444661715448\n",
      "Batch: 41800,train loss is: 0.00028926194346074244\n",
      "test loss is 0.00033771788207696775\n",
      "Batch: 41900,train loss is: 0.00030370454318591825\n",
      "test loss is 0.000384887201194166\n",
      "Batch: 42000,train loss is: 0.0002477353133637363\n",
      "test loss is 0.00030930560985821174\n",
      "Batch: 42100,train loss is: 0.0001657514344293909\n",
      "test loss is 0.0003112891327407704\n",
      "Batch: 42200,train loss is: 0.00033513430844294735\n",
      "test loss is 0.0003338584563215447\n",
      "Batch: 42300,train loss is: 0.0003078867823621406\n",
      "test loss is 0.0003712135067826916\n",
      "Batch: 42400,train loss is: 0.00020544497108158946\n",
      "test loss is 0.0003612455837236255\n",
      "Batch: 42500,train loss is: 0.00038738161940970046\n",
      "test loss is 0.0003854740966666645\n",
      "Batch: 42600,train loss is: 0.0004327762662440784\n",
      "test loss is 0.00031907750641786746\n",
      "Batch: 42700,train loss is: 0.00030679129699488636\n",
      "test loss is 0.00038525643828491926\n",
      "Batch: 42800,train loss is: 0.0004204229880998202\n",
      "test loss is 0.00041063595851567315\n",
      "Batch: 42900,train loss is: 0.00030310108447470675\n",
      "test loss is 0.000372531841510682\n",
      "Batch: 43000,train loss is: 0.00023920719908999304\n",
      "test loss is 0.0003057836933948609\n",
      "Batch: 43100,train loss is: 0.0003284147755888227\n",
      "test loss is 0.00038450966746743603\n",
      "Batch: 43200,train loss is: 0.0003765833682129096\n",
      "test loss is 0.0003371337693807518\n",
      "Batch: 43300,train loss is: 0.00043961587818818\n",
      "test loss is 0.00030921309069062674\n",
      "Batch: 43400,train loss is: 0.0006391759454191679\n",
      "test loss is 0.0003162164007650362\n",
      "Batch: 43500,train loss is: 0.000250223127947321\n",
      "test loss is 0.0003794561234119046\n",
      "Batch: 43600,train loss is: 0.00044574717894633293\n",
      "test loss is 0.00031342807412603466\n",
      "Batch: 43700,train loss is: 0.0005570425748710113\n",
      "test loss is 0.0003211237255082921\n",
      "Batch: 43800,train loss is: 0.0004438336283385995\n",
      "test loss is 0.0004258136370719933\n",
      "Batch: 43900,train loss is: 0.0004297344759733063\n",
      "test loss is 0.0003577435240174912\n",
      "Batch: 44000,train loss is: 0.0004045709499253826\n",
      "test loss is 0.0003152842537365125\n",
      "Batch: 44100,train loss is: 0.0004887532739603942\n",
      "test loss is 0.0003665193858851103\n",
      "Batch: 44200,train loss is: 0.0003381971963463293\n",
      "test loss is 0.000308217226876729\n",
      "Batch: 44300,train loss is: 0.0004298784394948103\n",
      "test loss is 0.00029976689769537754\n",
      "Batch: 44400,train loss is: 0.00030294847115742003\n",
      "test loss is 0.00032311459987754856\n",
      "Batch: 44500,train loss is: 0.00018721988327788705\n",
      "test loss is 0.00029987856283392015\n",
      "Batch: 44600,train loss is: 0.0002697700921667481\n",
      "test loss is 0.00029401015915005743\n",
      "Batch: 44700,train loss is: 0.000437851430349751\n",
      "test loss is 0.0003134260529583438\n",
      "Batch: 44800,train loss is: 0.00030369881065879186\n",
      "test loss is 0.0003910387367959996\n",
      "Batch: 44900,train loss is: 0.00026818393000109156\n",
      "test loss is 0.0003107475787260131\n",
      "Batch: 45000,train loss is: 0.0004668311250433014\n",
      "test loss is 0.0003426976126678196\n",
      "Batch: 45100,train loss is: 0.0003538867229546188\n",
      "test loss is 0.000348801583888819\n",
      "Batch: 45200,train loss is: 0.0003062823944711325\n",
      "test loss is 0.0003446359744499818\n",
      "Batch: 45300,train loss is: 0.0002900962693390353\n",
      "test loss is 0.00031496236257137875\n",
      "Batch: 45400,train loss is: 0.0002372338072631122\n",
      "test loss is 0.0003146711646682939\n",
      "Batch: 45500,train loss is: 0.0002790915671676631\n",
      "test loss is 0.0002963489983103541\n",
      "Batch: 45600,train loss is: 0.00040089186985638756\n",
      "test loss is 0.0003178255299901961\n",
      "Batch: 45700,train loss is: 0.0005654100357923347\n",
      "test loss is 0.00031061449715368846\n",
      "Batch: 45800,train loss is: 0.000335075605299558\n",
      "test loss is 0.00033276181206296174\n",
      "Batch: 45900,train loss is: 0.00024076042607370943\n",
      "test loss is 0.0003336512189295979\n",
      "Batch: 46000,train loss is: 0.000768918985223923\n",
      "test loss is 0.0003502288035124621\n",
      "Batch: 46100,train loss is: 0.0002322406328982691\n",
      "test loss is 0.00029826554215602524\n",
      "Batch: 46200,train loss is: 0.00034144215994411206\n",
      "test loss is 0.0003167509788241624\n",
      "Batch: 46300,train loss is: 0.0006521791347949765\n",
      "test loss is 0.00045130839792326615\n",
      "Batch: 46400,train loss is: 0.00026527097321086065\n",
      "test loss is 0.00036615615703685446\n",
      "Batch: 46500,train loss is: 0.0004399949311979251\n",
      "test loss is 0.00031725949046683183\n",
      "Batch: 46600,train loss is: 0.00032672283729210175\n",
      "test loss is 0.00041831249041285383\n",
      "Batch: 46700,train loss is: 0.00023531314923456412\n",
      "test loss is 0.0003530221517917776\n",
      "-----------------------Epoch: 1----------------------------------\n",
      "Batch: 0,train loss is: 0.0008661643470680718\n",
      "test loss is 0.00037687028517213234\n",
      "Batch: 100,train loss is: 0.0003112860545226759\n",
      "test loss is 0.00032993883789162663\n",
      "Batch: 200,train loss is: 0.0004222267099300218\n",
      "test loss is 0.0003403776147323206\n",
      "Batch: 300,train loss is: 0.000340216274399502\n",
      "test loss is 0.0003845836688495293\n",
      "Batch: 400,train loss is: 0.0003881296778402124\n",
      "test loss is 0.0003827050267590136\n",
      "Batch: 500,train loss is: 0.0003475773682525586\n",
      "test loss is 0.000315601326370525\n",
      "Batch: 600,train loss is: 0.0003062498611932415\n",
      "test loss is 0.0003099313959717682\n",
      "Batch: 700,train loss is: 0.00042777974972206594\n",
      "test loss is 0.00033062598341978796\n",
      "Batch: 800,train loss is: 0.000367537618574534\n",
      "test loss is 0.0003008406048944317\n",
      "Batch: 900,train loss is: 0.0001682640857142286\n",
      "test loss is 0.00032421873997932383\n",
      "Batch: 1000,train loss is: 0.0004231335023311485\n",
      "test loss is 0.00038604795656660244\n",
      "Batch: 1100,train loss is: 0.0002426288580824707\n",
      "test loss is 0.0003119818944996904\n",
      "Batch: 1200,train loss is: 0.00039009698295910933\n",
      "test loss is 0.000345612917424333\n",
      "Batch: 1300,train loss is: 0.0005429907352489157\n",
      "test loss is 0.0003299734042755283\n",
      "Batch: 1400,train loss is: 0.0002530630507127452\n",
      "test loss is 0.00035321353930570877\n",
      "Batch: 1500,train loss is: 0.0009232064950967318\n",
      "test loss is 0.00035675756125432863\n",
      "Batch: 1600,train loss is: 0.00020644379947187835\n",
      "test loss is 0.0003114772500306987\n",
      "Batch: 1700,train loss is: 0.00027375851566778184\n",
      "test loss is 0.00029047210398421827\n",
      "Batch: 1800,train loss is: 0.0003464641816295543\n",
      "test loss is 0.00031524385005262767\n",
      "Batch: 1900,train loss is: 0.0002671757670100515\n",
      "test loss is 0.00033367629430939053\n",
      "Batch: 2000,train loss is: 0.00044006078579221844\n",
      "test loss is 0.00037925703121608433\n",
      "Batch: 2100,train loss is: 0.0005197852132309299\n",
      "test loss is 0.00036269620592151847\n",
      "Batch: 2200,train loss is: 0.00021585379213877207\n",
      "test loss is 0.0003336695751451504\n",
      "Batch: 2300,train loss is: 0.0002985267014620004\n",
      "test loss is 0.0003401215345092066\n",
      "Batch: 2400,train loss is: 0.00029535020039827675\n",
      "test loss is 0.00030876269406575693\n",
      "Batch: 2500,train loss is: 0.0004093080089147875\n",
      "test loss is 0.0004334719268686145\n",
      "Batch: 2600,train loss is: 0.0002549427680959556\n",
      "test loss is 0.0003098796144301384\n",
      "Batch: 2700,train loss is: 0.0002506152579228415\n",
      "test loss is 0.00039636159116762415\n",
      "Batch: 2800,train loss is: 0.00028213650246180697\n",
      "test loss is 0.0003096793562491653\n",
      "Batch: 2900,train loss is: 0.00026150376094130493\n",
      "test loss is 0.0003771977195463084\n",
      "Batch: 3000,train loss is: 0.0003623058751759138\n",
      "test loss is 0.0003271643462234256\n",
      "Batch: 3100,train loss is: 0.00026935454214288367\n",
      "test loss is 0.00037666119632304216\n",
      "Batch: 3200,train loss is: 0.00023171477004520083\n",
      "test loss is 0.00030490000724714915\n",
      "Batch: 3300,train loss is: 0.00033388443354193445\n",
      "test loss is 0.00032733769927877966\n",
      "Batch: 3400,train loss is: 0.0002866406710898935\n",
      "test loss is 0.0002976592387256464\n",
      "Batch: 3500,train loss is: 0.00034213156382814863\n",
      "test loss is 0.0003173648776752911\n",
      "Batch: 3600,train loss is: 0.0003563853714460228\n",
      "test loss is 0.0003312131372765043\n",
      "Batch: 3700,train loss is: 0.00023318333005233215\n",
      "test loss is 0.00031623464378664073\n",
      "Batch: 3800,train loss is: 0.0003477843634429596\n",
      "test loss is 0.00032080675084206856\n",
      "Batch: 3900,train loss is: 0.00031890527222540544\n",
      "test loss is 0.00030237605814502474\n",
      "Batch: 4000,train loss is: 0.0003346106189085096\n",
      "test loss is 0.00032344304544490053\n",
      "Batch: 4100,train loss is: 0.00023306805146102843\n",
      "test loss is 0.00034889314636177174\n",
      "Batch: 4200,train loss is: 0.00029283154542607726\n",
      "test loss is 0.0003467469593923609\n",
      "Batch: 4300,train loss is: 0.000273887374184974\n",
      "test loss is 0.00029233271632133034\n",
      "Batch: 4400,train loss is: 0.0002772836869800988\n",
      "test loss is 0.000308420040574313\n",
      "Batch: 4500,train loss is: 0.000620088260738858\n",
      "test loss is 0.00045379578716839335\n",
      "Batch: 4600,train loss is: 0.0002609621208580072\n",
      "test loss is 0.0003060752131724135\n",
      "Batch: 4700,train loss is: 0.00025228556819625934\n",
      "test loss is 0.0003339609566562458\n",
      "Batch: 4800,train loss is: 0.00044326564997266085\n",
      "test loss is 0.00035169651302945754\n",
      "Batch: 4900,train loss is: 0.0003148715547898907\n",
      "test loss is 0.00030858113588111993\n",
      "Batch: 5000,train loss is: 0.0003425471745710283\n",
      "test loss is 0.0003085477460207753\n",
      "Batch: 5100,train loss is: 0.0002749081102757408\n",
      "test loss is 0.0003503587866464688\n",
      "Batch: 5200,train loss is: 0.00025682201820132497\n",
      "test loss is 0.0003023350594294785\n",
      "Batch: 5300,train loss is: 0.00030470301860831836\n",
      "test loss is 0.00036559313339874343\n",
      "Batch: 5400,train loss is: 0.00033219186247344186\n",
      "test loss is 0.00033671573336126626\n",
      "Batch: 5500,train loss is: 0.00027206421898657706\n",
      "test loss is 0.0003445387107431338\n",
      "Batch: 5600,train loss is: 0.0003052287330773339\n",
      "test loss is 0.0003526845744203742\n",
      "Batch: 5700,train loss is: 0.00039500632002365357\n",
      "test loss is 0.0004081672709893234\n",
      "Batch: 5800,train loss is: 0.0003780322533555081\n",
      "test loss is 0.00045777437042386735\n",
      "Batch: 5900,train loss is: 0.0002238426613182257\n",
      "test loss is 0.0003240233109100479\n",
      "Batch: 6000,train loss is: 0.0002996757367401741\n",
      "test loss is 0.00037038683074317846\n",
      "Batch: 6100,train loss is: 0.00032994707340006806\n",
      "test loss is 0.0003063109569911566\n",
      "Batch: 6200,train loss is: 0.0003054067542742787\n",
      "test loss is 0.0003135878514479903\n",
      "Batch: 6300,train loss is: 0.0006106903512312951\n",
      "test loss is 0.0003078704607038303\n",
      "Batch: 6400,train loss is: 0.00020583243508649377\n",
      "test loss is 0.00033445380292248633\n",
      "Batch: 6500,train loss is: 0.000182081811792022\n",
      "test loss is 0.0003638018445287957\n",
      "Batch: 6600,train loss is: 0.00018632117412106537\n",
      "test loss is 0.0003120173757769372\n",
      "Batch: 6700,train loss is: 0.0003683240303368499\n",
      "test loss is 0.0003525766996661641\n",
      "Batch: 6800,train loss is: 0.00028606117359192685\n",
      "test loss is 0.0003195151632890284\n",
      "Batch: 6900,train loss is: 0.00024367527571829441\n",
      "test loss is 0.00029150529799720857\n",
      "Batch: 7000,train loss is: 0.00024176025393238945\n",
      "test loss is 0.0003074860347237659\n",
      "Batch: 7100,train loss is: 0.0004246776073020138\n",
      "test loss is 0.0003681063339454474\n",
      "Batch: 7200,train loss is: 0.0002487527497507052\n",
      "test loss is 0.00031933906385302657\n",
      "Batch: 7300,train loss is: 0.0002835855543459806\n",
      "test loss is 0.0003569630312329494\n",
      "Batch: 7400,train loss is: 0.00022871297881410486\n",
      "test loss is 0.0003177980655547889\n",
      "Batch: 7500,train loss is: 0.0002689255904466403\n",
      "test loss is 0.00029352128623314096\n",
      "Batch: 7600,train loss is: 0.00036953037805182815\n",
      "test loss is 0.00032299861003740255\n",
      "Batch: 7700,train loss is: 0.0003989174739085035\n",
      "test loss is 0.0003141797917518376\n",
      "Batch: 7800,train loss is: 0.00039202894034281804\n",
      "test loss is 0.0002894616010319104\n",
      "Batch: 7900,train loss is: 0.0002753979931783525\n",
      "test loss is 0.00031296863234988486\n",
      "Batch: 8000,train loss is: 0.0002457555008204807\n",
      "test loss is 0.0003051136327927471\n",
      "Batch: 8100,train loss is: 0.0005117828892529615\n",
      "test loss is 0.00035020928065759523\n",
      "Batch: 8200,train loss is: 0.00030706359010977756\n",
      "test loss is 0.0003373667412220463\n",
      "Batch: 8300,train loss is: 0.00027653416757744204\n",
      "test loss is 0.0003506034341507685\n",
      "Batch: 8400,train loss is: 0.0003938394339962387\n",
      "test loss is 0.0003166360392447903\n",
      "Batch: 8500,train loss is: 0.00027374840401450803\n",
      "test loss is 0.00032056387724317947\n",
      "Batch: 8600,train loss is: 0.0001839756535710066\n",
      "test loss is 0.0003261617645372186\n",
      "Batch: 8700,train loss is: 0.00024654030641581314\n",
      "test loss is 0.00030400673114904626\n",
      "Batch: 8800,train loss is: 0.00033616226255563\n",
      "test loss is 0.00032274700847609484\n",
      "Batch: 8900,train loss is: 0.00032324476054437063\n",
      "test loss is 0.0003439713930045716\n",
      "Batch: 9000,train loss is: 0.00021591242540775743\n",
      "test loss is 0.0002977025008302272\n",
      "Batch: 9100,train loss is: 0.0002556253297571797\n",
      "test loss is 0.00032220033998196296\n",
      "Batch: 9200,train loss is: 0.00032733619306325604\n",
      "test loss is 0.0003247852429482328\n",
      "Batch: 9300,train loss is: 0.0003219687950826924\n",
      "test loss is 0.0003211337880913973\n",
      "Batch: 9400,train loss is: 0.00029787554788277893\n",
      "test loss is 0.00033095666211005394\n",
      "Batch: 9500,train loss is: 0.00033646945434546305\n",
      "test loss is 0.0003020067539936708\n",
      "Batch: 9600,train loss is: 0.000563443680443853\n",
      "test loss is 0.0003249306672907144\n",
      "Batch: 9700,train loss is: 0.00035775743005698535\n",
      "test loss is 0.00032361515386700776\n",
      "Batch: 9800,train loss is: 0.00025671163492259217\n",
      "test loss is 0.00030786949611391487\n",
      "Batch: 9900,train loss is: 0.0002965318088963405\n",
      "test loss is 0.0003416998274492013\n",
      "Batch: 10000,train loss is: 0.0004896285975403248\n",
      "test loss is 0.00032156429879057517\n",
      "Batch: 10100,train loss is: 0.0003120118427445233\n",
      "test loss is 0.00048790769433871184\n",
      "Batch: 10200,train loss is: 0.00030396648405093944\n",
      "test loss is 0.0003279765062563067\n",
      "Batch: 10300,train loss is: 0.0002664303233342939\n",
      "test loss is 0.0003043597681976197\n",
      "Batch: 10400,train loss is: 0.0003036981589982395\n",
      "test loss is 0.0003095865516806912\n",
      "Batch: 10500,train loss is: 0.0003828146435849139\n",
      "test loss is 0.0003569827952609775\n",
      "Batch: 10600,train loss is: 0.00021270253075860097\n",
      "test loss is 0.00034101516833580576\n",
      "Batch: 10700,train loss is: 0.0002826175470035733\n",
      "test loss is 0.0003011553788667905\n",
      "Batch: 10800,train loss is: 0.00027614745900757633\n",
      "test loss is 0.00035440150560286135\n",
      "Batch: 10900,train loss is: 0.0002112191625514263\n",
      "test loss is 0.0002961473536413914\n",
      "Batch: 11000,train loss is: 0.00023270613350407835\n",
      "test loss is 0.00031947590271032966\n",
      "Batch: 11100,train loss is: 0.00026000974902823826\n",
      "test loss is 0.0003868223780560933\n",
      "Batch: 11200,train loss is: 0.000406893906125096\n",
      "test loss is 0.00034790974585777537\n",
      "Batch: 11300,train loss is: 0.0004153227137000976\n",
      "test loss is 0.0003617611287176109\n",
      "Batch: 11400,train loss is: 0.0002012816201863051\n",
      "test loss is 0.000303964870262223\n",
      "Batch: 11500,train loss is: 0.00022576164072911253\n",
      "test loss is 0.0003101062524040964\n",
      "Batch: 11600,train loss is: 0.00041094445963942467\n",
      "test loss is 0.00032575136440041495\n",
      "Batch: 11700,train loss is: 0.0003412394559183362\n",
      "test loss is 0.00032907614993962477\n",
      "Batch: 11800,train loss is: 0.00021735580525525123\n",
      "test loss is 0.0003211298494103086\n",
      "Batch: 11900,train loss is: 0.00019315969791170438\n",
      "test loss is 0.00030329364542965324\n",
      "Batch: 12000,train loss is: 0.0003208480565717255\n",
      "test loss is 0.00038067206133889775\n",
      "Batch: 12100,train loss is: 0.00028911636953018484\n",
      "test loss is 0.0003045214477660514\n",
      "Batch: 12200,train loss is: 0.0003595654220361484\n",
      "test loss is 0.0003001684173820264\n",
      "Batch: 12300,train loss is: 0.00024135817419421843\n",
      "test loss is 0.00029074199469052516\n",
      "Batch: 12400,train loss is: 0.00026048693185721367\n",
      "test loss is 0.0003038871157140746\n",
      "Batch: 12500,train loss is: 0.00032589622087703216\n",
      "test loss is 0.00033319180837571005\n",
      "Batch: 12600,train loss is: 0.0003107170729709661\n",
      "test loss is 0.0003068797979385055\n",
      "Batch: 12700,train loss is: 0.00023702839288334328\n",
      "test loss is 0.0003065035264017058\n",
      "Batch: 12800,train loss is: 0.000755212440942471\n",
      "test loss is 0.0003932767190344426\n",
      "Batch: 12900,train loss is: 0.00030040576849890634\n",
      "test loss is 0.0002969316044846626\n",
      "Batch: 13000,train loss is: 0.00042068646622674534\n",
      "test loss is 0.00029619621686129155\n",
      "Batch: 13100,train loss is: 0.0002428196067653384\n",
      "test loss is 0.0002983880312385514\n",
      "Batch: 13200,train loss is: 0.00031734362836741115\n",
      "test loss is 0.00033268708333174945\n",
      "Batch: 13300,train loss is: 0.00037679213395311795\n",
      "test loss is 0.00035072770500224154\n",
      "Batch: 13400,train loss is: 0.0002613816080374989\n",
      "test loss is 0.00030310818912156033\n",
      "Batch: 13500,train loss is: 0.00022193741162571692\n",
      "test loss is 0.0003191283467121075\n",
      "Batch: 13600,train loss is: 0.00024187753348124284\n",
      "test loss is 0.0003011363291932066\n",
      "Batch: 13700,train loss is: 0.0003091623525181459\n",
      "test loss is 0.0003108846061265774\n",
      "Batch: 13800,train loss is: 0.00024193364663377357\n",
      "test loss is 0.0002922366035380705\n",
      "Batch: 13900,train loss is: 0.00036849970953002734\n",
      "test loss is 0.0004089258247351481\n",
      "Batch: 14000,train loss is: 0.00016233837026108009\n",
      "test loss is 0.00032229258044363967\n",
      "Batch: 14100,train loss is: 0.00033310715629538545\n",
      "test loss is 0.0003072161910004605\n",
      "Batch: 14200,train loss is: 0.0003424346345818758\n",
      "test loss is 0.0003334041949244213\n",
      "Batch: 14300,train loss is: 0.00047778183068909036\n",
      "test loss is 0.0003100491176681663\n",
      "Batch: 14400,train loss is: 0.00036308874974102893\n",
      "test loss is 0.0003118736183009378\n",
      "Batch: 14500,train loss is: 0.0003979269422230014\n",
      "test loss is 0.00035855640820545154\n",
      "Batch: 14600,train loss is: 0.00022605322368908251\n",
      "test loss is 0.00031033062178772904\n",
      "Batch: 14700,train loss is: 0.00025404167480723937\n",
      "test loss is 0.0003278064782367701\n",
      "Batch: 14800,train loss is: 0.00033183481063640477\n",
      "test loss is 0.000402205055193773\n",
      "Batch: 14900,train loss is: 0.0002906734420645114\n",
      "test loss is 0.00033120966586097064\n",
      "Batch: 15000,train loss is: 0.0002179441187325272\n",
      "test loss is 0.00030756678999313233\n",
      "Batch: 15100,train loss is: 0.00019237340892736175\n",
      "test loss is 0.00030484841968197074\n",
      "Batch: 15200,train loss is: 0.00024488657181279393\n",
      "test loss is 0.0003105660466265866\n",
      "Batch: 15300,train loss is: 0.00035063213016206106\n",
      "test loss is 0.000313138892416299\n",
      "Batch: 15400,train loss is: 0.00026830905498954796\n",
      "test loss is 0.00030019879925871114\n",
      "Batch: 15500,train loss is: 0.00019582503545502127\n",
      "test loss is 0.0003167433927301768\n",
      "Batch: 15600,train loss is: 0.0003991183717250291\n",
      "test loss is 0.0003739543381459542\n",
      "Batch: 15700,train loss is: 0.0003432480782942675\n",
      "test loss is 0.0003081923527899287\n",
      "Batch: 15800,train loss is: 0.0003517829122887592\n",
      "test loss is 0.00030596014702103324\n",
      "Batch: 15900,train loss is: 0.0002595180818199799\n",
      "test loss is 0.000343078145512432\n",
      "Batch: 16000,train loss is: 0.0003145930407042781\n",
      "test loss is 0.0003235951176612452\n",
      "Batch: 16100,train loss is: 0.00024320992243794532\n",
      "test loss is 0.00042640325149097036\n",
      "Batch: 16200,train loss is: 0.00027846174372568507\n",
      "test loss is 0.00033751422633231516\n",
      "Batch: 16300,train loss is: 0.0005589792687155292\n",
      "test loss is 0.0003728297007466153\n",
      "Batch: 16400,train loss is: 0.0003906860479110113\n",
      "test loss is 0.0003415813448308\n",
      "Batch: 16500,train loss is: 0.0002690388008296221\n",
      "test loss is 0.00033279720217820616\n",
      "Batch: 16600,train loss is: 0.00021275650251215763\n",
      "test loss is 0.0003331126290855457\n",
      "Batch: 16700,train loss is: 0.00017015288890800056\n",
      "test loss is 0.00030624450525504045\n",
      "Batch: 16800,train loss is: 0.0004739084845323915\n",
      "test loss is 0.00033912724931518427\n",
      "Batch: 16900,train loss is: 0.0003349561468863302\n",
      "test loss is 0.00031353630064854794\n",
      "Batch: 17000,train loss is: 0.00033423452377866637\n",
      "test loss is 0.0003775231563819735\n",
      "Batch: 17100,train loss is: 0.00026339916139000373\n",
      "test loss is 0.0003301122987615207\n",
      "Batch: 17200,train loss is: 0.000298547833619149\n",
      "test loss is 0.0003196158919456425\n",
      "Batch: 17300,train loss is: 0.0008865574778474793\n",
      "test loss is 0.0003016364363780421\n",
      "Batch: 17400,train loss is: 0.0002705843417919525\n",
      "test loss is 0.0003007905293368244\n",
      "Batch: 17500,train loss is: 0.0002802021385020693\n",
      "test loss is 0.00047474934129824314\n",
      "Batch: 17600,train loss is: 0.0003014800299134993\n",
      "test loss is 0.00030804081011531146\n",
      "Batch: 17700,train loss is: 0.0005757336666499403\n",
      "test loss is 0.00032746551163012936\n",
      "Batch: 17800,train loss is: 0.0002866914986463927\n",
      "test loss is 0.00031337037523762547\n",
      "Batch: 17900,train loss is: 0.0002787988348374714\n",
      "test loss is 0.00030548073257858743\n",
      "Batch: 18000,train loss is: 0.00017460170890468864\n",
      "test loss is 0.00032521284886201737\n",
      "Batch: 18100,train loss is: 0.0003441595436118447\n",
      "test loss is 0.0003443027531499111\n",
      "Batch: 18200,train loss is: 0.00023287444518621685\n",
      "test loss is 0.0003261322981331237\n",
      "Batch: 18300,train loss is: 0.0004922626117140406\n",
      "test loss is 0.0003761844528826862\n",
      "Batch: 18400,train loss is: 0.00025900743590479117\n",
      "test loss is 0.00031252962627936634\n",
      "Batch: 18500,train loss is: 0.0004093670466204167\n",
      "test loss is 0.00031312173852805097\n",
      "Batch: 18600,train loss is: 0.0003010766355150885\n",
      "test loss is 0.00041028440766847923\n",
      "Batch: 18700,train loss is: 0.0003054331806180933\n",
      "test loss is 0.000310521904859039\n",
      "Batch: 18800,train loss is: 0.0003090251375011975\n",
      "test loss is 0.0003250226632842736\n",
      "Batch: 18900,train loss is: 0.00023050373049924205\n",
      "test loss is 0.00034031211899600457\n",
      "Batch: 19000,train loss is: 0.00030438374125884714\n",
      "test loss is 0.0003199228361455645\n",
      "Batch: 19100,train loss is: 0.0002678712181169296\n",
      "test loss is 0.00030963443666099397\n",
      "Batch: 19200,train loss is: 0.00027173030747171885\n",
      "test loss is 0.0003090589799146004\n",
      "Batch: 19300,train loss is: 0.0002597701881748201\n",
      "test loss is 0.00029424973158684444\n",
      "Batch: 19400,train loss is: 0.0002504050792150144\n",
      "test loss is 0.0003221040833682142\n",
      "Batch: 19500,train loss is: 0.0003778543110538313\n",
      "test loss is 0.00030776428678754275\n",
      "Batch: 19600,train loss is: 0.0003336165343987992\n",
      "test loss is 0.00031967462057694524\n",
      "Batch: 19700,train loss is: 0.0003531584795259384\n",
      "test loss is 0.0003172717245689832\n",
      "Batch: 19800,train loss is: 0.0002659023392291618\n",
      "test loss is 0.00032745448218675447\n",
      "Batch: 19900,train loss is: 0.00018297751991052424\n",
      "test loss is 0.0002969539183982723\n",
      "Batch: 20000,train loss is: 0.0002811246181835881\n",
      "test loss is 0.0004083639824412743\n",
      "Batch: 20100,train loss is: 0.00023139450292024018\n",
      "test loss is 0.0003691248100247899\n",
      "Batch: 20200,train loss is: 0.0003475600454621344\n",
      "test loss is 0.0003203941371150207\n",
      "Batch: 20300,train loss is: 0.0004005730426622631\n",
      "test loss is 0.0003102498897691618\n",
      "Batch: 20400,train loss is: 0.00030015296079942866\n",
      "test loss is 0.0003549365929991035\n",
      "Batch: 20500,train loss is: 0.00020895471771461223\n",
      "test loss is 0.00030045303541104704\n",
      "Batch: 20600,train loss is: 0.0002833371162656139\n",
      "test loss is 0.000326576261826875\n",
      "Batch: 20700,train loss is: 0.0002710676292908503\n",
      "test loss is 0.00031381986593990627\n",
      "Batch: 20800,train loss is: 0.00028356401437867375\n",
      "test loss is 0.00030257558572419253\n",
      "Batch: 20900,train loss is: 0.00024268246146557593\n",
      "test loss is 0.00033145759659549475\n",
      "Batch: 21000,train loss is: 0.00022104408879949997\n",
      "test loss is 0.0003215066735023676\n",
      "Batch: 21100,train loss is: 0.0003114044056887183\n",
      "test loss is 0.00035764597325960867\n",
      "Batch: 21200,train loss is: 0.00028365079178922054\n",
      "test loss is 0.000347625209266087\n",
      "Batch: 21300,train loss is: 0.00035051098864651356\n",
      "test loss is 0.0003758290252308966\n",
      "Batch: 21400,train loss is: 0.00031083528264683343\n",
      "test loss is 0.0003194868241386407\n",
      "Batch: 21500,train loss is: 0.00036625329867757137\n",
      "test loss is 0.0003374713449888921\n",
      "Batch: 21600,train loss is: 0.00034528144728919485\n",
      "test loss is 0.0003043512116557836\n",
      "Batch: 21700,train loss is: 0.0003039487144629386\n",
      "test loss is 0.00034503088806206146\n",
      "Batch: 21800,train loss is: 0.0001950985783279108\n",
      "test loss is 0.00033094239649489364\n",
      "Batch: 21900,train loss is: 0.0002357204222409429\n",
      "test loss is 0.00032419598308905807\n",
      "Batch: 22000,train loss is: 0.0004888951194493842\n",
      "test loss is 0.000344536252090934\n",
      "Batch: 22100,train loss is: 0.00027205221806496854\n",
      "test loss is 0.00037071650978506355\n",
      "Batch: 22200,train loss is: 0.00039481560233835147\n",
      "test loss is 0.00032431417591511247\n",
      "Batch: 22300,train loss is: 0.00027936755867174454\n",
      "test loss is 0.000345530224775512\n",
      "Batch: 22400,train loss is: 0.000452020038239124\n",
      "test loss is 0.00038973401855881045\n",
      "Batch: 22500,train loss is: 0.00032112300916566284\n",
      "test loss is 0.000331074726539168\n",
      "Batch: 22600,train loss is: 0.0003254324758969574\n",
      "test loss is 0.0003452607159557225\n",
      "Batch: 22700,train loss is: 0.00031553275491214233\n",
      "test loss is 0.00032907213458233266\n",
      "Batch: 22800,train loss is: 0.0006694568775631367\n",
      "test loss is 0.00037684561822319996\n",
      "Batch: 22900,train loss is: 0.00030052079874321575\n",
      "test loss is 0.0003137288146551518\n",
      "Batch: 23000,train loss is: 0.0003675275405261618\n",
      "test loss is 0.00035232232824203454\n",
      "Batch: 23100,train loss is: 0.0005739229977187459\n",
      "test loss is 0.0003340673970267901\n",
      "Batch: 23200,train loss is: 0.00033506468942650123\n",
      "test loss is 0.0003270539216508903\n",
      "Batch: 23300,train loss is: 0.00017484829260644837\n",
      "test loss is 0.0003110266036033726\n",
      "Batch: 23400,train loss is: 0.0005924938832481811\n",
      "test loss is 0.0006215794487357789\n",
      "Batch: 23500,train loss is: 0.0005051443661147432\n",
      "test loss is 0.00030961496016827295\n",
      "Batch: 23600,train loss is: 0.0002652282753531103\n",
      "test loss is 0.00032097701523435627\n",
      "Batch: 23700,train loss is: 0.00021870508773634762\n",
      "test loss is 0.00033099659341869365\n",
      "Batch: 23800,train loss is: 0.0004323297505042954\n",
      "test loss is 0.00033351043967755784\n",
      "Batch: 23900,train loss is: 0.0002691502057157139\n",
      "test loss is 0.0003238706185591817\n",
      "Batch: 24000,train loss is: 0.00032184355975025064\n",
      "test loss is 0.00037046694302431924\n",
      "Batch: 24100,train loss is: 0.00021779617964569128\n",
      "test loss is 0.00035753927371653607\n",
      "Batch: 24200,train loss is: 0.00024537760586723086\n",
      "test loss is 0.0003358652797286955\n",
      "Batch: 24300,train loss is: 0.0002077794481045185\n",
      "test loss is 0.00029389389412227523\n",
      "Batch: 24400,train loss is: 0.0003287603871777742\n",
      "test loss is 0.00043866596697358354\n",
      "Batch: 24500,train loss is: 0.00029165071439674127\n",
      "test loss is 0.0003335395200577338\n",
      "Batch: 24600,train loss is: 0.0003298090093506344\n",
      "test loss is 0.00028761899451470096\n",
      "Batch: 24700,train loss is: 0.00026665969668222605\n",
      "test loss is 0.00032878338216800034\n",
      "Batch: 24800,train loss is: 0.00021827455755932303\n",
      "test loss is 0.0003475295565374424\n",
      "Batch: 24900,train loss is: 0.00019154829365818893\n",
      "test loss is 0.00031269551961268305\n",
      "Batch: 25000,train loss is: 0.00047146437378958977\n",
      "test loss is 0.00042506129571323094\n",
      "Batch: 25100,train loss is: 0.0002631606564864762\n",
      "test loss is 0.0002952984315917336\n",
      "Batch: 25200,train loss is: 0.00018748238671212661\n",
      "test loss is 0.00034133315385409324\n",
      "Batch: 25300,train loss is: 0.0003229865384822171\n",
      "test loss is 0.0003446666728901254\n",
      "Batch: 25400,train loss is: 0.0002891089486307763\n",
      "test loss is 0.00028599688414610274\n",
      "Batch: 25500,train loss is: 0.00044852420441436093\n",
      "test loss is 0.00031385384819286816\n",
      "Batch: 25600,train loss is: 0.00030377682878139\n",
      "test loss is 0.0003447192776134055\n",
      "Batch: 25700,train loss is: 0.0002599015810692004\n",
      "test loss is 0.0003373374104668272\n",
      "Batch: 25800,train loss is: 0.00032535210286439723\n",
      "test loss is 0.0004043888677425416\n",
      "Batch: 25900,train loss is: 0.00023354184514770343\n",
      "test loss is 0.0003164200966471174\n",
      "Batch: 26000,train loss is: 0.00042666397120532897\n",
      "test loss is 0.0003471712166390544\n",
      "Batch: 26100,train loss is: 0.00028979915625427334\n",
      "test loss is 0.000322126005594131\n",
      "Batch: 26200,train loss is: 0.00039199052774461336\n",
      "test loss is 0.00032051191848166154\n",
      "Batch: 26300,train loss is: 0.00016721246634567618\n",
      "test loss is 0.00031774446869515534\n",
      "Batch: 26400,train loss is: 0.00029018383717713183\n",
      "test loss is 0.0003059971055247029\n",
      "Batch: 26500,train loss is: 0.00019770856946732624\n",
      "test loss is 0.0003073835341089313\n",
      "Batch: 26600,train loss is: 0.00022409613794879048\n",
      "test loss is 0.000325700889296921\n",
      "Batch: 26700,train loss is: 0.000328624162117756\n",
      "test loss is 0.0003256278254448491\n",
      "Batch: 26800,train loss is: 0.00037336370903409493\n",
      "test loss is 0.00030323071887668565\n",
      "Batch: 26900,train loss is: 0.00025673826869506756\n",
      "test loss is 0.00034000485255004443\n",
      "Batch: 27000,train loss is: 0.0003611780398717828\n",
      "test loss is 0.00033926752044833355\n",
      "Batch: 27100,train loss is: 0.0002977026529585216\n",
      "test loss is 0.00030835079041080326\n",
      "Batch: 27200,train loss is: 0.00024382057627170275\n",
      "test loss is 0.0003436011311374074\n",
      "Batch: 27300,train loss is: 0.0001860157913748679\n",
      "test loss is 0.0002984911581391981\n",
      "Batch: 27400,train loss is: 0.0003948158669723165\n",
      "test loss is 0.00033251171075916297\n",
      "Batch: 27500,train loss is: 0.0002526493751511979\n",
      "test loss is 0.00033561675543233974\n",
      "Batch: 27600,train loss is: 0.00032645204786871374\n",
      "test loss is 0.00031429030822958975\n",
      "Batch: 27700,train loss is: 0.0009002847280325309\n",
      "test loss is 0.00032542529677809907\n",
      "Batch: 27800,train loss is: 0.0002929403689707245\n",
      "test loss is 0.00037191097626236694\n",
      "Batch: 27900,train loss is: 0.00021369783391976532\n",
      "test loss is 0.00031405134489909255\n",
      "Batch: 28000,train loss is: 0.000262071185061808\n",
      "test loss is 0.0003047039349543106\n",
      "Batch: 28100,train loss is: 0.0002119838684849099\n",
      "test loss is 0.0003141754324785516\n",
      "Batch: 28200,train loss is: 0.00031166836994169086\n",
      "test loss is 0.00030859061949149925\n",
      "Batch: 28300,train loss is: 0.00035179113593812166\n",
      "test loss is 0.0003802804376501815\n",
      "Batch: 28400,train loss is: 0.00039446678518202996\n",
      "test loss is 0.0003028213045893236\n",
      "Batch: 28500,train loss is: 0.00025318756585316547\n",
      "test loss is 0.00033591180262442476\n",
      "Batch: 28600,train loss is: 0.000338624710807413\n",
      "test loss is 0.00029974109581764946\n",
      "Batch: 28700,train loss is: 0.0005187770304709348\n",
      "test loss is 0.0004114810045657769\n",
      "Batch: 28800,train loss is: 0.0003825425113765469\n",
      "test loss is 0.0003298478153417671\n",
      "Batch: 28900,train loss is: 0.00024243874271609637\n",
      "test loss is 0.0003087512380952041\n",
      "Batch: 29000,train loss is: 0.00023894627670297255\n",
      "test loss is 0.00030346327219408706\n",
      "Batch: 29100,train loss is: 0.00038340854249281305\n",
      "test loss is 0.0003075702226357281\n",
      "Batch: 29200,train loss is: 0.00020202293418765534\n",
      "test loss is 0.00032096951574194687\n",
      "Batch: 29300,train loss is: 0.000250020911146369\n",
      "test loss is 0.00031638225502643333\n",
      "Batch: 29400,train loss is: 0.00027287225090901846\n",
      "test loss is 0.0003352399996828067\n",
      "Batch: 29500,train loss is: 0.0002696228100204168\n",
      "test loss is 0.0003179934630770013\n",
      "Batch: 29600,train loss is: 0.0003077700253703564\n",
      "test loss is 0.0003710074691976964\n",
      "Batch: 29700,train loss is: 0.000221017703255954\n",
      "test loss is 0.00030871476723000406\n",
      "Batch: 29800,train loss is: 0.0005524855381806425\n",
      "test loss is 0.00032460288835869754\n",
      "Batch: 29900,train loss is: 0.000289524044867773\n",
      "test loss is 0.00031805603427302587\n",
      "Batch: 30000,train loss is: 0.0002876983596459813\n",
      "test loss is 0.0003336487052601097\n",
      "Batch: 30100,train loss is: 0.0003325888652763805\n",
      "test loss is 0.0002975812387669497\n",
      "Batch: 30200,train loss is: 0.0004506841335290064\n",
      "test loss is 0.000356805505913594\n",
      "Batch: 30300,train loss is: 0.0003829696134617144\n",
      "test loss is 0.0003202860496589484\n",
      "Batch: 30400,train loss is: 0.00033725286069795314\n",
      "test loss is 0.00028833222420241345\n",
      "Batch: 30500,train loss is: 0.0003068672437737158\n",
      "test loss is 0.00033775930641181517\n",
      "Batch: 30600,train loss is: 0.00035816985101982734\n",
      "test loss is 0.00037005393163706937\n",
      "Batch: 30700,train loss is: 0.00028540473071215475\n",
      "test loss is 0.00031845656127615717\n",
      "Batch: 30800,train loss is: 0.0002216712172359232\n",
      "test loss is 0.0003235199574703278\n",
      "Batch: 30900,train loss is: 0.0002630626659783922\n",
      "test loss is 0.00030482095344364286\n",
      "Batch: 31000,train loss is: 0.00039183059200954937\n",
      "test loss is 0.00032993006541640437\n",
      "Batch: 31100,train loss is: 0.0003464431936908405\n",
      "test loss is 0.0003643257983249197\n",
      "Batch: 31200,train loss is: 0.00034648666492845444\n",
      "test loss is 0.000335703969136407\n",
      "Batch: 31300,train loss is: 0.0005180287070906489\n",
      "test loss is 0.0004914342372363679\n",
      "Batch: 31400,train loss is: 0.00033319475998518035\n",
      "test loss is 0.00041032300998095\n",
      "Batch: 31500,train loss is: 0.0005529911462259429\n",
      "test loss is 0.00031289045227044133\n",
      "Batch: 31600,train loss is: 0.0002313282235863068\n",
      "test loss is 0.00030362676087150587\n",
      "Batch: 31700,train loss is: 0.00021066670019717312\n",
      "test loss is 0.00042518488580727036\n",
      "Batch: 31800,train loss is: 0.000316383265975311\n",
      "test loss is 0.0003396689179354703\n",
      "Batch: 31900,train loss is: 0.0002882497934734975\n",
      "test loss is 0.0003128663590943596\n",
      "Batch: 32000,train loss is: 0.000298604956203297\n",
      "test loss is 0.0003312494901482789\n",
      "Batch: 32100,train loss is: 0.0002795266260900912\n",
      "test loss is 0.00029759813261712127\n",
      "Batch: 32200,train loss is: 0.00029774235271758375\n",
      "test loss is 0.00044956119224091607\n",
      "Batch: 32300,train loss is: 0.000271029058431826\n",
      "test loss is 0.00031370137000829397\n",
      "Batch: 32400,train loss is: 0.00032502127864147246\n",
      "test loss is 0.0003520407400054272\n",
      "Batch: 32500,train loss is: 0.00032457752239537863\n",
      "test loss is 0.0003123921450530604\n",
      "Batch: 32600,train loss is: 0.0002154234966758224\n",
      "test loss is 0.00030531877850449167\n",
      "Batch: 32700,train loss is: 0.0003083275538405321\n",
      "test loss is 0.00031291519791464765\n",
      "Batch: 32800,train loss is: 0.0004255470957465366\n",
      "test loss is 0.00030795536907503616\n",
      "Batch: 32900,train loss is: 0.00034276432846819954\n",
      "test loss is 0.00037322150763864143\n",
      "Batch: 33000,train loss is: 0.0001997260990527564\n",
      "test loss is 0.0003047402983063254\n",
      "Batch: 33100,train loss is: 0.0003417255497038394\n",
      "test loss is 0.00041946678886415674\n",
      "Batch: 33200,train loss is: 0.00031270224056648057\n",
      "test loss is 0.00029444425469519846\n",
      "Batch: 33300,train loss is: 0.0002685048613796593\n",
      "test loss is 0.00032154983869799616\n",
      "Batch: 33400,train loss is: 0.0003191247138293429\n",
      "test loss is 0.00028261903092805706\n",
      "Batch: 33500,train loss is: 0.0006858433766599341\n",
      "test loss is 0.0003395652935663974\n",
      "Batch: 33600,train loss is: 0.0002667379521272368\n",
      "test loss is 0.000298409340393278\n",
      "Batch: 33700,train loss is: 0.00023012877428846217\n",
      "test loss is 0.00033198612627371256\n",
      "Batch: 33800,train loss is: 0.0003045485906734213\n",
      "test loss is 0.0003286283060806233\n",
      "Batch: 33900,train loss is: 0.00034884420795538616\n",
      "test loss is 0.00029555942924984355\n",
      "Batch: 34000,train loss is: 0.0002392157590641236\n",
      "test loss is 0.0003572985995177519\n",
      "Batch: 34100,train loss is: 0.0005301349670487637\n",
      "test loss is 0.00033880598006642707\n",
      "Batch: 34200,train loss is: 0.00022049600598466278\n",
      "test loss is 0.0003008012388615955\n",
      "Batch: 34300,train loss is: 0.0003859244893187333\n",
      "test loss is 0.00032173069476863764\n",
      "Batch: 34400,train loss is: 0.000341740335952225\n",
      "test loss is 0.00031631761207201437\n",
      "Batch: 34500,train loss is: 0.0004257180130183859\n",
      "test loss is 0.000297702891577902\n",
      "Batch: 34600,train loss is: 0.00028244062657700774\n",
      "test loss is 0.0003200214738541112\n",
      "Batch: 34700,train loss is: 0.0002998369696191183\n",
      "test loss is 0.0003264570747988749\n",
      "Batch: 34800,train loss is: 0.00033764121415095317\n",
      "test loss is 0.00033906650268712533\n",
      "Batch: 34900,train loss is: 0.0003808303398138905\n",
      "test loss is 0.0003125168492146627\n",
      "Batch: 35000,train loss is: 0.00033441788607680323\n",
      "test loss is 0.0003182971731850576\n",
      "Batch: 35100,train loss is: 0.00040739965278873525\n",
      "test loss is 0.0003001867527256729\n",
      "Batch: 35200,train loss is: 0.0002374686766203214\n",
      "test loss is 0.0003552422355983791\n",
      "Batch: 35300,train loss is: 0.0002776120972337981\n",
      "test loss is 0.00028933416455639106\n",
      "Batch: 35400,train loss is: 0.00043673045436827356\n",
      "test loss is 0.0002940990066210683\n",
      "Batch: 35500,train loss is: 0.00021922258955659913\n",
      "test loss is 0.0003198186238329256\n",
      "Batch: 35600,train loss is: 0.00046834661846473897\n",
      "test loss is 0.0003352484549520528\n",
      "Batch: 35700,train loss is: 0.00026315955160109136\n",
      "test loss is 0.00036250050251157703\n",
      "Batch: 35800,train loss is: 0.0003477592423115917\n",
      "test loss is 0.00029626946102927947\n",
      "Batch: 35900,train loss is: 0.0003666438975660299\n",
      "test loss is 0.00035479305273035215\n",
      "Batch: 36000,train loss is: 9.878792221575838e-05\n",
      "test loss is 0.00031285896426483633\n",
      "Batch: 36100,train loss is: 0.0002773003088671936\n",
      "test loss is 0.0003621213560946876\n",
      "Batch: 36200,train loss is: 0.00029409754454096495\n",
      "test loss is 0.00031414622639155124\n",
      "Batch: 36300,train loss is: 0.00032235890623495754\n",
      "test loss is 0.00029093222406731384\n",
      "Batch: 36400,train loss is: 0.000319905887306652\n",
      "test loss is 0.00029556824733325155\n",
      "Batch: 36500,train loss is: 0.00027666933204682916\n",
      "test loss is 0.0003403622898058336\n",
      "Batch: 36600,train loss is: 0.00029994417936801004\n",
      "test loss is 0.0003747713489862609\n",
      "Batch: 36700,train loss is: 0.0004402423225323598\n",
      "test loss is 0.00033999774987934225\n",
      "Batch: 36800,train loss is: 0.0003136685774791252\n",
      "test loss is 0.00033606645183787255\n",
      "Batch: 36900,train loss is: 0.0004307441432021731\n",
      "test loss is 0.0003845277304134171\n",
      "Batch: 37000,train loss is: 0.00037605673709572966\n",
      "test loss is 0.00030814823565457\n",
      "Batch: 37100,train loss is: 0.0003950144564721955\n",
      "test loss is 0.0003061574850092142\n",
      "Batch: 37200,train loss is: 0.0002743559427784081\n",
      "test loss is 0.0002961547542629542\n",
      "Batch: 37300,train loss is: 0.00044989368239541755\n",
      "test loss is 0.00031759734702965585\n",
      "Batch: 37400,train loss is: 0.00029544923547995935\n",
      "test loss is 0.00030013669212282157\n",
      "Batch: 37500,train loss is: 0.00022386081979563436\n",
      "test loss is 0.00033667261356807557\n",
      "Batch: 37600,train loss is: 0.0002333896494737534\n",
      "test loss is 0.00037696590432104626\n",
      "Batch: 37700,train loss is: 0.0008754670968362607\n",
      "test loss is 0.0003797119507177188\n",
      "Batch: 37800,train loss is: 0.0003185570081749505\n",
      "test loss is 0.00030532110849467814\n",
      "Batch: 37900,train loss is: 0.0002528442004144027\n",
      "test loss is 0.00035002282946212395\n",
      "Batch: 38000,train loss is: 0.0004280073619928206\n",
      "test loss is 0.00036430578341869384\n",
      "Batch: 38100,train loss is: 0.00036027444364703485\n",
      "test loss is 0.00034406647961892825\n",
      "Batch: 38200,train loss is: 0.000311116212147617\n",
      "test loss is 0.0003817142057259584\n",
      "Batch: 38300,train loss is: 0.00019212221946226794\n",
      "test loss is 0.00030718093572847506\n",
      "Batch: 38400,train loss is: 0.00025630900943831485\n",
      "test loss is 0.0003281763406068191\n",
      "Batch: 38500,train loss is: 0.0003870438614421165\n",
      "test loss is 0.0003381765183695195\n",
      "Batch: 38600,train loss is: 0.0004504837340757351\n",
      "test loss is 0.000308726211523852\n",
      "Batch: 38700,train loss is: 0.00043893188244567875\n",
      "test loss is 0.0003222018265842929\n",
      "Batch: 38800,train loss is: 0.0002556230430490493\n",
      "test loss is 0.00034002473323452894\n",
      "Batch: 38900,train loss is: 0.0003252382075964686\n",
      "test loss is 0.00030931300620075136\n",
      "Batch: 39000,train loss is: 0.00042820569127268604\n",
      "test loss is 0.0005275027386880499\n",
      "Batch: 39100,train loss is: 0.00026069722652885655\n",
      "test loss is 0.00035305210004292926\n",
      "Batch: 39200,train loss is: 0.0003099353447924585\n",
      "test loss is 0.00029947326448624516\n",
      "Batch: 39300,train loss is: 0.00036697350015962296\n",
      "test loss is 0.0003371143389908552\n",
      "Batch: 39400,train loss is: 0.0002635614087389575\n",
      "test loss is 0.0003221714003391145\n",
      "Batch: 39500,train loss is: 0.0003626152717375067\n",
      "test loss is 0.0002872882343303372\n",
      "Batch: 39600,train loss is: 0.00022241794919101545\n",
      "test loss is 0.0003049586899476354\n",
      "Batch: 39700,train loss is: 0.00025428995932373253\n",
      "test loss is 0.00030076425872064557\n",
      "Batch: 39800,train loss is: 0.0003377342425283726\n",
      "test loss is 0.0003206424760683284\n",
      "Batch: 39900,train loss is: 0.00029733954204633087\n",
      "test loss is 0.0003208020946805734\n",
      "Batch: 40000,train loss is: 0.0001824941921500396\n",
      "test loss is 0.00032466066950279034\n",
      "Batch: 40100,train loss is: 0.0002056317080195241\n",
      "test loss is 0.0003279603659175292\n",
      "Batch: 40200,train loss is: 0.0002578851870159448\n",
      "test loss is 0.00030232167854742477\n",
      "Batch: 40300,train loss is: 0.0003250441446025385\n",
      "test loss is 0.0003210750831957121\n",
      "Batch: 40400,train loss is: 0.00028345866508714746\n",
      "test loss is 0.0003461322774740687\n",
      "Batch: 40500,train loss is: 0.00019878633980363283\n",
      "test loss is 0.00032077320793667466\n",
      "Batch: 40600,train loss is: 0.00035832483403044034\n",
      "test loss is 0.0003335172184823244\n",
      "Batch: 40700,train loss is: 0.0001734023442063279\n",
      "test loss is 0.00037488249905848957\n",
      "Batch: 40800,train loss is: 0.0002603334887106356\n",
      "test loss is 0.0002811718841828069\n",
      "Batch: 40900,train loss is: 0.0002849023889148626\n",
      "test loss is 0.0003339035287135268\n",
      "Batch: 41000,train loss is: 0.00023263199212367956\n",
      "test loss is 0.00033756887144709196\n",
      "Batch: 41100,train loss is: 0.00028693421511005594\n",
      "test loss is 0.0003386359235658625\n",
      "Batch: 41200,train loss is: 0.0003832045069930081\n",
      "test loss is 0.00031095421072266626\n",
      "Batch: 41300,train loss is: 0.000472957693974216\n",
      "test loss is 0.00035382017095637473\n",
      "Batch: 41400,train loss is: 0.00028910429904614287\n",
      "test loss is 0.0003349694330106663\n",
      "Batch: 41500,train loss is: 0.00021046640728018847\n",
      "test loss is 0.0002906245865470612\n",
      "Batch: 41600,train loss is: 0.0002265874517444854\n",
      "test loss is 0.00033491040982142436\n",
      "Batch: 41700,train loss is: 0.0002890144284273861\n",
      "test loss is 0.0003413397836636815\n",
      "Batch: 41800,train loss is: 0.0002689395114119789\n",
      "test loss is 0.00031618216497209705\n",
      "Batch: 41900,train loss is: 0.00022448529474678038\n",
      "test loss is 0.00034261405692599564\n",
      "Batch: 42000,train loss is: 0.0002495336853866578\n",
      "test loss is 0.00029827876164140724\n",
      "Batch: 42100,train loss is: 0.00024784199302032564\n",
      "test loss is 0.00028458089193078876\n",
      "Batch: 42200,train loss is: 0.00030354013107957963\n",
      "test loss is 0.0002887560940446749\n",
      "Batch: 42300,train loss is: 0.00023247326631279302\n",
      "test loss is 0.00032988034939859557\n",
      "Batch: 42400,train loss is: 0.0001866436804376052\n",
      "test loss is 0.00029870541563991693\n",
      "Batch: 42500,train loss is: 0.00038286384275329147\n",
      "test loss is 0.0004009917135138756\n",
      "Batch: 42600,train loss is: 0.0002743792358655897\n",
      "test loss is 0.0003102896069417421\n",
      "Batch: 42700,train loss is: 0.000194817647591367\n",
      "test loss is 0.0003328414433488496\n",
      "Batch: 42800,train loss is: 0.00024210397869942654\n",
      "test loss is 0.00028668227124811166\n",
      "Batch: 42900,train loss is: 0.00042195681967550723\n",
      "test loss is 0.000296661707794987\n",
      "Batch: 43000,train loss is: 0.0003581312910115377\n",
      "test loss is 0.00032095210577755083\n",
      "Batch: 43100,train loss is: 0.0002722673929734268\n",
      "test loss is 0.0003506980688289036\n",
      "Batch: 43200,train loss is: 0.00038965364849215657\n",
      "test loss is 0.00031220550406551564\n",
      "Batch: 43300,train loss is: 0.00028162143060660454\n",
      "test loss is 0.00031917109903786974\n",
      "Batch: 43400,train loss is: 0.00022968621124722747\n",
      "test loss is 0.00029727719454617396\n",
      "Batch: 43500,train loss is: 0.00019966016024771562\n",
      "test loss is 0.0003263804614143797\n",
      "Batch: 43600,train loss is: 0.00027598350022862126\n",
      "test loss is 0.00032470382224522685\n",
      "Batch: 43700,train loss is: 0.0003353807631646064\n",
      "test loss is 0.0003008283300160074\n",
      "Batch: 43800,train loss is: 0.0003308864141121612\n",
      "test loss is 0.00033501831499717236\n",
      "Batch: 43900,train loss is: 0.00027897001768647535\n",
      "test loss is 0.00033499940057680734\n",
      "Batch: 44000,train loss is: 0.0002681106125504468\n",
      "test loss is 0.00033494023425278415\n",
      "Batch: 44100,train loss is: 0.00020980401321934612\n",
      "test loss is 0.0003167723685707184\n",
      "Batch: 44200,train loss is: 0.00023980616716897166\n",
      "test loss is 0.0002971073505571708\n",
      "Batch: 44300,train loss is: 0.00030933274819540683\n",
      "test loss is 0.0003073267851321495\n",
      "Batch: 44400,train loss is: 0.0003232504334601676\n",
      "test loss is 0.0002849199722180429\n",
      "Batch: 44500,train loss is: 0.00021368059494729797\n",
      "test loss is 0.0003378499419516078\n",
      "Batch: 44600,train loss is: 0.00017653458044422356\n",
      "test loss is 0.0003011752717392333\n",
      "Batch: 44700,train loss is: 0.00032018762994304975\n",
      "test loss is 0.0003316031043337485\n",
      "Batch: 44800,train loss is: 0.0001691378507951256\n",
      "test loss is 0.0002924248915954585\n",
      "Batch: 44900,train loss is: 0.0003492897379249394\n",
      "test loss is 0.00031347439184620414\n",
      "Batch: 45000,train loss is: 0.0003092487210040398\n",
      "test loss is 0.0003344197462989042\n",
      "Batch: 45100,train loss is: 0.0001898252288884785\n",
      "test loss is 0.0003029501338726081\n",
      "Batch: 45200,train loss is: 0.00030312959327941167\n",
      "test loss is 0.0003023333746056379\n",
      "Batch: 45300,train loss is: 0.0003474443787815285\n",
      "test loss is 0.0003143995578768606\n",
      "Batch: 45400,train loss is: 0.00020931050239660738\n",
      "test loss is 0.0003063962526307671\n",
      "Batch: 45500,train loss is: 0.00033585625652340293\n",
      "test loss is 0.0003159235299877827\n",
      "Batch: 45600,train loss is: 0.00038014400551861785\n",
      "test loss is 0.00033562138040474295\n",
      "Batch: 45700,train loss is: 0.000227065761525219\n",
      "test loss is 0.00031126689255997166\n",
      "Batch: 45800,train loss is: 0.00019083114968496186\n",
      "test loss is 0.00030660148126332627\n",
      "Batch: 45900,train loss is: 0.00021060131055078058\n",
      "test loss is 0.0003162687505643946\n",
      "Batch: 46000,train loss is: 0.00018373237067696485\n",
      "test loss is 0.00030571431570802164\n",
      "Batch: 46100,train loss is: 0.0002622474915431926\n",
      "test loss is 0.0003730771233993389\n",
      "Batch: 46200,train loss is: 0.00027237299188425843\n",
      "test loss is 0.00032243957035335216\n",
      "Batch: 46300,train loss is: 0.00042349749924572457\n",
      "test loss is 0.0003114883603555281\n",
      "Batch: 46400,train loss is: 0.0002501798529881992\n",
      "test loss is 0.00031866769547517893\n",
      "Batch: 46500,train loss is: 0.00033341471400218407\n",
      "test loss is 0.00040094096877967304\n",
      "Batch: 46600,train loss is: 0.00026128084236295935\n",
      "test loss is 0.0003424678376341211\n",
      "Batch: 46700,train loss is: 0.00018331914939669896\n",
      "test loss is 0.00030306516904500975\n",
      "-----------------------Epoch: 2----------------------------------\n",
      "Batch: 0,train loss is: 0.0004711663895736708\n",
      "test loss is 0.0002986358541968676\n",
      "Batch: 100,train loss is: 0.00020343175014609177\n",
      "test loss is 0.00036957351954532424\n",
      "Batch: 200,train loss is: 0.0002486173313268851\n",
      "test loss is 0.00032469848542826855\n",
      "Batch: 300,train loss is: 0.00026231573000046204\n",
      "test loss is 0.0002969245621361393\n",
      "Batch: 400,train loss is: 0.0002730110079138282\n",
      "test loss is 0.00032082790373625357\n",
      "Batch: 500,train loss is: 0.00022303965785642663\n",
      "test loss is 0.000340940520004062\n",
      "Batch: 600,train loss is: 0.00021279533720456725\n",
      "test loss is 0.00032905480605429006\n",
      "Batch: 700,train loss is: 0.00036819636459671837\n",
      "test loss is 0.0003117945990829881\n",
      "Batch: 800,train loss is: 0.0005082383758526498\n",
      "test loss is 0.0003461584515999914\n",
      "Batch: 900,train loss is: 0.0002471503442902739\n",
      "test loss is 0.00029098537040581676\n",
      "Batch: 1000,train loss is: 0.00033526000817365634\n",
      "test loss is 0.00030235650943199636\n",
      "Batch: 1100,train loss is: 0.0005228419725717582\n",
      "test loss is 0.0003054343609619039\n",
      "Batch: 1200,train loss is: 0.00019448659780293422\n",
      "test loss is 0.0003040875956076829\n",
      "Batch: 1300,train loss is: 0.00022194387055516262\n",
      "test loss is 0.0003013014340129088\n",
      "Batch: 1400,train loss is: 0.0003478204062479682\n",
      "test loss is 0.0002998911111075063\n",
      "Batch: 1500,train loss is: 0.00022527039410294704\n",
      "test loss is 0.0003234479185466539\n",
      "Batch: 1600,train loss is: 0.00031412042654356445\n",
      "test loss is 0.00032304809376906916\n",
      "Batch: 1700,train loss is: 0.0004833565770209464\n",
      "test loss is 0.0003004880392730092\n",
      "Batch: 1800,train loss is: 0.00018395158859634735\n",
      "test loss is 0.0003882746466234609\n",
      "Batch: 1900,train loss is: 0.0002606122009617387\n",
      "test loss is 0.0003372887075900564\n",
      "Batch: 2000,train loss is: 0.00021713537026274993\n",
      "test loss is 0.00032142306622077636\n",
      "Batch: 2100,train loss is: 0.0004217305422804423\n",
      "test loss is 0.0003009524493997766\n",
      "Batch: 2200,train loss is: 0.00044124126162043017\n",
      "test loss is 0.0005398874158062274\n",
      "Batch: 2300,train loss is: 0.0003091782438657106\n",
      "test loss is 0.00029011384281304103\n",
      "Batch: 2400,train loss is: 0.0004538336676575195\n",
      "test loss is 0.00033856554812087663\n",
      "Batch: 2500,train loss is: 0.00037326817865515887\n",
      "test loss is 0.0003860421851946852\n",
      "Batch: 2600,train loss is: 0.00019994277429493404\n",
      "test loss is 0.00029869758073213594\n",
      "Batch: 2700,train loss is: 0.00030699934717011554\n",
      "test loss is 0.00032840093280448466\n",
      "Batch: 2800,train loss is: 0.00028515850482853085\n",
      "test loss is 0.0002967029313388484\n",
      "Batch: 2900,train loss is: 0.00030077111811955767\n",
      "test loss is 0.0002772388822773432\n",
      "Batch: 3000,train loss is: 0.0003456575349658937\n",
      "test loss is 0.0003202708691575126\n",
      "Batch: 3100,train loss is: 0.00030732525455210536\n",
      "test loss is 0.0003385655004424272\n",
      "Batch: 3200,train loss is: 0.0002433499304650612\n",
      "test loss is 0.00029411611319037\n",
      "Batch: 3300,train loss is: 0.00026614478626038466\n",
      "test loss is 0.0002926469338434532\n",
      "Batch: 3400,train loss is: 0.00022545671245690287\n",
      "test loss is 0.00032963387899346896\n",
      "Batch: 3500,train loss is: 0.00024188421962874148\n",
      "test loss is 0.0003097505533740041\n",
      "Batch: 3600,train loss is: 0.0002726939444473189\n",
      "test loss is 0.0003689559824694784\n",
      "Batch: 3700,train loss is: 0.0003130346397649044\n",
      "test loss is 0.00030839411871346996\n",
      "Batch: 3800,train loss is: 0.0004358365397104266\n",
      "test loss is 0.0003565004705629617\n",
      "Batch: 3900,train loss is: 0.00025472347000397356\n",
      "test loss is 0.00036988019215904665\n",
      "Batch: 4000,train loss is: 0.0002671081400458283\n",
      "test loss is 0.00028945602657532235\n",
      "Batch: 4100,train loss is: 0.0002644611584359447\n",
      "test loss is 0.00028548440859115664\n",
      "Batch: 4200,train loss is: 0.00041059447092274147\n",
      "test loss is 0.00035808076219451864\n",
      "Batch: 4300,train loss is: 0.00017999019006336998\n",
      "test loss is 0.00032677809788673345\n",
      "Batch: 4400,train loss is: 0.00031381290180629344\n",
      "test loss is 0.0003713305077617996\n",
      "Batch: 4500,train loss is: 0.00029618453202438734\n",
      "test loss is 0.0003638373263878894\n",
      "Batch: 4600,train loss is: 0.00042320879681943986\n",
      "test loss is 0.00029997437952175403\n",
      "Batch: 4700,train loss is: 0.000395748128513419\n",
      "test loss is 0.00036785821965774597\n",
      "Batch: 4800,train loss is: 0.00028043016721423803\n",
      "test loss is 0.0003120014161586591\n",
      "Batch: 4900,train loss is: 0.00034818134084791503\n",
      "test loss is 0.0002887359930287\n",
      "Batch: 5000,train loss is: 0.0002861206059629842\n",
      "test loss is 0.0003364571772899069\n",
      "Batch: 5100,train loss is: 0.0003042289234136985\n",
      "test loss is 0.0003806548494456205\n",
      "Batch: 5200,train loss is: 0.000184308890149354\n",
      "test loss is 0.00030907553834378786\n",
      "Batch: 5300,train loss is: 0.00014548805071607816\n",
      "test loss is 0.00028377148631008513\n",
      "Batch: 5400,train loss is: 0.0002310560579385674\n",
      "test loss is 0.0003000294239754897\n",
      "Batch: 5500,train loss is: 0.00034617857111780864\n",
      "test loss is 0.00032077072814247086\n",
      "Batch: 5600,train loss is: 0.00028721653106311777\n",
      "test loss is 0.0003126568498043682\n",
      "Batch: 5700,train loss is: 0.00021378313893971953\n",
      "test loss is 0.0003291357964551383\n",
      "Batch: 5800,train loss is: 0.0003322679286445337\n",
      "test loss is 0.000312236858575755\n",
      "Batch: 5900,train loss is: 0.00023407722957415102\n",
      "test loss is 0.00032573434608199487\n",
      "Batch: 6000,train loss is: 0.00024140121541089922\n",
      "test loss is 0.0003145837125873731\n",
      "Batch: 6100,train loss is: 0.00029532030946105147\n",
      "test loss is 0.0003826620715571387\n",
      "Batch: 6200,train loss is: 0.0006274302526955147\n",
      "test loss is 0.000310303951628161\n",
      "Batch: 6300,train loss is: 0.0003548030849357043\n",
      "test loss is 0.00028496136302084025\n",
      "Batch: 6400,train loss is: 0.00024387128342628808\n",
      "test loss is 0.0003353436195874048\n",
      "Batch: 6500,train loss is: 0.0003271521304285399\n",
      "test loss is 0.0003638952455273862\n",
      "Batch: 6600,train loss is: 0.00037789590002881184\n",
      "test loss is 0.0003228904010819858\n",
      "Batch: 6700,train loss is: 0.0003793983735058341\n",
      "test loss is 0.0003168653917782987\n",
      "Batch: 6800,train loss is: 0.0003069250514150646\n",
      "test loss is 0.00030427966550620507\n",
      "Batch: 6900,train loss is: 0.00032605093653853594\n",
      "test loss is 0.0003138030303151443\n",
      "Batch: 7000,train loss is: 0.00026681007778844737\n",
      "test loss is 0.00029892178325530014\n",
      "Batch: 7100,train loss is: 0.0003374705633834772\n",
      "test loss is 0.0003118075260541197\n",
      "Batch: 7200,train loss is: 0.00034735502080542985\n",
      "test loss is 0.00041678421035320633\n",
      "Batch: 7300,train loss is: 0.0002009799633285477\n",
      "test loss is 0.0003513978767080611\n",
      "Batch: 7400,train loss is: 0.0003055287371020582\n",
      "test loss is 0.00030668112791458447\n",
      "Batch: 7500,train loss is: 0.00024613459692740124\n",
      "test loss is 0.0003114300677600412\n",
      "Batch: 7600,train loss is: 0.0003281268823883258\n",
      "test loss is 0.000292704928200846\n",
      "Batch: 7700,train loss is: 0.0001891328376627342\n",
      "test loss is 0.0003530761618272193\n",
      "Batch: 7800,train loss is: 0.00035926397818011977\n",
      "test loss is 0.000299266918548329\n",
      "Batch: 7900,train loss is: 0.00029154583985882933\n",
      "test loss is 0.0003269011123177957\n",
      "Batch: 8000,train loss is: 0.0004286518608485777\n",
      "test loss is 0.0003349599848278129\n",
      "Batch: 8100,train loss is: 0.00020918313640106993\n",
      "test loss is 0.00029827203766709127\n",
      "Batch: 8200,train loss is: 0.0003187996206305113\n",
      "test loss is 0.00029318520073878595\n",
      "Batch: 8300,train loss is: 0.0002389623643894396\n",
      "test loss is 0.0003112620702117842\n",
      "Batch: 8400,train loss is: 0.00016535567894101921\n",
      "test loss is 0.0003291459380163703\n",
      "Batch: 8500,train loss is: 0.00037854825104881686\n",
      "test loss is 0.0002904383163703338\n",
      "Batch: 8600,train loss is: 0.0002561444650762175\n",
      "test loss is 0.00030714015979837366\n",
      "Batch: 8700,train loss is: 0.0003098398197068532\n",
      "test loss is 0.0003465387860635884\n",
      "Batch: 8800,train loss is: 0.00021139400726367653\n",
      "test loss is 0.0002984015719677135\n",
      "Batch: 8900,train loss is: 0.0002804507877271076\n",
      "test loss is 0.0003168231939188485\n",
      "Batch: 9000,train loss is: 0.00035846698463307006\n",
      "test loss is 0.000343022356566383\n",
      "Batch: 9100,train loss is: 0.00028070948215994606\n",
      "test loss is 0.0003630374625585343\n",
      "Batch: 9200,train loss is: 0.0003332152930124433\n",
      "test loss is 0.0003216710773158078\n",
      "Batch: 9300,train loss is: 0.00022499139208350224\n",
      "test loss is 0.0003067985180269293\n",
      "Batch: 9400,train loss is: 0.00038586849962685046\n",
      "test loss is 0.0003169274732013423\n",
      "Batch: 9500,train loss is: 0.00022677105923151668\n",
      "test loss is 0.0003088249168578851\n",
      "Batch: 9600,train loss is: 0.00032314371755202693\n",
      "test loss is 0.0002996143803861041\n",
      "Batch: 9700,train loss is: 0.0002673844710553271\n",
      "test loss is 0.0003441209903920093\n",
      "Batch: 9800,train loss is: 0.00026990273234598277\n",
      "test loss is 0.00030535034363423605\n",
      "Batch: 9900,train loss is: 0.0002812476383083555\n",
      "test loss is 0.00029189700742877526\n",
      "Batch: 10000,train loss is: 0.00034194167897025053\n",
      "test loss is 0.00030525260351291575\n",
      "Batch: 10100,train loss is: 0.00016812669982603146\n",
      "test loss is 0.0002892998823361871\n",
      "Batch: 10200,train loss is: 0.00022420511907467828\n",
      "test loss is 0.0003165016622468129\n",
      "Batch: 10300,train loss is: 0.00041738918592808575\n",
      "test loss is 0.0002829977492606004\n",
      "Batch: 10400,train loss is: 0.0003729404180802318\n",
      "test loss is 0.00030242962886768736\n",
      "Batch: 10500,train loss is: 0.00029010540852097717\n",
      "test loss is 0.0002927603841469828\n",
      "Batch: 10600,train loss is: 0.0003936105043724992\n",
      "test loss is 0.00029283557327178445\n",
      "Batch: 10700,train loss is: 0.00029196549923616726\n",
      "test loss is 0.0003250612112779561\n",
      "Batch: 10800,train loss is: 0.0004686812097142146\n",
      "test loss is 0.00033871786575518807\n",
      "Batch: 10900,train loss is: 0.0005478384260815385\n",
      "test loss is 0.0003315981337251553\n",
      "Batch: 11000,train loss is: 0.00030828965016988316\n",
      "test loss is 0.0003400668371367501\n",
      "Batch: 11100,train loss is: 0.00026395092871193374\n",
      "test loss is 0.0003134138288771705\n",
      "Batch: 11200,train loss is: 0.00034520767131467317\n",
      "test loss is 0.0003266338103375627\n",
      "Batch: 11300,train loss is: 0.0002053802678813673\n",
      "test loss is 0.0002966964059929242\n",
      "Batch: 11400,train loss is: 0.0003396130222803333\n",
      "test loss is 0.00033083528551037324\n",
      "Batch: 11500,train loss is: 0.0003329642963295482\n",
      "test loss is 0.00030691903376265484\n",
      "Batch: 11600,train loss is: 0.0003148536745051951\n",
      "test loss is 0.0003717385608852789\n",
      "Batch: 11700,train loss is: 0.00034674855069724727\n",
      "test loss is 0.0003514896898921815\n",
      "Batch: 11800,train loss is: 0.00022894424605161755\n",
      "test loss is 0.00028834248372375823\n",
      "Batch: 11900,train loss is: 0.00042790584730603546\n",
      "test loss is 0.0003271061198529434\n",
      "Batch: 12000,train loss is: 0.0006876025966030025\n",
      "test loss is 0.00030720339186684565\n",
      "Batch: 12100,train loss is: 0.00027964050275740455\n",
      "test loss is 0.0003480216835433051\n",
      "Batch: 12200,train loss is: 0.00023068374829516279\n",
      "test loss is 0.0003026521108419077\n",
      "Batch: 12300,train loss is: 0.0011434713276496705\n",
      "test loss is 0.00030459939728173514\n",
      "Batch: 12400,train loss is: 0.0001981773165679365\n",
      "test loss is 0.00035382367364530145\n",
      "Batch: 12500,train loss is: 0.0003485338493776023\n",
      "test loss is 0.0003686519790639325\n",
      "Batch: 12600,train loss is: 0.00036892414464930964\n",
      "test loss is 0.0003261294602920793\n",
      "Batch: 12700,train loss is: 0.00035474323826301243\n",
      "test loss is 0.00032052737448275695\n",
      "Batch: 12800,train loss is: 0.0003183685216294205\n",
      "test loss is 0.0002997896243796204\n",
      "Batch: 12900,train loss is: 0.00026184964752080887\n",
      "test loss is 0.0003006547347207941\n",
      "Batch: 13000,train loss is: 0.0002854134368262901\n",
      "test loss is 0.0003490078614292025\n",
      "Batch: 13100,train loss is: 0.000263070402397024\n",
      "test loss is 0.00030127114214591834\n",
      "Batch: 13200,train loss is: 0.00037380921156245106\n",
      "test loss is 0.0002985329251051417\n",
      "Batch: 13300,train loss is: 0.0002600539577963724\n",
      "test loss is 0.00033359750413048804\n",
      "Batch: 13400,train loss is: 0.00023574559539683675\n",
      "test loss is 0.0002973115473353037\n",
      "Batch: 13500,train loss is: 0.00033256742688622285\n",
      "test loss is 0.0003028271403750247\n",
      "Batch: 13600,train loss is: 0.0005279222677375777\n",
      "test loss is 0.0003225995060865614\n",
      "Batch: 13700,train loss is: 0.0002618835122196602\n",
      "test loss is 0.00031955578330905153\n",
      "Batch: 13800,train loss is: 0.0002921994022906107\n",
      "test loss is 0.00030081202527093727\n",
      "Batch: 13900,train loss is: 0.0002178138523462714\n",
      "test loss is 0.00030909528835506553\n",
      "Batch: 14000,train loss is: 0.00027326055914763693\n",
      "test loss is 0.0003411515170205521\n",
      "Batch: 14100,train loss is: 0.00023756268978170194\n",
      "test loss is 0.00032153844853392973\n",
      "Batch: 14200,train loss is: 0.00029376868320469163\n",
      "test loss is 0.00034265103570407454\n",
      "Batch: 14300,train loss is: 0.0001859037818301514\n",
      "test loss is 0.00031284729796107497\n",
      "Batch: 14400,train loss is: 0.00031689331933588147\n",
      "test loss is 0.0003226133605461432\n",
      "Batch: 14500,train loss is: 0.0002259345266688614\n",
      "test loss is 0.00030127894974419804\n",
      "Batch: 14600,train loss is: 0.00022555352366868667\n",
      "test loss is 0.00029541147491035885\n",
      "Batch: 14700,train loss is: 0.00033059079304142785\n",
      "test loss is 0.0003218295671707626\n",
      "Batch: 14800,train loss is: 0.0004252064199914799\n",
      "test loss is 0.00038129532771937776\n",
      "Batch: 14900,train loss is: 0.00035448586638163037\n",
      "test loss is 0.00031364491324362023\n",
      "Batch: 15000,train loss is: 0.0001833028986522417\n",
      "test loss is 0.0003385120237153824\n",
      "Batch: 15100,train loss is: 0.00040418562795394226\n",
      "test loss is 0.0002916435221705928\n",
      "Batch: 15200,train loss is: 0.0004400932791662662\n",
      "test loss is 0.000306681796974804\n",
      "Batch: 15300,train loss is: 0.0006754684153931849\n",
      "test loss is 0.0003741007565861044\n",
      "Batch: 15400,train loss is: 0.00039751801759550814\n",
      "test loss is 0.0002963535983609689\n",
      "Batch: 15500,train loss is: 0.0002681830132156422\n",
      "test loss is 0.0002902857702367926\n",
      "Batch: 15600,train loss is: 0.0005047113552020387\n",
      "test loss is 0.0004653454009158674\n",
      "Batch: 15700,train loss is: 0.0002558748638136826\n",
      "test loss is 0.0002900987546835373\n",
      "Batch: 15800,train loss is: 0.0002945558917156138\n",
      "test loss is 0.0003846622977493887\n",
      "Batch: 15900,train loss is: 0.0002862873495627727\n",
      "test loss is 0.00036858537851378173\n",
      "Batch: 16000,train loss is: 0.0004013955359073445\n",
      "test loss is 0.00037103664087994903\n",
      "Batch: 16100,train loss is: 0.00028977355466253845\n",
      "test loss is 0.0002910496163316863\n",
      "Batch: 16200,train loss is: 0.00023604676317271617\n",
      "test loss is 0.0003219839326992176\n",
      "Batch: 16300,train loss is: 0.0003874829675653623\n",
      "test loss is 0.00034034064593239385\n",
      "Batch: 16400,train loss is: 0.00029433663345557626\n",
      "test loss is 0.0003034371097048684\n",
      "Batch: 16500,train loss is: 0.0004173416546474637\n",
      "test loss is 0.0004992748505443094\n",
      "Batch: 16600,train loss is: 0.00041655472473273766\n",
      "test loss is 0.0003213474537388582\n",
      "Batch: 16700,train loss is: 0.0002251685807089914\n",
      "test loss is 0.0003152369314055775\n",
      "Batch: 16800,train loss is: 0.0006496125356868917\n",
      "test loss is 0.0003003226486738338\n",
      "Batch: 16900,train loss is: 0.000368925302067694\n",
      "test loss is 0.0003205759614652984\n",
      "Batch: 17000,train loss is: 0.0002240226198269515\n",
      "test loss is 0.00032889198721383953\n",
      "Batch: 17100,train loss is: 0.00019644090492485855\n",
      "test loss is 0.00033925098495249595\n",
      "Batch: 17200,train loss is: 0.0002294614417957871\n",
      "test loss is 0.00032370043980461613\n",
      "Batch: 17300,train loss is: 0.00037425813936730945\n",
      "test loss is 0.0003316425854606944\n",
      "Batch: 17400,train loss is: 0.00036521398883660247\n",
      "test loss is 0.000302162980592766\n",
      "Batch: 17500,train loss is: 0.00018478856131891812\n",
      "test loss is 0.0003166930711764331\n",
      "Batch: 17600,train loss is: 0.00020598583237499055\n",
      "test loss is 0.0003095374492272609\n",
      "Batch: 17700,train loss is: 0.0004914067251212437\n",
      "test loss is 0.0003697026922047573\n",
      "Batch: 17800,train loss is: 0.00023506201400400105\n",
      "test loss is 0.00029864106558125016\n",
      "Batch: 17900,train loss is: 0.0004444095466943067\n",
      "test loss is 0.00029754525225330773\n",
      "Batch: 18000,train loss is: 0.0002742897831054098\n",
      "test loss is 0.0003459614058304149\n",
      "Batch: 18100,train loss is: 0.0002013440161305439\n",
      "test loss is 0.00028026322914613506\n",
      "Batch: 18200,train loss is: 0.0003275848739817482\n",
      "test loss is 0.0003164571518992982\n",
      "Batch: 18300,train loss is: 0.00021545776943124976\n",
      "test loss is 0.000332099057049981\n",
      "Batch: 18400,train loss is: 0.00020098978728691117\n",
      "test loss is 0.00029351834725459725\n",
      "Batch: 18500,train loss is: 0.0002481392830964161\n",
      "test loss is 0.00029984750813294344\n",
      "Batch: 18600,train loss is: 0.0004489492736151306\n",
      "test loss is 0.000387272691001363\n",
      "Batch: 18700,train loss is: 0.00043070970522956093\n",
      "test loss is 0.00036732993163124053\n",
      "Batch: 18800,train loss is: 0.0002686102617752695\n",
      "test loss is 0.0003200494448352628\n",
      "Batch: 18900,train loss is: 0.00038963274845606514\n",
      "test loss is 0.0003071642556301832\n",
      "Batch: 19000,train loss is: 0.00027818376700561526\n",
      "test loss is 0.00030438247733554616\n",
      "Batch: 19100,train loss is: 0.0004942105459722173\n",
      "test loss is 0.00032408241059377264\n",
      "Batch: 19200,train loss is: 0.0002995253369433835\n",
      "test loss is 0.0003428394003235022\n",
      "Batch: 19300,train loss is: 0.00025754362132474295\n",
      "test loss is 0.0003048196088090268\n",
      "Batch: 19400,train loss is: 0.00031025156173096236\n",
      "test loss is 0.0002921178637670667\n",
      "Batch: 19500,train loss is: 0.00032732921784524457\n",
      "test loss is 0.00029314494621893714\n",
      "Batch: 19600,train loss is: 0.0002534427501984307\n",
      "test loss is 0.00031359154437700344\n",
      "Batch: 19700,train loss is: 0.00021417873900615478\n",
      "test loss is 0.00031977178940364195\n",
      "Batch: 19800,train loss is: 0.00017349770410042127\n",
      "test loss is 0.0002990098948730796\n",
      "Batch: 19900,train loss is: 0.0003822738563816166\n",
      "test loss is 0.00038941511556090926\n",
      "Batch: 20000,train loss is: 0.00035446399023529704\n",
      "test loss is 0.00032684518531236846\n",
      "Batch: 20100,train loss is: 0.00022744070566385014\n",
      "test loss is 0.0003390391782536375\n",
      "Batch: 20200,train loss is: 0.00028450045906501823\n",
      "test loss is 0.0003170596650667577\n",
      "Batch: 20300,train loss is: 0.00027938846823520873\n",
      "test loss is 0.0003470623187672305\n",
      "Batch: 20400,train loss is: 0.0003257006064770584\n",
      "test loss is 0.00037953755056469566\n",
      "Batch: 20500,train loss is: 0.0002452200576546767\n",
      "test loss is 0.0003814789438343692\n",
      "Batch: 20600,train loss is: 0.00047069636800851444\n",
      "test loss is 0.0003489429687811438\n",
      "Batch: 20700,train loss is: 0.00023577932693074168\n",
      "test loss is 0.00032506275976970433\n",
      "Batch: 20800,train loss is: 0.00026349778874759037\n",
      "test loss is 0.00029147051581680555\n",
      "Batch: 20900,train loss is: 0.0005198934356188593\n",
      "test loss is 0.00032105647105320573\n",
      "Batch: 21000,train loss is: 0.00037611518433225345\n",
      "test loss is 0.0003253691875367277\n",
      "Batch: 21100,train loss is: 0.0003567081898582695\n",
      "test loss is 0.0002916383835421901\n",
      "Batch: 21200,train loss is: 0.00039211369225639037\n",
      "test loss is 0.00031141085775713\n",
      "Batch: 21300,train loss is: 0.0007206955906959348\n",
      "test loss is 0.00029254251047902615\n",
      "Batch: 21400,train loss is: 0.0009125760787047684\n",
      "test loss is 0.00030959178617682596\n",
      "Batch: 21500,train loss is: 0.00023224948985544285\n",
      "test loss is 0.0003329639233635881\n",
      "Batch: 21600,train loss is: 0.000270855480281759\n",
      "test loss is 0.0003099975164427862\n",
      "Batch: 21700,train loss is: 0.0002652839090375516\n",
      "test loss is 0.00029668510961364497\n",
      "Batch: 21800,train loss is: 0.00029261155614820195\n",
      "test loss is 0.0003076173803502635\n",
      "Batch: 21900,train loss is: 0.00015069401754078327\n",
      "test loss is 0.00030410314927921585\n",
      "Batch: 22000,train loss is: 0.00022214309790170105\n",
      "test loss is 0.00029176500015463616\n",
      "Batch: 22100,train loss is: 0.00028255758266073125\n",
      "test loss is 0.00032829650507896097\n",
      "Batch: 22200,train loss is: 0.00023447403481034157\n",
      "test loss is 0.00032870918055494686\n",
      "Batch: 22300,train loss is: 0.00031714973273795817\n",
      "test loss is 0.0002909355736236284\n",
      "Batch: 22400,train loss is: 0.00025614136564956937\n",
      "test loss is 0.0002916806071182342\n",
      "Batch: 22500,train loss is: 0.00036318474367708054\n",
      "test loss is 0.00035616818135258515\n",
      "Batch: 22600,train loss is: 0.000439082616488312\n",
      "test loss is 0.00034338162957795175\n",
      "Batch: 22700,train loss is: 0.000547877135643835\n",
      "test loss is 0.00032823879991697723\n",
      "Batch: 22800,train loss is: 0.0004153085763875562\n",
      "test loss is 0.0003923836127459459\n",
      "Batch: 22900,train loss is: 0.000270842047332347\n",
      "test loss is 0.0003319258251198106\n",
      "Batch: 23000,train loss is: 0.0003392467358115561\n",
      "test loss is 0.0003069465698180481\n",
      "Batch: 23100,train loss is: 0.0003753844412454861\n",
      "test loss is 0.00034133765385428055\n",
      "Batch: 23200,train loss is: 0.0002934619780458576\n",
      "test loss is 0.0003302007549584926\n",
      "Batch: 23300,train loss is: 0.00032435904205661164\n",
      "test loss is 0.00030494230706308323\n",
      "Batch: 23400,train loss is: 0.00018075229240809668\n",
      "test loss is 0.0003207018002685435\n",
      "Batch: 23500,train loss is: 0.0002830797618943851\n",
      "test loss is 0.0003147001460105546\n",
      "Batch: 23600,train loss is: 0.000274076178063705\n",
      "test loss is 0.0003490457971369035\n",
      "Batch: 23700,train loss is: 0.0002596854606574207\n",
      "test loss is 0.0003587292999655352\n",
      "Batch: 23800,train loss is: 0.0003093783274218914\n",
      "test loss is 0.00030249938820312445\n",
      "Batch: 23900,train loss is: 0.00025516557448067805\n",
      "test loss is 0.00031081992737003704\n",
      "Batch: 24000,train loss is: 0.00024389838501734764\n",
      "test loss is 0.0003258273591551137\n",
      "Batch: 24100,train loss is: 0.000282041347823293\n",
      "test loss is 0.00028054407972461686\n",
      "Batch: 24200,train loss is: 0.0002969568555824647\n",
      "test loss is 0.00031352740782833823\n",
      "Batch: 24300,train loss is: 0.0003661425778323162\n",
      "test loss is 0.00033729527422236773\n",
      "Batch: 24400,train loss is: 0.00021497710807005105\n",
      "test loss is 0.0003135742406558409\n",
      "Batch: 24500,train loss is: 0.0005434086684903091\n",
      "test loss is 0.00034234974359546064\n",
      "Batch: 24600,train loss is: 0.00021955870152660543\n",
      "test loss is 0.000296045911141778\n",
      "Batch: 24700,train loss is: 0.0002406582617165758\n",
      "test loss is 0.0003017691261298799\n",
      "Batch: 24800,train loss is: 0.00030293330194842234\n",
      "test loss is 0.00031582722896905994\n",
      "Batch: 24900,train loss is: 0.0004637294776521975\n",
      "test loss is 0.00030706961729552176\n",
      "Batch: 25000,train loss is: 0.000257450716503197\n",
      "test loss is 0.0003316809060120449\n",
      "Batch: 25100,train loss is: 0.0002278475281569831\n",
      "test loss is 0.0002851790248760795\n",
      "Batch: 25200,train loss is: 0.0003297738756859673\n",
      "test loss is 0.00027712341352496963\n",
      "Batch: 25300,train loss is: 0.00028961567807084666\n",
      "test loss is 0.0003165758474476112\n",
      "Batch: 25400,train loss is: 0.0003203349526946288\n",
      "test loss is 0.00032783089381941614\n",
      "Batch: 25500,train loss is: 0.00045392132626546824\n",
      "test loss is 0.00039053632546594557\n",
      "Batch: 25600,train loss is: 0.000348110174201082\n",
      "test loss is 0.00030417168912762047\n",
      "Batch: 25700,train loss is: 0.00035776944275578833\n",
      "test loss is 0.0003071825693390373\n",
      "Batch: 25800,train loss is: 0.00031605613584818025\n",
      "test loss is 0.0003044112237846888\n",
      "Batch: 25900,train loss is: 0.000267339085682251\n",
      "test loss is 0.0003268390395232605\n",
      "Batch: 26000,train loss is: 0.00024003553359297304\n",
      "test loss is 0.00033326131234453593\n",
      "Batch: 26100,train loss is: 0.0004027969774731097\n",
      "test loss is 0.00029884431347072533\n",
      "Batch: 26200,train loss is: 0.0004382271167489519\n",
      "test loss is 0.0003315060529595433\n",
      "Batch: 26300,train loss is: 0.00024756355812905124\n",
      "test loss is 0.00027989026733914326\n",
      "Batch: 26400,train loss is: 0.0003565324318445696\n",
      "test loss is 0.00035838694586298644\n",
      "Batch: 26500,train loss is: 0.000291772285562899\n",
      "test loss is 0.0003423458622100431\n",
      "Batch: 26600,train loss is: 0.00030213001814193875\n",
      "test loss is 0.0003120724351514185\n",
      "Batch: 26700,train loss is: 0.00029270629184301183\n",
      "test loss is 0.0003120301642470194\n",
      "Batch: 26800,train loss is: 0.0003652359000604372\n",
      "test loss is 0.00037362671687045005\n",
      "Batch: 26900,train loss is: 0.0002485793699598639\n",
      "test loss is 0.00029448920382923696\n",
      "Batch: 27000,train loss is: 0.00038251315244287377\n",
      "test loss is 0.00029462583815004207\n",
      "Batch: 27100,train loss is: 0.00022547254814568295\n",
      "test loss is 0.00030601901581317844\n",
      "Batch: 27200,train loss is: 0.0004531910658082597\n",
      "test loss is 0.0003086999872345562\n",
      "Batch: 27300,train loss is: 0.00025713972317504093\n",
      "test loss is 0.0002978101073211241\n",
      "Batch: 27400,train loss is: 0.00033115484512657225\n",
      "test loss is 0.0003103694879870293\n",
      "Batch: 27500,train loss is: 0.0004679039578930332\n",
      "test loss is 0.0003148285956924283\n",
      "Batch: 27600,train loss is: 0.00023439847932273897\n",
      "test loss is 0.00029416600943287576\n",
      "Batch: 27700,train loss is: 0.0032514569860410077\n",
      "test loss is 0.00033649461293994387\n",
      "Batch: 27800,train loss is: 0.0004070148229483678\n",
      "test loss is 0.00030708978807107316\n",
      "Batch: 27900,train loss is: 0.000287684199907376\n",
      "test loss is 0.00031893138511200397\n",
      "Batch: 28000,train loss is: 0.0003314204459844834\n",
      "test loss is 0.00028896682761101193\n",
      "Batch: 28100,train loss is: 0.00041557618849997476\n",
      "test loss is 0.0003309682247186685\n",
      "Batch: 28200,train loss is: 0.0002205678588630265\n",
      "test loss is 0.0002949245679189282\n",
      "Batch: 28300,train loss is: 0.0002237992612980011\n",
      "test loss is 0.0003218294315587117\n",
      "Batch: 28400,train loss is: 0.00020479939922827192\n",
      "test loss is 0.0003194323796771965\n",
      "Batch: 28500,train loss is: 0.00025505017441578324\n",
      "test loss is 0.00032397906722839224\n",
      "Batch: 28600,train loss is: 0.000248307132464044\n",
      "test loss is 0.000412410450885506\n",
      "Batch: 28700,train loss is: 0.0002431848728507675\n",
      "test loss is 0.00029845221892389084\n",
      "Batch: 28800,train loss is: 0.00039336371043416655\n",
      "test loss is 0.00030591173510610014\n",
      "Batch: 28900,train loss is: 0.00016074690757761925\n",
      "test loss is 0.0002882950996465263\n",
      "Batch: 29000,train loss is: 0.00022165550180242252\n",
      "test loss is 0.00030170229386411294\n",
      "Batch: 29100,train loss is: 0.0003796536184315681\n",
      "test loss is 0.0003005830148552897\n",
      "Batch: 29200,train loss is: 0.00018938593579071134\n",
      "test loss is 0.0002872117903893979\n",
      "Batch: 29300,train loss is: 0.00016933561890453407\n",
      "test loss is 0.0003277179869686353\n",
      "Batch: 29400,train loss is: 0.00019704973261672285\n",
      "test loss is 0.00035689972946826464\n",
      "Batch: 29500,train loss is: 0.00043314698579596117\n",
      "test loss is 0.00032985830059727297\n",
      "Batch: 29600,train loss is: 0.00026184062803810297\n",
      "test loss is 0.00030565788592064504\n",
      "Batch: 29700,train loss is: 0.0005046208458725505\n",
      "test loss is 0.0003167082959531532\n",
      "Batch: 29800,train loss is: 0.00020408583338191785\n",
      "test loss is 0.0002924708249227061\n",
      "Batch: 29900,train loss is: 0.00028105448404819626\n",
      "test loss is 0.00031143199235654756\n",
      "Batch: 30000,train loss is: 0.00032230686348267936\n",
      "test loss is 0.00029082945959039655\n",
      "Batch: 30100,train loss is: 0.00043578464934592576\n",
      "test loss is 0.00029539811439002696\n",
      "Batch: 30200,train loss is: 0.0003585402587193699\n",
      "test loss is 0.0003506539642469438\n",
      "Batch: 30300,train loss is: 0.00031146691945303697\n",
      "test loss is 0.00032532045203394113\n",
      "Batch: 30400,train loss is: 0.0003645350036096013\n",
      "test loss is 0.000418805449923832\n",
      "Batch: 30500,train loss is: 0.0001686521575903916\n",
      "test loss is 0.00031580778695054345\n",
      "Batch: 30600,train loss is: 0.0003256323919076804\n",
      "test loss is 0.00029978659781235037\n",
      "Batch: 30700,train loss is: 0.00023896753892012178\n",
      "test loss is 0.00030530518739361106\n",
      "Batch: 30800,train loss is: 0.00025198848724063236\n",
      "test loss is 0.00033788477851806397\n",
      "Batch: 30900,train loss is: 0.0003207961371443509\n",
      "test loss is 0.00027738225141643315\n",
      "Batch: 31000,train loss is: 0.00023784852061800466\n",
      "test loss is 0.0002799783208324408\n",
      "Batch: 31100,train loss is: 0.0002195336762240463\n",
      "test loss is 0.0002774328408138194\n",
      "Batch: 31200,train loss is: 0.00022943188850316174\n",
      "test loss is 0.00030589781016285446\n",
      "Batch: 31300,train loss is: 0.00034771926989802496\n",
      "test loss is 0.0003339595373894101\n",
      "Batch: 31400,train loss is: 0.000202470093279716\n",
      "test loss is 0.00029772764711046153\n",
      "Batch: 31500,train loss is: 0.00020716481809946545\n",
      "test loss is 0.0003141129109067472\n",
      "Batch: 31600,train loss is: 0.00034595066837464413\n",
      "test loss is 0.0003060253022384939\n",
      "Batch: 31700,train loss is: 0.0003303076178086111\n",
      "test loss is 0.00028887837756073046\n",
      "Batch: 31800,train loss is: 0.00025389989658161\n",
      "test loss is 0.0003086451156117024\n",
      "Batch: 31900,train loss is: 0.00031543417002877975\n",
      "test loss is 0.00033956846603669155\n",
      "Batch: 32000,train loss is: 0.00027074642130995853\n",
      "test loss is 0.00032254884266409576\n",
      "Batch: 32100,train loss is: 0.0002921097219473315\n",
      "test loss is 0.00028302738945968264\n",
      "Batch: 32200,train loss is: 0.00022941967434399665\n",
      "test loss is 0.0003996178825015044\n",
      "Batch: 32300,train loss is: 0.0003090326140377267\n",
      "test loss is 0.0002980398618278793\n",
      "Batch: 32400,train loss is: 0.00014350585851777091\n",
      "test loss is 0.0003174791337936561\n",
      "Batch: 32500,train loss is: 0.000463604097501889\n",
      "test loss is 0.00029783573742129643\n",
      "Batch: 32600,train loss is: 0.00025170029565733384\n",
      "test loss is 0.0002797611732303062\n",
      "Batch: 32700,train loss is: 0.00017153700788369422\n",
      "test loss is 0.0003175953275924925\n",
      "Batch: 32800,train loss is: 0.0002659892418180466\n",
      "test loss is 0.00030642097043883016\n",
      "Batch: 32900,train loss is: 0.00030588469666591655\n",
      "test loss is 0.00031160558337444034\n",
      "Batch: 33000,train loss is: 0.0002715958434110475\n",
      "test loss is 0.0003130272446608911\n",
      "Batch: 33100,train loss is: 0.0002870302340004841\n",
      "test loss is 0.00027980180337728433\n",
      "Batch: 33200,train loss is: 0.00031374311418824703\n",
      "test loss is 0.0003125327560456789\n",
      "Batch: 33300,train loss is: 0.0002074149243379607\n",
      "test loss is 0.00028666966889018817\n",
      "Batch: 33400,train loss is: 0.0002154504057452055\n",
      "test loss is 0.0002980765230049639\n",
      "Batch: 33500,train loss is: 0.0003677940496163458\n",
      "test loss is 0.00031179404310488674\n",
      "Batch: 33600,train loss is: 0.0001814068256066514\n",
      "test loss is 0.00028717863536746477\n",
      "Batch: 33700,train loss is: 0.00019004780962559976\n",
      "test loss is 0.00036829485390094764\n",
      "Batch: 33800,train loss is: 0.000371855663245651\n",
      "test loss is 0.00038948744545374403\n",
      "Batch: 33900,train loss is: 0.00029316971055849065\n",
      "test loss is 0.00033504309909192746\n",
      "Batch: 34000,train loss is: 0.0002925007700747105\n",
      "test loss is 0.00030011596594609047\n",
      "Batch: 34100,train loss is: 0.0002733935073232479\n",
      "test loss is 0.00030097216501136345\n",
      "Batch: 34200,train loss is: 0.00018103633034462685\n",
      "test loss is 0.00035994513780902344\n",
      "Batch: 34300,train loss is: 0.0002069664657487647\n",
      "test loss is 0.00030004971016461157\n",
      "Batch: 34400,train loss is: 0.00041326420744728216\n",
      "test loss is 0.0003147174720868397\n",
      "Batch: 34500,train loss is: 0.00020265072381794795\n",
      "test loss is 0.00031322981044018826\n",
      "Batch: 34600,train loss is: 0.00026910623201463853\n",
      "test loss is 0.00035181589596517825\n",
      "Batch: 34700,train loss is: 0.0003618953827181153\n",
      "test loss is 0.00033339024451409124\n",
      "Batch: 34800,train loss is: 0.0002437622239595168\n",
      "test loss is 0.00028705673151045415\n",
      "Batch: 34900,train loss is: 0.0003596924022376106\n",
      "test loss is 0.00033705229570879996\n",
      "Batch: 35000,train loss is: 0.00021221926942022288\n",
      "test loss is 0.00030396154402041276\n",
      "Batch: 35100,train loss is: 0.00033640037922977415\n",
      "test loss is 0.0003035936957509375\n",
      "Batch: 35200,train loss is: 0.000200463037004279\n",
      "test loss is 0.0002908491919647379\n",
      "Batch: 35300,train loss is: 0.00026449691912852977\n",
      "test loss is 0.0004505285790625806\n",
      "Batch: 35400,train loss is: 0.0003807881300402137\n",
      "test loss is 0.0003116447918265714\n",
      "Batch: 35500,train loss is: 0.00021868279097174724\n",
      "test loss is 0.0002846920931798618\n",
      "Batch: 35600,train loss is: 0.00029474899563376657\n",
      "test loss is 0.00031880628612764516\n",
      "Batch: 35700,train loss is: 0.00036110532930676883\n",
      "test loss is 0.00044148027188743103\n",
      "Batch: 35800,train loss is: 0.0004444736655955818\n",
      "test loss is 0.00029159218358478525\n",
      "Batch: 35900,train loss is: 0.0002301507940455354\n",
      "test loss is 0.00031179252832079106\n",
      "Batch: 36000,train loss is: 0.0003228050806605694\n",
      "test loss is 0.00032370230924406735\n",
      "Batch: 36100,train loss is: 0.0002465087075337592\n",
      "test loss is 0.0002810007254730418\n",
      "Batch: 36200,train loss is: 0.0002280389395812865\n",
      "test loss is 0.000287774376161021\n",
      "Batch: 36300,train loss is: 0.00033066055565325906\n",
      "test loss is 0.00032759442152603473\n",
      "Batch: 36400,train loss is: 0.0004279711130721113\n",
      "test loss is 0.0002884135336366242\n",
      "Batch: 36500,train loss is: 0.000293751246121481\n",
      "test loss is 0.0003115902782610244\n",
      "Batch: 36600,train loss is: 0.00047912623002139323\n",
      "test loss is 0.00030845985211138397\n",
      "Batch: 36700,train loss is: 0.00032124920772429644\n",
      "test loss is 0.00029881303691277187\n",
      "Batch: 36800,train loss is: 0.0005978110330585306\n",
      "test loss is 0.00027680382203552716\n",
      "Batch: 36900,train loss is: 0.00033084485339958923\n",
      "test loss is 0.00032949871194330906\n",
      "Batch: 37000,train loss is: 0.00020634232902921824\n",
      "test loss is 0.00031223854833395545\n",
      "Batch: 37100,train loss is: 0.0003435392485100521\n",
      "test loss is 0.0003030213173664343\n",
      "Batch: 37200,train loss is: 0.00021879582647493207\n",
      "test loss is 0.00030758831476907926\n",
      "Batch: 37300,train loss is: 0.00030675923551363586\n",
      "test loss is 0.00030828912761875915\n",
      "Batch: 37400,train loss is: 0.0005092036919157565\n",
      "test loss is 0.0003711978940842303\n",
      "Batch: 37500,train loss is: 0.00037666349442165564\n",
      "test loss is 0.00031289291757827936\n",
      "Batch: 37600,train loss is: 0.00021381713045971263\n",
      "test loss is 0.00028918815948824256\n",
      "Batch: 37700,train loss is: 0.0002273618054323794\n",
      "test loss is 0.0002898306939473077\n",
      "Batch: 37800,train loss is: 0.000372359399427985\n",
      "test loss is 0.00031607554291541946\n",
      "Batch: 37900,train loss is: 0.0002465232139370285\n",
      "test loss is 0.00030019023441955206\n",
      "Batch: 38000,train loss is: 0.0002609329891305274\n",
      "test loss is 0.00030399491610788186\n",
      "Batch: 38100,train loss is: 0.00037684007859651995\n",
      "test loss is 0.00029952151242097075\n",
      "Batch: 38200,train loss is: 0.0003475990003834032\n",
      "test loss is 0.0002966239960851567\n",
      "Batch: 38300,train loss is: 0.00044464729406360403\n",
      "test loss is 0.00031233099540549125\n",
      "Batch: 38400,train loss is: 0.000442560554283558\n",
      "test loss is 0.00029807426437684616\n",
      "Batch: 38500,train loss is: 0.00018846435401511925\n",
      "test loss is 0.0003036022464379563\n",
      "Batch: 38600,train loss is: 0.00019940808722747572\n",
      "test loss is 0.00029980779134034777\n",
      "Batch: 38700,train loss is: 0.00033067470994976194\n",
      "test loss is 0.0003408551946585767\n",
      "Batch: 38800,train loss is: 0.000244388581301402\n",
      "test loss is 0.00029185542737378896\n",
      "Batch: 38900,train loss is: 0.000455665829815334\n",
      "test loss is 0.0002830228253191596\n",
      "Batch: 39000,train loss is: 0.00036157016873384517\n",
      "test loss is 0.0003142859885346197\n",
      "Batch: 39100,train loss is: 0.00019133037526153872\n",
      "test loss is 0.0003361747121442029\n",
      "Batch: 39200,train loss is: 0.00028328165461111225\n",
      "test loss is 0.00030815288455948866\n",
      "Batch: 39300,train loss is: 0.00022398528598272394\n",
      "test loss is 0.000288724493274492\n",
      "Batch: 39400,train loss is: 0.0003384762645849584\n",
      "test loss is 0.0003208566222382796\n",
      "Batch: 39500,train loss is: 0.00024092780252150267\n",
      "test loss is 0.0002894325248639482\n",
      "Batch: 39600,train loss is: 0.0002518017217720929\n",
      "test loss is 0.0003689048826606776\n",
      "Batch: 39700,train loss is: 0.0003625725869483188\n",
      "test loss is 0.0003025869903991352\n",
      "Batch: 39800,train loss is: 0.0005596993543951792\n",
      "test loss is 0.00032750854408190844\n",
      "Batch: 39900,train loss is: 0.00027613233979218143\n",
      "test loss is 0.0003306373246012793\n",
      "Batch: 40000,train loss is: 0.00028349108362585984\n",
      "test loss is 0.00027892908019050715\n",
      "Batch: 40100,train loss is: 0.0002674404963914212\n",
      "test loss is 0.0003084357306423736\n",
      "Batch: 40200,train loss is: 0.00038773458229510453\n",
      "test loss is 0.00029099426237067037\n",
      "Batch: 40300,train loss is: 0.00039738978211686227\n",
      "test loss is 0.0003104445765430683\n",
      "Batch: 40400,train loss is: 0.000404683986954903\n",
      "test loss is 0.00029864886794867733\n",
      "Batch: 40500,train loss is: 0.00043223033670184026\n",
      "test loss is 0.00037109937805721775\n",
      "Batch: 40600,train loss is: 0.00021473867814866268\n",
      "test loss is 0.0003542243417632375\n",
      "Batch: 40700,train loss is: 0.0003213894864677394\n",
      "test loss is 0.00030585593913007753\n",
      "Batch: 40800,train loss is: 0.0002833062840011313\n",
      "test loss is 0.00030719466512123245\n",
      "Batch: 40900,train loss is: 0.00022014854878489793\n",
      "test loss is 0.00030748189944080105\n",
      "Batch: 41000,train loss is: 0.0004651470457862733\n",
      "test loss is 0.00031739395645790806\n",
      "Batch: 41100,train loss is: 0.000361440090868307\n",
      "test loss is 0.00031859389185281744\n",
      "Batch: 41200,train loss is: 0.0004905756225934635\n",
      "test loss is 0.0002925523384918467\n",
      "Batch: 41300,train loss is: 0.000303127407099751\n",
      "test loss is 0.0002976049247096039\n",
      "Batch: 41400,train loss is: 0.0003211827426644583\n",
      "test loss is 0.0003293440889724173\n",
      "Batch: 41500,train loss is: 0.00020438589724826707\n",
      "test loss is 0.0003499189722549489\n",
      "Batch: 41600,train loss is: 0.00024056157609362322\n",
      "test loss is 0.0003338129549746188\n",
      "Batch: 41700,train loss is: 0.0002887324835513264\n",
      "test loss is 0.0003780328780029307\n",
      "Batch: 41800,train loss is: 0.0003203090219236359\n",
      "test loss is 0.0003076034840323457\n",
      "Batch: 41900,train loss is: 0.00020513834521347456\n",
      "test loss is 0.0003247670703106807\n",
      "Batch: 42000,train loss is: 0.00021979177196833105\n",
      "test loss is 0.00029926240278586196\n",
      "Batch: 42100,train loss is: 0.0002435934119872647\n",
      "test loss is 0.0002658099901276286\n",
      "Batch: 42200,train loss is: 0.00028663847139942327\n",
      "test loss is 0.0002963150297797429\n",
      "Batch: 42300,train loss is: 0.0003674104313936407\n",
      "test loss is 0.00035125034161022356\n",
      "Batch: 42400,train loss is: 0.000230783766476652\n",
      "test loss is 0.0003587993062999942\n",
      "Batch: 42500,train loss is: 0.00033969390007228045\n",
      "test loss is 0.0003747788411306046\n",
      "Batch: 42600,train loss is: 0.0002761823874009296\n",
      "test loss is 0.0002856440414778992\n",
      "Batch: 42700,train loss is: 0.00022596539076355444\n",
      "test loss is 0.0003116035146024776\n",
      "Batch: 42800,train loss is: 0.0003906444788370956\n",
      "test loss is 0.0003289609177926943\n",
      "Batch: 42900,train loss is: 0.00021483773877060822\n",
      "test loss is 0.00030747091681579517\n",
      "Batch: 43000,train loss is: 0.00024965506821174907\n",
      "test loss is 0.0003007113843828096\n",
      "Batch: 43100,train loss is: 0.0003043030662412925\n",
      "test loss is 0.0003225641104934434\n",
      "Batch: 43200,train loss is: 0.0002825547347203288\n",
      "test loss is 0.0003013087792854647\n",
      "Batch: 43300,train loss is: 0.00023848892847814763\n",
      "test loss is 0.0002740353552772256\n",
      "Batch: 43400,train loss is: 0.0002051673091361362\n",
      "test loss is 0.000379190219418929\n",
      "Batch: 43500,train loss is: 0.0002731200449418982\n",
      "test loss is 0.0003144402116841849\n",
      "Batch: 43600,train loss is: 0.00029419548139158845\n",
      "test loss is 0.00029686488153724027\n",
      "Batch: 43700,train loss is: 0.00046388986404752286\n",
      "test loss is 0.00030473968323424153\n",
      "Batch: 43800,train loss is: 0.00042324432292470263\n",
      "test loss is 0.0003532674623146761\n",
      "Batch: 43900,train loss is: 0.00027812590460546416\n",
      "test loss is 0.0003270789816802512\n",
      "Batch: 44000,train loss is: 0.00034346611400723064\n",
      "test loss is 0.00043892635425956967\n",
      "Batch: 44100,train loss is: 0.0002556265115509782\n",
      "test loss is 0.0002885355191581596\n",
      "Batch: 44200,train loss is: 0.00036789705069948276\n",
      "test loss is 0.0002759935317060572\n",
      "Batch: 44300,train loss is: 0.00037324330942308556\n",
      "test loss is 0.0003833801224596139\n",
      "Batch: 44400,train loss is: 0.00023516872690324396\n",
      "test loss is 0.00030828410246028475\n",
      "Batch: 44500,train loss is: 0.0002667386355790489\n",
      "test loss is 0.00031702090253109163\n",
      "Batch: 44600,train loss is: 0.0002663460016885202\n",
      "test loss is 0.00033598766015825504\n",
      "Batch: 44700,train loss is: 0.00033804733299798456\n",
      "test loss is 0.0003227775835604771\n",
      "Batch: 44800,train loss is: 0.0003480449604174371\n",
      "test loss is 0.00037782089488491634\n",
      "Batch: 44900,train loss is: 0.0003786599035350336\n",
      "test loss is 0.00034051818622641503\n",
      "Batch: 45000,train loss is: 0.0004108106765675361\n",
      "test loss is 0.0003199079017641098\n",
      "Batch: 45100,train loss is: 0.00026157658239365383\n",
      "test loss is 0.000322776824097521\n",
      "Batch: 45200,train loss is: 0.0003130141463769098\n",
      "test loss is 0.0003194107726633244\n",
      "Batch: 45300,train loss is: 0.00033784179400892406\n",
      "test loss is 0.00028795359722480127\n",
      "Batch: 45400,train loss is: 0.0003672517591140693\n",
      "test loss is 0.00028206942344854947\n",
      "Batch: 45500,train loss is: 0.00018837103480031156\n",
      "test loss is 0.0003125309156940709\n",
      "Batch: 45600,train loss is: 0.00022364599429462254\n",
      "test loss is 0.00031458570863655204\n",
      "Batch: 45700,train loss is: 0.0003123243332778313\n",
      "test loss is 0.00030640129148273653\n",
      "Batch: 45800,train loss is: 0.0003553343495807568\n",
      "test loss is 0.0003634211650542388\n",
      "Batch: 45900,train loss is: 0.00020138471497126593\n",
      "test loss is 0.0004041130948349021\n",
      "Batch: 46000,train loss is: 0.00021943854875862212\n",
      "test loss is 0.00030190339761479543\n",
      "Batch: 46100,train loss is: 0.00021838366725632354\n",
      "test loss is 0.0002811585289126618\n",
      "Batch: 46200,train loss is: 0.00025538064848640936\n",
      "test loss is 0.00032912279770226096\n",
      "Batch: 46300,train loss is: 0.00022595080836762288\n",
      "test loss is 0.00031884607054612473\n",
      "Batch: 46400,train loss is: 0.00024993580955401905\n",
      "test loss is 0.00031258526670065705\n",
      "Batch: 46500,train loss is: 0.00031706041073035313\n",
      "test loss is 0.0003183884184074809\n",
      "Batch: 46600,train loss is: 0.00036518778942242326\n",
      "test loss is 0.0003356411955910871\n",
      "Batch: 46700,train loss is: 0.0006649350564703567\n",
      "test loss is 0.00033617881904409683\n",
      "-----------------------Epoch: 3----------------------------------\n",
      "Batch: 0,train loss is: 0.0003789555557195685\n",
      "test loss is 0.0003740663858091171\n",
      "Batch: 100,train loss is: 0.000249794798301532\n",
      "test loss is 0.000283955847303386\n",
      "Batch: 200,train loss is: 0.00030478267183057893\n",
      "test loss is 0.00029845674771154053\n",
      "Batch: 300,train loss is: 0.0003896377779642084\n",
      "test loss is 0.00033726995302151783\n",
      "Batch: 400,train loss is: 0.00020954089561768063\n",
      "test loss is 0.0003132166474860316\n",
      "Batch: 500,train loss is: 0.0003482975844086452\n",
      "test loss is 0.00029121910515069366\n",
      "Batch: 600,train loss is: 0.00022050441946306606\n",
      "test loss is 0.0003067222831039623\n",
      "Batch: 700,train loss is: 0.00022192670041671893\n",
      "test loss is 0.00032510133624537107\n",
      "Batch: 800,train loss is: 0.0003340939326866993\n",
      "test loss is 0.00032808615170986267\n",
      "Batch: 900,train loss is: 0.00023766782406144723\n",
      "test loss is 0.00032776794588040885\n",
      "Batch: 1000,train loss is: 0.00048341422839316743\n",
      "test loss is 0.00039825821258047993\n",
      "Batch: 1100,train loss is: 0.000361043933816145\n",
      "test loss is 0.00032636573882061895\n",
      "Batch: 1200,train loss is: 0.0003699768815770108\n",
      "test loss is 0.00040089947351247356\n",
      "Batch: 1300,train loss is: 0.0002612198377666876\n",
      "test loss is 0.0003103269801348907\n",
      "Batch: 1400,train loss is: 0.00037197284005421915\n",
      "test loss is 0.00033264973588223126\n",
      "Batch: 1500,train loss is: 0.00021659550665257198\n",
      "test loss is 0.0002955366199688856\n",
      "Batch: 1600,train loss is: 0.0004483732620267875\n",
      "test loss is 0.000348476062710388\n",
      "Batch: 1700,train loss is: 0.0004587219574204461\n",
      "test loss is 0.00030673468204421903\n",
      "Batch: 1800,train loss is: 0.0002624635758907599\n",
      "test loss is 0.0003186661625567491\n",
      "Batch: 1900,train loss is: 0.00031652714042582964\n",
      "test loss is 0.0002931742977322059\n",
      "Batch: 2000,train loss is: 0.0003950004436533755\n",
      "test loss is 0.00032037665696747216\n",
      "Batch: 2100,train loss is: 0.00019330867438205486\n",
      "test loss is 0.000296474806109917\n",
      "Batch: 2200,train loss is: 0.00017351703813635897\n",
      "test loss is 0.0002949457812630811\n",
      "Batch: 2300,train loss is: 0.00015813595738310888\n",
      "test loss is 0.0002741360780841053\n",
      "Batch: 2400,train loss is: 0.00024619412596093974\n",
      "test loss is 0.0003086113707009278\n",
      "Batch: 2500,train loss is: 0.0001784144417863369\n",
      "test loss is 0.00029912292598113067\n",
      "Batch: 2600,train loss is: 0.00034322152010895547\n",
      "test loss is 0.00033081456493998094\n",
      "Batch: 2700,train loss is: 0.00033336405979436765\n",
      "test loss is 0.0003637351576068967\n",
      "Batch: 2800,train loss is: 0.00033845156681492863\n",
      "test loss is 0.00030431567393621444\n",
      "Batch: 2900,train loss is: 0.0003588950763168806\n",
      "test loss is 0.00034782653080833595\n",
      "Batch: 3000,train loss is: 0.00031521700262990054\n",
      "test loss is 0.00031554817229904723\n",
      "Batch: 3100,train loss is: 0.00036889144886830807\n",
      "test loss is 0.0003098829367219862\n",
      "Batch: 3200,train loss is: 0.00033542079453287483\n",
      "test loss is 0.0003049021808657819\n",
      "Batch: 3300,train loss is: 0.00023767396462735118\n",
      "test loss is 0.00029820633262065793\n",
      "Batch: 3400,train loss is: 0.0005335633560473549\n",
      "test loss is 0.0003350180277748371\n",
      "Batch: 3500,train loss is: 0.0003345884799017383\n",
      "test loss is 0.0003850847700060744\n",
      "Batch: 3600,train loss is: 0.0003114148865167211\n",
      "test loss is 0.000323479869708179\n",
      "Batch: 3700,train loss is: 0.00032455989695053043\n",
      "test loss is 0.0002980321293015868\n",
      "Batch: 3800,train loss is: 0.00026778260989692506\n",
      "test loss is 0.0003493453872206062\n",
      "Batch: 3900,train loss is: 0.000298694666505105\n",
      "test loss is 0.00033085484898020856\n",
      "Batch: 4000,train loss is: 0.0003658937730389866\n",
      "test loss is 0.0002986690722164396\n",
      "Batch: 4100,train loss is: 0.0003733382594368963\n",
      "test loss is 0.0003059517770521397\n",
      "Batch: 4200,train loss is: 0.00024075083206250627\n",
      "test loss is 0.0003372622977548312\n",
      "Batch: 4300,train loss is: 0.0008911678648438998\n",
      "test loss is 0.00034239135715460184\n",
      "Batch: 4400,train loss is: 0.00033249575155633884\n",
      "test loss is 0.00029949255581703427\n",
      "Batch: 4500,train loss is: 0.0003490394620391899\n",
      "test loss is 0.0002871196536767362\n",
      "Batch: 4600,train loss is: 0.0002371043065686096\n",
      "test loss is 0.0002913891414266003\n",
      "Batch: 4700,train loss is: 0.00017702118937425992\n",
      "test loss is 0.0002818161848476963\n",
      "Batch: 4800,train loss is: 0.0004532802741924781\n",
      "test loss is 0.0003454795390687181\n",
      "Batch: 4900,train loss is: 0.005092034856984443\n",
      "test loss is 0.0003219642441819863\n",
      "Batch: 5000,train loss is: 0.00018070421469119194\n",
      "test loss is 0.00029920497870741423\n",
      "Batch: 5100,train loss is: 0.0003125741577976838\n",
      "test loss is 0.0002990382541190747\n",
      "Batch: 5200,train loss is: 0.00020593600358895827\n",
      "test loss is 0.0003211857760528001\n",
      "Batch: 5300,train loss is: 0.00023636737396851774\n",
      "test loss is 0.00028246153773987424\n",
      "Batch: 5400,train loss is: 0.00021882878016931501\n",
      "test loss is 0.00030934472981461664\n",
      "Batch: 5500,train loss is: 0.0004139711804193503\n",
      "test loss is 0.0003519180666719867\n",
      "Batch: 5600,train loss is: 0.0002811320277804006\n",
      "test loss is 0.00029533241720820536\n",
      "Batch: 5700,train loss is: 0.00032591923526469106\n",
      "test loss is 0.0002866648782940114\n",
      "Batch: 5800,train loss is: 0.00041478004114060265\n",
      "test loss is 0.0003016519256104063\n",
      "Batch: 5900,train loss is: 0.0002841039660361884\n",
      "test loss is 0.00029091986982468656\n",
      "Batch: 6000,train loss is: 0.0002180412827790875\n",
      "test loss is 0.00031398250126568833\n",
      "Batch: 6100,train loss is: 0.0003875673748080367\n",
      "test loss is 0.00031207750564475904\n",
      "Batch: 6200,train loss is: 0.00038429278456369445\n",
      "test loss is 0.00029844582224826995\n",
      "Batch: 6300,train loss is: 0.00043813547734818103\n",
      "test loss is 0.00028819249578084937\n",
      "Batch: 6400,train loss is: 0.00030273251797332233\n",
      "test loss is 0.000329326174941858\n",
      "Batch: 6500,train loss is: 0.00021667861870709542\n",
      "test loss is 0.00027767784045856244\n",
      "Batch: 6600,train loss is: 0.0003755518200400867\n",
      "test loss is 0.00028976040913999154\n",
      "Batch: 6700,train loss is: 0.00017957598656743743\n",
      "test loss is 0.0003501915058691326\n",
      "Batch: 6800,train loss is: 0.00025434684130243074\n",
      "test loss is 0.0003139057750198911\n",
      "Batch: 6900,train loss is: 0.00027959397364632493\n",
      "test loss is 0.00030393447170506775\n",
      "Batch: 7000,train loss is: 0.00022386472560235446\n",
      "test loss is 0.00031888418690656224\n",
      "Batch: 7100,train loss is: 0.00046757454910576477\n",
      "test loss is 0.00030647519900571193\n",
      "Batch: 7200,train loss is: 0.00026005328859857917\n",
      "test loss is 0.00032242836075013146\n",
      "Batch: 7300,train loss is: 0.00031039108848216927\n",
      "test loss is 0.00032826241776223405\n",
      "Batch: 7400,train loss is: 0.0002654243584513563\n",
      "test loss is 0.00030369560486503033\n",
      "Batch: 7500,train loss is: 0.0003178844050453688\n",
      "test loss is 0.00030813235896531366\n",
      "Batch: 7600,train loss is: 0.0006795388483913664\n",
      "test loss is 0.00032010130956847653\n",
      "Batch: 7700,train loss is: 0.00023848929323467858\n",
      "test loss is 0.00032692446514466733\n",
      "Batch: 7800,train loss is: 0.00024742231445948527\n",
      "test loss is 0.00035019788480342883\n",
      "Batch: 7900,train loss is: 0.000319437545360655\n",
      "test loss is 0.00031057689196950646\n",
      "Batch: 8000,train loss is: 0.0003132783371642272\n",
      "test loss is 0.0003012076845200041\n",
      "Batch: 8100,train loss is: 0.0002657164916114174\n",
      "test loss is 0.0002876695812038984\n",
      "Batch: 8200,train loss is: 0.00030436094804669225\n",
      "test loss is 0.0003300987654702249\n",
      "Batch: 8300,train loss is: 0.0004770146173843397\n",
      "test loss is 0.00032541478609651606\n",
      "Batch: 8400,train loss is: 0.00035314084070447896\n",
      "test loss is 0.0005082985631199853\n",
      "Batch: 8500,train loss is: 0.00024165296105373786\n",
      "test loss is 0.0003971634386620732\n",
      "Batch: 8600,train loss is: 0.00028742257252421994\n",
      "test loss is 0.0002845602102752281\n",
      "Batch: 8700,train loss is: 0.0002553980625184114\n",
      "test loss is 0.000385212547644391\n",
      "Batch: 8800,train loss is: 0.00024430889801137603\n",
      "test loss is 0.0003184574335755341\n",
      "Batch: 8900,train loss is: 0.0005833851672606836\n",
      "test loss is 0.0003445669162315164\n",
      "Batch: 9000,train loss is: 0.00042296545015998847\n",
      "test loss is 0.00043831068844261376\n",
      "Batch: 9100,train loss is: 0.00032947400766716964\n",
      "test loss is 0.0002797464670915558\n",
      "Batch: 9200,train loss is: 0.00028873044311817427\n",
      "test loss is 0.0003332918436826187\n",
      "Batch: 9300,train loss is: 0.00023085491836735068\n",
      "test loss is 0.0002882296549948817\n",
      "Batch: 9400,train loss is: 0.0004180402330766407\n",
      "test loss is 0.00029164768036175193\n",
      "Batch: 9500,train loss is: 0.0003496923020644184\n",
      "test loss is 0.00034627396917303753\n",
      "Batch: 9600,train loss is: 0.0004901648650158268\n",
      "test loss is 0.00040374756372192703\n",
      "Batch: 9700,train loss is: 0.0003954938959548876\n",
      "test loss is 0.0002770084764034548\n",
      "Batch: 9800,train loss is: 0.00025911023711723617\n",
      "test loss is 0.0003434294157216432\n",
      "Batch: 9900,train loss is: 0.000417616234516884\n",
      "test loss is 0.00030449384891417233\n",
      "Batch: 10000,train loss is: 0.00025364330445654175\n",
      "test loss is 0.0003213958771923738\n",
      "Batch: 10100,train loss is: 0.0002723750828320877\n",
      "test loss is 0.00028513983747290397\n",
      "Batch: 10200,train loss is: 0.00021482501398301985\n",
      "test loss is 0.000346476673373295\n",
      "Batch: 10300,train loss is: 0.0003232361800797858\n",
      "test loss is 0.0003201226367005436\n",
      "Batch: 10400,train loss is: 0.00023173113518029913\n",
      "test loss is 0.00032172237865783624\n",
      "Batch: 10500,train loss is: 0.00028604354087202174\n",
      "test loss is 0.000343180952071765\n",
      "Batch: 10600,train loss is: 0.00020030461211351547\n",
      "test loss is 0.00029514381104586577\n",
      "Batch: 10700,train loss is: 0.00034308348089888825\n",
      "test loss is 0.0002879127980916113\n",
      "Batch: 10800,train loss is: 0.00023668648927227797\n",
      "test loss is 0.00029136180203580725\n",
      "Batch: 10900,train loss is: 0.0004010584502501767\n",
      "test loss is 0.0003150306363191522\n",
      "Batch: 11000,train loss is: 0.00040441690738926464\n",
      "test loss is 0.00034561221318930564\n",
      "Batch: 11100,train loss is: 0.0003284201474196468\n",
      "test loss is 0.00031621540742509963\n",
      "Batch: 11200,train loss is: 0.00019535349574524412\n",
      "test loss is 0.00029340263265865084\n",
      "Batch: 11300,train loss is: 0.00044214424847350165\n",
      "test loss is 0.00031799701296958367\n",
      "Batch: 11400,train loss is: 0.00018124640916127857\n",
      "test loss is 0.000286839619187624\n",
      "Batch: 11500,train loss is: 0.00036434901455431314\n",
      "test loss is 0.00029920832624194624\n",
      "Batch: 11600,train loss is: 0.00036652027454346734\n",
      "test loss is 0.0002949862780931132\n",
      "Batch: 11700,train loss is: 0.0003354259330270345\n",
      "test loss is 0.00032393294596478104\n",
      "Batch: 11800,train loss is: 0.00033288895170083765\n",
      "test loss is 0.00031681935294529054\n",
      "Batch: 11900,train loss is: 0.000260020258652934\n",
      "test loss is 0.000327663596692176\n",
      "Batch: 12000,train loss is: 0.00017766065452453505\n",
      "test loss is 0.0003248542153835476\n",
      "Batch: 12100,train loss is: 0.00028867589416433666\n",
      "test loss is 0.00032997566959005336\n",
      "Batch: 12200,train loss is: 0.00019707029382206765\n",
      "test loss is 0.00030590431298283455\n",
      "Batch: 12300,train loss is: 0.00021976849267444883\n",
      "test loss is 0.00027634055623191033\n",
      "Batch: 12400,train loss is: 0.0002886289879856427\n",
      "test loss is 0.0003034441594984516\n",
      "Batch: 12500,train loss is: 0.00016567030695774446\n",
      "test loss is 0.00032131732085978037\n",
      "Batch: 12600,train loss is: 0.00021347618258957425\n",
      "test loss is 0.0003047104793107065\n",
      "Batch: 12700,train loss is: 0.0003019138158784646\n",
      "test loss is 0.00034920625280353827\n",
      "Batch: 12800,train loss is: 0.00022370979023578864\n",
      "test loss is 0.0002824943842020702\n",
      "Batch: 12900,train loss is: 0.00027152715762080934\n",
      "test loss is 0.0003008060753450904\n",
      "Batch: 13000,train loss is: 0.00035097037929975527\n",
      "test loss is 0.00032313171693833053\n",
      "Batch: 13100,train loss is: 0.00032952423100678226\n",
      "test loss is 0.00029391966514040517\n",
      "Batch: 13200,train loss is: 0.000296459787863444\n",
      "test loss is 0.00028433911496327473\n",
      "Batch: 13300,train loss is: 0.00047526565199989147\n",
      "test loss is 0.00031366835917116765\n",
      "Batch: 13400,train loss is: 0.00024341228297235484\n",
      "test loss is 0.00029059598153358856\n",
      "Batch: 13500,train loss is: 0.0002590597685775634\n",
      "test loss is 0.00029719906771264587\n",
      "Batch: 13600,train loss is: 0.0002915992587706022\n",
      "test loss is 0.0002973072964409262\n",
      "Batch: 13700,train loss is: 0.00028263379553737444\n",
      "test loss is 0.0003242941034839966\n",
      "Batch: 13800,train loss is: 0.0002812805497005347\n",
      "test loss is 0.0003168039577394121\n",
      "Batch: 13900,train loss is: 0.0003236316440676636\n",
      "test loss is 0.00029611075748271315\n",
      "Batch: 14000,train loss is: 0.0002732973610191574\n",
      "test loss is 0.0002895196455858804\n",
      "Batch: 14100,train loss is: 0.00031235158503793595\n",
      "test loss is 0.00035240581279345404\n",
      "Batch: 14200,train loss is: 0.0002292610320359351\n",
      "test loss is 0.00033716763650044944\n",
      "Batch: 14300,train loss is: 0.000237846062959957\n",
      "test loss is 0.000279979146031215\n",
      "Batch: 14400,train loss is: 0.0002272472455721795\n",
      "test loss is 0.0003082467783650168\n",
      "Batch: 14500,train loss is: 0.0003301774037717699\n",
      "test loss is 0.00036749536162770756\n",
      "Batch: 14600,train loss is: 0.0003079245141514886\n",
      "test loss is 0.00029774040445484766\n",
      "Batch: 14700,train loss is: 0.0003585220621242335\n",
      "test loss is 0.0003238411086992357\n",
      "Batch: 14800,train loss is: 0.00021289447246478505\n",
      "test loss is 0.00030637496127299614\n",
      "Batch: 14900,train loss is: 0.00031556416712276794\n",
      "test loss is 0.00037465798664992303\n",
      "Batch: 15000,train loss is: 0.000472689065189999\n",
      "test loss is 0.0003858068530802809\n",
      "Batch: 15100,train loss is: 0.0002052635004564938\n",
      "test loss is 0.000318879185757315\n",
      "Batch: 15200,train loss is: 0.00030389742296848567\n",
      "test loss is 0.0002939801835368539\n",
      "Batch: 15300,train loss is: 0.00029772714268022974\n",
      "test loss is 0.0003079585618221681\n",
      "Batch: 15400,train loss is: 0.0002768029259725581\n",
      "test loss is 0.00034379369615771845\n",
      "Batch: 15500,train loss is: 0.000513254663322047\n",
      "test loss is 0.0002950711708074194\n",
      "Batch: 15600,train loss is: 0.00030588535344775147\n",
      "test loss is 0.00037726688469905096\n",
      "Batch: 15700,train loss is: 0.00023848957015433587\n",
      "test loss is 0.0003082810260237025\n",
      "Batch: 15800,train loss is: 0.00033706195891018795\n",
      "test loss is 0.00030690269445437677\n",
      "Batch: 15900,train loss is: 0.00021277706198197883\n",
      "test loss is 0.0003021532498248948\n",
      "Batch: 16000,train loss is: 0.0002760500293456893\n",
      "test loss is 0.0002914060081782809\n",
      "Batch: 16100,train loss is: 0.00030051404007430747\n",
      "test loss is 0.0003037664760273316\n",
      "Batch: 16200,train loss is: 0.00039546724489141974\n",
      "test loss is 0.0003386486109505307\n",
      "Batch: 16300,train loss is: 0.0002752386171624222\n",
      "test loss is 0.0002977292266190248\n",
      "Batch: 16400,train loss is: 0.0005575833028823857\n",
      "test loss is 0.00028303996267129604\n",
      "Batch: 16500,train loss is: 0.0002908488907528731\n",
      "test loss is 0.00027983209383658703\n",
      "Batch: 16600,train loss is: 0.00039743149645067995\n",
      "test loss is 0.0003112714279226361\n",
      "Batch: 16700,train loss is: 0.0002121911963475859\n",
      "test loss is 0.00029264152927412025\n",
      "Batch: 16800,train loss is: 0.0002831234274741091\n",
      "test loss is 0.00031129655814432467\n",
      "Batch: 16900,train loss is: 0.00028249617077533276\n",
      "test loss is 0.0003351070846028764\n",
      "Batch: 17000,train loss is: 0.00020966045574455255\n",
      "test loss is 0.00029216170869909933\n",
      "Batch: 17100,train loss is: 0.000257138832704565\n",
      "test loss is 0.00030940280590517415\n",
      "Batch: 17200,train loss is: 0.000290246016730507\n",
      "test loss is 0.0003035144083989242\n",
      "Batch: 17300,train loss is: 0.00037026949212173985\n",
      "test loss is 0.0003486685736551809\n",
      "Batch: 17400,train loss is: 0.0001723637074135703\n",
      "test loss is 0.0002935118958548326\n",
      "Batch: 17500,train loss is: 0.0004236213106058688\n",
      "test loss is 0.00029648594914871387\n",
      "Batch: 17600,train loss is: 0.0003094871415688645\n",
      "test loss is 0.0002942799516218655\n",
      "Batch: 17700,train loss is: 0.00029452163479167035\n",
      "test loss is 0.00033501699512058196\n",
      "Batch: 17800,train loss is: 0.00033919723713476236\n",
      "test loss is 0.00032104478352548563\n",
      "Batch: 17900,train loss is: 0.0003984155672869456\n",
      "test loss is 0.00032633089269796844\n",
      "Batch: 18000,train loss is: 0.00039523706443194117\n",
      "test loss is 0.00031159104746631387\n",
      "Batch: 18100,train loss is: 0.0003132440563745423\n",
      "test loss is 0.0003045310197029205\n",
      "Batch: 18200,train loss is: 0.0003787571288225553\n",
      "test loss is 0.00029030719835009347\n",
      "Batch: 18300,train loss is: 0.0002185894723810182\n",
      "test loss is 0.00028430699909595637\n",
      "Batch: 18400,train loss is: 0.0006127792340234106\n",
      "test loss is 0.0003417578749379347\n",
      "Batch: 18500,train loss is: 0.0002719004213574727\n",
      "test loss is 0.0002984918557015556\n",
      "Batch: 18600,train loss is: 0.00018481513946936822\n",
      "test loss is 0.0003064497971001168\n",
      "Batch: 18700,train loss is: 0.00033773282546792024\n",
      "test loss is 0.00028108011803343455\n",
      "Batch: 18800,train loss is: 0.0002839890856346439\n",
      "test loss is 0.0003993363566403929\n",
      "Batch: 18900,train loss is: 0.00020648678118954472\n",
      "test loss is 0.0002976558207171849\n",
      "Batch: 19000,train loss is: 0.00019349162961211305\n",
      "test loss is 0.00027521339434281677\n",
      "Batch: 19100,train loss is: 0.00020581147634208923\n",
      "test loss is 0.00028170639724408615\n",
      "Batch: 19200,train loss is: 0.0005646789306820054\n",
      "test loss is 0.0003234094294461565\n",
      "Batch: 19300,train loss is: 0.00031290135387561987\n",
      "test loss is 0.0003116518756345671\n",
      "Batch: 19400,train loss is: 0.0002678007334508752\n",
      "test loss is 0.00030382062660677016\n",
      "Batch: 19500,train loss is: 0.00023828485457548195\n",
      "test loss is 0.00030786306314918035\n",
      "Batch: 19600,train loss is: 0.0002759865160099239\n",
      "test loss is 0.0004074471131835583\n",
      "Batch: 19700,train loss is: 0.00029968882676790977\n",
      "test loss is 0.0003114065158818407\n",
      "Batch: 19800,train loss is: 0.00034909262450806967\n",
      "test loss is 0.00032349117498490723\n",
      "Batch: 19900,train loss is: 0.00033254674014409626\n",
      "test loss is 0.00036566539338765854\n",
      "Batch: 20000,train loss is: 0.0001749676166809951\n",
      "test loss is 0.000287215216748946\n",
      "Batch: 20100,train loss is: 0.0002310193821411211\n",
      "test loss is 0.00029188181555764525\n",
      "Batch: 20200,train loss is: 0.0003614039465505523\n",
      "test loss is 0.0004130362646557064\n",
      "Batch: 20300,train loss is: 0.0002549118933919306\n",
      "test loss is 0.00033923640545527805\n",
      "Batch: 20400,train loss is: 0.0002632014995598688\n",
      "test loss is 0.0003569615862769352\n",
      "Batch: 20500,train loss is: 0.00015049851448731483\n",
      "test loss is 0.00029555715997132997\n",
      "Batch: 20600,train loss is: 0.00021899138990354126\n",
      "test loss is 0.000350482013743573\n",
      "Batch: 20700,train loss is: 0.00022994261418562365\n",
      "test loss is 0.0003676750050410039\n",
      "Batch: 20800,train loss is: 0.00028818349509809714\n",
      "test loss is 0.00028811257123594893\n",
      "Batch: 20900,train loss is: 0.0006825235143221999\n",
      "test loss is 0.0003234190935665557\n",
      "Batch: 21000,train loss is: 0.00026953290153550235\n",
      "test loss is 0.00030506198217289567\n",
      "Batch: 21100,train loss is: 0.00030835951420040157\n",
      "test loss is 0.0003915832468808422\n",
      "Batch: 21200,train loss is: 0.0005720852789573526\n",
      "test loss is 0.0003080034629072338\n",
      "Batch: 21300,train loss is: 0.0003479093044156631\n",
      "test loss is 0.00029241524699866303\n",
      "Batch: 21400,train loss is: 0.00017707177941447204\n",
      "test loss is 0.00027824879727619334\n",
      "Batch: 21500,train loss is: 0.000285870923505687\n",
      "test loss is 0.0003051700733260004\n",
      "Batch: 21600,train loss is: 0.00029686863506185645\n",
      "test loss is 0.00029144865702840746\n",
      "Batch: 21700,train loss is: 0.0002898061882731992\n",
      "test loss is 0.0002940478338000822\n",
      "Batch: 21800,train loss is: 0.0003216728305949579\n",
      "test loss is 0.0003614018867288677\n",
      "Batch: 21900,train loss is: 0.0002641370407435705\n",
      "test loss is 0.0003101438227202478\n",
      "Batch: 22000,train loss is: 0.00022307341243210866\n",
      "test loss is 0.0002815087839616065\n",
      "Batch: 22100,train loss is: 0.00031085415073990813\n",
      "test loss is 0.00027317240135967875\n",
      "Batch: 22200,train loss is: 0.0003533093827081765\n",
      "test loss is 0.00044223781009921026\n",
      "Batch: 22300,train loss is: 0.0003300633389115496\n",
      "test loss is 0.0002961151447596409\n",
      "Batch: 22400,train loss is: 0.00027976611064489264\n",
      "test loss is 0.0002898356636498542\n",
      "Batch: 22500,train loss is: 0.00030290610823570195\n",
      "test loss is 0.00028814546167806363\n",
      "Batch: 22600,train loss is: 0.000359137268428246\n",
      "test loss is 0.0003415275149494371\n",
      "Batch: 22700,train loss is: 0.00023877134695235647\n",
      "test loss is 0.00027156374476922633\n",
      "Batch: 22800,train loss is: 0.00034337880705652564\n",
      "test loss is 0.000286710808896123\n",
      "Batch: 22900,train loss is: 0.0003594879746017059\n",
      "test loss is 0.0003552847279084429\n",
      "Batch: 23000,train loss is: 0.0002143807678976817\n",
      "test loss is 0.0003138708238023107\n",
      "Batch: 23100,train loss is: 0.00037854356271881855\n",
      "test loss is 0.00027446781272872115\n",
      "Batch: 23200,train loss is: 0.00021770386054544607\n",
      "test loss is 0.00032033139165265994\n",
      "Batch: 23300,train loss is: 0.000299055957675369\n",
      "test loss is 0.0002819883325458623\n",
      "Batch: 23400,train loss is: 0.00037774872822936924\n",
      "test loss is 0.00029585941753322207\n",
      "Batch: 23500,train loss is: 0.0005526084354415993\n",
      "test loss is 0.000361093207694959\n",
      "Batch: 23600,train loss is: 0.00027252764043032114\n",
      "test loss is 0.00029483230814543443\n",
      "Batch: 23700,train loss is: 0.0004226460959685166\n",
      "test loss is 0.0003737015739263042\n",
      "Batch: 23800,train loss is: 0.00026593058590699913\n",
      "test loss is 0.0002825609610177647\n",
      "Batch: 23900,train loss is: 0.00018912562787866302\n",
      "test loss is 0.0002801837364319612\n",
      "Batch: 24000,train loss is: 0.000255659207323943\n",
      "test loss is 0.0002757289398396178\n",
      "Batch: 24100,train loss is: 0.00025736270355110433\n",
      "test loss is 0.0003282488881507037\n",
      "Batch: 24200,train loss is: 0.0004192476539760729\n",
      "test loss is 0.00030628784131914513\n",
      "Batch: 24300,train loss is: 0.0002993815659297688\n",
      "test loss is 0.0003325563179736995\n",
      "Batch: 24400,train loss is: 0.00027253006199707186\n",
      "test loss is 0.0003402646943550907\n",
      "Batch: 24500,train loss is: 0.00022730202585512308\n",
      "test loss is 0.0003104074980982839\n",
      "Batch: 24600,train loss is: 0.000405848337446916\n",
      "test loss is 0.00035433491533462427\n",
      "Batch: 24700,train loss is: 0.00012508189989432592\n",
      "test loss is 0.00027088535390493927\n",
      "Batch: 24800,train loss is: 0.00023755957197662343\n",
      "test loss is 0.0003139489514519352\n",
      "Batch: 24900,train loss is: 0.00032108093596844924\n",
      "test loss is 0.0003110418066256877\n",
      "Batch: 25000,train loss is: 0.0002800079529208341\n",
      "test loss is 0.0003545866842517463\n",
      "Batch: 25100,train loss is: 0.0007081159943199803\n",
      "test loss is 0.0002762669899102271\n",
      "Batch: 25200,train loss is: 0.00027766612164082803\n",
      "test loss is 0.0003032073533625492\n",
      "Batch: 25300,train loss is: 0.0005950758464151202\n",
      "test loss is 0.0003496400846465343\n",
      "Batch: 25400,train loss is: 0.00033809369993864885\n",
      "test loss is 0.00033280166167132876\n",
      "Batch: 25500,train loss is: 0.00029545478395258526\n",
      "test loss is 0.00033479197126890767\n",
      "Batch: 25600,train loss is: 0.0003724777554171108\n",
      "test loss is 0.00033346838595694914\n",
      "Batch: 25700,train loss is: 0.0004475049640388731\n",
      "test loss is 0.00032139491440384623\n",
      "Batch: 25800,train loss is: 0.0001995790081022475\n",
      "test loss is 0.0002827851054762198\n",
      "Batch: 25900,train loss is: 0.000168910088755686\n",
      "test loss is 0.000301867324085668\n",
      "Batch: 26000,train loss is: 0.0002654147742714237\n",
      "test loss is 0.00034104714237182763\n",
      "Batch: 26100,train loss is: 0.00016464173659131045\n",
      "test loss is 0.0002950830757926815\n",
      "Batch: 26200,train loss is: 0.0006925648295222781\n",
      "test loss is 0.00033623771060708264\n",
      "Batch: 26300,train loss is: 0.0002498284439381147\n",
      "test loss is 0.00028355932803025857\n",
      "Batch: 26400,train loss is: 0.00025596342804739376\n",
      "test loss is 0.0003411465854777777\n",
      "Batch: 26500,train loss is: 0.00042554670252238646\n",
      "test loss is 0.00030178208501859525\n",
      "Batch: 26600,train loss is: 0.0002493538451222624\n",
      "test loss is 0.00033808819954603866\n",
      "Batch: 26700,train loss is: 0.0001976388376739096\n",
      "test loss is 0.0002922046907091118\n",
      "Batch: 26800,train loss is: 0.00021443959599670904\n",
      "test loss is 0.00034776515679167615\n",
      "Batch: 26900,train loss is: 0.0002818130979677733\n",
      "test loss is 0.00027476161518647874\n",
      "Batch: 27000,train loss is: 0.00044022222807312965\n",
      "test loss is 0.0003380412269472439\n",
      "Batch: 27100,train loss is: 0.00019089967712894397\n",
      "test loss is 0.0002903752984657325\n",
      "Batch: 27200,train loss is: 0.00022490791450622657\n",
      "test loss is 0.00027962584887603444\n",
      "Batch: 27300,train loss is: 0.0002583168771115213\n",
      "test loss is 0.0003014822570595671\n",
      "Batch: 27400,train loss is: 0.0003593098382099621\n",
      "test loss is 0.0002952116433354805\n",
      "Batch: 27500,train loss is: 0.0004504163714399321\n",
      "test loss is 0.0002741206825255957\n",
      "Batch: 27600,train loss is: 0.00037121415476268447\n",
      "test loss is 0.0003355208504549557\n",
      "Batch: 27700,train loss is: 0.00032726804585789834\n",
      "test loss is 0.00030672510884007876\n",
      "Batch: 27800,train loss is: 0.00024646458979278195\n",
      "test loss is 0.00033751212620993236\n",
      "Batch: 27900,train loss is: 0.00041364406753679496\n",
      "test loss is 0.0002939245377155447\n",
      "Batch: 28000,train loss is: 0.00024655149480666956\n",
      "test loss is 0.00027860501695460565\n",
      "Batch: 28100,train loss is: 0.000787959089421285\n",
      "test loss is 0.00047373618400758835\n",
      "Batch: 28200,train loss is: 0.0003627738147800732\n",
      "test loss is 0.00043059930844000237\n",
      "Batch: 28300,train loss is: 0.00026449909053349326\n",
      "test loss is 0.0003046135669095762\n",
      "Batch: 28400,train loss is: 0.0005043165298246041\n",
      "test loss is 0.00038964437226861255\n",
      "Batch: 28500,train loss is: 0.0003706246402226457\n",
      "test loss is 0.00030284744563051217\n",
      "Batch: 28600,train loss is: 0.00028820165956284214\n",
      "test loss is 0.00029750297515542456\n",
      "Batch: 28700,train loss is: 0.00026433403506427117\n",
      "test loss is 0.0002722877134249515\n",
      "Batch: 28800,train loss is: 0.00033167170639281367\n",
      "test loss is 0.0002962703410792016\n",
      "Batch: 28900,train loss is: 0.0002925909804090085\n",
      "test loss is 0.00030548600151935587\n",
      "Batch: 29000,train loss is: 0.00021210773306304216\n",
      "test loss is 0.0002955492869821005\n",
      "Batch: 29100,train loss is: 0.00027615522828173167\n",
      "test loss is 0.00030868207547518785\n",
      "Batch: 29200,train loss is: 0.0003665544139107418\n",
      "test loss is 0.00033192024263627433\n",
      "Batch: 29300,train loss is: 0.00029696840138173047\n",
      "test loss is 0.00028427280275938226\n",
      "Batch: 29400,train loss is: 0.0002264139245641728\n",
      "test loss is 0.00032737300515150227\n",
      "Batch: 29500,train loss is: 0.00025152777690712005\n",
      "test loss is 0.00034117614592595323\n",
      "Batch: 29600,train loss is: 0.000226848158578109\n",
      "test loss is 0.0002694937490627257\n",
      "Batch: 29700,train loss is: 0.00021590227315921573\n",
      "test loss is 0.0002864668733827231\n",
      "Batch: 29800,train loss is: 0.00026539779021047107\n",
      "test loss is 0.00028331624795769554\n",
      "Batch: 29900,train loss is: 0.0002539071188748036\n",
      "test loss is 0.00030578274538322584\n",
      "Batch: 30000,train loss is: 0.0002595426526516586\n",
      "test loss is 0.0003738548466694743\n",
      "Batch: 30100,train loss is: 0.00035529256309168244\n",
      "test loss is 0.0003128783964617571\n",
      "Batch: 30200,train loss is: 0.000377370532982117\n",
      "test loss is 0.00029064129676175364\n",
      "Batch: 30300,train loss is: 0.0003288768390801214\n",
      "test loss is 0.00027145609965941527\n",
      "Batch: 30400,train loss is: 0.0002513973518011308\n",
      "test loss is 0.00030772916847662064\n",
      "Batch: 30500,train loss is: 0.00038767948761452757\n",
      "test loss is 0.0002973676259915972\n",
      "Batch: 30600,train loss is: 0.0002486798574584553\n",
      "test loss is 0.00029442359033594407\n",
      "Batch: 30700,train loss is: 0.0003978217100517757\n",
      "test loss is 0.00032554983886656536\n",
      "Batch: 30800,train loss is: 0.0003448795409752304\n",
      "test loss is 0.00029474847758642484\n",
      "Batch: 30900,train loss is: 0.00026169714512682264\n",
      "test loss is 0.00027681254361875166\n",
      "Batch: 31000,train loss is: 0.0002736497910675251\n",
      "test loss is 0.00031569314161492565\n",
      "Batch: 31100,train loss is: 0.0004076721311104315\n",
      "test loss is 0.00031243999012126945\n",
      "Batch: 31200,train loss is: 0.0005255643558731852\n",
      "test loss is 0.00044419010449143283\n",
      "Batch: 31300,train loss is: 0.00034162290419965035\n",
      "test loss is 0.0002899380257666274\n",
      "Batch: 31400,train loss is: 0.0009649527513185605\n",
      "test loss is 0.000294180334196318\n",
      "Batch: 31500,train loss is: 0.00032423272121665707\n",
      "test loss is 0.00032925933329864876\n",
      "Batch: 31600,train loss is: 0.0003110243645603317\n",
      "test loss is 0.0002694087752409114\n",
      "Batch: 31700,train loss is: 0.0003799198959214831\n",
      "test loss is 0.0004401939610997251\n",
      "Batch: 31800,train loss is: 0.00032624095624522053\n",
      "test loss is 0.00033672006991934434\n",
      "Batch: 31900,train loss is: 0.0003567002003580849\n",
      "test loss is 0.00030397849679259913\n",
      "Batch: 32000,train loss is: 0.0001889468404516137\n",
      "test loss is 0.0002986001731876305\n",
      "Batch: 32100,train loss is: 0.00039491468685344696\n",
      "test loss is 0.0003385345890465753\n",
      "Batch: 32200,train loss is: 0.000434114624481228\n",
      "test loss is 0.0002891391016328252\n",
      "Batch: 32300,train loss is: 0.0004165440001331341\n",
      "test loss is 0.00033547887358313015\n",
      "Batch: 32400,train loss is: 0.00023300033172502408\n",
      "test loss is 0.0003393971181787507\n",
      "Batch: 32500,train loss is: 0.0003664130512946597\n",
      "test loss is 0.00027004784313011166\n",
      "Batch: 32600,train loss is: 0.00021819806626205068\n",
      "test loss is 0.0003125056716936316\n",
      "Batch: 32700,train loss is: 0.00043386818607542955\n",
      "test loss is 0.00027880208449800894\n",
      "Batch: 32800,train loss is: 0.0002415397157132812\n",
      "test loss is 0.000438809410081162\n",
      "Batch: 32900,train loss is: 0.00028353416942900967\n",
      "test loss is 0.0002838521398330613\n",
      "Batch: 33000,train loss is: 0.00063900528243527\n",
      "test loss is 0.00028436127852794237\n",
      "Batch: 33100,train loss is: 0.0002444649686508711\n",
      "test loss is 0.00033047699926358617\n",
      "Batch: 33200,train loss is: 0.00029170393121170956\n",
      "test loss is 0.00029788246607402374\n",
      "Batch: 33300,train loss is: 0.00024715886338317906\n",
      "test loss is 0.0002885666153381827\n",
      "Batch: 33400,train loss is: 0.00016433271683718818\n",
      "test loss is 0.0002901375180065211\n",
      "Batch: 33500,train loss is: 0.00024308203976753132\n",
      "test loss is 0.0003708760709074407\n",
      "Batch: 33600,train loss is: 0.00044914441967146496\n",
      "test loss is 0.0003185425244430672\n",
      "Batch: 33700,train loss is: 0.00022263885432874598\n",
      "test loss is 0.0002891400873173844\n",
      "Batch: 33800,train loss is: 0.0002266654549104662\n",
      "test loss is 0.00029860321075458445\n",
      "Batch: 33900,train loss is: 0.00025837551876177413\n",
      "test loss is 0.0002976463908001266\n",
      "Batch: 34000,train loss is: 0.00023051453069340672\n",
      "test loss is 0.00030616190882402835\n",
      "Batch: 34100,train loss is: 0.0004010207102240396\n",
      "test loss is 0.0002976187668473035\n",
      "Batch: 34200,train loss is: 0.0003382768070610046\n",
      "test loss is 0.0002813791957678417\n",
      "Batch: 34300,train loss is: 0.00028887389957888256\n",
      "test loss is 0.0003638965512458443\n",
      "Batch: 34400,train loss is: 0.00035539688496599557\n",
      "test loss is 0.0003366472425966746\n",
      "Batch: 34500,train loss is: 0.00017943547488119972\n",
      "test loss is 0.00031755394104171725\n",
      "Batch: 34600,train loss is: 0.00030934848884565523\n",
      "test loss is 0.0003317312202761121\n",
      "Batch: 34700,train loss is: 0.0004018288773578032\n",
      "test loss is 0.0003078161883758793\n",
      "Batch: 34800,train loss is: 0.0005778923114140394\n",
      "test loss is 0.000268656960468473\n",
      "Batch: 34900,train loss is: 0.0002861856466605411\n",
      "test loss is 0.00031306075674010546\n",
      "Batch: 35000,train loss is: 0.0003503665721307197\n",
      "test loss is 0.0003261452088906564\n",
      "Batch: 35100,train loss is: 0.0002170781194048268\n",
      "test loss is 0.00035493976346050345\n",
      "Batch: 35200,train loss is: 0.00021285691087462614\n",
      "test loss is 0.0003991238199290653\n",
      "Batch: 35300,train loss is: 0.00028086199208685275\n",
      "test loss is 0.0003032883890723132\n",
      "Batch: 35400,train loss is: 0.0002980194859399085\n",
      "test loss is 0.00027851344920721237\n",
      "Batch: 35500,train loss is: 0.0005247170982508833\n",
      "test loss is 0.00029917596481251465\n",
      "Batch: 35600,train loss is: 0.00027239527445716753\n",
      "test loss is 0.0003005718401128129\n",
      "Batch: 35700,train loss is: 0.0004230954378115181\n",
      "test loss is 0.00030340709187776023\n",
      "Batch: 35800,train loss is: 0.00017138912067468973\n",
      "test loss is 0.00029642702433962194\n",
      "Batch: 35900,train loss is: 0.00042808698323970444\n",
      "test loss is 0.0003087663372283524\n",
      "Batch: 36000,train loss is: 0.0002674335861735596\n",
      "test loss is 0.00028291716458139884\n",
      "Batch: 36100,train loss is: 0.00031369383828355865\n",
      "test loss is 0.0003200213087725029\n",
      "Batch: 36200,train loss is: 0.00016119224974573987\n",
      "test loss is 0.00029602470812428543\n",
      "Batch: 36300,train loss is: 0.00017085920734370908\n",
      "test loss is 0.00030160750028411076\n",
      "Batch: 36400,train loss is: 0.00017470672672864707\n",
      "test loss is 0.0002976082215763881\n",
      "Batch: 36500,train loss is: 0.0002749574779796369\n",
      "test loss is 0.00027196197205891305\n",
      "Batch: 36600,train loss is: 0.00021442957760567762\n",
      "test loss is 0.00029637061700822767\n",
      "Batch: 36700,train loss is: 0.00025161419215172936\n",
      "test loss is 0.0003884285876671896\n",
      "Batch: 36800,train loss is: 0.00020355571938322005\n",
      "test loss is 0.0003126935470781355\n",
      "Batch: 36900,train loss is: 0.0002583451076151866\n",
      "test loss is 0.000290206955512485\n",
      "Batch: 37000,train loss is: 0.0002502306083165776\n",
      "test loss is 0.00032189403620241734\n",
      "Batch: 37100,train loss is: 0.00020351995371484916\n",
      "test loss is 0.0002762102880185194\n",
      "Batch: 37200,train loss is: 0.0002966449545359216\n",
      "test loss is 0.00029082669087765185\n",
      "Batch: 37300,train loss is: 0.00039447650591721155\n",
      "test loss is 0.0003007497703711388\n",
      "Batch: 37400,train loss is: 0.00044884044833350334\n",
      "test loss is 0.0002948725584866452\n",
      "Batch: 37500,train loss is: 0.0004231695143206464\n",
      "test loss is 0.0003323688005939903\n",
      "Batch: 37600,train loss is: 0.0002654226612732906\n",
      "test loss is 0.0002985979437380347\n",
      "Batch: 37700,train loss is: 0.0002376765867690787\n",
      "test loss is 0.0003103131732760472\n",
      "Batch: 37800,train loss is: 0.0002617460785747437\n",
      "test loss is 0.00031809275423346845\n",
      "Batch: 37900,train loss is: 0.00024398110004620146\n",
      "test loss is 0.00027774865633127657\n",
      "Batch: 38000,train loss is: 0.00022807848824119595\n",
      "test loss is 0.00029727430043921483\n",
      "Batch: 38100,train loss is: 0.0002262397313364021\n",
      "test loss is 0.0002912434164294084\n",
      "Batch: 38200,train loss is: 0.0002992830283601078\n",
      "test loss is 0.0003342939350528236\n",
      "Batch: 38300,train loss is: 0.00030116251301155815\n",
      "test loss is 0.00033218137856524184\n",
      "Batch: 38400,train loss is: 0.0002687035588857473\n",
      "test loss is 0.0002984968739931205\n",
      "Batch: 38500,train loss is: 0.00025998724968058766\n",
      "test loss is 0.00033251690371667597\n",
      "Batch: 38600,train loss is: 0.0002296160507336788\n",
      "test loss is 0.00037934420302646716\n",
      "Batch: 38700,train loss is: 0.00030039828721339625\n",
      "test loss is 0.00028386237983275646\n",
      "Batch: 38800,train loss is: 0.00043964199458792894\n",
      "test loss is 0.0003195293106012454\n",
      "Batch: 38900,train loss is: 0.0002003082283475716\n",
      "test loss is 0.0002718587314517415\n",
      "Batch: 39000,train loss is: 0.0003217199745382465\n",
      "test loss is 0.0002966570520634923\n",
      "Batch: 39100,train loss is: 0.0003124033877617468\n",
      "test loss is 0.00031571849651270534\n",
      "Batch: 39200,train loss is: 0.0004633786309732635\n",
      "test loss is 0.0003485125679728169\n",
      "Batch: 39300,train loss is: 0.0002527177220495836\n",
      "test loss is 0.0002935474306744426\n",
      "Batch: 39400,train loss is: 0.0003506970073140712\n",
      "test loss is 0.00029491589438184184\n",
      "Batch: 39500,train loss is: 0.0002523034166372546\n",
      "test loss is 0.00026955399136935977\n",
      "Batch: 39600,train loss is: 0.0002537177471921476\n",
      "test loss is 0.00030010202377481887\n",
      "Batch: 39700,train loss is: 0.00029313619979024017\n",
      "test loss is 0.0002851746500697085\n",
      "Batch: 39800,train loss is: 0.0005269227681871018\n",
      "test loss is 0.0003400946340312904\n",
      "Batch: 39900,train loss is: 0.0003116947408049714\n",
      "test loss is 0.0003026987375961296\n",
      "Batch: 40000,train loss is: 0.00020385701014354076\n",
      "test loss is 0.00030255845581945946\n",
      "Batch: 40100,train loss is: 0.00021679016992680783\n",
      "test loss is 0.0003073904741505673\n",
      "Batch: 40200,train loss is: 0.00028785189446028144\n",
      "test loss is 0.0002881397956385801\n",
      "Batch: 40300,train loss is: 0.00026418119778205077\n",
      "test loss is 0.0002783176383791316\n",
      "Batch: 40400,train loss is: 0.0002954233826144815\n",
      "test loss is 0.0002857352398110594\n",
      "Batch: 40500,train loss is: 0.0006986136106613773\n",
      "test loss is 0.00029801647032399927\n",
      "Batch: 40600,train loss is: 0.00032378794917736645\n",
      "test loss is 0.0002894019123092577\n",
      "Batch: 40700,train loss is: 0.00029151357332939684\n",
      "test loss is 0.00029775555287493775\n",
      "Batch: 40800,train loss is: 0.0002808546962658969\n",
      "test loss is 0.0003102583830355607\n",
      "Batch: 40900,train loss is: 0.00036852642703514275\n",
      "test loss is 0.00029281269904786445\n",
      "Batch: 41000,train loss is: 0.00018172089196244083\n",
      "test loss is 0.0003012265538462021\n",
      "Batch: 41100,train loss is: 0.00019567049512160718\n",
      "test loss is 0.00028539472800316433\n",
      "Batch: 41200,train loss is: 0.0002670143317549024\n",
      "test loss is 0.00028358830981109094\n",
      "Batch: 41300,train loss is: 0.00017265874422514753\n",
      "test loss is 0.0004002001697926256\n",
      "Batch: 41400,train loss is: 0.0003678106056296829\n",
      "test loss is 0.00038985834751640414\n",
      "Batch: 41500,train loss is: 0.00036601177052174804\n",
      "test loss is 0.00029918570184779363\n",
      "Batch: 41600,train loss is: 0.00028197403823752886\n",
      "test loss is 0.0002796652995653705\n",
      "Batch: 41700,train loss is: 0.0005215607944942322\n",
      "test loss is 0.0002880625918433778\n",
      "Batch: 41800,train loss is: 0.00024240785061911782\n",
      "test loss is 0.0002920492536219322\n",
      "Batch: 41900,train loss is: 0.0003360902876252934\n",
      "test loss is 0.00028916821216504966\n",
      "Batch: 42000,train loss is: 0.00023177941409594923\n",
      "test loss is 0.00031273133048984064\n",
      "Batch: 42100,train loss is: 0.00022486849291780757\n",
      "test loss is 0.00030222550787162495\n",
      "Batch: 42200,train loss is: 0.0006306441846549904\n",
      "test loss is 0.00028707981378163136\n",
      "Batch: 42300,train loss is: 0.00018108426629224645\n",
      "test loss is 0.0003175309017742124\n",
      "Batch: 42400,train loss is: 0.00014847365137755697\n",
      "test loss is 0.0002740289140965212\n",
      "Batch: 42500,train loss is: 0.00023739077007924666\n",
      "test loss is 0.00028118513706713774\n",
      "Batch: 42600,train loss is: 0.0003187593239517532\n",
      "test loss is 0.00040514129580667384\n",
      "Batch: 42700,train loss is: 0.0002231192562940911\n",
      "test loss is 0.00027710079398860095\n",
      "Batch: 42800,train loss is: 0.0006589041331018714\n",
      "test loss is 0.00032501622293424796\n",
      "Batch: 42900,train loss is: 0.00028807469996579055\n",
      "test loss is 0.0003177301556872765\n",
      "Batch: 43000,train loss is: 0.00022765853540176564\n",
      "test loss is 0.0003089587726697476\n",
      "Batch: 43100,train loss is: 0.00020363272261277922\n",
      "test loss is 0.0003323271067372337\n",
      "Batch: 43200,train loss is: 0.0003200000476112724\n",
      "test loss is 0.0002862317011270012\n",
      "Batch: 43300,train loss is: 0.0009404492446048633\n",
      "test loss is 0.0002950206188728236\n",
      "Batch: 43400,train loss is: 0.00023998705296853073\n",
      "test loss is 0.0002846775762681141\n",
      "Batch: 43500,train loss is: 0.00023967567046666012\n",
      "test loss is 0.0003154706303895541\n",
      "Batch: 43600,train loss is: 0.00023959655812046213\n",
      "test loss is 0.00031336561674895753\n",
      "Batch: 43700,train loss is: 0.00015446913242209734\n",
      "test loss is 0.0004374795378657467\n",
      "Batch: 43800,train loss is: 0.0004272211863060434\n",
      "test loss is 0.0002856092193516722\n",
      "Batch: 43900,train loss is: 0.00022754566545989254\n",
      "test loss is 0.00030207317439823775\n",
      "Batch: 44000,train loss is: 0.000286249269490612\n",
      "test loss is 0.0003376064712944617\n",
      "Batch: 44100,train loss is: 0.00024647663374822217\n",
      "test loss is 0.0003052338196828934\n",
      "Batch: 44200,train loss is: 0.00032752230708680635\n",
      "test loss is 0.0002766194927084876\n",
      "Batch: 44300,train loss is: 0.0004284265205287635\n",
      "test loss is 0.00027096977958363314\n",
      "Batch: 44400,train loss is: 0.000244426426349169\n",
      "test loss is 0.0003829448155853586\n",
      "Batch: 44500,train loss is: 0.0004293529906238063\n",
      "test loss is 0.00032888900402257455\n",
      "Batch: 44600,train loss is: 0.0001564680182692941\n",
      "test loss is 0.0003023534142819351\n",
      "Batch: 44700,train loss is: 0.00030503346109179963\n",
      "test loss is 0.00029750265857167864\n",
      "Batch: 44800,train loss is: 0.00021393349153121257\n",
      "test loss is 0.00030493987292787696\n",
      "Batch: 44900,train loss is: 0.0001698821189369067\n",
      "test loss is 0.00028435744799857254\n",
      "Batch: 45000,train loss is: 0.00020321136109293817\n",
      "test loss is 0.00028098137026795505\n",
      "Batch: 45100,train loss is: 0.00021600720391115477\n",
      "test loss is 0.0003318727476233765\n",
      "Batch: 45200,train loss is: 0.00026181949558817016\n",
      "test loss is 0.0002703670468381746\n",
      "Batch: 45300,train loss is: 0.0002707115298007493\n",
      "test loss is 0.00032072054677784265\n",
      "Batch: 45400,train loss is: 0.0002572080535231266\n",
      "test loss is 0.0003475063966970144\n",
      "Batch: 45500,train loss is: 0.0002986353665490795\n",
      "test loss is 0.00028074045471468336\n",
      "Batch: 45600,train loss is: 0.0003007072085607214\n",
      "test loss is 0.0002757888873318073\n",
      "Batch: 45700,train loss is: 0.00040433296977474703\n",
      "test loss is 0.00027387827880836596\n",
      "Batch: 45800,train loss is: 0.00034130644286946445\n",
      "test loss is 0.00028289767677145783\n",
      "Batch: 45900,train loss is: 0.0004795653788679374\n",
      "test loss is 0.000306666928752056\n",
      "Batch: 46000,train loss is: 0.0004992745253702667\n",
      "test loss is 0.0003916801088577057\n",
      "Batch: 46100,train loss is: 0.0002608576404378925\n",
      "test loss is 0.00026893246982085425\n",
      "Batch: 46200,train loss is: 0.0002447285835110286\n",
      "test loss is 0.00028938963550328794\n",
      "Batch: 46300,train loss is: 0.0003277880970530585\n",
      "test loss is 0.00034345283205545736\n",
      "Batch: 46400,train loss is: 0.00031313824576681344\n",
      "test loss is 0.0002947155013551205\n",
      "Batch: 46500,train loss is: 0.00040024736195588237\n",
      "test loss is 0.0002943879853745011\n",
      "Batch: 46600,train loss is: 0.00033393266543519707\n",
      "test loss is 0.00029035188162280693\n",
      "Batch: 46700,train loss is: 0.0002543213782350219\n",
      "test loss is 0.00029107439598523744\n",
      "-----------------------Epoch: 4----------------------------------\n",
      "Batch: 0,train loss is: 0.0003493980100567224\n",
      "test loss is 0.000284905652118274\n",
      "Batch: 100,train loss is: 0.0001860133275112877\n",
      "test loss is 0.00028188616811126825\n",
      "Batch: 200,train loss is: 0.00030137754228781976\n",
      "test loss is 0.000274029872474919\n",
      "Batch: 300,train loss is: 0.0002749321348037237\n",
      "test loss is 0.00030247211707343816\n",
      "Batch: 400,train loss is: 0.00018434463063467596\n",
      "test loss is 0.00028204432251041997\n",
      "Batch: 500,train loss is: 0.0005378678853815442\n",
      "test loss is 0.00030282663514390315\n",
      "Batch: 600,train loss is: 0.00022054971951236658\n",
      "test loss is 0.0002834983197552212\n",
      "Batch: 700,train loss is: 0.00039441607006647007\n",
      "test loss is 0.00032495565386562764\n",
      "Batch: 800,train loss is: 0.00024749770836258994\n",
      "test loss is 0.0002915052462375167\n",
      "Batch: 900,train loss is: 0.00031422953581149756\n",
      "test loss is 0.00028864178748349336\n",
      "Batch: 1000,train loss is: 0.0002731527170544177\n",
      "test loss is 0.00030421627664522073\n",
      "Batch: 1100,train loss is: 0.00027986406067339616\n",
      "test loss is 0.00032320979732583364\n",
      "Batch: 1200,train loss is: 0.000178089523521442\n",
      "test loss is 0.000283838585757206\n",
      "Batch: 1300,train loss is: 0.00018999272035738132\n",
      "test loss is 0.0003084216893927083\n",
      "Batch: 1400,train loss is: 0.00020348151490980374\n",
      "test loss is 0.0002802637335244384\n",
      "Batch: 1500,train loss is: 0.00034701058635124417\n",
      "test loss is 0.00031360910016631595\n",
      "Batch: 1600,train loss is: 0.00042651347710286183\n",
      "test loss is 0.000302948653914524\n",
      "Batch: 1700,train loss is: 0.00028960118653301327\n",
      "test loss is 0.0002889616436483962\n",
      "Batch: 1800,train loss is: 0.0002881896874279968\n",
      "test loss is 0.0003137140448106716\n",
      "Batch: 1900,train loss is: 0.00014580604823809744\n",
      "test loss is 0.0002725842803282154\n",
      "Batch: 2000,train loss is: 0.00026100009731453756\n",
      "test loss is 0.0003196555083432627\n",
      "Batch: 2100,train loss is: 0.0002557884367256559\n",
      "test loss is 0.0003229608488680981\n",
      "Batch: 2200,train loss is: 0.0003159621310288512\n",
      "test loss is 0.0003419959004453414\n",
      "Batch: 2300,train loss is: 0.0001954216690150982\n",
      "test loss is 0.0002806039227929099\n",
      "Batch: 2400,train loss is: 0.0002555907641418157\n",
      "test loss is 0.00028252947743953046\n",
      "Batch: 2500,train loss is: 0.0003144075147755258\n",
      "test loss is 0.00027576340052570987\n",
      "Batch: 2600,train loss is: 0.00031415107274642394\n",
      "test loss is 0.0002931916304046154\n",
      "Batch: 2700,train loss is: 0.0004696345627408061\n",
      "test loss is 0.0003297720758582804\n",
      "Batch: 2800,train loss is: 0.0002105061790821172\n",
      "test loss is 0.0003024813904965049\n",
      "Batch: 2900,train loss is: 0.0004192161149499397\n",
      "test loss is 0.000305964255659702\n",
      "Batch: 3000,train loss is: 0.000252734532960008\n",
      "test loss is 0.00029370251057057824\n",
      "Batch: 3100,train loss is: 0.0002510629726394746\n",
      "test loss is 0.0003327233558981437\n",
      "Batch: 3200,train loss is: 0.0003938186969357808\n",
      "test loss is 0.00033246433946492447\n",
      "Batch: 3300,train loss is: 0.0002837155988806003\n",
      "test loss is 0.000329916491096211\n",
      "Batch: 3400,train loss is: 0.00030468684347015275\n",
      "test loss is 0.0002659899841363622\n",
      "Batch: 3500,train loss is: 0.00025763084944507706\n",
      "test loss is 0.00031005481364922973\n",
      "Batch: 3600,train loss is: 0.00025402348082388687\n",
      "test loss is 0.0003100855059538635\n",
      "Batch: 3700,train loss is: 0.0002199996215801054\n",
      "test loss is 0.00029210189291565647\n",
      "Batch: 3800,train loss is: 0.00026198673179886475\n",
      "test loss is 0.0003091003970097044\n",
      "Batch: 3900,train loss is: 0.00030051613545367765\n",
      "test loss is 0.0002767674097551676\n",
      "Batch: 4000,train loss is: 0.0003679892188508067\n",
      "test loss is 0.00039161961679203634\n",
      "Batch: 4100,train loss is: 0.000363589207160747\n",
      "test loss is 0.0002806711939559237\n",
      "Batch: 4200,train loss is: 0.00029491997699950613\n",
      "test loss is 0.0002871137096408704\n",
      "Batch: 4300,train loss is: 0.00030361486547492774\n",
      "test loss is 0.0003449199916559641\n",
      "Batch: 4400,train loss is: 0.0002880433306406368\n",
      "test loss is 0.000317547155282578\n",
      "Batch: 4500,train loss is: 0.0001904160264317544\n",
      "test loss is 0.0002790457025091941\n",
      "Batch: 4600,train loss is: 0.0003319344321246012\n",
      "test loss is 0.00030528414480937105\n",
      "Batch: 4700,train loss is: 0.0002296325024955472\n",
      "test loss is 0.0003231044784103551\n",
      "Batch: 4800,train loss is: 0.0004717458736203106\n",
      "test loss is 0.00027891833129514655\n",
      "Batch: 4900,train loss is: 0.000520843050311696\n",
      "test loss is 0.00029660663377207826\n",
      "Batch: 5000,train loss is: 0.00030603754861860985\n",
      "test loss is 0.0003450614131041233\n",
      "Batch: 5100,train loss is: 0.00020555818885831362\n",
      "test loss is 0.00028928901361038303\n",
      "Batch: 5200,train loss is: 0.0003031926682317594\n",
      "test loss is 0.00037625212418542674\n",
      "Batch: 5300,train loss is: 0.0002902800040891334\n",
      "test loss is 0.0002751531545603547\n",
      "Batch: 5400,train loss is: 0.00027941628565790255\n",
      "test loss is 0.0002805764329512044\n",
      "Batch: 5500,train loss is: 0.00023611694857221885\n",
      "test loss is 0.0002706551163736698\n",
      "Batch: 5600,train loss is: 0.000188333599765583\n",
      "test loss is 0.00028978024475358096\n",
      "Batch: 5700,train loss is: 0.00023247696609550678\n",
      "test loss is 0.00030430651553582416\n",
      "Batch: 5800,train loss is: 0.00013727484229063085\n",
      "test loss is 0.000335778997254148\n",
      "Batch: 5900,train loss is: 0.00026421621033582006\n",
      "test loss is 0.0002834438105541974\n",
      "Batch: 6000,train loss is: 0.0003179743387974988\n",
      "test loss is 0.00043441435427469887\n",
      "Batch: 6100,train loss is: 0.0004516063768448135\n",
      "test loss is 0.0003546687209894053\n",
      "Batch: 6200,train loss is: 0.00039233348501799053\n",
      "test loss is 0.00032815805283156323\n",
      "Batch: 6300,train loss is: 0.000383693638234877\n",
      "test loss is 0.0002908556748122552\n",
      "Batch: 6400,train loss is: 0.0006096116414340171\n",
      "test loss is 0.0003343729207788548\n",
      "Batch: 6500,train loss is: 0.0003339203172826911\n",
      "test loss is 0.0003072973466186266\n",
      "Batch: 6600,train loss is: 0.00028185166197186335\n",
      "test loss is 0.00032595732597034803\n",
      "Batch: 6700,train loss is: 0.0003548240140932984\n",
      "test loss is 0.00028372787209393445\n",
      "Batch: 6800,train loss is: 0.00040791833697282427\n",
      "test loss is 0.0003605668954117509\n",
      "Batch: 6900,train loss is: 0.0003267207842726099\n",
      "test loss is 0.0003729555722688052\n",
      "Batch: 7000,train loss is: 0.00021924463105988968\n",
      "test loss is 0.00029312746650185306\n",
      "Batch: 7100,train loss is: 0.0003831126377505632\n",
      "test loss is 0.00027905053351931817\n",
      "Batch: 7200,train loss is: 0.00021374141881689288\n",
      "test loss is 0.00027297318440528046\n",
      "Batch: 7300,train loss is: 0.00024354477326005963\n",
      "test loss is 0.00030110184414224505\n",
      "Batch: 7400,train loss is: 0.00022856589764836348\n",
      "test loss is 0.00028453658381829677\n",
      "Batch: 7500,train loss is: 0.00032100481055985096\n",
      "test loss is 0.00028165165453708713\n",
      "Batch: 7600,train loss is: 0.0002894980361219335\n",
      "test loss is 0.0003900460749908301\n",
      "Batch: 7700,train loss is: 0.0002132658598983965\n",
      "test loss is 0.00030742939035217876\n",
      "Batch: 7800,train loss is: 0.00020762854761476236\n",
      "test loss is 0.00027172309810733587\n",
      "Batch: 7900,train loss is: 0.00026812766471785547\n",
      "test loss is 0.00031120844756848834\n",
      "Batch: 8000,train loss is: 0.00026023166292161364\n",
      "test loss is 0.0002741988302557337\n",
      "Batch: 8100,train loss is: 0.0002594268651581446\n",
      "test loss is 0.0003029866441208894\n",
      "Batch: 8200,train loss is: 0.00022510507252600722\n",
      "test loss is 0.00026871647822669073\n",
      "Batch: 8300,train loss is: 0.00029216243612254384\n",
      "test loss is 0.00029833537161190097\n",
      "Batch: 8400,train loss is: 0.0003024948052753069\n",
      "test loss is 0.0002711446264884921\n",
      "Batch: 8500,train loss is: 0.0003418577348572356\n",
      "test loss is 0.00041444394650498844\n",
      "Batch: 8600,train loss is: 0.0003002585223442167\n",
      "test loss is 0.0003007784755375148\n",
      "Batch: 8700,train loss is: 0.0003073909916140026\n",
      "test loss is 0.000322566378220229\n",
      "Batch: 8800,train loss is: 0.00026145720318522753\n",
      "test loss is 0.00029204433880691876\n",
      "Batch: 8900,train loss is: 0.0005427053686925518\n",
      "test loss is 0.0003714074828312006\n",
      "Batch: 9000,train loss is: 0.0002194058663716155\n",
      "test loss is 0.00026982337179566276\n",
      "Batch: 9100,train loss is: 0.0002645625167882063\n",
      "test loss is 0.00027573358450509917\n",
      "Batch: 9200,train loss is: 0.00023381059479378138\n",
      "test loss is 0.00031800882823154194\n",
      "Batch: 9300,train loss is: 0.00021655845898693691\n",
      "test loss is 0.0003326332264743164\n",
      "Batch: 9400,train loss is: 0.00029114775459368534\n",
      "test loss is 0.00038929031804651125\n",
      "Batch: 9500,train loss is: 0.00021989747196076294\n",
      "test loss is 0.00026943598106763544\n",
      "Batch: 9600,train loss is: 0.0002228645911184729\n",
      "test loss is 0.00033690998694443637\n",
      "Batch: 9700,train loss is: 0.0003140101077572265\n",
      "test loss is 0.00028713038259766993\n",
      "Batch: 9800,train loss is: 0.00016316587119447282\n",
      "test loss is 0.00026869329284180075\n",
      "Batch: 9900,train loss is: 0.00030325831623823963\n",
      "test loss is 0.00026865702264479484\n",
      "Batch: 10000,train loss is: 0.00031098152763747066\n",
      "test loss is 0.0003144548988513856\n",
      "Batch: 10100,train loss is: 0.00034641930621363127\n",
      "test loss is 0.0004156237695874319\n",
      "Batch: 10200,train loss is: 0.00035494373899658087\n",
      "test loss is 0.00027549036319179834\n",
      "Batch: 10300,train loss is: 0.00025765624828105407\n",
      "test loss is 0.00030952354264085296\n",
      "Batch: 10400,train loss is: 0.0002335381063397008\n",
      "test loss is 0.0003493874113069223\n",
      "Batch: 10500,train loss is: 0.0002968548810087147\n",
      "test loss is 0.0003119721903857202\n",
      "Batch: 10600,train loss is: 0.0003480287878883499\n",
      "test loss is 0.00031677305791063593\n",
      "Batch: 10700,train loss is: 0.00024890835450796855\n",
      "test loss is 0.00028658838416576403\n",
      "Batch: 10800,train loss is: 0.0003350628117580486\n",
      "test loss is 0.0002899464315891823\n",
      "Batch: 10900,train loss is: 0.0002278820225634749\n",
      "test loss is 0.00030642316726305026\n",
      "Batch: 11000,train loss is: 0.00041612027920908713\n",
      "test loss is 0.0003047074506126055\n",
      "Batch: 11100,train loss is: 0.0003742573525431213\n",
      "test loss is 0.0003695924504050559\n",
      "Batch: 11200,train loss is: 0.000301537987532556\n",
      "test loss is 0.0002999310578827502\n",
      "Batch: 11300,train loss is: 0.00015819059082288378\n",
      "test loss is 0.00029020499941657425\n",
      "Batch: 11400,train loss is: 0.000340389188796025\n",
      "test loss is 0.00029155904706260603\n",
      "Batch: 11500,train loss is: 0.0002721154554925057\n",
      "test loss is 0.00027195853890555927\n",
      "Batch: 11600,train loss is: 0.00032166536580124493\n",
      "test loss is 0.0002859236075184883\n",
      "Batch: 11700,train loss is: 0.0002634175240804189\n",
      "test loss is 0.00026217909216094204\n",
      "Batch: 11800,train loss is: 0.00018147133788811576\n",
      "test loss is 0.00031171559494110447\n",
      "Batch: 11900,train loss is: 0.00036265945336520105\n",
      "test loss is 0.0002905525926511751\n",
      "Batch: 12000,train loss is: 0.0003668255566264631\n",
      "test loss is 0.00028559142720266297\n",
      "Batch: 12100,train loss is: 0.00027952499869844964\n",
      "test loss is 0.00030920702293709123\n",
      "Batch: 12200,train loss is: 0.0002224120552635611\n",
      "test loss is 0.000316311893090546\n",
      "Batch: 12300,train loss is: 0.0002399168985611219\n",
      "test loss is 0.00029621877832710405\n",
      "Batch: 12400,train loss is: 0.0002310882250576386\n",
      "test loss is 0.00028847555459628784\n",
      "Batch: 12500,train loss is: 0.0002563132369820852\n",
      "test loss is 0.0003340711682398603\n",
      "Batch: 12600,train loss is: 0.00022484704880026008\n",
      "test loss is 0.0003289303987087456\n",
      "Batch: 12700,train loss is: 0.0001864560913057252\n",
      "test loss is 0.0003580352349798244\n",
      "Batch: 12800,train loss is: 0.00036258917413960635\n",
      "test loss is 0.0002795504764001773\n",
      "Batch: 12900,train loss is: 0.000322469332947815\n",
      "test loss is 0.00032279473447452775\n",
      "Batch: 13000,train loss is: 0.00032573904757861945\n",
      "test loss is 0.00027338853908403973\n",
      "Batch: 13100,train loss is: 0.0002696184288400555\n",
      "test loss is 0.0003090857470727345\n",
      "Batch: 13200,train loss is: 0.00023651283298627834\n",
      "test loss is 0.00031598290346490995\n",
      "Batch: 13300,train loss is: 0.00024397516964357217\n",
      "test loss is 0.0002982636050641685\n",
      "Batch: 13400,train loss is: 0.0003158578825363256\n",
      "test loss is 0.00029185664784630693\n",
      "Batch: 13500,train loss is: 0.0002951395750000054\n",
      "test loss is 0.0003224096842366964\n",
      "Batch: 13600,train loss is: 0.0002476617805901228\n",
      "test loss is 0.0003195305741943014\n",
      "Batch: 13700,train loss is: 0.0001999935407376299\n",
      "test loss is 0.00029473063662422897\n",
      "Batch: 13800,train loss is: 0.0002488771716579381\n",
      "test loss is 0.00032256783888237264\n",
      "Batch: 13900,train loss is: 0.0003352329823509294\n",
      "test loss is 0.0002845313925630521\n",
      "Batch: 14000,train loss is: 0.00018139561406923724\n",
      "test loss is 0.0002766582305955151\n",
      "Batch: 14100,train loss is: 0.0002616930835454646\n",
      "test loss is 0.00032357626114629085\n",
      "Batch: 14200,train loss is: 0.0002958334923680144\n",
      "test loss is 0.0003388216376226814\n",
      "Batch: 14300,train loss is: 0.0004343811274044471\n",
      "test loss is 0.0003664034071531836\n",
      "Batch: 14400,train loss is: 0.0003521512954902646\n",
      "test loss is 0.0002888709300230537\n",
      "Batch: 14500,train loss is: 0.00028029933834754477\n",
      "test loss is 0.0004118339993340001\n",
      "Batch: 14600,train loss is: 0.00029064546418311487\n",
      "test loss is 0.00031284097268225114\n",
      "Batch: 14700,train loss is: 0.0001894726984268673\n",
      "test loss is 0.00027848595249770454\n",
      "Batch: 14800,train loss is: 0.0002658067319505644\n",
      "test loss is 0.0002884443611592947\n",
      "Batch: 14900,train loss is: 0.000335222966902713\n",
      "test loss is 0.0003109680280166276\n",
      "Batch: 15000,train loss is: 0.0002936781983276902\n",
      "test loss is 0.0002843428765208333\n",
      "Batch: 15100,train loss is: 0.00023491805432800678\n",
      "test loss is 0.0002949880851572377\n",
      "Batch: 15200,train loss is: 0.0003762195572014255\n",
      "test loss is 0.00033508291331179463\n",
      "Batch: 15300,train loss is: 0.0005192312946666162\n",
      "test loss is 0.00027395534833136325\n",
      "Batch: 15400,train loss is: 0.000352364781825681\n",
      "test loss is 0.00028022348185115177\n",
      "Batch: 15500,train loss is: 0.00016867895865837107\n",
      "test loss is 0.00029940303905160885\n",
      "Batch: 15600,train loss is: 0.00023689754431666405\n",
      "test loss is 0.0003366985690972077\n",
      "Batch: 15700,train loss is: 0.00030794206977784774\n",
      "test loss is 0.00034188197527846133\n",
      "Batch: 15800,train loss is: 0.0003480231393145572\n",
      "test loss is 0.00029566052010164496\n",
      "Batch: 15900,train loss is: 0.0001870263408774103\n",
      "test loss is 0.000288123104623861\n",
      "Batch: 16000,train loss is: 0.00018629750279679696\n",
      "test loss is 0.0002635105948867015\n",
      "Batch: 16100,train loss is: 0.00032440129598481054\n",
      "test loss is 0.00027492354516282016\n",
      "Batch: 16200,train loss is: 0.00019863761272303316\n",
      "test loss is 0.0002876874655960758\n",
      "Batch: 16300,train loss is: 0.00022191635193343601\n",
      "test loss is 0.0002769110108626013\n",
      "Batch: 16400,train loss is: 0.00026510663090789107\n",
      "test loss is 0.00027626872001617654\n",
      "Batch: 16500,train loss is: 0.00017181030497002438\n",
      "test loss is 0.00028185303042155594\n",
      "Batch: 16600,train loss is: 0.0003080524409396551\n",
      "test loss is 0.00031921328864031493\n",
      "Batch: 16700,train loss is: 0.0008389468184471314\n",
      "test loss is 0.00031460325766359906\n",
      "Batch: 16800,train loss is: 0.00047664932127384606\n",
      "test loss is 0.0003150919765547683\n",
      "Batch: 16900,train loss is: 0.00017874244902066404\n",
      "test loss is 0.00030351340528924476\n",
      "Batch: 17000,train loss is: 0.00023841427086461809\n",
      "test loss is 0.0002992598090024209\n",
      "Batch: 17100,train loss is: 0.000446251824209872\n",
      "test loss is 0.00037025415268943557\n",
      "Batch: 17200,train loss is: 0.00039210160777484137\n",
      "test loss is 0.0004214236852315193\n",
      "Batch: 17300,train loss is: 0.0002534778828015773\n",
      "test loss is 0.00031086750039479487\n",
      "Batch: 17400,train loss is: 0.0003031323923468551\n",
      "test loss is 0.0002908493038857487\n",
      "Batch: 17500,train loss is: 0.0004684455154514242\n",
      "test loss is 0.00030489235957855234\n",
      "Batch: 17600,train loss is: 0.00026497359830114067\n",
      "test loss is 0.0002715594319864767\n",
      "Batch: 17700,train loss is: 0.0004361294159624057\n",
      "test loss is 0.0002693940879031787\n",
      "Batch: 17800,train loss is: 0.00025137257015886915\n",
      "test loss is 0.0002705221184803193\n",
      "Batch: 17900,train loss is: 0.0003478221141901014\n",
      "test loss is 0.00036261426744624184\n",
      "Batch: 18000,train loss is: 0.0002181537127399067\n",
      "test loss is 0.0003206360482784047\n",
      "Batch: 18100,train loss is: 0.00021148116905532038\n",
      "test loss is 0.00035094789975527934\n",
      "Batch: 18200,train loss is: 0.0002713620065296389\n",
      "test loss is 0.00028540195108510516\n",
      "Batch: 18300,train loss is: 0.0003366001312556413\n",
      "test loss is 0.0003361832775597429\n",
      "Batch: 18400,train loss is: 0.0002953081463092839\n",
      "test loss is 0.00029725766635308636\n",
      "Batch: 18500,train loss is: 0.00020175998650842666\n",
      "test loss is 0.000277164872410744\n",
      "Batch: 18600,train loss is: 0.0003618959067245743\n",
      "test loss is 0.00034754435248357415\n",
      "Batch: 18700,train loss is: 0.00018461693985586083\n",
      "test loss is 0.0003030418793361703\n",
      "Batch: 18800,train loss is: 0.00027059232903465895\n",
      "test loss is 0.0002739129964569417\n",
      "Batch: 18900,train loss is: 0.0004129333734333573\n",
      "test loss is 0.0002855460836123324\n",
      "Batch: 19000,train loss is: 0.00033867653014303775\n",
      "test loss is 0.00028779178472116\n",
      "Batch: 19100,train loss is: 0.0002086737683842411\n",
      "test loss is 0.00029996732871638386\n",
      "Batch: 19200,train loss is: 0.00020487647963507463\n",
      "test loss is 0.0003071335917591723\n",
      "Batch: 19300,train loss is: 0.00037212438686377923\n",
      "test loss is 0.0003448684194690146\n",
      "Batch: 19400,train loss is: 0.00027830232426931476\n",
      "test loss is 0.00032579467470412625\n",
      "Batch: 19500,train loss is: 0.0003536260296491321\n",
      "test loss is 0.0002894757203544551\n",
      "Batch: 19600,train loss is: 0.0003465660025129304\n",
      "test loss is 0.000323191006127119\n",
      "Batch: 19700,train loss is: 0.0001882592234212043\n",
      "test loss is 0.00029773000225060184\n",
      "Batch: 19800,train loss is: 0.0002706630304879171\n",
      "test loss is 0.000294210469411151\n",
      "Batch: 19900,train loss is: 0.0002208849182145393\n",
      "test loss is 0.0002873356266877949\n",
      "Batch: 20000,train loss is: 0.0001984266964017568\n",
      "test loss is 0.0002909517268079449\n",
      "Batch: 20100,train loss is: 0.0003357975334347047\n",
      "test loss is 0.00032710694390524305\n",
      "Batch: 20200,train loss is: 0.00021772409296068844\n",
      "test loss is 0.00028127373893261677\n",
      "Batch: 20300,train loss is: 0.00033180254731879236\n",
      "test loss is 0.00029542861615774514\n",
      "Batch: 20400,train loss is: 0.000298777374836384\n",
      "test loss is 0.00031090997780938945\n",
      "Batch: 20500,train loss is: 0.0002989413270343226\n",
      "test loss is 0.0002740197811777088\n",
      "Batch: 20600,train loss is: 0.000281705008286139\n",
      "test loss is 0.0003576196379390984\n",
      "Batch: 20700,train loss is: 0.0002725871796828227\n",
      "test loss is 0.00028713530347198864\n",
      "Batch: 20800,train loss is: 0.0003455412025508588\n",
      "test loss is 0.0003603546852741361\n",
      "Batch: 20900,train loss is: 0.0003085669401186988\n",
      "test loss is 0.0003110305712081122\n",
      "Batch: 21000,train loss is: 0.00018549087430109735\n",
      "test loss is 0.00027766138597236297\n",
      "Batch: 21100,train loss is: 0.00018762509033607523\n",
      "test loss is 0.000268247517495388\n",
      "Batch: 21200,train loss is: 0.0003093696643765598\n",
      "test loss is 0.00031782767051276013\n",
      "Batch: 21300,train loss is: 0.00023953856031465513\n",
      "test loss is 0.00033809434256340445\n",
      "Batch: 21400,train loss is: 0.0002652427877429636\n",
      "test loss is 0.00038010381258182953\n",
      "Batch: 21500,train loss is: 0.00021870943219219895\n",
      "test loss is 0.00031277457568103104\n",
      "Batch: 21600,train loss is: 0.00024523822382044444\n",
      "test loss is 0.0002964766687434501\n",
      "Batch: 21700,train loss is: 0.00022159321408389971\n",
      "test loss is 0.00032746782698467856\n",
      "Batch: 21800,train loss is: 0.00030514198029181754\n",
      "test loss is 0.000296026141308247\n",
      "Batch: 21900,train loss is: 0.0003078017957889974\n",
      "test loss is 0.0004000418982626125\n",
      "Batch: 22000,train loss is: 0.0002209484966179609\n",
      "test loss is 0.00029815730963788365\n",
      "Batch: 22100,train loss is: 0.00023337808901078633\n",
      "test loss is 0.0003126072336065353\n",
      "Batch: 22200,train loss is: 0.00028744106530995665\n",
      "test loss is 0.0002868439676371526\n",
      "Batch: 22300,train loss is: 0.0003419897080919821\n",
      "test loss is 0.00031085038487823377\n",
      "Batch: 22400,train loss is: 0.00038459055347814675\n",
      "test loss is 0.00032887888646551303\n",
      "Batch: 22500,train loss is: 0.0002825246790433124\n",
      "test loss is 0.0003170303009667761\n",
      "Batch: 22600,train loss is: 0.00027136847619228994\n",
      "test loss is 0.000306179778000991\n",
      "Batch: 22700,train loss is: 0.00028789511546482307\n",
      "test loss is 0.00028975682651091153\n",
      "Batch: 22800,train loss is: 0.00022456329735349068\n",
      "test loss is 0.00033916651569548427\n",
      "Batch: 22900,train loss is: 0.00021666579489415947\n",
      "test loss is 0.00029161693025979353\n",
      "Batch: 23000,train loss is: 0.00022791649605302726\n",
      "test loss is 0.00027291096885749387\n",
      "Batch: 23100,train loss is: 0.0002351846008551222\n",
      "test loss is 0.00028429357133730247\n",
      "Batch: 23200,train loss is: 0.00025331104601710716\n",
      "test loss is 0.0002737438776633162\n",
      "Batch: 23300,train loss is: 0.00041562372290146336\n",
      "test loss is 0.0003307016869722276\n",
      "Batch: 23400,train loss is: 0.00027620842629170995\n",
      "test loss is 0.00029981582417103767\n",
      "Batch: 23500,train loss is: 0.00028921919944988555\n",
      "test loss is 0.00029665674414105283\n",
      "Batch: 23600,train loss is: 0.0003900901022173641\n",
      "test loss is 0.00029302067895167554\n",
      "Batch: 23700,train loss is: 0.0002425889431801043\n",
      "test loss is 0.00028283791681303134\n",
      "Batch: 23800,train loss is: 0.0003160688756096595\n",
      "test loss is 0.00027741243759141844\n",
      "Batch: 23900,train loss is: 0.00032752372065229877\n",
      "test loss is 0.000272449585607635\n",
      "Batch: 24000,train loss is: 0.00026986774502548523\n",
      "test loss is 0.00028467299991047\n",
      "Batch: 24100,train loss is: 0.00018042751439830828\n",
      "test loss is 0.00029976702489931145\n",
      "Batch: 24200,train loss is: 0.00033727839622540784\n",
      "test loss is 0.0003807367144915312\n",
      "Batch: 24300,train loss is: 0.0003271273684886098\n",
      "test loss is 0.00029480339156426463\n",
      "Batch: 24400,train loss is: 0.0002756280056715058\n",
      "test loss is 0.00035403367900104996\n",
      "Batch: 24500,train loss is: 0.0003021979515281217\n",
      "test loss is 0.00029778734447061245\n",
      "Batch: 24600,train loss is: 0.0010647107616536527\n",
      "test loss is 0.00028267318472976945\n",
      "Batch: 24700,train loss is: 0.00019842161744140514\n",
      "test loss is 0.00033354120740678193\n",
      "Batch: 24800,train loss is: 0.0003420243259928108\n",
      "test loss is 0.0002825305373215243\n",
      "Batch: 24900,train loss is: 0.00019343101976499914\n",
      "test loss is 0.00031331217449013306\n",
      "Batch: 25000,train loss is: 0.00019255980980771229\n",
      "test loss is 0.00033721545431588605\n",
      "Batch: 25100,train loss is: 0.00038491848308016543\n",
      "test loss is 0.00029734798323238895\n",
      "Batch: 25200,train loss is: 0.00032969216940857823\n",
      "test loss is 0.0002826315360645788\n",
      "Batch: 25300,train loss is: 0.00022174177935891488\n",
      "test loss is 0.0002738082173544442\n",
      "Batch: 25400,train loss is: 0.0004335369158797671\n",
      "test loss is 0.0002777700268179051\n",
      "Batch: 25500,train loss is: 0.0002820771596789326\n",
      "test loss is 0.0002908611499862704\n",
      "Batch: 25600,train loss is: 0.00020766595513210215\n",
      "test loss is 0.0002845084399496099\n",
      "Batch: 25700,train loss is: 0.00024352803700357484\n",
      "test loss is 0.00027589966959833557\n",
      "Batch: 25800,train loss is: 0.00024342955751825411\n",
      "test loss is 0.0002882027876028564\n",
      "Batch: 25900,train loss is: 0.0002294406240707905\n",
      "test loss is 0.00027146953191653397\n",
      "Batch: 26000,train loss is: 0.0002122518921744327\n",
      "test loss is 0.00029512618731912646\n",
      "Batch: 26100,train loss is: 0.00029484194902486656\n",
      "test loss is 0.00027063132449186363\n",
      "Batch: 26200,train loss is: 0.0002531110078711155\n",
      "test loss is 0.000329572819873396\n",
      "Batch: 26300,train loss is: 0.0003205845818617777\n",
      "test loss is 0.0003162875853335979\n",
      "Batch: 26400,train loss is: 0.00019169671948415833\n",
      "test loss is 0.00034364431243902267\n",
      "Batch: 26500,train loss is: 0.00026376728211406717\n",
      "test loss is 0.0002839580483970263\n",
      "Batch: 26600,train loss is: 0.0005205967905922148\n",
      "test loss is 0.00035009006506827044\n",
      "Batch: 26700,train loss is: 0.00019053080906609424\n",
      "test loss is 0.0002864796575553439\n",
      "Batch: 26800,train loss is: 0.00023460732139628514\n",
      "test loss is 0.0003514414913228977\n",
      "Batch: 26900,train loss is: 0.00024266918547825473\n",
      "test loss is 0.000284976798949998\n",
      "Batch: 27000,train loss is: 0.0005147993812942965\n",
      "test loss is 0.0002825747829931413\n",
      "Batch: 27100,train loss is: 0.00032367097142179735\n",
      "test loss is 0.00030912211751821933\n",
      "Batch: 27200,train loss is: 0.00028874047240290174\n",
      "test loss is 0.0003327116752995057\n",
      "Batch: 27300,train loss is: 0.00020323690691742053\n",
      "test loss is 0.0002635773175723192\n",
      "Batch: 27400,train loss is: 0.0002671732357096146\n",
      "test loss is 0.000317717178391118\n",
      "Batch: 27500,train loss is: 0.000242019957702017\n",
      "test loss is 0.00028391763077450567\n",
      "Batch: 27600,train loss is: 0.0003110709628232442\n",
      "test loss is 0.00042285071872859574\n",
      "Batch: 27700,train loss is: 0.00032157049177535446\n",
      "test loss is 0.000294719802975667\n",
      "Batch: 27800,train loss is: 0.00023788292931026776\n",
      "test loss is 0.00028887931681755236\n",
      "Batch: 27900,train loss is: 0.00018233511591711992\n",
      "test loss is 0.00028116450658243185\n",
      "Batch: 28000,train loss is: 0.00015468492675447778\n",
      "test loss is 0.0002618704452784674\n",
      "Batch: 28100,train loss is: 0.0002548668094905847\n",
      "test loss is 0.00026361084754240004\n",
      "Batch: 28200,train loss is: 0.000266337426203668\n",
      "test loss is 0.00027907628843745915\n",
      "Batch: 28300,train loss is: 0.0002479650386505996\n",
      "test loss is 0.00029336307014437435\n",
      "Batch: 28400,train loss is: 0.00038143454548334013\n",
      "test loss is 0.00029463913927625846\n",
      "Batch: 28500,train loss is: 0.0002920872051083738\n",
      "test loss is 0.0002985884801898104\n",
      "Batch: 28600,train loss is: 0.0002945185221238587\n",
      "test loss is 0.00031224784755467123\n",
      "Batch: 28700,train loss is: 0.00042999563478747765\n",
      "test loss is 0.0003345639104242549\n",
      "Batch: 28800,train loss is: 0.0002752983582927715\n",
      "test loss is 0.0002646342071621775\n",
      "Batch: 28900,train loss is: 0.0004670584029272528\n",
      "test loss is 0.00028390890616320776\n",
      "Batch: 29000,train loss is: 0.0002831365645395907\n",
      "test loss is 0.0003090492948114266\n",
      "Batch: 29100,train loss is: 0.00023695101024110912\n",
      "test loss is 0.00028713354668717645\n",
      "Batch: 29200,train loss is: 0.0003386221951171164\n",
      "test loss is 0.00029766821169382047\n",
      "Batch: 29300,train loss is: 0.00020589106778658097\n",
      "test loss is 0.00032030405575044526\n",
      "Batch: 29400,train loss is: 0.0002664596438746709\n",
      "test loss is 0.00029653619826226313\n",
      "Batch: 29500,train loss is: 0.0002322848529206696\n",
      "test loss is 0.00032029238407773773\n",
      "Batch: 29600,train loss is: 0.0002008918938061622\n",
      "test loss is 0.0002750766588241058\n",
      "Batch: 29700,train loss is: 0.0003245577631262395\n",
      "test loss is 0.0003034851790665177\n",
      "Batch: 29800,train loss is: 0.0002702393107607921\n",
      "test loss is 0.000283475390188215\n",
      "Batch: 29900,train loss is: 0.00041713216579553883\n",
      "test loss is 0.00027598372978415443\n",
      "Batch: 30000,train loss is: 0.00023849248138442584\n",
      "test loss is 0.0002872397867304547\n",
      "Batch: 30100,train loss is: 0.00025805017475210115\n",
      "test loss is 0.00029249753267196596\n",
      "Batch: 30200,train loss is: 0.00017785156338116887\n",
      "test loss is 0.0002752070937871055\n",
      "Batch: 30300,train loss is: 0.0006252282360545789\n",
      "test loss is 0.0002916913123035847\n",
      "Batch: 30400,train loss is: 0.00034516730331917247\n",
      "test loss is 0.0003238792419120986\n",
      "Batch: 30500,train loss is: 0.0002896460079484806\n",
      "test loss is 0.00027622143110227074\n",
      "Batch: 30600,train loss is: 0.00022769117132422529\n",
      "test loss is 0.00026775608062957416\n",
      "Batch: 30700,train loss is: 0.00026796338203864467\n",
      "test loss is 0.0002778263034652418\n",
      "Batch: 30800,train loss is: 0.00030350309650185674\n",
      "test loss is 0.00035004340836769824\n",
      "Batch: 30900,train loss is: 0.00020783283751179892\n",
      "test loss is 0.0002977793526480051\n",
      "Batch: 31000,train loss is: 0.000405830017275846\n",
      "test loss is 0.0003161800800440176\n",
      "Batch: 31100,train loss is: 0.000310335584422934\n",
      "test loss is 0.00028483287439225\n",
      "Batch: 31200,train loss is: 0.0002820861726077579\n",
      "test loss is 0.0003310181769520754\n",
      "Batch: 31300,train loss is: 0.0003267651248017332\n",
      "test loss is 0.0002980105093177379\n",
      "Batch: 31400,train loss is: 0.0001835291633374837\n",
      "test loss is 0.00029046809049509904\n",
      "Batch: 31500,train loss is: 0.00015211633742828978\n",
      "test loss is 0.0002563953718786365\n",
      "Batch: 31600,train loss is: 0.0002457659789836178\n",
      "test loss is 0.00027080916695798157\n",
      "Batch: 31700,train loss is: 0.0002493829190070875\n",
      "test loss is 0.00027705007514136566\n",
      "Batch: 31800,train loss is: 0.00020916590268051598\n",
      "test loss is 0.00032051183720463276\n",
      "Batch: 31900,train loss is: 0.00025828611705239353\n",
      "test loss is 0.0002981853278487979\n",
      "Batch: 32000,train loss is: 0.00030959808285866027\n",
      "test loss is 0.00033243551100487303\n",
      "Batch: 32100,train loss is: 0.0003787267851970058\n",
      "test loss is 0.00029833392517917016\n",
      "Batch: 32200,train loss is: 0.00024577914874686417\n",
      "test loss is 0.0003756128267951776\n",
      "Batch: 32300,train loss is: 0.0004083160577426836\n",
      "test loss is 0.00035127340205583007\n",
      "Batch: 32400,train loss is: 0.00018166244830107483\n",
      "test loss is 0.0002984081552460203\n",
      "Batch: 32500,train loss is: 0.0003067914057402459\n",
      "test loss is 0.0002829480401090778\n",
      "Batch: 32600,train loss is: 0.0002278687523493349\n",
      "test loss is 0.00034458175795268336\n",
      "Batch: 32700,train loss is: 0.00018843221527240856\n",
      "test loss is 0.0002689656279354219\n",
      "Batch: 32800,train loss is: 0.0003690225642404803\n",
      "test loss is 0.00039064638026888135\n",
      "Batch: 32900,train loss is: 0.00035358485177646717\n",
      "test loss is 0.0002900628946247454\n",
      "Batch: 33000,train loss is: 0.00020930209558213677\n",
      "test loss is 0.00027370803695696744\n",
      "Batch: 33100,train loss is: 0.0005400664539212332\n",
      "test loss is 0.0003302687052350398\n",
      "Batch: 33200,train loss is: 0.0002695998950337117\n",
      "test loss is 0.00030261986810909466\n",
      "Batch: 33300,train loss is: 0.00023568132828897758\n",
      "test loss is 0.00028088651077196844\n",
      "Batch: 33400,train loss is: 0.00031487599161189887\n",
      "test loss is 0.0003001071692816794\n",
      "Batch: 33500,train loss is: 0.0003786525741600801\n",
      "test loss is 0.0002666359919379835\n",
      "Batch: 33600,train loss is: 0.0001957996230514214\n",
      "test loss is 0.00031497848341662037\n",
      "Batch: 33700,train loss is: 0.0003707589521830192\n",
      "test loss is 0.00035958556321349694\n",
      "Batch: 33800,train loss is: 0.0002967178383044204\n",
      "test loss is 0.00027336188087588424\n",
      "Batch: 33900,train loss is: 0.00023724097380342808\n",
      "test loss is 0.0003180760455789242\n",
      "Batch: 34000,train loss is: 0.0002267036896710616\n",
      "test loss is 0.0002871260743731161\n",
      "Batch: 34100,train loss is: 0.00023574226311987845\n",
      "test loss is 0.00027818340541646243\n",
      "Batch: 34200,train loss is: 0.0003704362380094242\n",
      "test loss is 0.00028259406093230195\n",
      "Batch: 34300,train loss is: 0.0003274508302420231\n",
      "test loss is 0.000301667179787251\n",
      "Batch: 34400,train loss is: 0.00023305517013161628\n",
      "test loss is 0.0002819896249480667\n",
      "Batch: 34500,train loss is: 0.00024470720975891487\n",
      "test loss is 0.00031551572519395934\n",
      "Batch: 34600,train loss is: 0.0003637935981037632\n",
      "test loss is 0.000276356278188617\n",
      "Batch: 34700,train loss is: 0.00022833650350538164\n",
      "test loss is 0.00028143288374091126\n",
      "Batch: 34800,train loss is: 0.00027735498132268484\n",
      "test loss is 0.0003307838182858795\n",
      "Batch: 34900,train loss is: 0.00042567767385603684\n",
      "test loss is 0.0003885887771786153\n",
      "Batch: 35000,train loss is: 0.0002819957185980246\n",
      "test loss is 0.000272470420372892\n",
      "Batch: 35100,train loss is: 0.00018459440586585118\n",
      "test loss is 0.00028452439197198176\n",
      "Batch: 35200,train loss is: 0.00017707312130320801\n",
      "test loss is 0.0002872136398294548\n",
      "Batch: 35300,train loss is: 0.0002722570976491604\n",
      "test loss is 0.0002954285024280018\n",
      "Batch: 35400,train loss is: 0.00030156324711606483\n",
      "test loss is 0.00033178725825631575\n",
      "Batch: 35500,train loss is: 0.00018177902972354277\n",
      "test loss is 0.00029388348811174736\n",
      "Batch: 35600,train loss is: 0.00032323371324829655\n",
      "test loss is 0.0003006290878312371\n",
      "Batch: 35700,train loss is: 0.0003034293571463991\n",
      "test loss is 0.0003048887246034532\n",
      "Batch: 35800,train loss is: 0.0002986095017634569\n",
      "test loss is 0.0002788442946386382\n",
      "Batch: 35900,train loss is: 0.00016645718714530374\n",
      "test loss is 0.0003469267166059224\n",
      "Batch: 36000,train loss is: 0.000298679843898105\n",
      "test loss is 0.00027181594639618886\n",
      "Batch: 36100,train loss is: 0.0003319542940296222\n",
      "test loss is 0.00030995822764625286\n",
      "Batch: 36200,train loss is: 0.000499248284947066\n",
      "test loss is 0.00031731000191815526\n",
      "Batch: 36300,train loss is: 0.00044444079280203907\n",
      "test loss is 0.00035306058202401485\n",
      "Batch: 36400,train loss is: 0.00033582199984420616\n",
      "test loss is 0.0002658369181568151\n",
      "Batch: 36500,train loss is: 0.00029602480814106886\n",
      "test loss is 0.0002813940054534384\n",
      "Batch: 36600,train loss is: 0.00031058623114176324\n",
      "test loss is 0.00032639886637458735\n",
      "Batch: 36700,train loss is: 0.00038018740526111767\n",
      "test loss is 0.0003165078491919865\n",
      "Batch: 36800,train loss is: 0.00025792730364496353\n",
      "test loss is 0.000287195243480418\n",
      "Batch: 36900,train loss is: 0.00024700393589938325\n",
      "test loss is 0.0002818705334218288\n",
      "Batch: 37000,train loss is: 0.0002634418786672851\n",
      "test loss is 0.0002951830461988777\n",
      "Batch: 37100,train loss is: 0.00019876950979446641\n",
      "test loss is 0.0003235444092633911\n",
      "Batch: 37200,train loss is: 0.00040560958950867053\n",
      "test loss is 0.0002956226280585824\n",
      "Batch: 37300,train loss is: 0.0002416142164513146\n",
      "test loss is 0.00031932008181416785\n",
      "Batch: 37400,train loss is: 0.00019815194318177322\n",
      "test loss is 0.00035455427330219417\n",
      "Batch: 37500,train loss is: 0.00031357187442869526\n",
      "test loss is 0.0002913473923130125\n",
      "Batch: 37600,train loss is: 0.0002566921454848415\n",
      "test loss is 0.0003262045869216835\n",
      "Batch: 37700,train loss is: 0.0003847293658441707\n",
      "test loss is 0.0003045115386829159\n",
      "Batch: 37800,train loss is: 0.00019920525159750492\n",
      "test loss is 0.0002947584056384373\n",
      "Batch: 37900,train loss is: 0.0004016080213892889\n",
      "test loss is 0.0003323970393878256\n",
      "Batch: 38000,train loss is: 0.00024464123330355706\n",
      "test loss is 0.0002959166117404647\n",
      "Batch: 38100,train loss is: 0.00030002459512432544\n",
      "test loss is 0.00029547731625029905\n",
      "Batch: 38200,train loss is: 0.00013158757661565245\n",
      "test loss is 0.00028170868584210184\n",
      "Batch: 38300,train loss is: 0.0002855897714587539\n",
      "test loss is 0.00027912676041314384\n",
      "Batch: 38400,train loss is: 0.0002757014340794912\n",
      "test loss is 0.0003458497280781453\n",
      "Batch: 38500,train loss is: 0.00022186279159531146\n",
      "test loss is 0.0003461108780017922\n",
      "Batch: 38600,train loss is: 0.0002838320023275538\n",
      "test loss is 0.0003441707843418455\n",
      "Batch: 38700,train loss is: 0.00040875872844917767\n",
      "test loss is 0.00028625980428747383\n",
      "Batch: 38800,train loss is: 0.0003215185412365527\n",
      "test loss is 0.000352207074261215\n",
      "Batch: 38900,train loss is: 0.0004427626522477575\n",
      "test loss is 0.000325419642360963\n",
      "Batch: 39000,train loss is: 0.0004692832682846646\n",
      "test loss is 0.00031341315084092294\n",
      "Batch: 39100,train loss is: 0.00023973564756683922\n",
      "test loss is 0.0003041772517619832\n",
      "Batch: 39200,train loss is: 0.000257231034357507\n",
      "test loss is 0.00027676251343489524\n",
      "Batch: 39300,train loss is: 0.00031983116320063716\n",
      "test loss is 0.0004147336141762187\n",
      "Batch: 39400,train loss is: 0.0002691302584167781\n",
      "test loss is 0.00026490021562640154\n",
      "Batch: 39500,train loss is: 0.00022717877621160646\n",
      "test loss is 0.000297627727760877\n",
      "Batch: 39600,train loss is: 0.0004576242155398269\n",
      "test loss is 0.00027334567532392187\n",
      "Batch: 39700,train loss is: 0.00026761288917223473\n",
      "test loss is 0.00026719012022266716\n",
      "Batch: 39800,train loss is: 0.00022889987559974805\n",
      "test loss is 0.00029498451555520956\n",
      "Batch: 39900,train loss is: 0.0002847049004483564\n",
      "test loss is 0.00028051756776777354\n",
      "Batch: 40000,train loss is: 0.00028210155079979064\n",
      "test loss is 0.00026712486768426015\n",
      "Batch: 40100,train loss is: 0.00023588297049992115\n",
      "test loss is 0.000328738387781417\n",
      "Batch: 40200,train loss is: 0.0007108170971516093\n",
      "test loss is 0.0004349137684931282\n",
      "Batch: 40300,train loss is: 0.00027240192258429284\n",
      "test loss is 0.00029727097339664836\n",
      "Batch: 40400,train loss is: 0.00022314512571456442\n",
      "test loss is 0.00028756075254573094\n",
      "Batch: 40500,train loss is: 0.000260041239569214\n",
      "test loss is 0.00028918418732118076\n",
      "Batch: 40600,train loss is: 0.0002457681944036248\n",
      "test loss is 0.0002775569599794439\n",
      "Batch: 40700,train loss is: 0.00018887138645114175\n",
      "test loss is 0.0002855779417787805\n",
      "Batch: 40800,train loss is: 0.0002964218000772355\n",
      "test loss is 0.0002747645006574604\n",
      "Batch: 40900,train loss is: 0.000181599242089029\n",
      "test loss is 0.00027081293988196343\n",
      "Batch: 41000,train loss is: 0.00018103624693169233\n",
      "test loss is 0.00027430453770420295\n",
      "Batch: 41100,train loss is: 0.0003980517152913067\n",
      "test loss is 0.0003204367648647605\n",
      "Batch: 41200,train loss is: 0.00027415057946615754\n",
      "test loss is 0.0002960567337443833\n",
      "Batch: 41300,train loss is: 0.00023364486931420688\n",
      "test loss is 0.00032491126273153894\n",
      "Batch: 41400,train loss is: 0.00031785987522765834\n",
      "test loss is 0.0002713502138831144\n",
      "Batch: 41500,train loss is: 0.00023088140524714023\n",
      "test loss is 0.0002813769952957756\n",
      "Batch: 41600,train loss is: 0.0003048138328423897\n",
      "test loss is 0.0002801879182957662\n",
      "Batch: 41700,train loss is: 0.0002908351102428671\n",
      "test loss is 0.00029864904029789074\n",
      "Batch: 41800,train loss is: 0.0003092958430558814\n",
      "test loss is 0.00028897817889205206\n",
      "Batch: 41900,train loss is: 0.0003389188283136941\n",
      "test loss is 0.0003501159368795098\n",
      "Batch: 42000,train loss is: 0.00043134980604487704\n",
      "test loss is 0.0002767361936786827\n",
      "Batch: 42100,train loss is: 0.00032624486632434484\n",
      "test loss is 0.00028071169274594427\n",
      "Batch: 42200,train loss is: 0.0004591029406314098\n",
      "test loss is 0.0003317071172786657\n",
      "Batch: 42300,train loss is: 0.00025847244496384636\n",
      "test loss is 0.00030870601852569824\n",
      "Batch: 42400,train loss is: 0.00024914528104822786\n",
      "test loss is 0.00028239226367870226\n",
      "Batch: 42500,train loss is: 0.00032488909575924176\n",
      "test loss is 0.0003163256484320052\n",
      "Batch: 42600,train loss is: 0.000322926415283289\n",
      "test loss is 0.00028245363621803556\n",
      "Batch: 42700,train loss is: 0.00024066860851900296\n",
      "test loss is 0.0002695334668138943\n",
      "Batch: 42800,train loss is: 0.00023627921114577555\n",
      "test loss is 0.00027240403680941474\n",
      "Batch: 42900,train loss is: 0.00037744414192691824\n",
      "test loss is 0.00031062614397666706\n",
      "Batch: 43000,train loss is: 0.0001957031356994356\n",
      "test loss is 0.0002662929826631035\n",
      "Batch: 43100,train loss is: 0.00033040216581744174\n",
      "test loss is 0.00033355035450160803\n",
      "Batch: 43200,train loss is: 0.0002581866512949108\n",
      "test loss is 0.0003689845246900802\n",
      "Batch: 43300,train loss is: 0.0003119101826721299\n",
      "test loss is 0.00028450118968558966\n",
      "Batch: 43400,train loss is: 0.00021447472607426704\n",
      "test loss is 0.00033106146472083035\n",
      "Batch: 43500,train loss is: 0.00023392117799246414\n",
      "test loss is 0.00030802096382674893\n",
      "Batch: 43600,train loss is: 0.00020071514350492568\n",
      "test loss is 0.00027320224277640673\n",
      "Batch: 43700,train loss is: 0.0002657559998558086\n",
      "test loss is 0.0002846825392816218\n",
      "Batch: 43800,train loss is: 0.00017915879354254998\n",
      "test loss is 0.0003008229705283364\n",
      "Batch: 43900,train loss is: 0.00025046052807449904\n",
      "test loss is 0.00026505840759880224\n",
      "Batch: 44000,train loss is: 0.00019473855708775867\n",
      "test loss is 0.0002620412106445783\n",
      "Batch: 44100,train loss is: 0.00031267822777706917\n",
      "test loss is 0.000280012141875697\n",
      "Batch: 44200,train loss is: 0.0001526318642734473\n",
      "test loss is 0.00026744117997450366\n",
      "Batch: 44300,train loss is: 0.0003539741437341949\n",
      "test loss is 0.0002793306108542257\n",
      "Batch: 44400,train loss is: 0.00044027614203118485\n",
      "test loss is 0.00029430105722538655\n",
      "Batch: 44500,train loss is: 0.00018307195230010117\n",
      "test loss is 0.0003013618698303753\n",
      "Batch: 44600,train loss is: 0.00044186829873313555\n",
      "test loss is 0.0002930037274238922\n",
      "Batch: 44700,train loss is: 0.0001770066634630901\n",
      "test loss is 0.0003214528213101696\n",
      "Batch: 44800,train loss is: 0.00025991675409130693\n",
      "test loss is 0.00027326656394838553\n",
      "Batch: 44900,train loss is: 0.0002761820090599262\n",
      "test loss is 0.00030918248500968706\n",
      "Batch: 45000,train loss is: 0.0001732658634082122\n",
      "test loss is 0.000303692646302072\n",
      "Batch: 45100,train loss is: 0.0002766373408871355\n",
      "test loss is 0.0003154096492860368\n",
      "Batch: 45200,train loss is: 0.0004565609629497785\n",
      "test loss is 0.00029055221981178853\n",
      "Batch: 45300,train loss is: 0.0005257207558497582\n",
      "test loss is 0.0004062495427376983\n",
      "Batch: 45400,train loss is: 0.00033038802961973487\n",
      "test loss is 0.000329042233534019\n",
      "Batch: 45500,train loss is: 0.0003828751596544965\n",
      "test loss is 0.00034611067776533117\n",
      "Batch: 45600,train loss is: 0.0002929540472582512\n",
      "test loss is 0.000273886682517971\n",
      "Batch: 45700,train loss is: 0.0003201620867341635\n",
      "test loss is 0.00027665985461090115\n",
      "Batch: 45800,train loss is: 0.00025643856790072574\n",
      "test loss is 0.00029638178164794783\n",
      "Batch: 45900,train loss is: 0.000344270195413218\n",
      "test loss is 0.0003296350893055404\n",
      "Batch: 46000,train loss is: 0.0003541154314671409\n",
      "test loss is 0.0002703825420091238\n",
      "Batch: 46100,train loss is: 0.00016142499323067334\n",
      "test loss is 0.00031693075053403004\n",
      "Batch: 46200,train loss is: 0.0002990851272364511\n",
      "test loss is 0.0003094618596019779\n",
      "Batch: 46300,train loss is: 0.00024362286802807269\n",
      "test loss is 0.0002878306122216215\n",
      "Batch: 46400,train loss is: 0.00040474062389515906\n",
      "test loss is 0.00026633973670613505\n",
      "Batch: 46500,train loss is: 0.00035844195179937345\n",
      "test loss is 0.0002775667463216206\n",
      "Batch: 46600,train loss is: 0.00031086379466105255\n",
      "test loss is 0.0004374402719455979\n",
      "Batch: 46700,train loss is: 0.00017108381583089377\n",
      "test loss is 0.00028349016378718256\n",
      "-----------------------Epoch: 5----------------------------------\n",
      "Batch: 0,train loss is: 0.0002164416773000071\n",
      "test loss is 0.0002994886375497491\n",
      "Batch: 100,train loss is: 0.0002836299733684069\n",
      "test loss is 0.0003040274613833311\n",
      "Batch: 200,train loss is: 0.0001902440365259884\n",
      "test loss is 0.00026154628012633704\n",
      "Batch: 300,train loss is: 0.00038866165165964183\n",
      "test loss is 0.00034331418291333064\n",
      "Batch: 400,train loss is: 0.00026166363702190775\n",
      "test loss is 0.0003117398235952673\n",
      "Batch: 500,train loss is: 0.0005057180963821662\n",
      "test loss is 0.000300866542733474\n",
      "Batch: 600,train loss is: 0.0002886206623085054\n",
      "test loss is 0.00035326168953974844\n",
      "Batch: 700,train loss is: 0.00022749499423608632\n",
      "test loss is 0.00027196651451736866\n",
      "Batch: 800,train loss is: 0.00034263942855813944\n",
      "test loss is 0.00028423324305873834\n",
      "Batch: 900,train loss is: 0.0002704922528731421\n",
      "test loss is 0.0003207552132282062\n",
      "Batch: 1000,train loss is: 0.00022931056509022178\n",
      "test loss is 0.00028533035583857874\n",
      "Batch: 1100,train loss is: 0.0002745310095220853\n",
      "test loss is 0.0003008132209707254\n",
      "Batch: 1200,train loss is: 0.00014200445916537832\n",
      "test loss is 0.00028542686314043383\n",
      "Batch: 1300,train loss is: 0.00022511673784525132\n",
      "test loss is 0.00026159536220773927\n",
      "Batch: 1400,train loss is: 0.0002454027282303309\n",
      "test loss is 0.00027850478251865366\n",
      "Batch: 1500,train loss is: 0.00044807755508129556\n",
      "test loss is 0.00032733964716373444\n",
      "Batch: 1600,train loss is: 0.0001274626027480367\n",
      "test loss is 0.00029374995273929457\n",
      "Batch: 1700,train loss is: 0.00020046141424834779\n",
      "test loss is 0.0002916246687468481\n",
      "Batch: 1800,train loss is: 0.0002447104298155091\n",
      "test loss is 0.0002932904717339552\n",
      "Batch: 1900,train loss is: 0.0003843498107661905\n",
      "test loss is 0.00033938050149993204\n",
      "Batch: 2000,train loss is: 0.00032819566334593074\n",
      "test loss is 0.0003100463695942422\n",
      "Batch: 2100,train loss is: 0.00028951569866328517\n",
      "test loss is 0.00028306606342071253\n",
      "Batch: 2200,train loss is: 0.0003925390436697113\n",
      "test loss is 0.00027014991300114953\n",
      "Batch: 2300,train loss is: 0.0003850589366489091\n",
      "test loss is 0.00030278678717159173\n",
      "Batch: 2400,train loss is: 0.0006252684724776619\n",
      "test loss is 0.00039563972508626705\n",
      "Batch: 2500,train loss is: 0.00024302129340062566\n",
      "test loss is 0.0003198209412928737\n",
      "Batch: 2600,train loss is: 0.0002393490223790356\n",
      "test loss is 0.0002617055201034336\n",
      "Batch: 2700,train loss is: 0.0002179862182492053\n",
      "test loss is 0.0002914425955266113\n",
      "Batch: 2800,train loss is: 0.0003848794281255786\n",
      "test loss is 0.000361587227623877\n",
      "Batch: 2900,train loss is: 0.0001879901591149627\n",
      "test loss is 0.0002665605506398191\n",
      "Batch: 3000,train loss is: 0.00027283516429724034\n",
      "test loss is 0.0002759829950530366\n",
      "Batch: 3100,train loss is: 0.00023121788197521203\n",
      "test loss is 0.0002931176270537548\n",
      "Batch: 3200,train loss is: 0.00028260633137903795\n",
      "test loss is 0.00027468557665590086\n",
      "Batch: 3300,train loss is: 0.00022485603086065583\n",
      "test loss is 0.00028843763549078565\n",
      "Batch: 3400,train loss is: 0.0002804746989753562\n",
      "test loss is 0.00029381889300267\n",
      "Batch: 3500,train loss is: 0.00032825476843885285\n",
      "test loss is 0.0003019372696606345\n",
      "Batch: 3600,train loss is: 0.0002398089170085295\n",
      "test loss is 0.0002884740125726296\n",
      "Batch: 3700,train loss is: 0.00026398565223910935\n",
      "test loss is 0.0003229718427691614\n",
      "Batch: 3800,train loss is: 0.0005019243336413662\n",
      "test loss is 0.000300552197933417\n",
      "Batch: 3900,train loss is: 0.00019746369284920484\n",
      "test loss is 0.0002792623080473028\n",
      "Batch: 4000,train loss is: 0.00017928573519748967\n",
      "test loss is 0.0003201044306101316\n",
      "Batch: 4100,train loss is: 0.00026922295355949325\n",
      "test loss is 0.00028163349563343435\n",
      "Batch: 4200,train loss is: 0.0005490121481586184\n",
      "test loss is 0.00028080099136211463\n",
      "Batch: 4300,train loss is: 0.0005244262589955478\n",
      "test loss is 0.00037257146452085584\n",
      "Batch: 4400,train loss is: 0.0003322905530761922\n",
      "test loss is 0.0002713627559611715\n",
      "Batch: 4500,train loss is: 0.00021802539963739712\n",
      "test loss is 0.0002673510591422902\n",
      "Batch: 4600,train loss is: 0.00045699258545365217\n",
      "test loss is 0.00028911245275771885\n",
      "Batch: 4700,train loss is: 0.0006742265800792327\n",
      "test loss is 0.00029926823012989967\n",
      "Batch: 4800,train loss is: 0.00041555874120990965\n",
      "test loss is 0.0002622439115305462\n",
      "Batch: 4900,train loss is: 0.00018492584967470013\n",
      "test loss is 0.0002742244999033284\n",
      "Batch: 5000,train loss is: 0.0003172566593218836\n",
      "test loss is 0.00032476989176692585\n",
      "Batch: 5100,train loss is: 0.00022869034831342336\n",
      "test loss is 0.0003273632382021209\n",
      "Batch: 5200,train loss is: 0.00040787131896697905\n",
      "test loss is 0.0002952284077225263\n",
      "Batch: 5300,train loss is: 0.00031322586580753687\n",
      "test loss is 0.0004168558353375822\n",
      "Batch: 5400,train loss is: 0.0003055256635557472\n",
      "test loss is 0.0002951814244213876\n",
      "Batch: 5500,train loss is: 0.00021176037388995642\n",
      "test loss is 0.00030770840749400497\n",
      "Batch: 5600,train loss is: 0.00040855102878137415\n",
      "test loss is 0.00027719958211350264\n",
      "Batch: 5700,train loss is: 0.00013569608826594453\n",
      "test loss is 0.00027601517922157633\n",
      "Batch: 5800,train loss is: 0.0002168698014973783\n",
      "test loss is 0.0003104035956574012\n",
      "Batch: 5900,train loss is: 0.00027383253025928885\n",
      "test loss is 0.0002788294119326928\n",
      "Batch: 6000,train loss is: 0.00021477856897781739\n",
      "test loss is 0.00025738424319820554\n",
      "Batch: 6100,train loss is: 0.000264539079848462\n",
      "test loss is 0.0003299845108840634\n",
      "Batch: 6200,train loss is: 0.0002950815662263671\n",
      "test loss is 0.0002814792151002919\n",
      "Batch: 6300,train loss is: 0.00021080481862412772\n",
      "test loss is 0.0002964532741902003\n",
      "Batch: 6400,train loss is: 0.0003396399372191494\n",
      "test loss is 0.00037314751730003446\n",
      "Batch: 6500,train loss is: 0.00026952481399116477\n",
      "test loss is 0.0002770201487566415\n",
      "Batch: 6600,train loss is: 0.00045629764243033445\n",
      "test loss is 0.0002848689730930658\n",
      "Batch: 6700,train loss is: 0.00046138260352362884\n",
      "test loss is 0.00030293098494435093\n",
      "Batch: 6800,train loss is: 0.00020526131811733755\n",
      "test loss is 0.000387325924826798\n",
      "Batch: 6900,train loss is: 0.0003617226917495574\n",
      "test loss is 0.0003000996907137012\n",
      "Batch: 7000,train loss is: 0.00032159288555256313\n",
      "test loss is 0.000305818860348808\n",
      "Batch: 7100,train loss is: 0.0002642268100226316\n",
      "test loss is 0.0003074140739445063\n",
      "Batch: 7200,train loss is: 0.00033361543247902806\n",
      "test loss is 0.0002731420780753271\n",
      "Batch: 7300,train loss is: 0.00018964077834049083\n",
      "test loss is 0.00029700605110476126\n",
      "Batch: 7400,train loss is: 0.0002479573460206093\n",
      "test loss is 0.00030364678345029886\n",
      "Batch: 7500,train loss is: 0.00020067805531152303\n",
      "test loss is 0.00027538580595688506\n",
      "Batch: 7600,train loss is: 0.00029445670399391293\n",
      "test loss is 0.00027720160589488795\n",
      "Batch: 7700,train loss is: 0.00024181380623927658\n",
      "test loss is 0.00029404788718514417\n",
      "Batch: 7800,train loss is: 0.0002576744021955597\n",
      "test loss is 0.00026530799704148117\n",
      "Batch: 7900,train loss is: 0.00014605006441563523\n",
      "test loss is 0.00027177630731452105\n",
      "Batch: 8000,train loss is: 0.00025392423259773544\n",
      "test loss is 0.0003335631986563168\n",
      "Batch: 8100,train loss is: 0.0003796476615837204\n",
      "test loss is 0.00027221177666696876\n",
      "Batch: 8200,train loss is: 0.00025641333708193695\n",
      "test loss is 0.00028825854036720146\n",
      "Batch: 8300,train loss is: 0.00023208531330229295\n",
      "test loss is 0.0002698082078689901\n",
      "Batch: 8400,train loss is: 0.000216721126257746\n",
      "test loss is 0.0002756349017192164\n",
      "Batch: 8500,train loss is: 0.0003286402472377189\n",
      "test loss is 0.00031714932933603335\n",
      "Batch: 8600,train loss is: 0.00022090760769581387\n",
      "test loss is 0.00030450710313616164\n",
      "Batch: 8700,train loss is: 0.0001268346702472366\n",
      "test loss is 0.00028276796670242727\n",
      "Batch: 8800,train loss is: 0.0002029779155945543\n",
      "test loss is 0.00029216905878477425\n",
      "Batch: 8900,train loss is: 0.0003063382243894394\n",
      "test loss is 0.0002773513124095658\n",
      "Batch: 9000,train loss is: 0.00034976910313761227\n",
      "test loss is 0.0002911752061717768\n",
      "Batch: 9100,train loss is: 0.0002511166655963982\n",
      "test loss is 0.00032960851738731214\n",
      "Batch: 9200,train loss is: 0.00030765544201027094\n",
      "test loss is 0.0002661906102169026\n",
      "Batch: 9300,train loss is: 0.00019849987740757122\n",
      "test loss is 0.0002913660357459994\n",
      "Batch: 9400,train loss is: 0.0002901833352890983\n",
      "test loss is 0.00028895362107128993\n",
      "Batch: 9500,train loss is: 0.00033216540870915163\n",
      "test loss is 0.0003038012727942993\n",
      "Batch: 9600,train loss is: 0.00022189510126587725\n",
      "test loss is 0.0003455383797875643\n",
      "Batch: 9700,train loss is: 0.00025374178857173215\n",
      "test loss is 0.00029224035187853947\n",
      "Batch: 9800,train loss is: 0.00034282293066337406\n",
      "test loss is 0.00030765753614830135\n",
      "Batch: 9900,train loss is: 0.0001624910585600089\n",
      "test loss is 0.0002964271758163577\n",
      "Batch: 10000,train loss is: 0.0002622859832804455\n",
      "test loss is 0.0002683284656509817\n",
      "Batch: 10100,train loss is: 0.00016966348603182933\n",
      "test loss is 0.00027556993014058795\n",
      "Batch: 10200,train loss is: 0.00018949855663397055\n",
      "test loss is 0.0003250991205181473\n",
      "Batch: 10300,train loss is: 0.0002898184822885764\n",
      "test loss is 0.0002668864474608875\n",
      "Batch: 10400,train loss is: 0.00022227519370491682\n",
      "test loss is 0.0002985612208044367\n",
      "Batch: 10500,train loss is: 0.00023651247762670625\n",
      "test loss is 0.0003065283768706605\n",
      "Batch: 10600,train loss is: 0.00024475284199435184\n",
      "test loss is 0.0003116808695643688\n",
      "Batch: 10700,train loss is: 0.0002266215489775986\n",
      "test loss is 0.0003580294031006476\n",
      "Batch: 10800,train loss is: 0.00022721548987876303\n",
      "test loss is 0.00029687463361557215\n",
      "Batch: 10900,train loss is: 0.0002502936714951743\n",
      "test loss is 0.00029006595699811687\n",
      "Batch: 11000,train loss is: 0.0002639862859801339\n",
      "test loss is 0.00027409297835564897\n",
      "Batch: 11100,train loss is: 0.00022673159053575466\n",
      "test loss is 0.00026974182374097646\n",
      "Batch: 11200,train loss is: 0.00035996093781268416\n",
      "test loss is 0.00031442676302397346\n",
      "Batch: 11300,train loss is: 0.00018686735200735508\n",
      "test loss is 0.0003274830281839947\n",
      "Batch: 11400,train loss is: 0.0003454314386312379\n",
      "test loss is 0.00030806931007641675\n",
      "Batch: 11500,train loss is: 0.00034579137227337534\n",
      "test loss is 0.00030114449480560804\n",
      "Batch: 11600,train loss is: 0.0003056645430922707\n",
      "test loss is 0.00030235181450263035\n",
      "Batch: 11700,train loss is: 0.00033142346106487813\n",
      "test loss is 0.0002976120163756137\n",
      "Batch: 11800,train loss is: 0.0003087319646725142\n",
      "test loss is 0.00029286107894330925\n",
      "Batch: 11900,train loss is: 0.0002578588044316513\n",
      "test loss is 0.0002904799984165018\n",
      "Batch: 12000,train loss is: 0.00021879698939316283\n",
      "test loss is 0.0002714888890078663\n",
      "Batch: 12100,train loss is: 0.00027896998203266017\n",
      "test loss is 0.0003277029197563396\n",
      "Batch: 12200,train loss is: 0.00020611832091285378\n",
      "test loss is 0.0002783513599526431\n",
      "Batch: 12300,train loss is: 0.00025297835058866373\n",
      "test loss is 0.0003017431274356455\n",
      "Batch: 12400,train loss is: 0.00041851720921984943\n",
      "test loss is 0.00039456101832573295\n",
      "Batch: 12500,train loss is: 0.00024999591847355425\n",
      "test loss is 0.0002909187118481042\n",
      "Batch: 12600,train loss is: 0.00029641835542071343\n",
      "test loss is 0.0002894565260196589\n",
      "Batch: 12700,train loss is: 0.00020583084365816042\n",
      "test loss is 0.000335328519284181\n",
      "Batch: 12800,train loss is: 0.0002453400377368273\n",
      "test loss is 0.00030919828211013186\n",
      "Batch: 12900,train loss is: 0.00028830222568256134\n",
      "test loss is 0.0002932243463499388\n",
      "Batch: 13000,train loss is: 0.0003178778217294827\n",
      "test loss is 0.00026572784332996104\n",
      "Batch: 13100,train loss is: 0.0003025533270315366\n",
      "test loss is 0.00027419952229757903\n",
      "Batch: 13200,train loss is: 0.0004280159357345728\n",
      "test loss is 0.00030440438154976545\n",
      "Batch: 13300,train loss is: 0.0002124521873280519\n",
      "test loss is 0.0003399330816251169\n",
      "Batch: 13400,train loss is: 0.00032232099268512124\n",
      "test loss is 0.00026767850976092724\n",
      "Batch: 13500,train loss is: 0.00019742093219255933\n",
      "test loss is 0.00029266117181970516\n",
      "Batch: 13600,train loss is: 0.00026522011219538984\n",
      "test loss is 0.0003118765102061742\n",
      "Batch: 13700,train loss is: 0.0001677402630622471\n",
      "test loss is 0.0003082624997619904\n",
      "Batch: 13800,train loss is: 0.00022300035485473267\n",
      "test loss is 0.00031100194532989004\n",
      "Batch: 13900,train loss is: 0.0003805106636628502\n",
      "test loss is 0.00033314494325466227\n",
      "Batch: 14000,train loss is: 0.0002924001928402327\n",
      "test loss is 0.0003016676969841899\n",
      "Batch: 14100,train loss is: 0.0002991948108301089\n",
      "test loss is 0.000286525729006081\n",
      "Batch: 14200,train loss is: 0.0003191323073763204\n",
      "test loss is 0.00029435575776860405\n",
      "Batch: 14300,train loss is: 0.00033942743550053397\n",
      "test loss is 0.00025905605886752535\n",
      "Batch: 14400,train loss is: 0.0002943570029559019\n",
      "test loss is 0.00031941584160171245\n",
      "Batch: 14500,train loss is: 0.0002449927673786194\n",
      "test loss is 0.0002947488391740135\n",
      "Batch: 14600,train loss is: 0.0004084799123738563\n",
      "test loss is 0.0003075089516683967\n",
      "Batch: 14700,train loss is: 0.0003551702844692521\n",
      "test loss is 0.0003219590040397022\n",
      "Batch: 14800,train loss is: 0.00027469455966778745\n",
      "test loss is 0.0003300563428504081\n",
      "Batch: 14900,train loss is: 0.00022050472595604246\n",
      "test loss is 0.0003762311519786546\n",
      "Batch: 15000,train loss is: 0.0003971033319299722\n",
      "test loss is 0.00032460304932546144\n",
      "Batch: 15100,train loss is: 0.0003415526840613854\n",
      "test loss is 0.00027346832544372223\n",
      "Batch: 15200,train loss is: 0.0004275603471963687\n",
      "test loss is 0.0002928114324234413\n",
      "Batch: 15300,train loss is: 0.0002677727744311104\n",
      "test loss is 0.0003434812212866257\n",
      "Batch: 15400,train loss is: 0.00035324525365589086\n",
      "test loss is 0.00027726621013294004\n",
      "Batch: 15500,train loss is: 0.00021445339658542837\n",
      "test loss is 0.0002958059445519725\n",
      "Batch: 15600,train loss is: 0.00028322365679885975\n",
      "test loss is 0.00030797087292755844\n",
      "Batch: 15700,train loss is: 0.0004361603922798193\n",
      "test loss is 0.00028454785604715966\n",
      "Batch: 15800,train loss is: 0.00020184093294939076\n",
      "test loss is 0.0002894044927018594\n",
      "Batch: 15900,train loss is: 0.00016391095396245481\n",
      "test loss is 0.00030795107347518714\n",
      "Batch: 16000,train loss is: 0.0003414788912150822\n",
      "test loss is 0.0003036551853920117\n",
      "Batch: 16100,train loss is: 0.0002550188294828649\n",
      "test loss is 0.0003025898530367516\n",
      "Batch: 16200,train loss is: 0.0002266440848048008\n",
      "test loss is 0.0002783957354431426\n",
      "Batch: 16300,train loss is: 0.00032606130731715333\n",
      "test loss is 0.000282975910619072\n",
      "Batch: 16400,train loss is: 0.0003562354921787964\n",
      "test loss is 0.0002662883034028996\n",
      "Batch: 16500,train loss is: 0.00024040523358917064\n",
      "test loss is 0.0002693629668376734\n",
      "Batch: 16600,train loss is: 0.00025738009464018315\n",
      "test loss is 0.0002809409838922486\n",
      "Batch: 16700,train loss is: 0.00023008013969795358\n",
      "test loss is 0.000337211304885167\n",
      "Batch: 16800,train loss is: 0.0002488804335226766\n",
      "test loss is 0.0002561414143413358\n",
      "Batch: 16900,train loss is: 0.000411152692070417\n",
      "test loss is 0.000420020671131286\n",
      "Batch: 17000,train loss is: 0.0002094826119581367\n",
      "test loss is 0.0002986265427479988\n",
      "Batch: 17100,train loss is: 0.00037272999579786054\n",
      "test loss is 0.00032032871827437036\n",
      "Batch: 17200,train loss is: 0.00022307622952624095\n",
      "test loss is 0.00026133525778132766\n",
      "Batch: 17300,train loss is: 0.0003236436043907717\n",
      "test loss is 0.0002920334506282992\n",
      "Batch: 17400,train loss is: 0.00025226795360113006\n",
      "test loss is 0.00033726409218937244\n",
      "Batch: 17500,train loss is: 0.0001906207346287235\n",
      "test loss is 0.00027978455521175677\n",
      "Batch: 17600,train loss is: 0.00040959849496484207\n",
      "test loss is 0.0003734204685575553\n",
      "Batch: 17700,train loss is: 0.00029594624468459133\n",
      "test loss is 0.0002693182485560286\n",
      "Batch: 17800,train loss is: 0.00024249904132362033\n",
      "test loss is 0.00028704442164149084\n",
      "Batch: 17900,train loss is: 0.0005782330519797447\n",
      "test loss is 0.0003165532131875196\n",
      "Batch: 18000,train loss is: 0.00028367871224403154\n",
      "test loss is 0.000277040452204559\n",
      "Batch: 18100,train loss is: 0.0002369810068639038\n",
      "test loss is 0.000253834733967611\n",
      "Batch: 18200,train loss is: 0.0005117269925089959\n",
      "test loss is 0.00037602599050154205\n",
      "Batch: 18300,train loss is: 0.0001815182360329073\n",
      "test loss is 0.0002676452693197617\n",
      "Batch: 18400,train loss is: 0.00040890050998611694\n",
      "test loss is 0.0002949134433166814\n",
      "Batch: 18500,train loss is: 0.00016184837449549137\n",
      "test loss is 0.0002734154870454223\n",
      "Batch: 18600,train loss is: 0.00021107421032604378\n",
      "test loss is 0.0002745244139832538\n",
      "Batch: 18700,train loss is: 0.0002773811597000729\n",
      "test loss is 0.00034042405110019686\n",
      "Batch: 18800,train loss is: 0.0002548567992734492\n",
      "test loss is 0.00029249077175115155\n",
      "Batch: 18900,train loss is: 0.0003005292031605739\n",
      "test loss is 0.0003259455842491388\n",
      "Batch: 19000,train loss is: 0.0003048758200355446\n",
      "test loss is 0.00027907955882903654\n",
      "Batch: 19100,train loss is: 0.00023840791686503176\n",
      "test loss is 0.00027789375635757795\n",
      "Batch: 19200,train loss is: 0.0002024116509111033\n",
      "test loss is 0.0002698202730185219\n",
      "Batch: 19300,train loss is: 0.00026156252472051196\n",
      "test loss is 0.00030815865723037825\n",
      "Batch: 19400,train loss is: 0.0001971946569358139\n",
      "test loss is 0.0002971007491080276\n",
      "Batch: 19500,train loss is: 0.0002669962211063323\n",
      "test loss is 0.00030268870585261097\n",
      "Batch: 19600,train loss is: 0.0002260235891630096\n",
      "test loss is 0.00028865118678907\n",
      "Batch: 19700,train loss is: 0.00023734171748041752\n",
      "test loss is 0.0003067851479866536\n",
      "Batch: 19800,train loss is: 0.00031247522691645946\n",
      "test loss is 0.0003055220095356192\n",
      "Batch: 19900,train loss is: 0.00024872001864083846\n",
      "test loss is 0.00027669235699168614\n",
      "Batch: 20000,train loss is: 0.0002523066579559587\n",
      "test loss is 0.00033097035621678603\n",
      "Batch: 20100,train loss is: 0.00034776735602658\n",
      "test loss is 0.0004733494579237482\n",
      "Batch: 20200,train loss is: 0.0002099671778508813\n",
      "test loss is 0.0002798785215936596\n",
      "Batch: 20300,train loss is: 0.00032041932790321907\n",
      "test loss is 0.00030825244016272925\n",
      "Batch: 20400,train loss is: 0.0002092803599998631\n",
      "test loss is 0.0002767265077016443\n",
      "Batch: 20500,train loss is: 0.0003284490484717944\n",
      "test loss is 0.0002610564141625621\n",
      "Batch: 20600,train loss is: 0.0003244969875119568\n",
      "test loss is 0.00032052806007331746\n",
      "Batch: 20700,train loss is: 0.00033649231033078925\n",
      "test loss is 0.00034916517845695583\n",
      "Batch: 20800,train loss is: 0.00021759270905431116\n",
      "test loss is 0.00032413103294547964\n",
      "Batch: 20900,train loss is: 0.0003070397719484015\n",
      "test loss is 0.00033374372368252195\n",
      "Batch: 21000,train loss is: 0.0002496433725873102\n",
      "test loss is 0.0003012261037519498\n",
      "Batch: 21100,train loss is: 0.00023439003095010276\n",
      "test loss is 0.00033573665948922446\n",
      "Batch: 21200,train loss is: 0.00036735874889299067\n",
      "test loss is 0.0002963153745750268\n",
      "Batch: 21300,train loss is: 0.00026795523978250674\n",
      "test loss is 0.0003046654214962002\n",
      "Batch: 21400,train loss is: 0.0003524658316940503\n",
      "test loss is 0.0002677720708205169\n",
      "Batch: 21500,train loss is: 0.00021139115609662007\n",
      "test loss is 0.000276869965944289\n",
      "Batch: 21600,train loss is: 0.00025714376855823077\n",
      "test loss is 0.0003275153271031451\n",
      "Batch: 21700,train loss is: 0.0001885458663439879\n",
      "test loss is 0.00029274199235221023\n",
      "Batch: 21800,train loss is: 0.00024173042105465068\n",
      "test loss is 0.0002722513739686072\n",
      "Batch: 21900,train loss is: 0.0002783040113111801\n",
      "test loss is 0.0002973215258074678\n",
      "Batch: 22000,train loss is: 0.00020794686710903985\n",
      "test loss is 0.00028909238290156296\n",
      "Batch: 22100,train loss is: 0.0002457675010792471\n",
      "test loss is 0.0003151471289837659\n",
      "Batch: 22200,train loss is: 0.00017781817436900027\n",
      "test loss is 0.00031628065506769936\n",
      "Batch: 22300,train loss is: 0.0004026310836460615\n",
      "test loss is 0.0002834453382154479\n",
      "Batch: 22400,train loss is: 0.00031448504386671363\n",
      "test loss is 0.0004927209861454395\n",
      "Batch: 22500,train loss is: 0.0001567038202748661\n",
      "test loss is 0.00027158215820457957\n",
      "Batch: 22600,train loss is: 0.000938390583808132\n",
      "test loss is 0.0002749173258782654\n",
      "Batch: 22700,train loss is: 0.0002926861823076463\n",
      "test loss is 0.0002923697778238412\n",
      "Batch: 22800,train loss is: 0.0003157186598471187\n",
      "test loss is 0.000296911072680882\n",
      "Batch: 22900,train loss is: 0.00023490725021754386\n",
      "test loss is 0.000269287017176943\n",
      "Batch: 23000,train loss is: 0.0002935879454416746\n",
      "test loss is 0.00030473465438190465\n",
      "Batch: 23100,train loss is: 0.000178861097452673\n",
      "test loss is 0.00031521571398706677\n",
      "Batch: 23200,train loss is: 0.0001447935898009126\n",
      "test loss is 0.0002679427697906945\n",
      "Batch: 23300,train loss is: 0.00017465486499135347\n",
      "test loss is 0.0002762410025164195\n",
      "Batch: 23400,train loss is: 0.00016296201025608797\n",
      "test loss is 0.00027882183660270856\n",
      "Batch: 23500,train loss is: 0.00017146951651964248\n",
      "test loss is 0.0002673184154063277\n",
      "Batch: 23600,train loss is: 0.00039787184750492957\n",
      "test loss is 0.000329538468345523\n",
      "Batch: 23700,train loss is: 0.0002977940795905704\n",
      "test loss is 0.0002748658950619818\n",
      "Batch: 23800,train loss is: 0.00027309911783333733\n",
      "test loss is 0.00030548778420654357\n",
      "Batch: 23900,train loss is: 0.000239013153593421\n",
      "test loss is 0.00027570933171365634\n",
      "Batch: 24000,train loss is: 0.0003876518746414393\n",
      "test loss is 0.0002730444591589626\n",
      "Batch: 24100,train loss is: 0.00031983998282604335\n",
      "test loss is 0.00029424155261464983\n",
      "Batch: 24200,train loss is: 0.0005608367494453931\n",
      "test loss is 0.00029833510035587183\n",
      "Batch: 24300,train loss is: 0.00023933030316769245\n",
      "test loss is 0.00027680585370733646\n",
      "Batch: 24400,train loss is: 0.00027218088565148733\n",
      "test loss is 0.00028342681735760345\n",
      "Batch: 24500,train loss is: 0.000578364108546118\n",
      "test loss is 0.00032465053237977633\n",
      "Batch: 24600,train loss is: 0.0001949706324125245\n",
      "test loss is 0.0002745810850183937\n",
      "Batch: 24700,train loss is: 0.0003587609246813898\n",
      "test loss is 0.00035266407960944147\n",
      "Batch: 24800,train loss is: 0.00020704548407063172\n",
      "test loss is 0.0002743650203549275\n",
      "Batch: 24900,train loss is: 0.00033521261982360904\n",
      "test loss is 0.0002822004085498403\n",
      "Batch: 25000,train loss is: 0.0003999718206822969\n",
      "test loss is 0.00030522109393758096\n",
      "Batch: 25100,train loss is: 0.0002766940126228252\n",
      "test loss is 0.0003314542347284833\n",
      "Batch: 25200,train loss is: 0.0002171906343715783\n",
      "test loss is 0.000337693106120647\n",
      "Batch: 25300,train loss is: 0.00026311553560568824\n",
      "test loss is 0.0003035093811181784\n",
      "Batch: 25400,train loss is: 0.0002933950461455736\n",
      "test loss is 0.0002892868828203217\n",
      "Batch: 25500,train loss is: 0.0002627193975684213\n",
      "test loss is 0.0002545743846454042\n",
      "Batch: 25600,train loss is: 0.00020696077405624923\n",
      "test loss is 0.0003119225499608365\n",
      "Batch: 25700,train loss is: 0.0004291323005342496\n",
      "test loss is 0.0003541264066892117\n",
      "Batch: 25800,train loss is: 0.00029191468228980625\n",
      "test loss is 0.0002647252799597925\n",
      "Batch: 25900,train loss is: 0.00046682509407801516\n",
      "test loss is 0.0002647349718897094\n",
      "Batch: 26000,train loss is: 0.00016444722683158316\n",
      "test loss is 0.00026895728887996135\n",
      "Batch: 26100,train loss is: 0.0003019569166961133\n",
      "test loss is 0.00030397743119080884\n",
      "Batch: 26200,train loss is: 0.0003226180325339395\n",
      "test loss is 0.0003353898489437363\n",
      "Batch: 26300,train loss is: 0.0002642166775574571\n",
      "test loss is 0.0002742304277273307\n",
      "Batch: 26400,train loss is: 0.0002456719083893907\n",
      "test loss is 0.0003242466993974344\n",
      "Batch: 26500,train loss is: 0.00023432687849392735\n",
      "test loss is 0.000272051779023667\n",
      "Batch: 26600,train loss is: 0.00039565368972577903\n",
      "test loss is 0.0002973158525557526\n",
      "Batch: 26700,train loss is: 0.00019720596863250523\n",
      "test loss is 0.00029060801505212316\n",
      "Batch: 26800,train loss is: 0.00041245071582500417\n",
      "test loss is 0.0002869917760459869\n",
      "Batch: 26900,train loss is: 0.00027165416633065836\n",
      "test loss is 0.0003580084771589243\n",
      "Batch: 27000,train loss is: 0.00035432393685979767\n",
      "test loss is 0.0002809784291543926\n",
      "Batch: 27100,train loss is: 0.00030298108608360034\n",
      "test loss is 0.0002974920804877928\n",
      "Batch: 27200,train loss is: 0.0002735329261987998\n",
      "test loss is 0.0003027516463618983\n",
      "Batch: 27300,train loss is: 0.0002788367239205031\n",
      "test loss is 0.0002544194145840894\n",
      "Batch: 27400,train loss is: 0.0003731788464613092\n",
      "test loss is 0.000262505412424694\n",
      "Batch: 27500,train loss is: 0.00024315100697717604\n",
      "test loss is 0.00028625395028644414\n",
      "Batch: 27600,train loss is: 0.00036223730118505103\n",
      "test loss is 0.00034271698286785515\n",
      "Batch: 27700,train loss is: 0.00039777236314363643\n",
      "test loss is 0.00027063149856398216\n",
      "Batch: 27800,train loss is: 0.0003223119935874004\n",
      "test loss is 0.0002833532891672245\n",
      "Batch: 27900,train loss is: 0.00029510315930591987\n",
      "test loss is 0.0002654745214892693\n",
      "Batch: 28000,train loss is: 0.0002994424767511522\n",
      "test loss is 0.00028201394443902163\n",
      "Batch: 28100,train loss is: 0.00017744613512272724\n",
      "test loss is 0.0002857412825445775\n",
      "Batch: 28200,train loss is: 0.0003321123786962176\n",
      "test loss is 0.0002864754019661912\n",
      "Batch: 28300,train loss is: 0.0006031478516397805\n",
      "test loss is 0.00029980664010498053\n",
      "Batch: 28400,train loss is: 0.00029515273896613576\n",
      "test loss is 0.0003201799953946972\n",
      "Batch: 28500,train loss is: 0.0005039317049181497\n",
      "test loss is 0.00037334701421582866\n",
      "Batch: 28600,train loss is: 0.000208217083162004\n",
      "test loss is 0.000277526765064418\n",
      "Batch: 28700,train loss is: 0.0002956395722812265\n",
      "test loss is 0.0002697498802658304\n",
      "Batch: 28800,train loss is: 0.0003452949764638729\n",
      "test loss is 0.0003187760541986744\n",
      "Batch: 28900,train loss is: 0.00030689167682504566\n",
      "test loss is 0.00026691910142201495\n",
      "Batch: 29000,train loss is: 0.000716963490939615\n",
      "test loss is 0.0004104499009227549\n",
      "Batch: 29100,train loss is: 0.0003263929832109429\n",
      "test loss is 0.00028559426303967036\n",
      "Batch: 29200,train loss is: 0.00027221896315034465\n",
      "test loss is 0.0002959283598876759\n",
      "Batch: 29300,train loss is: 0.0002251113626792574\n",
      "test loss is 0.00029173464142092514\n",
      "Batch: 29400,train loss is: 0.00024624831014144136\n",
      "test loss is 0.00030498465973692\n",
      "Batch: 29500,train loss is: 0.00040987429087396425\n",
      "test loss is 0.00033571839273519754\n",
      "Batch: 29600,train loss is: 0.00023219249999740978\n",
      "test loss is 0.0003310097987663943\n",
      "Batch: 29700,train loss is: 0.000227660363235253\n",
      "test loss is 0.0003020303465149589\n",
      "Batch: 29800,train loss is: 0.0002717192263681834\n",
      "test loss is 0.0003148343777818897\n",
      "Batch: 29900,train loss is: 0.00036006704877478434\n",
      "test loss is 0.00030534104761912315\n",
      "Batch: 30000,train loss is: 0.0004272401072892112\n",
      "test loss is 0.00026774902170187723\n",
      "Batch: 30100,train loss is: 0.00021427335403361822\n",
      "test loss is 0.0002865273975246001\n",
      "Batch: 30200,train loss is: 0.00034710707375868377\n",
      "test loss is 0.0002836377201393226\n",
      "Batch: 30300,train loss is: 0.0003453780102791773\n",
      "test loss is 0.0002985999102067227\n",
      "Batch: 30400,train loss is: 0.0001851662942655632\n",
      "test loss is 0.00028323943947145436\n",
      "Batch: 30500,train loss is: 0.00021793501407178583\n",
      "test loss is 0.00029140872684061965\n",
      "Batch: 30600,train loss is: 0.0002352346200338673\n",
      "test loss is 0.0003371429296836302\n",
      "Batch: 30700,train loss is: 0.0002150578989275677\n",
      "test loss is 0.0003085367836461134\n",
      "Batch: 30800,train loss is: 0.0003039668354013882\n",
      "test loss is 0.0003189890464894302\n",
      "Batch: 30900,train loss is: 0.00024928367709884774\n",
      "test loss is 0.00033593443428014995\n",
      "Batch: 31000,train loss is: 0.00022712003024957854\n",
      "test loss is 0.0002682642859190432\n",
      "Batch: 31100,train loss is: 0.0003122791423058193\n",
      "test loss is 0.0003199981669004331\n",
      "Batch: 31200,train loss is: 0.0002488941721056433\n",
      "test loss is 0.00027709188721577614\n",
      "Batch: 31300,train loss is: 0.00036158399687306116\n",
      "test loss is 0.00030806038927650234\n",
      "Batch: 31400,train loss is: 0.00022621331699419826\n",
      "test loss is 0.00028906051638884645\n",
      "Batch: 31500,train loss is: 0.0003087185632196386\n",
      "test loss is 0.00032407645037762757\n",
      "Batch: 31600,train loss is: 0.0002508279698180509\n",
      "test loss is 0.0002908618341628998\n",
      "Batch: 31700,train loss is: 0.0004726801237880864\n",
      "test loss is 0.0003888485331408029\n",
      "Batch: 31800,train loss is: 0.00025288117422432685\n",
      "test loss is 0.0003044028179667932\n",
      "Batch: 31900,train loss is: 0.0003341414191382361\n",
      "test loss is 0.00033569648913060426\n",
      "Batch: 32000,train loss is: 0.0003817303364929889\n",
      "test loss is 0.0002824366763812812\n",
      "Batch: 32100,train loss is: 0.0002601812971604354\n",
      "test loss is 0.00026368536596618275\n",
      "Batch: 32200,train loss is: 0.00028284437199145433\n",
      "test loss is 0.00026848643642115634\n",
      "Batch: 32300,train loss is: 0.00025012363507070554\n",
      "test loss is 0.00031440460524530424\n",
      "Batch: 32400,train loss is: 0.00025585327550559154\n",
      "test loss is 0.0002871876707820838\n",
      "Batch: 32500,train loss is: 0.0002682408770525223\n",
      "test loss is 0.00030984150021357975\n",
      "Batch: 32600,train loss is: 0.00027870363290957836\n",
      "test loss is 0.00034894434913974805\n",
      "Batch: 32700,train loss is: 0.00027696482399752113\n",
      "test loss is 0.00028390875296707376\n",
      "Batch: 32800,train loss is: 0.00019333234351322272\n",
      "test loss is 0.00026386127621928464\n",
      "Batch: 32900,train loss is: 0.0002773406073104694\n",
      "test loss is 0.000282707650552991\n",
      "Batch: 33000,train loss is: 0.0003427785202846794\n",
      "test loss is 0.00027539662850371664\n",
      "Batch: 33100,train loss is: 0.0004962627360242045\n",
      "test loss is 0.0002930939716880599\n",
      "Batch: 33200,train loss is: 0.00024205695689657943\n",
      "test loss is 0.00026537497549384106\n",
      "Batch: 33300,train loss is: 0.00023312017371429812\n",
      "test loss is 0.00028743808553539934\n",
      "Batch: 33400,train loss is: 0.00039722584963442864\n",
      "test loss is 0.0002793870587200845\n",
      "Batch: 33500,train loss is: 0.0003433230610067364\n",
      "test loss is 0.0003703708260782932\n",
      "Batch: 33600,train loss is: 0.00026781991943265445\n",
      "test loss is 0.0003049688933158668\n",
      "Batch: 33700,train loss is: 0.00027041756024470177\n",
      "test loss is 0.000269025069833925\n",
      "Batch: 33800,train loss is: 0.0002466238698640824\n",
      "test loss is 0.000302730545376397\n",
      "Batch: 33900,train loss is: 0.00026083079696078554\n",
      "test loss is 0.00027599459513355013\n",
      "Batch: 34000,train loss is: 0.00030564187051881\n",
      "test loss is 0.0003048005505267532\n",
      "Batch: 34100,train loss is: 0.00020095066564973716\n",
      "test loss is 0.0002905258673252764\n",
      "Batch: 34200,train loss is: 0.00021419424798335183\n",
      "test loss is 0.0002709875310759895\n",
      "Batch: 34300,train loss is: 0.0002374367500584895\n",
      "test loss is 0.0003062856568361017\n",
      "Batch: 34400,train loss is: 0.0003505117632363933\n",
      "test loss is 0.00027087076709558593\n",
      "Batch: 34500,train loss is: 0.0002286383645935832\n",
      "test loss is 0.0002975264281585259\n",
      "Batch: 34600,train loss is: 0.0002340754233273232\n",
      "test loss is 0.00033428600037224774\n",
      "Batch: 34700,train loss is: 0.00028432055437514997\n",
      "test loss is 0.0004793457597112143\n",
      "Batch: 34800,train loss is: 0.000199317770119488\n",
      "test loss is 0.00027649235161407546\n",
      "Batch: 34900,train loss is: 0.0005924403690897812\n",
      "test loss is 0.00029186765035361614\n",
      "Batch: 35000,train loss is: 0.0002227376420552861\n",
      "test loss is 0.0003043502063020779\n",
      "Batch: 35100,train loss is: 0.0004924591616036156\n",
      "test loss is 0.00030074793357142234\n",
      "Batch: 35200,train loss is: 0.00022246683923291148\n",
      "test loss is 0.0002661736533440214\n",
      "Batch: 35300,train loss is: 0.00029665363804623544\n",
      "test loss is 0.000265837751926011\n",
      "Batch: 35400,train loss is: 0.000362513793760519\n",
      "test loss is 0.0002666276689915128\n",
      "Batch: 35500,train loss is: 0.0002476729481618701\n",
      "test loss is 0.00026561991139452184\n",
      "Batch: 35600,train loss is: 0.0002749697211883858\n",
      "test loss is 0.00027560429615950164\n",
      "Batch: 35700,train loss is: 0.000183402198900917\n",
      "test loss is 0.0003317213487495774\n",
      "Batch: 35800,train loss is: 0.0003139759909600411\n",
      "test loss is 0.00032285490131936156\n",
      "Batch: 35900,train loss is: 0.00022281048549216341\n",
      "test loss is 0.00030140348545838367\n",
      "Batch: 36000,train loss is: 0.0003492266157615944\n",
      "test loss is 0.00029427268560324875\n",
      "Batch: 36100,train loss is: 0.0003630750717517645\n",
      "test loss is 0.00028874738716834893\n",
      "Batch: 36200,train loss is: 0.0002051764637211239\n",
      "test loss is 0.0002564313973946635\n",
      "Batch: 36300,train loss is: 0.00031910669363017195\n",
      "test loss is 0.00026194926992564806\n",
      "Batch: 36400,train loss is: 0.00031839450605744476\n",
      "test loss is 0.0002807564072998994\n",
      "Batch: 36500,train loss is: 0.00023745411438682885\n",
      "test loss is 0.00026369042115150177\n",
      "Batch: 36600,train loss is: 0.0003251113320940943\n",
      "test loss is 0.0002590915953878623\n",
      "Batch: 36700,train loss is: 0.00021657576653432673\n",
      "test loss is 0.00027061563513618663\n",
      "Batch: 36800,train loss is: 0.0003094198958005478\n",
      "test loss is 0.0002840404041837527\n",
      "Batch: 36900,train loss is: 0.0003342459214061717\n",
      "test loss is 0.00034958421038057375\n",
      "Batch: 37000,train loss is: 0.00037490874399907716\n",
      "test loss is 0.00031901713548300974\n",
      "Batch: 37100,train loss is: 0.00035338123907262844\n",
      "test loss is 0.0003223197163861851\n",
      "Batch: 37200,train loss is: 0.00031982590063868725\n",
      "test loss is 0.0003190781703960371\n",
      "Batch: 37300,train loss is: 0.00019056774119583742\n",
      "test loss is 0.00028185693433331604\n",
      "Batch: 37400,train loss is: 0.0002007998369703083\n",
      "test loss is 0.00027408120247745653\n",
      "Batch: 37500,train loss is: 0.000327113145629798\n",
      "test loss is 0.0003112279150520981\n",
      "Batch: 37600,train loss is: 0.00024450939737710926\n",
      "test loss is 0.0003050512027028496\n",
      "Batch: 37700,train loss is: 0.0003224750866680328\n",
      "test loss is 0.0002895309070483367\n",
      "Batch: 37800,train loss is: 0.00014679161638139743\n",
      "test loss is 0.0002706461381494235\n",
      "Batch: 37900,train loss is: 0.0002676775752781407\n",
      "test loss is 0.00028734120059045314\n",
      "Batch: 38000,train loss is: 0.00015342416853455533\n",
      "test loss is 0.00025549953846287744\n",
      "Batch: 38100,train loss is: 0.0002898707908398878\n",
      "test loss is 0.00030733386063473293\n",
      "Batch: 38200,train loss is: 0.00032534808178282274\n",
      "test loss is 0.0003308904530134667\n",
      "Batch: 38300,train loss is: 0.000265958014245188\n",
      "test loss is 0.000298845745662084\n",
      "Batch: 38400,train loss is: 0.00028365564492540736\n",
      "test loss is 0.00029983501295178274\n",
      "Batch: 38500,train loss is: 0.00032089545999388627\n",
      "test loss is 0.00029681868195521004\n",
      "Batch: 38600,train loss is: 0.0002513518820704033\n",
      "test loss is 0.0002800735918970713\n",
      "Batch: 38700,train loss is: 0.0002488371318483366\n",
      "test loss is 0.00026564264858132755\n",
      "Batch: 38800,train loss is: 0.00039838061379639396\n",
      "test loss is 0.00038097100492101425\n",
      "Batch: 38900,train loss is: 0.00022118490491215007\n",
      "test loss is 0.0003080118336398774\n",
      "Batch: 39000,train loss is: 0.00023296646415450588\n",
      "test loss is 0.0002822118068741834\n",
      "Batch: 39100,train loss is: 0.0002409486848097372\n",
      "test loss is 0.0004931108068694013\n",
      "Batch: 39200,train loss is: 0.00031881336001254987\n",
      "test loss is 0.0002766929953903497\n",
      "Batch: 39300,train loss is: 0.0002598844358489351\n",
      "test loss is 0.00028339195561235604\n",
      "Batch: 39400,train loss is: 0.00022023515518107057\n",
      "test loss is 0.00027403543659074625\n",
      "Batch: 39500,train loss is: 0.00025815004536559937\n",
      "test loss is 0.0002808235136476478\n",
      "Batch: 39600,train loss is: 0.0003427184165103684\n",
      "test loss is 0.0002843629215869626\n",
      "Batch: 39700,train loss is: 0.00042219474429325987\n",
      "test loss is 0.0002963995339769156\n",
      "Batch: 39800,train loss is: 0.00024070727997871523\n",
      "test loss is 0.0003077963172714592\n",
      "Batch: 39900,train loss is: 0.0002841585203143688\n",
      "test loss is 0.00026624543118736947\n",
      "Batch: 40000,train loss is: 0.0002603668091197918\n",
      "test loss is 0.00028533598651224034\n",
      "Batch: 40100,train loss is: 0.0002967247356161131\n",
      "test loss is 0.00026027359963246726\n",
      "Batch: 40200,train loss is: 0.0002829098111463555\n",
      "test loss is 0.00028968320991225815\n",
      "Batch: 40300,train loss is: 0.00015763202047275682\n",
      "test loss is 0.0002658178080885351\n",
      "Batch: 40400,train loss is: 0.0002142445554699737\n",
      "test loss is 0.00026423377575026134\n",
      "Batch: 40500,train loss is: 0.0002633485747212286\n",
      "test loss is 0.00031493995165329054\n",
      "Batch: 40600,train loss is: 0.00040421104885653635\n",
      "test loss is 0.00035908804488445024\n",
      "Batch: 40700,train loss is: 0.00046788492529100593\n",
      "test loss is 0.00047710334688797455\n",
      "Batch: 40800,train loss is: 0.0002806866088351027\n",
      "test loss is 0.00026075416060426944\n",
      "Batch: 40900,train loss is: 0.0001673474151171552\n",
      "test loss is 0.00028290102687293666\n",
      "Batch: 41000,train loss is: 0.00036406610828922624\n",
      "test loss is 0.0003145014955187991\n",
      "Batch: 41100,train loss is: 0.00023421548990723383\n",
      "test loss is 0.0002946265008109463\n",
      "Batch: 41200,train loss is: 0.000384946777261395\n",
      "test loss is 0.0003464804882669468\n",
      "Batch: 41300,train loss is: 0.00021109955095115212\n",
      "test loss is 0.00027635329157019097\n",
      "Batch: 41400,train loss is: 0.0002949169772469701\n",
      "test loss is 0.0002637852997855236\n",
      "Batch: 41500,train loss is: 0.00031568582327146804\n",
      "test loss is 0.0002822928773683819\n",
      "Batch: 41600,train loss is: 0.00024552852624513304\n",
      "test loss is 0.00035242782299839864\n",
      "Batch: 41700,train loss is: 0.0004365389650546462\n",
      "test loss is 0.0003507681204703127\n",
      "Batch: 41800,train loss is: 0.00036143781863165177\n",
      "test loss is 0.0002837361047474383\n",
      "Batch: 41900,train loss is: 0.0002450709761039053\n",
      "test loss is 0.0002951585252556811\n",
      "Batch: 42000,train loss is: 0.00019771770331965878\n",
      "test loss is 0.00029420692377985625\n",
      "Batch: 42100,train loss is: 0.0002347930501019448\n",
      "test loss is 0.00039762127366497124\n",
      "Batch: 42200,train loss is: 0.00023856376531834923\n",
      "test loss is 0.00026300680684215134\n",
      "Batch: 42300,train loss is: 0.00024166300756636407\n",
      "test loss is 0.00027915964362318146\n",
      "Batch: 42400,train loss is: 0.00023725044872012664\n",
      "test loss is 0.0002755881225618785\n",
      "Batch: 42500,train loss is: 0.0003500802230660907\n",
      "test loss is 0.00026963431138333994\n",
      "Batch: 42600,train loss is: 0.0004203642777967956\n",
      "test loss is 0.0003889231441885298\n",
      "Batch: 42700,train loss is: 0.0002462228309008191\n",
      "test loss is 0.0003031258106260061\n",
      "Batch: 42800,train loss is: 0.00018102162281011957\n",
      "test loss is 0.0002739619632577173\n",
      "Batch: 42900,train loss is: 0.0001929068073977446\n",
      "test loss is 0.00030080727251952956\n",
      "Batch: 43000,train loss is: 0.00017742193197763844\n",
      "test loss is 0.0003197357593516847\n",
      "Batch: 43100,train loss is: 0.00023067950852358134\n",
      "test loss is 0.00028216866986639907\n",
      "Batch: 43200,train loss is: 0.00026423133524142856\n",
      "test loss is 0.0002559677861069444\n",
      "Batch: 43300,train loss is: 0.00019553617957806298\n",
      "test loss is 0.00026734673178280315\n",
      "Batch: 43400,train loss is: 0.00028525451101936634\n",
      "test loss is 0.00030959636290802154\n",
      "Batch: 43500,train loss is: 0.0006348998788011458\n",
      "test loss is 0.0002875674918542878\n",
      "Batch: 43600,train loss is: 0.00036091797304535283\n",
      "test loss is 0.0002774485555310956\n",
      "Batch: 43700,train loss is: 0.0002318812644361651\n",
      "test loss is 0.00030003639498068433\n",
      "Batch: 43800,train loss is: 0.00021727875819301407\n",
      "test loss is 0.00027547193563907675\n",
      "Batch: 43900,train loss is: 0.00028654011755800885\n",
      "test loss is 0.0002649916833087356\n",
      "Batch: 44000,train loss is: 0.00033759389448833113\n",
      "test loss is 0.0002858947225316846\n",
      "Batch: 44100,train loss is: 0.0003163428390847594\n",
      "test loss is 0.0002937015292256801\n",
      "Batch: 44200,train loss is: 0.0001912383063295028\n",
      "test loss is 0.00025415749209633403\n",
      "Batch: 44300,train loss is: 0.00020119135218893094\n",
      "test loss is 0.0002851992951916318\n",
      "Batch: 44400,train loss is: 0.00023423204293642825\n",
      "test loss is 0.0002621634157933121\n",
      "Batch: 44500,train loss is: 0.00029008568731682524\n",
      "test loss is 0.00029147834502772163\n",
      "Batch: 44600,train loss is: 0.0003115857855881338\n",
      "test loss is 0.00026134935433430394\n",
      "Batch: 44700,train loss is: 0.0002598219168631629\n",
      "test loss is 0.0002872901889883043\n",
      "Batch: 44800,train loss is: 0.0002909905561611169\n",
      "test loss is 0.0003029416134860855\n",
      "Batch: 44900,train loss is: 0.0002347130941526898\n",
      "test loss is 0.00030423011845027986\n",
      "Batch: 45000,train loss is: 0.0003496441397199772\n",
      "test loss is 0.0003496423190791406\n",
      "Batch: 45100,train loss is: 0.0002147418130438652\n",
      "test loss is 0.00027342820485262096\n",
      "Batch: 45200,train loss is: 0.0003191568595500585\n",
      "test loss is 0.00028190403687562433\n",
      "Batch: 45300,train loss is: 0.00033220456141765754\n",
      "test loss is 0.00027158659031203193\n",
      "Batch: 45400,train loss is: 0.00022448284792305658\n",
      "test loss is 0.00029067924196949366\n",
      "Batch: 45500,train loss is: 0.00034101602429634123\n",
      "test loss is 0.00026151920704851065\n",
      "Batch: 45600,train loss is: 0.00020063383498107156\n",
      "test loss is 0.00031022286180161415\n",
      "Batch: 45700,train loss is: 0.0007230524944350152\n",
      "test loss is 0.0002776350449066543\n",
      "Batch: 45800,train loss is: 0.000239984960818344\n",
      "test loss is 0.0002781522449778317\n",
      "Batch: 45900,train loss is: 0.00029513290064761663\n",
      "test loss is 0.0003178104731264056\n",
      "Batch: 46000,train loss is: 0.0002959974304245152\n",
      "test loss is 0.00028770949282703247\n",
      "Batch: 46100,train loss is: 0.00026487287990076237\n",
      "test loss is 0.00027387996007001826\n",
      "Batch: 46200,train loss is: 0.0005466457178344055\n",
      "test loss is 0.0003293980168135673\n",
      "Batch: 46300,train loss is: 0.0002957103437239376\n",
      "test loss is 0.0002713577142764792\n",
      "Batch: 46400,train loss is: 0.00018955319339013677\n",
      "test loss is 0.0002947245864720091\n",
      "Batch: 46500,train loss is: 0.0002211786579424358\n",
      "test loss is 0.000275247411545042\n",
      "Batch: 46600,train loss is: 0.0003002067382021037\n",
      "test loss is 0.00031972918796734534\n",
      "Batch: 46700,train loss is: 0.0002059261506162662\n",
      "test loss is 0.00028123779014449876\n",
      "-----------------------Epoch: 6----------------------------------\n",
      "Batch: 0,train loss is: 0.00021956237907095724\n",
      "test loss is 0.0002642982895456231\n",
      "Batch: 100,train loss is: 0.0003350551086538164\n",
      "test loss is 0.0002942144530904326\n",
      "Batch: 200,train loss is: 0.00029771732363049536\n",
      "test loss is 0.0002690980783896284\n",
      "Batch: 300,train loss is: 0.00017910990734298075\n",
      "test loss is 0.00026268788371249977\n",
      "Batch: 400,train loss is: 0.0004410698893649135\n",
      "test loss is 0.00026765808091721785\n",
      "Batch: 500,train loss is: 0.0002339493623923018\n",
      "test loss is 0.0002970320396669645\n",
      "Batch: 600,train loss is: 0.0002522882356468757\n",
      "test loss is 0.0003059648628280416\n",
      "Batch: 700,train loss is: 0.00020248360995148856\n",
      "test loss is 0.0002944441355751235\n",
      "Batch: 800,train loss is: 0.0002254171577204084\n",
      "test loss is 0.00028311504336828267\n",
      "Batch: 900,train loss is: 0.00022363234607625893\n",
      "test loss is 0.00029965905719130765\n",
      "Batch: 1000,train loss is: 0.0002204474350861359\n",
      "test loss is 0.00028950602867634665\n",
      "Batch: 1100,train loss is: 0.00024584686074242355\n",
      "test loss is 0.0002850218382667285\n",
      "Batch: 1200,train loss is: 0.0003073612760284204\n",
      "test loss is 0.00026349440268346635\n",
      "Batch: 1300,train loss is: 0.0006106552646607662\n",
      "test loss is 0.00030567555585993446\n",
      "Batch: 1400,train loss is: 0.0002518811958038296\n",
      "test loss is 0.00029398044202486403\n",
      "Batch: 1500,train loss is: 0.0002948203709090564\n",
      "test loss is 0.00029201034957589687\n",
      "Batch: 1600,train loss is: 0.0003015453983058895\n",
      "test loss is 0.0002727228285042908\n",
      "Batch: 1700,train loss is: 0.00022649658914338416\n",
      "test loss is 0.000279433066283039\n",
      "Batch: 1800,train loss is: 0.00037624281765641333\n",
      "test loss is 0.00026417842953901265\n",
      "Batch: 1900,train loss is: 0.0002641136521614505\n",
      "test loss is 0.00032213804410707446\n",
      "Batch: 2000,train loss is: 0.0003070026551641482\n",
      "test loss is 0.0002702765550479465\n",
      "Batch: 2100,train loss is: 0.00025224504484082453\n",
      "test loss is 0.0003049566669459816\n",
      "Batch: 2200,train loss is: 0.00021435211882924872\n",
      "test loss is 0.0002878465193945736\n",
      "Batch: 2300,train loss is: 0.000365869241819002\n",
      "test loss is 0.0003178917560844975\n",
      "Batch: 2400,train loss is: 0.00021820246939223967\n",
      "test loss is 0.000289686271962926\n",
      "Batch: 2500,train loss is: 0.0002375224071766092\n",
      "test loss is 0.0002773053566875201\n",
      "Batch: 2600,train loss is: 0.0001961478685133195\n",
      "test loss is 0.00026357287372172995\n",
      "Batch: 2700,train loss is: 0.0002248176927267592\n",
      "test loss is 0.0002968664109473563\n",
      "Batch: 2800,train loss is: 0.00029755419969953046\n",
      "test loss is 0.0002733270885641803\n",
      "Batch: 2900,train loss is: 0.00020496316262028454\n",
      "test loss is 0.00030780476522466305\n",
      "Batch: 3000,train loss is: 0.0002410713086462113\n",
      "test loss is 0.0003916691945392992\n",
      "Batch: 3100,train loss is: 0.0002745758728598266\n",
      "test loss is 0.0005549172355077929\n",
      "Batch: 3200,train loss is: 0.00018094413232347113\n",
      "test loss is 0.0003327755052744534\n",
      "Batch: 3300,train loss is: 0.00027513168544653435\n",
      "test loss is 0.0003194415846316\n",
      "Batch: 3400,train loss is: 0.00023524572938174963\n",
      "test loss is 0.00033033881831173435\n",
      "Batch: 3500,train loss is: 0.00018517355168609845\n",
      "test loss is 0.0002730801448653145\n",
      "Batch: 3600,train loss is: 0.00020848285988371416\n",
      "test loss is 0.0002832367647379898\n",
      "Batch: 3700,train loss is: 0.000222391467434347\n",
      "test loss is 0.0002597732122836751\n",
      "Batch: 3800,train loss is: 0.00027545211925393716\n",
      "test loss is 0.0002682228269610502\n",
      "Batch: 3900,train loss is: 0.00018213931219957354\n",
      "test loss is 0.0002542818729189615\n",
      "Batch: 4000,train loss is: 0.0003036942823497748\n",
      "test loss is 0.00029283199546618053\n",
      "Batch: 4100,train loss is: 0.00029651774671942213\n",
      "test loss is 0.00030374338216387263\n",
      "Batch: 4200,train loss is: 0.0002163735195202879\n",
      "test loss is 0.0002844775536411003\n",
      "Batch: 4300,train loss is: 0.00028058847598098685\n",
      "test loss is 0.00029714220857746863\n",
      "Batch: 4400,train loss is: 0.00025179620923365524\n",
      "test loss is 0.00027028747010396614\n",
      "Batch: 4500,train loss is: 0.0003025177139949642\n",
      "test loss is 0.00027240536845484306\n",
      "Batch: 4600,train loss is: 0.0002782610026071706\n",
      "test loss is 0.0002598829082238833\n",
      "Batch: 4700,train loss is: 0.0003182422495258651\n",
      "test loss is 0.0002883104873053024\n",
      "Batch: 4800,train loss is: 0.000260891878294523\n",
      "test loss is 0.00028831415897307246\n",
      "Batch: 4900,train loss is: 0.000212709496224667\n",
      "test loss is 0.00036039286889659304\n",
      "Batch: 5000,train loss is: 0.0003377231565710567\n",
      "test loss is 0.00028863461682903063\n",
      "Batch: 5100,train loss is: 0.000267904518315802\n",
      "test loss is 0.00031418064139523\n",
      "Batch: 5200,train loss is: 0.00026213901437224877\n",
      "test loss is 0.0002777983732712546\n",
      "Batch: 5300,train loss is: 0.0002858724638791833\n",
      "test loss is 0.0002942797725999334\n",
      "Batch: 5400,train loss is: 0.00015220072232050515\n",
      "test loss is 0.0002978730725119386\n",
      "Batch: 5500,train loss is: 0.0001840860294430222\n",
      "test loss is 0.00030316869519994566\n",
      "Batch: 5600,train loss is: 0.000254058531266188\n",
      "test loss is 0.0002862862956547464\n",
      "Batch: 5700,train loss is: 0.0002738516379870663\n",
      "test loss is 0.00026027765365057985\n",
      "Batch: 5800,train loss is: 0.00023731490629896206\n",
      "test loss is 0.0003013661858944366\n",
      "Batch: 5900,train loss is: 0.0004344443370526598\n",
      "test loss is 0.0002689860318937574\n",
      "Batch: 6000,train loss is: 0.00028617563317314743\n",
      "test loss is 0.0003284414475255955\n",
      "Batch: 6100,train loss is: 0.0001926815429653515\n",
      "test loss is 0.00031213648751435313\n",
      "Batch: 6200,train loss is: 0.00023968887321373553\n",
      "test loss is 0.00029753153302468143\n",
      "Batch: 6300,train loss is: 0.00021719376253291682\n",
      "test loss is 0.00031144808001037943\n",
      "Batch: 6400,train loss is: 0.0003467735401615627\n",
      "test loss is 0.00026508065836800535\n",
      "Batch: 6500,train loss is: 0.00034509140259100476\n",
      "test loss is 0.0002973841761445213\n",
      "Batch: 6600,train loss is: 0.00020370039258173222\n",
      "test loss is 0.00026165389209441563\n",
      "Batch: 6700,train loss is: 0.000194639114194204\n",
      "test loss is 0.0002564640308058744\n",
      "Batch: 6800,train loss is: 0.0001927781815688263\n",
      "test loss is 0.0002819881994496225\n",
      "Batch: 6900,train loss is: 0.0002385899596446247\n",
      "test loss is 0.00028444676306450437\n",
      "Batch: 7000,train loss is: 0.00025368194353260075\n",
      "test loss is 0.00028089788212650206\n",
      "Batch: 7100,train loss is: 0.0004017598965372264\n",
      "test loss is 0.00030478048512339083\n",
      "Batch: 7200,train loss is: 0.0004886444307178345\n",
      "test loss is 0.00031641302038550495\n",
      "Batch: 7300,train loss is: 0.0002819852963923755\n",
      "test loss is 0.0002958152806539803\n",
      "Batch: 7400,train loss is: 0.0003230171669295005\n",
      "test loss is 0.00030391234517358024\n",
      "Batch: 7500,train loss is: 0.00039656195983874383\n",
      "test loss is 0.0002573722989101399\n",
      "Batch: 7600,train loss is: 0.00047787706439584763\n",
      "test loss is 0.0002599775691054779\n",
      "Batch: 7700,train loss is: 0.00034885007005442195\n",
      "test loss is 0.00027105106442225406\n",
      "Batch: 7800,train loss is: 0.00016461563946423884\n",
      "test loss is 0.00026230780746739886\n",
      "Batch: 7900,train loss is: 0.0005502494759440527\n",
      "test loss is 0.0002666833001498341\n",
      "Batch: 8000,train loss is: 0.0001629512919040371\n",
      "test loss is 0.00025464930157032884\n",
      "Batch: 8100,train loss is: 0.00018958619095242943\n",
      "test loss is 0.0003043538410664702\n",
      "Batch: 8200,train loss is: 0.00036977935746183545\n",
      "test loss is 0.0002777793885630078\n",
      "Batch: 8300,train loss is: 0.00027531413910090975\n",
      "test loss is 0.0002622768038390477\n",
      "Batch: 8400,train loss is: 0.0003972791485362043\n",
      "test loss is 0.0002797377303858383\n",
      "Batch: 8500,train loss is: 0.00032306376419649694\n",
      "test loss is 0.00031120386667353363\n",
      "Batch: 8600,train loss is: 0.00019198490707631326\n",
      "test loss is 0.00026432937930715036\n",
      "Batch: 8700,train loss is: 0.00025055023661457775\n",
      "test loss is 0.00028555778284842793\n",
      "Batch: 8800,train loss is: 0.00032511092787599184\n",
      "test loss is 0.00028477220907446724\n",
      "Batch: 8900,train loss is: 0.00019750174706573145\n",
      "test loss is 0.0002855713428129486\n",
      "Batch: 9000,train loss is: 0.00017795978895059606\n",
      "test loss is 0.0002816628766003912\n",
      "Batch: 9100,train loss is: 0.00023727712242470602\n",
      "test loss is 0.00027687418606368717\n",
      "Batch: 9200,train loss is: 0.0005250384502639377\n",
      "test loss is 0.00030689723711111767\n",
      "Batch: 9300,train loss is: 0.00021219784629230163\n",
      "test loss is 0.00028228659249614885\n",
      "Batch: 9400,train loss is: 0.00025532300857203496\n",
      "test loss is 0.00026752826548632894\n",
      "Batch: 9500,train loss is: 0.00014403333879859254\n",
      "test loss is 0.0002774796769184904\n",
      "Batch: 9600,train loss is: 0.0002867321977700117\n",
      "test loss is 0.00028161116067205175\n",
      "Batch: 9700,train loss is: 0.0002784387336458117\n",
      "test loss is 0.0003013019062557383\n",
      "Batch: 9800,train loss is: 0.0002585308486722904\n",
      "test loss is 0.0002792971669570741\n",
      "Batch: 9900,train loss is: 0.0002920973604164142\n",
      "test loss is 0.0002634566242611499\n",
      "Batch: 10000,train loss is: 0.0002595715587312541\n",
      "test loss is 0.00033865377389056015\n",
      "Batch: 10100,train loss is: 0.0003112803271715909\n",
      "test loss is 0.00037369122315600555\n",
      "Batch: 10200,train loss is: 0.00035492353936718654\n",
      "test loss is 0.0002882631718927627\n",
      "Batch: 10300,train loss is: 0.0001894975342260603\n",
      "test loss is 0.00028212391666990026\n",
      "Batch: 10400,train loss is: 0.0003285523704510975\n",
      "test loss is 0.00026134194203806995\n",
      "Batch: 10500,train loss is: 0.000416176616252064\n",
      "test loss is 0.00041386602178327834\n",
      "Batch: 10600,train loss is: 0.0001870884586456889\n",
      "test loss is 0.0002824518679221828\n",
      "Batch: 10700,train loss is: 0.000439392323825541\n",
      "test loss is 0.00035595161143203897\n",
      "Batch: 10800,train loss is: 0.0002808073800669096\n",
      "test loss is 0.00028496969817235466\n",
      "Batch: 10900,train loss is: 0.0004012106790657059\n",
      "test loss is 0.00027384689396743795\n",
      "Batch: 11000,train loss is: 0.00033248631286377655\n",
      "test loss is 0.0002922392051005131\n",
      "Batch: 11100,train loss is: 0.000289202221901933\n",
      "test loss is 0.000276047027536473\n",
      "Batch: 11200,train loss is: 0.0004024724141393226\n",
      "test loss is 0.00026415433547669146\n",
      "Batch: 11300,train loss is: 0.00025210828513237265\n",
      "test loss is 0.00026824551652571875\n",
      "Batch: 11400,train loss is: 0.00013922416600541587\n",
      "test loss is 0.00028659730108951886\n",
      "Batch: 11500,train loss is: 0.0002733547244065937\n",
      "test loss is 0.0002608141712521792\n",
      "Batch: 11600,train loss is: 0.00041727712129882734\n",
      "test loss is 0.00030572974581211316\n",
      "Batch: 11700,train loss is: 0.00032308197361251474\n",
      "test loss is 0.0002988727561912253\n",
      "Batch: 11800,train loss is: 0.0002589833166292358\n",
      "test loss is 0.0002758896238416663\n",
      "Batch: 11900,train loss is: 0.00021967576938951465\n",
      "test loss is 0.00025797229759173666\n",
      "Batch: 12000,train loss is: 0.0003404343327164553\n",
      "test loss is 0.0003337961442965613\n",
      "Batch: 12100,train loss is: 0.0002176871654745814\n",
      "test loss is 0.0002883302330098951\n",
      "Batch: 12200,train loss is: 0.00021871231502468177\n",
      "test loss is 0.0002643966396584478\n",
      "Batch: 12300,train loss is: 0.00022692695619595723\n",
      "test loss is 0.000310251861793499\n",
      "Batch: 12400,train loss is: 0.000216486056405903\n",
      "test loss is 0.00028920561637276913\n",
      "Batch: 12500,train loss is: 0.00025139591621427493\n",
      "test loss is 0.00031008773936919674\n",
      "Batch: 12600,train loss is: 0.0005783879844074105\n",
      "test loss is 0.0003024013419838881\n",
      "Batch: 12700,train loss is: 0.00022456524916938811\n",
      "test loss is 0.000289183695458528\n",
      "Batch: 12800,train loss is: 0.0002563802582693104\n",
      "test loss is 0.0003119377643683409\n",
      "Batch: 12900,train loss is: 0.0007877123739702479\n",
      "test loss is 0.00026342340462123077\n",
      "Batch: 13000,train loss is: 0.0003028871980250705\n",
      "test loss is 0.00026719119706785277\n",
      "Batch: 13100,train loss is: 0.00023739914102663336\n",
      "test loss is 0.00026948938482491253\n",
      "Batch: 13200,train loss is: 0.00026594235127682015\n",
      "test loss is 0.0002885071756004076\n",
      "Batch: 13300,train loss is: 0.0003320504537927647\n",
      "test loss is 0.0002955447416550169\n",
      "Batch: 13400,train loss is: 0.00028646145644257455\n",
      "test loss is 0.0003151889231995242\n",
      "Batch: 13500,train loss is: 0.00022702532791494818\n",
      "test loss is 0.0002684612533484292\n",
      "Batch: 13600,train loss is: 0.0003053547589143367\n",
      "test loss is 0.00026817067321366453\n",
      "Batch: 13700,train loss is: 0.00022746549687652616\n",
      "test loss is 0.00032400089079549793\n",
      "Batch: 13800,train loss is: 0.00029672058237852425\n",
      "test loss is 0.0003428926855785969\n",
      "Batch: 13900,train loss is: 0.00046811863654643737\n",
      "test loss is 0.00026454082465017775\n",
      "Batch: 14000,train loss is: 0.00030770850417800313\n",
      "test loss is 0.0002590240986957131\n",
      "Batch: 14100,train loss is: 0.00041985292719573964\n",
      "test loss is 0.0003180836827405927\n",
      "Batch: 14200,train loss is: 0.0003204508057280378\n",
      "test loss is 0.0002601902349816917\n",
      "Batch: 14300,train loss is: 0.0002061795370367192\n",
      "test loss is 0.000259946548565747\n",
      "Batch: 14400,train loss is: 0.00032686472158186875\n",
      "test loss is 0.0002614839605961536\n",
      "Batch: 14500,train loss is: 0.00022908550825367463\n",
      "test loss is 0.0002619376899317929\n",
      "Batch: 14600,train loss is: 0.00024264730147735298\n",
      "test loss is 0.0002904916559466842\n",
      "Batch: 14700,train loss is: 0.00035398203015501377\n",
      "test loss is 0.00032235387692294875\n",
      "Batch: 14800,train loss is: 0.0002483861346363283\n",
      "test loss is 0.00027728625746106523\n",
      "Batch: 14900,train loss is: 0.00024404009481467087\n",
      "test loss is 0.00028533869792724936\n",
      "Batch: 15000,train loss is: 0.00025610944909066236\n",
      "test loss is 0.00028906178440143727\n",
      "Batch: 15100,train loss is: 0.0002006680840200781\n",
      "test loss is 0.00028926642109043774\n",
      "Batch: 15200,train loss is: 0.00024817167932795196\n",
      "test loss is 0.00031476518191613905\n",
      "Batch: 15300,train loss is: 0.00022020773619704008\n",
      "test loss is 0.00027310536166833704\n",
      "Batch: 15400,train loss is: 0.00027405325950118347\n",
      "test loss is 0.0002748911075342407\n",
      "Batch: 15500,train loss is: 0.0001514569011219973\n",
      "test loss is 0.00027165317870664156\n",
      "Batch: 15600,train loss is: 0.00024229847756591333\n",
      "test loss is 0.0002922695806156479\n",
      "Batch: 15700,train loss is: 0.0001816292266463652\n",
      "test loss is 0.0002810181646055988\n",
      "Batch: 15800,train loss is: 0.0001566798139852769\n",
      "test loss is 0.00029672489590044015\n",
      "Batch: 15900,train loss is: 0.0004500934988316907\n",
      "test loss is 0.0003352107667049325\n",
      "Batch: 16000,train loss is: 0.00033943775676941064\n",
      "test loss is 0.0002799847057294792\n",
      "Batch: 16100,train loss is: 0.00032245244057967163\n",
      "test loss is 0.00027010845886572447\n",
      "Batch: 16200,train loss is: 0.00022236064283526457\n",
      "test loss is 0.0002742847666422475\n",
      "Batch: 16300,train loss is: 0.00016095659794574448\n",
      "test loss is 0.00026268174440102293\n",
      "Batch: 16400,train loss is: 0.00026465412992021366\n",
      "test loss is 0.00030575514790273943\n",
      "Batch: 16500,train loss is: 0.0002100909179936457\n",
      "test loss is 0.0002959906523672695\n",
      "Batch: 16600,train loss is: 0.0002804600940961807\n",
      "test loss is 0.00032931996281153954\n",
      "Batch: 16700,train loss is: 0.0004648538804389623\n",
      "test loss is 0.0002895842310900925\n",
      "Batch: 16800,train loss is: 0.00022575542083292055\n",
      "test loss is 0.0002621599069634242\n",
      "Batch: 16900,train loss is: 0.00021836508155996247\n",
      "test loss is 0.0002904705880615435\n",
      "Batch: 17000,train loss is: 0.00032242498936510474\n",
      "test loss is 0.0002732354240288329\n",
      "Batch: 17100,train loss is: 0.0005059815689761141\n",
      "test loss is 0.00027868111984115964\n",
      "Batch: 17200,train loss is: 0.00041990869841032467\n",
      "test loss is 0.00028918345702165347\n",
      "Batch: 17300,train loss is: 0.0002407056777060639\n",
      "test loss is 0.00026584705969242466\n",
      "Batch: 17400,train loss is: 0.00020547199292666023\n",
      "test loss is 0.0002931789106997088\n",
      "Batch: 17500,train loss is: 0.00044391487183023\n",
      "test loss is 0.000261594638005802\n",
      "Batch: 17600,train loss is: 0.0002825194921150702\n",
      "test loss is 0.0002573124461050897\n",
      "Batch: 17700,train loss is: 0.000641876881207856\n",
      "test loss is 0.0002832830472642534\n",
      "Batch: 17800,train loss is: 0.0003230655248347156\n",
      "test loss is 0.0002787215194887758\n",
      "Batch: 17900,train loss is: 0.0002871647646300069\n",
      "test loss is 0.0003165758784838325\n",
      "Batch: 18000,train loss is: 0.000331502581519075\n",
      "test loss is 0.00028290351492550706\n",
      "Batch: 18100,train loss is: 0.0013849866338276832\n",
      "test loss is 0.0002807945678559427\n",
      "Batch: 18200,train loss is: 0.00027758835891138916\n",
      "test loss is 0.0002704060595779486\n",
      "Batch: 18300,train loss is: 0.00028893039598558293\n",
      "test loss is 0.0003022548144162108\n",
      "Batch: 18400,train loss is: 0.0002761172408457268\n",
      "test loss is 0.000270198084554453\n",
      "Batch: 18500,train loss is: 0.00026995964966535163\n",
      "test loss is 0.00027109887579899967\n",
      "Batch: 18600,train loss is: 0.0003522091748204959\n",
      "test loss is 0.0002832413014074689\n",
      "Batch: 18700,train loss is: 0.00035840690933343605\n",
      "test loss is 0.0002700226510796719\n",
      "Batch: 18800,train loss is: 0.0002568640217231068\n",
      "test loss is 0.00030343268648174197\n",
      "Batch: 18900,train loss is: 0.0002712919572280939\n",
      "test loss is 0.0002711456545192835\n",
      "Batch: 19000,train loss is: 0.0005326346402485025\n",
      "test loss is 0.0003085520372322995\n",
      "Batch: 19100,train loss is: 0.0002797061360354086\n",
      "test loss is 0.00033363055709548754\n",
      "Batch: 19200,train loss is: 0.0007013937118053345\n",
      "test loss is 0.0002698501865265559\n",
      "Batch: 19300,train loss is: 0.0002252510418286266\n",
      "test loss is 0.00027596184479823654\n",
      "Batch: 19400,train loss is: 0.0002480975044003661\n",
      "test loss is 0.0002857697698150289\n",
      "Batch: 19500,train loss is: 0.00019540290972587805\n",
      "test loss is 0.00025444090350620397\n",
      "Batch: 19600,train loss is: 0.0002621338441314179\n",
      "test loss is 0.00028956469903857986\n",
      "Batch: 19700,train loss is: 0.00019369236906072453\n",
      "test loss is 0.0002949969919954924\n",
      "Batch: 19800,train loss is: 0.00018608648067058858\n",
      "test loss is 0.00029564478270014576\n",
      "Batch: 19900,train loss is: 0.00024186828234797536\n",
      "test loss is 0.0002658473263023165\n",
      "Batch: 20000,train loss is: 0.00025112470312564964\n",
      "test loss is 0.0002539653311254915\n",
      "Batch: 20100,train loss is: 0.00033446563939915986\n",
      "test loss is 0.0003098060915267167\n",
      "Batch: 20200,train loss is: 0.0005362992786268124\n",
      "test loss is 0.0003242073113399818\n",
      "Batch: 20300,train loss is: 0.0004642214249462059\n",
      "test loss is 0.00035765636603607795\n",
      "Batch: 20400,train loss is: 0.000156061013171738\n",
      "test loss is 0.0002579507538943215\n",
      "Batch: 20500,train loss is: 0.00020352403753362612\n",
      "test loss is 0.0003003788007595345\n",
      "Batch: 20600,train loss is: 0.0002555978448083827\n",
      "test loss is 0.00027912099489420575\n",
      "Batch: 20700,train loss is: 0.0001848575657798228\n",
      "test loss is 0.00027597652266736977\n",
      "Batch: 20800,train loss is: 0.0002465193802450806\n",
      "test loss is 0.00030362820386677824\n",
      "Batch: 20900,train loss is: 0.000251991364628761\n",
      "test loss is 0.0002808566115591988\n",
      "Batch: 21000,train loss is: 0.00033664671902113616\n",
      "test loss is 0.0003085675855981153\n",
      "Batch: 21100,train loss is: 0.00025410338578145397\n",
      "test loss is 0.00027488815951373256\n",
      "Batch: 21200,train loss is: 0.0002400964474137834\n",
      "test loss is 0.0002707045340305929\n",
      "Batch: 21300,train loss is: 0.00021200059030783276\n",
      "test loss is 0.0002551706117598231\n",
      "Batch: 21400,train loss is: 0.0002511891947160463\n",
      "test loss is 0.00028550797025185775\n",
      "Batch: 21500,train loss is: 0.00030027786696162776\n",
      "test loss is 0.00027308680475219524\n",
      "Batch: 21600,train loss is: 0.00025727854783123225\n",
      "test loss is 0.00031633431952661424\n",
      "Batch: 21700,train loss is: 0.00018572886145364268\n",
      "test loss is 0.00027774262719827605\n",
      "Batch: 21800,train loss is: 0.00020073203122140585\n",
      "test loss is 0.0003225241383299546\n",
      "Batch: 21900,train loss is: 0.00022217658191956216\n",
      "test loss is 0.00025514871751195845\n",
      "Batch: 22000,train loss is: 0.00030309605318699386\n",
      "test loss is 0.00042464808232297544\n",
      "Batch: 22100,train loss is: 0.00032131379352238684\n",
      "test loss is 0.00027028283503762243\n",
      "Batch: 22200,train loss is: 0.0004042218062886687\n",
      "test loss is 0.0003231688464860994\n",
      "Batch: 22300,train loss is: 0.00044403865805484326\n",
      "test loss is 0.000292180249859386\n",
      "Batch: 22400,train loss is: 0.0003163614139147621\n",
      "test loss is 0.00030442239090856837\n",
      "Batch: 22500,train loss is: 0.000349475515332414\n",
      "test loss is 0.00031988118005046466\n",
      "Batch: 22600,train loss is: 0.0003714501104469327\n",
      "test loss is 0.0002656651653441455\n",
      "Batch: 22700,train loss is: 0.00031620001222480677\n",
      "test loss is 0.00035068760774998426\n",
      "Batch: 22800,train loss is: 0.00029619317445394905\n",
      "test loss is 0.0003243928077916906\n",
      "Batch: 22900,train loss is: 0.00021756395136915434\n",
      "test loss is 0.0002861780045129912\n",
      "Batch: 23000,train loss is: 0.0003051783047733647\n",
      "test loss is 0.000265123408868985\n",
      "Batch: 23100,train loss is: 0.00027430379906739016\n",
      "test loss is 0.0002671261767395666\n",
      "Batch: 23200,train loss is: 0.00030257550715926155\n",
      "test loss is 0.0002978868196487646\n",
      "Batch: 23300,train loss is: 0.000317167984355926\n",
      "test loss is 0.0003104044177688729\n",
      "Batch: 23400,train loss is: 0.00023735579821046706\n",
      "test loss is 0.0002853240838185826\n",
      "Batch: 23500,train loss is: 0.00018849347571249218\n",
      "test loss is 0.00027633503668219645\n",
      "Batch: 23600,train loss is: 0.0005003042728573299\n",
      "test loss is 0.00029854408051067704\n",
      "Batch: 23700,train loss is: 0.0004005724902010153\n",
      "test loss is 0.0003768887292806621\n",
      "Batch: 23800,train loss is: 0.00017532905569096795\n",
      "test loss is 0.00026309892687455255\n",
      "Batch: 23900,train loss is: 0.00024405106575730776\n",
      "test loss is 0.00025637140554548985\n",
      "Batch: 24000,train loss is: 0.00026669955947961803\n",
      "test loss is 0.00027023556680984777\n",
      "Batch: 24100,train loss is: 0.00025313873167662974\n",
      "test loss is 0.0003523303525276538\n",
      "Batch: 24200,train loss is: 0.00022224631593462847\n",
      "test loss is 0.00025191006626169903\n",
      "Batch: 24300,train loss is: 0.0002867230714286798\n",
      "test loss is 0.000267975856999175\n",
      "Batch: 24400,train loss is: 0.0003185161178248455\n",
      "test loss is 0.0002730153207316895\n",
      "Batch: 24500,train loss is: 0.0004237220061627514\n",
      "test loss is 0.0004428577162032357\n",
      "Batch: 24600,train loss is: 0.00023275810595666034\n",
      "test loss is 0.00028654910864368675\n",
      "Batch: 24700,train loss is: 0.00022260229442762027\n",
      "test loss is 0.0002895983871594093\n",
      "Batch: 24800,train loss is: 0.00020754037811321975\n",
      "test loss is 0.0002876982085413151\n",
      "Batch: 24900,train loss is: 0.0002819871922169031\n",
      "test loss is 0.00029754769356759875\n",
      "Batch: 25000,train loss is: 0.0002921357482474256\n",
      "test loss is 0.00041363808472147124\n",
      "Batch: 25100,train loss is: 0.00023649823988936504\n",
      "test loss is 0.0002613953737529271\n",
      "Batch: 25200,train loss is: 0.00014186936416151258\n",
      "test loss is 0.0002780826535447676\n",
      "Batch: 25300,train loss is: 0.00033724549557176555\n",
      "test loss is 0.0003522479278663746\n",
      "Batch: 25400,train loss is: 0.0001925139477222154\n",
      "test loss is 0.000301314133032373\n",
      "Batch: 25500,train loss is: 0.0002860529162740935\n",
      "test loss is 0.000383774995176118\n",
      "Batch: 25600,train loss is: 0.00030744625876077806\n",
      "test loss is 0.000260410632163125\n",
      "Batch: 25700,train loss is: 0.0003088130746542984\n",
      "test loss is 0.0003353491789167264\n",
      "Batch: 25800,train loss is: 0.00021358435090570687\n",
      "test loss is 0.0003111512980497942\n",
      "Batch: 25900,train loss is: 0.00023143387762251017\n",
      "test loss is 0.0002808514215541248\n",
      "Batch: 26000,train loss is: 0.0003088228380363815\n",
      "test loss is 0.0002816614774520543\n",
      "Batch: 26100,train loss is: 0.0001740530846513277\n",
      "test loss is 0.0002585634416328397\n",
      "Batch: 26200,train loss is: 0.00021807977484851498\n",
      "test loss is 0.0002678082180934043\n",
      "Batch: 26300,train loss is: 0.0002870221958081095\n",
      "test loss is 0.00027494168313373774\n",
      "Batch: 26400,train loss is: 0.00021082055446939045\n",
      "test loss is 0.0002663308621280315\n",
      "Batch: 26500,train loss is: 0.00032214272679127374\n",
      "test loss is 0.0002505102165516024\n",
      "Batch: 26600,train loss is: 0.00019052898302699455\n",
      "test loss is 0.00027380020980482427\n",
      "Batch: 26700,train loss is: 0.00030553944209811784\n",
      "test loss is 0.0003019649170594664\n",
      "Batch: 26800,train loss is: 0.00024948899250854255\n",
      "test loss is 0.0002932055271220555\n",
      "Batch: 26900,train loss is: 0.00024866014732234095\n",
      "test loss is 0.00025536993758292495\n",
      "Batch: 27000,train loss is: 0.0002006802116409715\n",
      "test loss is 0.000254713760187352\n",
      "Batch: 27100,train loss is: 0.00032516946593840514\n",
      "test loss is 0.0002871578226463932\n",
      "Batch: 27200,train loss is: 0.00031369586491200975\n",
      "test loss is 0.00027809502101002257\n",
      "Batch: 27300,train loss is: 0.00029353618292401367\n",
      "test loss is 0.0003216218054420349\n",
      "Batch: 27400,train loss is: 0.00017023756694152747\n",
      "test loss is 0.0002533621675305968\n",
      "Batch: 27500,train loss is: 0.00019881111539401096\n",
      "test loss is 0.00026403247539436107\n",
      "Batch: 27600,train loss is: 0.00028590228811582784\n",
      "test loss is 0.0002979762282579735\n",
      "Batch: 27700,train loss is: 0.0004671609806777177\n",
      "test loss is 0.00035357567683668117\n",
      "Batch: 27800,train loss is: 0.00017277971652493\n",
      "test loss is 0.00028879687684610545\n",
      "Batch: 27900,train loss is: 0.0003729359946994453\n",
      "test loss is 0.00032016865384041916\n",
      "Batch: 28000,train loss is: 0.00019325717755171767\n",
      "test loss is 0.00027092153856035785\n",
      "Batch: 28100,train loss is: 0.00020257681937199651\n",
      "test loss is 0.0002825037258457632\n",
      "Batch: 28200,train loss is: 0.00028346052367719517\n",
      "test loss is 0.00025144896038943\n",
      "Batch: 28300,train loss is: 0.00039927634059650196\n",
      "test loss is 0.00027680009733352005\n",
      "Batch: 28400,train loss is: 0.00019074973702344421\n",
      "test loss is 0.00024404726704844434\n",
      "Batch: 28500,train loss is: 0.0002670945092349197\n",
      "test loss is 0.0003562199181546648\n",
      "Batch: 28600,train loss is: 0.0002090052190191011\n",
      "test loss is 0.0002554460733537429\n",
      "Batch: 28700,train loss is: 0.00034462258139536593\n",
      "test loss is 0.0002673544486198098\n",
      "Batch: 28800,train loss is: 0.0003479269745382176\n",
      "test loss is 0.0002536759404311049\n",
      "Batch: 28900,train loss is: 0.0002630130228647629\n",
      "test loss is 0.00025457734449631177\n",
      "Batch: 29000,train loss is: 0.00021218204689876335\n",
      "test loss is 0.0002718824441548942\n",
      "Batch: 29100,train loss is: 0.00027105516438625285\n",
      "test loss is 0.0002884968297209268\n",
      "Batch: 29200,train loss is: 0.0002605973627828001\n",
      "test loss is 0.0002926388423631871\n",
      "Batch: 29300,train loss is: 0.0003491081164535568\n",
      "test loss is 0.0002826321866475363\n",
      "Batch: 29400,train loss is: 0.0002831583059596999\n",
      "test loss is 0.00029401117108753596\n",
      "Batch: 29500,train loss is: 0.00022564262332103933\n",
      "test loss is 0.0002993180956404938\n",
      "Batch: 29600,train loss is: 0.00034688078136510856\n",
      "test loss is 0.0003028979154086931\n",
      "Batch: 29700,train loss is: 0.00037960676625399173\n",
      "test loss is 0.0002664599904292593\n",
      "Batch: 29800,train loss is: 0.0006797205561081753\n",
      "test loss is 0.0002803758416994212\n",
      "Batch: 29900,train loss is: 0.001414685131933998\n",
      "test loss is 0.00027306437137645064\n",
      "Batch: 30000,train loss is: 0.00029272239321198585\n",
      "test loss is 0.0003162300282682601\n",
      "Batch: 30100,train loss is: 0.00028941456812479855\n",
      "test loss is 0.0003295966476635469\n",
      "Batch: 30200,train loss is: 0.00025709084082534194\n",
      "test loss is 0.000278268784423484\n",
      "Batch: 30300,train loss is: 0.0001781505521504836\n",
      "test loss is 0.00025767716842500733\n",
      "Batch: 30400,train loss is: 0.00021047988330833797\n",
      "test loss is 0.00031333612908478425\n",
      "Batch: 30500,train loss is: 0.0003255398553022637\n",
      "test loss is 0.0003289851004018915\n",
      "Batch: 30600,train loss is: 0.00026538545644044277\n",
      "test loss is 0.0003010620892659827\n",
      "Batch: 30700,train loss is: 0.00028194116891027245\n",
      "test loss is 0.0002698360424950483\n",
      "Batch: 30800,train loss is: 0.00025238227231051463\n",
      "test loss is 0.0002710491538835101\n",
      "Batch: 30900,train loss is: 0.00026462964860039643\n",
      "test loss is 0.0002760502662715157\n",
      "Batch: 31000,train loss is: 0.00029622984975874087\n",
      "test loss is 0.00026941623550067855\n",
      "Batch: 31100,train loss is: 0.0002772408184350682\n",
      "test loss is 0.00029781505682272247\n",
      "Batch: 31200,train loss is: 0.0003548011658007895\n",
      "test loss is 0.00026743772038072966\n",
      "Batch: 31300,train loss is: 0.00015786917803083496\n",
      "test loss is 0.00025594080575612265\n",
      "Batch: 31400,train loss is: 0.00028921919212736947\n",
      "test loss is 0.0002931574358781746\n",
      "Batch: 31500,train loss is: 0.0002479889131554694\n",
      "test loss is 0.00026424751958537034\n",
      "Batch: 31600,train loss is: 0.0004171493815177059\n",
      "test loss is 0.00027025377007255344\n",
      "Batch: 31700,train loss is: 0.00030115116589854625\n",
      "test loss is 0.00028105547855444694\n",
      "Batch: 31800,train loss is: 0.00022221668614411233\n",
      "test loss is 0.00029444262413862035\n",
      "Batch: 31900,train loss is: 0.00032584619467723437\n",
      "test loss is 0.00028180066506557885\n",
      "Batch: 32000,train loss is: 0.0002467999199009932\n",
      "test loss is 0.00026831376916014357\n",
      "Batch: 32100,train loss is: 0.0001778769956858533\n",
      "test loss is 0.0002562472449702519\n",
      "Batch: 32200,train loss is: 0.0002392368853720422\n",
      "test loss is 0.00027843553090603094\n",
      "Batch: 32300,train loss is: 0.0002257058499682674\n",
      "test loss is 0.00027920716942869147\n",
      "Batch: 32400,train loss is: 0.00026950793875948594\n",
      "test loss is 0.0003142924084562885\n",
      "Batch: 32500,train loss is: 0.0002659650850624221\n",
      "test loss is 0.0002698797047555871\n",
      "Batch: 32600,train loss is: 0.00021269662874896659\n",
      "test loss is 0.00026197675497723283\n",
      "Batch: 32700,train loss is: 0.00019465008675823361\n",
      "test loss is 0.0002677504684940931\n",
      "Batch: 32800,train loss is: 0.00027484761197291927\n",
      "test loss is 0.0003726978213793786\n",
      "Batch: 32900,train loss is: 0.0002477987678084018\n",
      "test loss is 0.0002971192753068602\n",
      "Batch: 33000,train loss is: 0.0003110786636283438\n",
      "test loss is 0.0002724905556245359\n",
      "Batch: 33100,train loss is: 0.00023940861464694958\n",
      "test loss is 0.0002617227125460059\n",
      "Batch: 33200,train loss is: 0.0002988706302054201\n",
      "test loss is 0.00031194215495237494\n",
      "Batch: 33300,train loss is: 0.00026600419293379375\n",
      "test loss is 0.0002589175587992856\n",
      "Batch: 33400,train loss is: 0.00016353337251861052\n",
      "test loss is 0.00026049725255612797\n",
      "Batch: 33500,train loss is: 0.0002462662451955695\n",
      "test loss is 0.0002619688689238442\n",
      "Batch: 33600,train loss is: 0.0001871240198067657\n",
      "test loss is 0.0002855509843379192\n",
      "Batch: 33700,train loss is: 0.00033327438924148783\n",
      "test loss is 0.00025776858015817987\n",
      "Batch: 33800,train loss is: 0.00027720419384610044\n",
      "test loss is 0.0002964168643569585\n",
      "Batch: 33900,train loss is: 0.00016274587118285248\n",
      "test loss is 0.00026130208317671274\n",
      "Batch: 34000,train loss is: 0.0001825154074096041\n",
      "test loss is 0.00027525771827844334\n",
      "Batch: 34100,train loss is: 0.00024876291016548413\n",
      "test loss is 0.0003106004842263303\n",
      "Batch: 34200,train loss is: 0.00019469868275160684\n",
      "test loss is 0.000297874368385677\n",
      "Batch: 34300,train loss is: 0.00025239429844235787\n",
      "test loss is 0.00029047563652837585\n",
      "Batch: 34400,train loss is: 0.00022430571321909509\n",
      "test loss is 0.00025517182946817064\n",
      "Batch: 34500,train loss is: 0.0002951720827234977\n",
      "test loss is 0.0002887244202999776\n",
      "Batch: 34600,train loss is: 0.00025093636504091584\n",
      "test loss is 0.0002861949919351772\n",
      "Batch: 34700,train loss is: 0.00030045779915297097\n",
      "test loss is 0.00026138381282318003\n",
      "Batch: 34800,train loss is: 0.0001493423532412078\n",
      "test loss is 0.00031676889524432197\n",
      "Batch: 34900,train loss is: 0.00021457547350849077\n",
      "test loss is 0.00027092163695417634\n",
      "Batch: 35000,train loss is: 0.000315087460881932\n",
      "test loss is 0.00030610010173186394\n",
      "Batch: 35100,train loss is: 0.00016098267381491098\n",
      "test loss is 0.0002535352235569914\n",
      "Batch: 35200,train loss is: 0.0002648354400318747\n",
      "test loss is 0.00028270645033145374\n",
      "Batch: 35300,train loss is: 0.00036100802926784104\n",
      "test loss is 0.00026763562813999776\n",
      "Batch: 35400,train loss is: 0.0004736922663530589\n",
      "test loss is 0.0003556012647237844\n",
      "Batch: 35500,train loss is: 0.0002782216302009493\n",
      "test loss is 0.0003182339930689846\n",
      "Batch: 35600,train loss is: 0.0002579824481669473\n",
      "test loss is 0.0002724155315083084\n",
      "Batch: 35700,train loss is: 0.0003281196656088386\n",
      "test loss is 0.00031344586535397763\n",
      "Batch: 35800,train loss is: 0.0002342064087810934\n",
      "test loss is 0.0002890756390339712\n",
      "Batch: 35900,train loss is: 0.00039761420799817556\n",
      "test loss is 0.00025780235698144203\n",
      "Batch: 36000,train loss is: 0.00032849469971416283\n",
      "test loss is 0.0002741105813151099\n",
      "Batch: 36100,train loss is: 0.00024897523936114\n",
      "test loss is 0.0002853066819317815\n",
      "Batch: 36200,train loss is: 0.00034862977065886803\n",
      "test loss is 0.0002997396998256671\n",
      "Batch: 36300,train loss is: 0.0003553577972101158\n",
      "test loss is 0.0002910846493264536\n",
      "Batch: 36400,train loss is: 0.0002116276118800156\n",
      "test loss is 0.00027628057486296984\n",
      "Batch: 36500,train loss is: 0.00015544381624942\n",
      "test loss is 0.0002631163718945063\n",
      "Batch: 36600,train loss is: 0.00017117187234577013\n",
      "test loss is 0.00025190829291581364\n",
      "Batch: 36700,train loss is: 0.0002675488491567219\n",
      "test loss is 0.00026553244102266155\n",
      "Batch: 36800,train loss is: 0.00014218049344511194\n",
      "test loss is 0.0002892072076026281\n",
      "Batch: 36900,train loss is: 0.0003461547893352784\n",
      "test loss is 0.00035542186856816636\n",
      "Batch: 37000,train loss is: 0.0002704202405400491\n",
      "test loss is 0.0002956998635740165\n",
      "Batch: 37100,train loss is: 0.00021036048489508983\n",
      "test loss is 0.0002401301672501437\n",
      "Batch: 37200,train loss is: 0.00036998653532669784\n",
      "test loss is 0.00026816662578499134\n",
      "Batch: 37300,train loss is: 0.000255373882174039\n",
      "test loss is 0.00029066059982001146\n",
      "Batch: 37400,train loss is: 0.000228770318499734\n",
      "test loss is 0.000268456538185084\n",
      "Batch: 37500,train loss is: 0.0002020380416677864\n",
      "test loss is 0.0002797474178174971\n",
      "Batch: 37600,train loss is: 0.00040239516015211634\n",
      "test loss is 0.0002916092814569617\n",
      "Batch: 37700,train loss is: 0.00031378953157313374\n",
      "test loss is 0.0003261296174892776\n",
      "Batch: 37800,train loss is: 0.00024992885315964017\n",
      "test loss is 0.00046426296359530024\n",
      "Batch: 37900,train loss is: 0.00019791013228387725\n",
      "test loss is 0.0002691558694909582\n",
      "Batch: 38000,train loss is: 0.00023966734586300815\n",
      "test loss is 0.0002757494271113127\n",
      "Batch: 38100,train loss is: 0.00040183696476965954\n",
      "test loss is 0.0002733376851332919\n",
      "Batch: 38200,train loss is: 0.0003228500408024086\n",
      "test loss is 0.0003089503714898217\n",
      "Batch: 38300,train loss is: 0.0003029768140820659\n",
      "test loss is 0.0002777714999810556\n",
      "Batch: 38400,train loss is: 0.0002403585625726485\n",
      "test loss is 0.0002822184204995201\n",
      "Batch: 38500,train loss is: 0.0002526947125742581\n",
      "test loss is 0.00036570205086877537\n",
      "Batch: 38600,train loss is: 0.00024472326610458907\n",
      "test loss is 0.00027335597275223616\n",
      "Batch: 38700,train loss is: 0.00039984203038056815\n",
      "test loss is 0.0002752684106221298\n",
      "Batch: 38800,train loss is: 0.0002773000478984239\n",
      "test loss is 0.0002861670401161313\n",
      "Batch: 38900,train loss is: 0.0004381170520196308\n",
      "test loss is 0.00026272059919300357\n",
      "Batch: 39000,train loss is: 0.0002820018750783066\n",
      "test loss is 0.00025257729972577516\n",
      "Batch: 39100,train loss is: 0.00021602660532769177\n",
      "test loss is 0.00026871242194257934\n",
      "Batch: 39200,train loss is: 0.00031704693980765105\n",
      "test loss is 0.00026269577640452625\n",
      "Batch: 39300,train loss is: 0.00014295519829497772\n",
      "test loss is 0.00026045750067056015\n",
      "Batch: 39400,train loss is: 0.0003379604716300939\n",
      "test loss is 0.0004169691036581739\n",
      "Batch: 39500,train loss is: 0.00017490873879210522\n",
      "test loss is 0.0002634016499645255\n",
      "Batch: 39600,train loss is: 0.00019265970757826844\n",
      "test loss is 0.00035336292013994115\n",
      "Batch: 39700,train loss is: 0.0002963843566043497\n",
      "test loss is 0.00026252797838844776\n",
      "Batch: 39800,train loss is: 0.0003888200525586012\n",
      "test loss is 0.000309243420064523\n",
      "Batch: 39900,train loss is: 0.00015621356300702353\n",
      "test loss is 0.00026787092651603606\n",
      "Batch: 40000,train loss is: 0.00025643043164933865\n",
      "test loss is 0.00026859399459761474\n",
      "Batch: 40100,train loss is: 0.00022872304886216232\n",
      "test loss is 0.00030458917171985277\n",
      "Batch: 40200,train loss is: 0.00029299533532381493\n",
      "test loss is 0.0004016731101221974\n",
      "Batch: 40300,train loss is: 0.00027566050067948946\n",
      "test loss is 0.000266679148755965\n",
      "Batch: 40400,train loss is: 0.00042201644624429487\n",
      "test loss is 0.0003245717067870991\n",
      "Batch: 40500,train loss is: 0.0002288574290287191\n",
      "test loss is 0.0003174897023035164\n",
      "Batch: 40600,train loss is: 0.00024393777844349993\n",
      "test loss is 0.0002784518820475938\n",
      "Batch: 40700,train loss is: 0.00020615880385404378\n",
      "test loss is 0.00025156286863224257\n",
      "Batch: 40800,train loss is: 0.0002458167159375003\n",
      "test loss is 0.00027995313956290344\n",
      "Batch: 40900,train loss is: 0.0002231476457643433\n",
      "test loss is 0.00027201893975261856\n",
      "Batch: 41000,train loss is: 0.00033660264203178264\n",
      "test loss is 0.00025743584350276365\n",
      "Batch: 41100,train loss is: 0.00015170362575219102\n",
      "test loss is 0.00027919456060878023\n",
      "Batch: 41200,train loss is: 0.00029870682455431424\n",
      "test loss is 0.00029607145608215297\n",
      "Batch: 41300,train loss is: 0.00021285821178946654\n",
      "test loss is 0.00029602080343323956\n",
      "Batch: 41400,train loss is: 0.00018053850205665593\n",
      "test loss is 0.0002740179153452594\n",
      "Batch: 41500,train loss is: 0.00036662181912651485\n",
      "test loss is 0.0003337829027991861\n",
      "Batch: 41600,train loss is: 0.0003236274256908359\n",
      "test loss is 0.000345865707438771\n",
      "Batch: 41700,train loss is: 0.00022266356321083046\n",
      "test loss is 0.00027339365288416316\n",
      "Batch: 41800,train loss is: 0.0002765504077347204\n",
      "test loss is 0.00027211584412443763\n",
      "Batch: 41900,train loss is: 0.00023560956906225843\n",
      "test loss is 0.00029731083011952624\n",
      "Batch: 42000,train loss is: 0.00022604344504829964\n",
      "test loss is 0.000264349358222138\n",
      "Batch: 42100,train loss is: 0.00022669070415948285\n",
      "test loss is 0.00026550752309296624\n",
      "Batch: 42200,train loss is: 0.00026405778461972195\n",
      "test loss is 0.00025965500565200373\n",
      "Batch: 42300,train loss is: 0.00022533923031455862\n",
      "test loss is 0.0002599399120203716\n",
      "Batch: 42400,train loss is: 0.00038073623120379884\n",
      "test loss is 0.00029059912589265177\n",
      "Batch: 42500,train loss is: 0.000210714353938666\n",
      "test loss is 0.00030184906577102814\n",
      "Batch: 42600,train loss is: 0.0003365676400213295\n",
      "test loss is 0.00026464676187544836\n",
      "Batch: 42700,train loss is: 0.00031207875295405027\n",
      "test loss is 0.0002632566897691986\n",
      "Batch: 42800,train loss is: 0.00031143313017278245\n",
      "test loss is 0.0003802516238773426\n",
      "Batch: 42900,train loss is: 0.0002371914152738626\n",
      "test loss is 0.00026855713101107964\n",
      "Batch: 43000,train loss is: 0.0002287564154988273\n",
      "test loss is 0.00027069783731778144\n",
      "Batch: 43100,train loss is: 0.00020610684110105898\n",
      "test loss is 0.0002868290075365956\n",
      "Batch: 43200,train loss is: 0.00023731009691669016\n",
      "test loss is 0.0002707567114468264\n",
      "Batch: 43300,train loss is: 0.0003255207641682446\n",
      "test loss is 0.00031899567826213393\n",
      "Batch: 43400,train loss is: 0.00021508309277261487\n",
      "test loss is 0.0002999728880535626\n",
      "Batch: 43500,train loss is: 0.0002661210044339029\n",
      "test loss is 0.0002750613711764931\n",
      "Batch: 43600,train loss is: 0.00029690720370935603\n",
      "test loss is 0.0002809419389554009\n",
      "Batch: 43700,train loss is: 0.00021654329084339275\n",
      "test loss is 0.0002765247540275432\n",
      "Batch: 43800,train loss is: 0.000894115420565896\n",
      "test loss is 0.00026014066862466997\n",
      "Batch: 43900,train loss is: 0.0002221958181915744\n",
      "test loss is 0.00028563529212542667\n",
      "Batch: 44000,train loss is: 0.0002809558335796406\n",
      "test loss is 0.000268628511956138\n",
      "Batch: 44100,train loss is: 0.0001921103743977307\n",
      "test loss is 0.00028226738131246896\n",
      "Batch: 44200,train loss is: 0.00027727480579370443\n",
      "test loss is 0.00031723639226692834\n",
      "Batch: 44300,train loss is: 0.0002544121115281263\n",
      "test loss is 0.0003194995938112792\n",
      "Batch: 44400,train loss is: 0.0002309215808694508\n",
      "test loss is 0.0002690484826489003\n",
      "Batch: 44500,train loss is: 0.00028132535231258126\n",
      "test loss is 0.00037946307960851175\n",
      "Batch: 44600,train loss is: 0.00032758817938003423\n",
      "test loss is 0.000254601111824857\n",
      "Batch: 44700,train loss is: 0.00029173286553678033\n",
      "test loss is 0.00028252331347520854\n",
      "Batch: 44800,train loss is: 0.0002420191281709491\n",
      "test loss is 0.0002751223918388272\n",
      "Batch: 44900,train loss is: 0.00032477151485158233\n",
      "test loss is 0.0003458806499492347\n",
      "Batch: 45000,train loss is: 0.0002564855420545373\n",
      "test loss is 0.0002759864488504778\n",
      "Batch: 45100,train loss is: 0.0004053632481956345\n",
      "test loss is 0.00029966838789799693\n",
      "Batch: 45200,train loss is: 0.00039997959840596824\n",
      "test loss is 0.0002606479439677625\n",
      "Batch: 45300,train loss is: 0.0002045670700990491\n",
      "test loss is 0.0002889040595621342\n",
      "Batch: 45400,train loss is: 0.0003318969217264101\n",
      "test loss is 0.0003117297509060275\n",
      "Batch: 45500,train loss is: 0.00019006003025509024\n",
      "test loss is 0.00028551051449974104\n",
      "Batch: 45600,train loss is: 0.00020195956489979064\n",
      "test loss is 0.00025899201025132956\n",
      "Batch: 45700,train loss is: 0.00035777243686137137\n",
      "test loss is 0.0003257872433719597\n",
      "Batch: 45800,train loss is: 0.00024619225034129625\n",
      "test loss is 0.00030001413591606777\n",
      "Batch: 45900,train loss is: 0.0002050136701191753\n",
      "test loss is 0.00028164280026680093\n",
      "Batch: 46000,train loss is: 0.00018724948861264742\n",
      "test loss is 0.00027535961966780224\n",
      "Batch: 46100,train loss is: 0.00023763134722500515\n",
      "test loss is 0.0002698123042904412\n",
      "Batch: 46200,train loss is: 0.0003904816580135732\n",
      "test loss is 0.0003378677702732281\n",
      "Batch: 46300,train loss is: 0.0002259983449868331\n",
      "test loss is 0.0002601639769490536\n",
      "Batch: 46400,train loss is: 0.00071097394323443\n",
      "test loss is 0.00029627984436650324\n",
      "Batch: 46500,train loss is: 0.00025611366613343745\n",
      "test loss is 0.00030036628655141887\n",
      "Batch: 46600,train loss is: 0.00032276229688340055\n",
      "test loss is 0.00028106057138765976\n",
      "Batch: 46700,train loss is: 0.0002002409024354235\n",
      "test loss is 0.00029316171202527544\n",
      "-----------------------Epoch: 7----------------------------------\n",
      "Batch: 0,train loss is: 0.0002845351234908566\n",
      "test loss is 0.00027083532680717125\n",
      "Batch: 100,train loss is: 0.00021346671066547893\n",
      "test loss is 0.0002628301488528103\n",
      "Batch: 200,train loss is: 0.0003206620300438876\n",
      "test loss is 0.0002756168500989707\n",
      "Batch: 300,train loss is: 0.0001878237729976722\n",
      "test loss is 0.00030015426099031233\n",
      "Batch: 400,train loss is: 0.0002867245080347897\n",
      "test loss is 0.0003138635888696037\n",
      "Batch: 500,train loss is: 0.0002713829104948729\n",
      "test loss is 0.00028738061046502716\n",
      "Batch: 600,train loss is: 0.00020057531291986361\n",
      "test loss is 0.0002867170584294909\n",
      "Batch: 700,train loss is: 0.0002628344858837072\n",
      "test loss is 0.0002661933133672672\n",
      "Batch: 800,train loss is: 0.00023325568775467225\n",
      "test loss is 0.0002697113307130412\n",
      "Batch: 900,train loss is: 0.0003158526527879112\n",
      "test loss is 0.0002558539436292403\n",
      "Batch: 1000,train loss is: 0.00021517720952500796\n",
      "test loss is 0.00026463449501963604\n",
      "Batch: 1100,train loss is: 0.00047762863116529935\n",
      "test loss is 0.0002798162810511549\n",
      "Batch: 1200,train loss is: 0.0003059260674858656\n",
      "test loss is 0.00027651650381458394\n",
      "Batch: 1300,train loss is: 0.00029494168008515107\n",
      "test loss is 0.000288105327432917\n",
      "Batch: 1400,train loss is: 0.0003709151804450273\n",
      "test loss is 0.0003282159407120507\n",
      "Batch: 1500,train loss is: 0.0006224050821247603\n",
      "test loss is 0.00028085087897760275\n",
      "Batch: 1600,train loss is: 0.0002604782539038561\n",
      "test loss is 0.0002638999153383287\n",
      "Batch: 1700,train loss is: 0.0001991278735658881\n",
      "test loss is 0.00027253820104402416\n",
      "Batch: 1800,train loss is: 0.00018418223731356484\n",
      "test loss is 0.00026593263402407844\n",
      "Batch: 1900,train loss is: 0.0003643223670868078\n",
      "test loss is 0.0002960396711412659\n",
      "Batch: 2000,train loss is: 0.000297458680380286\n",
      "test loss is 0.00030633802935721655\n",
      "Batch: 2100,train loss is: 0.00023501667100860385\n",
      "test loss is 0.0002868593612928365\n",
      "Batch: 2200,train loss is: 0.0001916663040597502\n",
      "test loss is 0.0002877923954889663\n",
      "Batch: 2300,train loss is: 0.0003844173958062565\n",
      "test loss is 0.0002660513903379318\n",
      "Batch: 2400,train loss is: 0.0004273145731057787\n",
      "test loss is 0.000335225117391646\n",
      "Batch: 2500,train loss is: 0.00020995462204794838\n",
      "test loss is 0.00025580148461137485\n",
      "Batch: 2600,train loss is: 0.0002387819358974601\n",
      "test loss is 0.00025684796871769513\n",
      "Batch: 2700,train loss is: 0.00017480375985305815\n",
      "test loss is 0.00026643010474686283\n",
      "Batch: 2800,train loss is: 0.00017925032922278097\n",
      "test loss is 0.0002703643623303\n",
      "Batch: 2900,train loss is: 0.0005947478721710659\n",
      "test loss is 0.00027729367531588266\n",
      "Batch: 3000,train loss is: 0.00038858021835429344\n",
      "test loss is 0.0003489365072169648\n",
      "Batch: 3100,train loss is: 0.0005109543472938331\n",
      "test loss is 0.00029872816939708127\n",
      "Batch: 3200,train loss is: 0.00030486932029059145\n",
      "test loss is 0.0003187697000135833\n",
      "Batch: 3300,train loss is: 0.00025502573922423676\n",
      "test loss is 0.00028800995714043\n",
      "Batch: 3400,train loss is: 0.00027554113148180843\n",
      "test loss is 0.00027033816730375084\n",
      "Batch: 3500,train loss is: 0.00012761556139242092\n",
      "test loss is 0.0002661838260709724\n",
      "Batch: 3600,train loss is: 0.0003423843072434685\n",
      "test loss is 0.0004244539892907123\n",
      "Batch: 3700,train loss is: 0.00024505660136952764\n",
      "test loss is 0.0003263543418885791\n",
      "Batch: 3800,train loss is: 0.00022919911395644327\n",
      "test loss is 0.0002504710864216009\n",
      "Batch: 3900,train loss is: 0.00016547580062020427\n",
      "test loss is 0.0002658224742844712\n",
      "Batch: 4000,train loss is: 0.0002414202971271463\n",
      "test loss is 0.00030662992407403614\n",
      "Batch: 4100,train loss is: 0.0003901497564759819\n",
      "test loss is 0.00026510056783941437\n",
      "Batch: 4200,train loss is: 0.0003654899143503656\n",
      "test loss is 0.0002667135871444663\n",
      "Batch: 4300,train loss is: 0.00027590044109340164\n",
      "test loss is 0.00026127837115853396\n",
      "Batch: 4400,train loss is: 0.00024025080117588972\n",
      "test loss is 0.0002592788461876707\n",
      "Batch: 4500,train loss is: 0.00025731465544082685\n",
      "test loss is 0.0002999955430140865\n",
      "Batch: 4600,train loss is: 0.00036424503849129524\n",
      "test loss is 0.0002829760890927991\n",
      "Batch: 4700,train loss is: 0.0002904588295450886\n",
      "test loss is 0.00027809501479086673\n",
      "Batch: 4800,train loss is: 0.00040506543475630645\n",
      "test loss is 0.000260437116834839\n",
      "Batch: 4900,train loss is: 0.00042266292137645785\n",
      "test loss is 0.000391147108199155\n",
      "Batch: 5000,train loss is: 0.00037508913616333023\n",
      "test loss is 0.0002907055492250229\n",
      "Batch: 5100,train loss is: 0.00025330900944510763\n",
      "test loss is 0.00028757183032712756\n",
      "Batch: 5200,train loss is: 0.00018403905195463167\n",
      "test loss is 0.0002522429215275958\n",
      "Batch: 5300,train loss is: 0.0001669370574006569\n",
      "test loss is 0.0002602614656448298\n",
      "Batch: 5400,train loss is: 0.00026667450677249924\n",
      "test loss is 0.0003556380034685858\n",
      "Batch: 5500,train loss is: 0.000356488419226028\n",
      "test loss is 0.0002768517403249548\n",
      "Batch: 5600,train loss is: 0.0001855907769996525\n",
      "test loss is 0.00028448814863226064\n",
      "Batch: 5700,train loss is: 0.00023657195201664538\n",
      "test loss is 0.0003022172767853045\n",
      "Batch: 5800,train loss is: 0.00023096057871852937\n",
      "test loss is 0.0002574809304783064\n",
      "Batch: 5900,train loss is: 0.00019696979332463338\n",
      "test loss is 0.00027028788900426324\n",
      "Batch: 6000,train loss is: 0.00022999240726844145\n",
      "test loss is 0.00026648596076804873\n",
      "Batch: 6100,train loss is: 0.00018680211158765333\n",
      "test loss is 0.0002967589951591366\n",
      "Batch: 6200,train loss is: 0.00038148715142004286\n",
      "test loss is 0.0002643861728747472\n",
      "Batch: 6300,train loss is: 0.0003447806793017932\n",
      "test loss is 0.00028012096491818357\n",
      "Batch: 6400,train loss is: 0.00023224466443613181\n",
      "test loss is 0.00032204518869378344\n",
      "Batch: 6500,train loss is: 0.0005720467623475395\n",
      "test loss is 0.0003381789211610123\n",
      "Batch: 6600,train loss is: 0.0005461743349348142\n",
      "test loss is 0.0003313668002158089\n",
      "Batch: 6700,train loss is: 0.00031027277166883685\n",
      "test loss is 0.00029762412569220265\n",
      "Batch: 6800,train loss is: 0.00035657088435606197\n",
      "test loss is 0.000305368726968445\n",
      "Batch: 6900,train loss is: 0.0005155089094398961\n",
      "test loss is 0.00030479478498986786\n",
      "Batch: 7000,train loss is: 0.00018871542739074551\n",
      "test loss is 0.0002611324061140471\n",
      "Batch: 7100,train loss is: 0.00025651056133688414\n",
      "test loss is 0.00024908956016368527\n",
      "Batch: 7200,train loss is: 0.0002275535658481993\n",
      "test loss is 0.0002889046234017697\n",
      "Batch: 7300,train loss is: 0.0003420797849119357\n",
      "test loss is 0.000255064756910903\n",
      "Batch: 7400,train loss is: 0.0003475233345463603\n",
      "test loss is 0.00027047311802219106\n",
      "Batch: 7500,train loss is: 0.0005018386731461136\n",
      "test loss is 0.00029238355409134974\n",
      "Batch: 7600,train loss is: 0.00034728404550519353\n",
      "test loss is 0.0002966221481039577\n",
      "Batch: 7700,train loss is: 0.0003241107937183768\n",
      "test loss is 0.00031253922402690615\n",
      "Batch: 7800,train loss is: 0.00029227624164949464\n",
      "test loss is 0.0002699170694115963\n",
      "Batch: 7900,train loss is: 0.000257713820941989\n",
      "test loss is 0.00025727934038240647\n",
      "Batch: 8000,train loss is: 0.00022626426295370216\n",
      "test loss is 0.00025970398906602045\n",
      "Batch: 8100,train loss is: 0.0001479538455913557\n",
      "test loss is 0.00025855312787314037\n",
      "Batch: 8200,train loss is: 0.00024490508885289684\n",
      "test loss is 0.0002507274573165318\n",
      "Batch: 8300,train loss is: 0.0002711595662186806\n",
      "test loss is 0.00027952722142952663\n",
      "Batch: 8400,train loss is: 0.0005745664937112168\n",
      "test loss is 0.0002990154439747925\n",
      "Batch: 8500,train loss is: 0.0001589524985628511\n",
      "test loss is 0.00026997658102708376\n",
      "Batch: 8600,train loss is: 0.00018287515430480349\n",
      "test loss is 0.00030439312390172235\n",
      "Batch: 8700,train loss is: 0.0002635460432234783\n",
      "test loss is 0.0002925828916281952\n",
      "Batch: 8800,train loss is: 0.00027840709553371705\n",
      "test loss is 0.0003615223792725746\n",
      "Batch: 8900,train loss is: 0.00028829907767035447\n",
      "test loss is 0.0002654823388919882\n",
      "Batch: 9000,train loss is: 0.00021511494049549932\n",
      "test loss is 0.0003095653591613272\n",
      "Batch: 9100,train loss is: 0.00024483227348479205\n",
      "test loss is 0.0002750449447989179\n",
      "Batch: 9200,train loss is: 0.0003457824866377493\n",
      "test loss is 0.00026876345445292304\n",
      "Batch: 9300,train loss is: 0.00036424148607879306\n",
      "test loss is 0.0002938387284192548\n",
      "Batch: 9400,train loss is: 0.0001636925910836175\n",
      "test loss is 0.00029494977536146863\n",
      "Batch: 9500,train loss is: 0.00027899877116806563\n",
      "test loss is 0.0002781295861028728\n",
      "Batch: 9600,train loss is: 0.00022249138782095749\n",
      "test loss is 0.00029723099025735284\n",
      "Batch: 9700,train loss is: 0.0005219577158867534\n",
      "test loss is 0.0004216096644107136\n",
      "Batch: 9800,train loss is: 0.00021882351465593057\n",
      "test loss is 0.0002766134482890195\n",
      "Batch: 9900,train loss is: 0.0004588009502235599\n",
      "test loss is 0.0002699079661981396\n",
      "Batch: 10000,train loss is: 0.000341745423260827\n",
      "test loss is 0.00030715481229325265\n",
      "Batch: 10100,train loss is: 0.00022003931071954588\n",
      "test loss is 0.0002788815740319472\n",
      "Batch: 10200,train loss is: 0.00032135825500980407\n",
      "test loss is 0.00028038602280122606\n",
      "Batch: 10300,train loss is: 0.00027052582212794914\n",
      "test loss is 0.00026352175096329135\n",
      "Batch: 10400,train loss is: 0.00021043932421653435\n",
      "test loss is 0.0002520458626164828\n",
      "Batch: 10500,train loss is: 0.0002649107405090769\n",
      "test loss is 0.0002793460375067616\n",
      "Batch: 10600,train loss is: 0.0004424862049605111\n",
      "test loss is 0.0002835279444322718\n",
      "Batch: 10700,train loss is: 0.0005574699333293844\n",
      "test loss is 0.00028828560603569466\n",
      "Batch: 10800,train loss is: 0.00032658489862645834\n",
      "test loss is 0.0002817729131612264\n",
      "Batch: 10900,train loss is: 0.00029595922260062304\n",
      "test loss is 0.00030838677310333863\n",
      "Batch: 11000,train loss is: 0.000585422809448315\n",
      "test loss is 0.0002593865484415994\n",
      "Batch: 11100,train loss is: 0.0002732392670051101\n",
      "test loss is 0.000259240206100753\n",
      "Batch: 11200,train loss is: 0.00016939458101343887\n",
      "test loss is 0.0002583367038164114\n",
      "Batch: 11300,train loss is: 0.00023685980095327312\n",
      "test loss is 0.00026645116803722694\n",
      "Batch: 11400,train loss is: 0.00038549904735318764\n",
      "test loss is 0.0003202733445595599\n",
      "Batch: 11500,train loss is: 0.00016851542231824293\n",
      "test loss is 0.00029649214070010964\n",
      "Batch: 11600,train loss is: 0.00033178781182012886\n",
      "test loss is 0.00034524399021515213\n",
      "Batch: 11700,train loss is: 0.0002996442329301268\n",
      "test loss is 0.000295814121367592\n",
      "Batch: 11800,train loss is: 0.00017272297115701672\n",
      "test loss is 0.0002940119870118342\n",
      "Batch: 11900,train loss is: 0.00021704393631942145\n",
      "test loss is 0.000250387833595469\n",
      "Batch: 12000,train loss is: 0.00028075738082494295\n",
      "test loss is 0.00033486104173760474\n",
      "Batch: 12100,train loss is: 0.00026691155705776395\n",
      "test loss is 0.00025973963971979384\n",
      "Batch: 12200,train loss is: 0.00020082358778586554\n",
      "test loss is 0.00033201014959270957\n",
      "Batch: 12300,train loss is: 0.0002544544974822784\n",
      "test loss is 0.0002500042903668204\n",
      "Batch: 12400,train loss is: 0.00020317377452831446\n",
      "test loss is 0.00026835086916299916\n",
      "Batch: 12500,train loss is: 0.00036051929807829784\n",
      "test loss is 0.00025671729859228176\n",
      "Batch: 12600,train loss is: 0.000340229827943783\n",
      "test loss is 0.00030894220863471864\n",
      "Batch: 12700,train loss is: 0.0008205967865753122\n",
      "test loss is 0.00026151857316190593\n",
      "Batch: 12800,train loss is: 0.0001897576997478281\n",
      "test loss is 0.0002890364846471756\n",
      "Batch: 12900,train loss is: 0.0002886097895426745\n",
      "test loss is 0.0002533268653077536\n",
      "Batch: 13000,train loss is: 0.00021614400080795693\n",
      "test loss is 0.0002825522214534467\n",
      "Batch: 13100,train loss is: 0.0003204539784452217\n",
      "test loss is 0.00031575490013796006\n",
      "Batch: 13200,train loss is: 0.00042615103068687605\n",
      "test loss is 0.00029242161123049494\n",
      "Batch: 13300,train loss is: 0.00018352657409980295\n",
      "test loss is 0.00029764239608274335\n",
      "Batch: 13400,train loss is: 0.00026252720470675803\n",
      "test loss is 0.0002695656901470954\n",
      "Batch: 13500,train loss is: 0.00020525636959081316\n",
      "test loss is 0.0002627775252937003\n",
      "Batch: 13600,train loss is: 0.00041269767657406864\n",
      "test loss is 0.00028889995122448716\n",
      "Batch: 13700,train loss is: 0.00029034940558474425\n",
      "test loss is 0.00024964993834236085\n",
      "Batch: 13800,train loss is: 0.0001664262259872992\n",
      "test loss is 0.000268506605584409\n",
      "Batch: 13900,train loss is: 0.0004957747555299032\n",
      "test loss is 0.00027803491254584843\n",
      "Batch: 14000,train loss is: 0.00021232627286742845\n",
      "test loss is 0.0002873234606168785\n",
      "Batch: 14100,train loss is: 0.00025238541648849364\n",
      "test loss is 0.0002675509915976592\n",
      "Batch: 14200,train loss is: 0.00022569153773273717\n",
      "test loss is 0.0002744558397225484\n",
      "Batch: 14300,train loss is: 0.00024134422212818452\n",
      "test loss is 0.000284215961076285\n",
      "Batch: 14400,train loss is: 0.00022379168282464908\n",
      "test loss is 0.0003210288309223646\n",
      "Batch: 14500,train loss is: 0.00022437457271564604\n",
      "test loss is 0.0002569694156431634\n",
      "Batch: 14600,train loss is: 0.0002227238993777495\n",
      "test loss is 0.00025727260640977695\n",
      "Batch: 14700,train loss is: 0.0003559602262316427\n",
      "test loss is 0.0002926070716139645\n",
      "Batch: 14800,train loss is: 0.00024574040758437797\n",
      "test loss is 0.0002603819143420511\n",
      "Batch: 14900,train loss is: 0.00027042131025128134\n",
      "test loss is 0.00026862955908185524\n",
      "Batch: 15000,train loss is: 0.0002221975762255683\n",
      "test loss is 0.00032953349825273374\n",
      "Batch: 15100,train loss is: 0.00031960955162284657\n",
      "test loss is 0.0003313612335745715\n",
      "Batch: 15200,train loss is: 0.00017712979361335268\n",
      "test loss is 0.0002511673515862323\n",
      "Batch: 15300,train loss is: 0.0002587875630328614\n",
      "test loss is 0.000278379824275791\n",
      "Batch: 15400,train loss is: 0.0003530763514564556\n",
      "test loss is 0.0003220313087390331\n",
      "Batch: 15500,train loss is: 0.0004112274057795507\n",
      "test loss is 0.00030464182749942545\n",
      "Batch: 15600,train loss is: 0.0003683973720376514\n",
      "test loss is 0.00028296279057351094\n",
      "Batch: 15700,train loss is: 0.00019501002714936315\n",
      "test loss is 0.0003022037829053028\n",
      "Batch: 15800,train loss is: 0.00026757572456035636\n",
      "test loss is 0.0002704926012416607\n",
      "Batch: 15900,train loss is: 0.00017062619680309222\n",
      "test loss is 0.00029145219325240403\n",
      "Batch: 16000,train loss is: 0.0003433747389573607\n",
      "test loss is 0.0003088564097305853\n",
      "Batch: 16100,train loss is: 0.00018544642415400448\n",
      "test loss is 0.000270504722442327\n",
      "Batch: 16200,train loss is: 0.00034735460668020794\n",
      "test loss is 0.00028971778631363773\n",
      "Batch: 16300,train loss is: 0.00018432364310330497\n",
      "test loss is 0.0003232496143306907\n",
      "Batch: 16400,train loss is: 0.00022674509754693435\n",
      "test loss is 0.0003108089038780906\n",
      "Batch: 16500,train loss is: 0.0002842955113451618\n",
      "test loss is 0.0002960852422886101\n",
      "Batch: 16600,train loss is: 0.00018031369393493716\n",
      "test loss is 0.00028824189637389764\n",
      "Batch: 16700,train loss is: 0.00019408952142951344\n",
      "test loss is 0.0002535187767572392\n",
      "Batch: 16800,train loss is: 0.0003441592278145077\n",
      "test loss is 0.0002920502971990758\n",
      "Batch: 16900,train loss is: 0.00022899235086913222\n",
      "test loss is 0.00025754800261803743\n",
      "Batch: 17000,train loss is: 0.0003057866507635438\n",
      "test loss is 0.0002858825857448844\n",
      "Batch: 17100,train loss is: 0.0002364840545648335\n",
      "test loss is 0.00029394222071765536\n",
      "Batch: 17200,train loss is: 0.000270618771811707\n",
      "test loss is 0.0002790275901484893\n",
      "Batch: 17300,train loss is: 0.00019690784071576276\n",
      "test loss is 0.0002581275546213133\n",
      "Batch: 17400,train loss is: 0.0001784779470090437\n",
      "test loss is 0.00028645704018788514\n",
      "Batch: 17500,train loss is: 0.00020154479827579652\n",
      "test loss is 0.0002713583447395442\n",
      "Batch: 17600,train loss is: 0.00021401842403517558\n",
      "test loss is 0.0002565082545797897\n",
      "Batch: 17700,train loss is: 0.00028690679449628893\n",
      "test loss is 0.0003449870939066795\n",
      "Batch: 17800,train loss is: 0.00017833657865906838\n",
      "test loss is 0.00035056756330815374\n",
      "Batch: 17900,train loss is: 0.00018667615314693143\n",
      "test loss is 0.0002775954765699497\n",
      "Batch: 18000,train loss is: 0.000177650972740146\n",
      "test loss is 0.00028319733475089747\n",
      "Batch: 18100,train loss is: 0.0005089065106110752\n",
      "test loss is 0.0003149733763722414\n",
      "Batch: 18200,train loss is: 0.00021175750885152144\n",
      "test loss is 0.00025656145048019035\n",
      "Batch: 18300,train loss is: 0.00016030624903528762\n",
      "test loss is 0.0002727791264118338\n",
      "Batch: 18400,train loss is: 0.0002320228190909263\n",
      "test loss is 0.00026324218873848275\n",
      "Batch: 18500,train loss is: 0.00018530911574896676\n",
      "test loss is 0.0002613956086780332\n",
      "Batch: 18600,train loss is: 0.0003206968052693566\n",
      "test loss is 0.0002545653061636963\n",
      "Batch: 18700,train loss is: 0.00024094404823342077\n",
      "test loss is 0.0002703931875027642\n",
      "Batch: 18800,train loss is: 0.00035591458139817486\n",
      "test loss is 0.00025960675138874317\n",
      "Batch: 18900,train loss is: 0.0003211666381686466\n",
      "test loss is 0.0002733200409362748\n",
      "Batch: 19000,train loss is: 0.0002378591401021837\n",
      "test loss is 0.0002763591729661857\n",
      "Batch: 19100,train loss is: 0.0002954527477112508\n",
      "test loss is 0.00024603254934606693\n",
      "Batch: 19200,train loss is: 0.00027487671685994447\n",
      "test loss is 0.00026408977800619534\n",
      "Batch: 19300,train loss is: 0.0003254787224843827\n",
      "test loss is 0.0002756477182467197\n",
      "Batch: 19400,train loss is: 0.00018951356851236125\n",
      "test loss is 0.00033734102308450236\n",
      "Batch: 19500,train loss is: 0.00032790913701404175\n",
      "test loss is 0.00026010100059186775\n",
      "Batch: 19600,train loss is: 0.0003016198460751322\n",
      "test loss is 0.00026466905996685476\n",
      "Batch: 19700,train loss is: 0.0002278914215818096\n",
      "test loss is 0.00025264097117033506\n",
      "Batch: 19800,train loss is: 0.00024222678232440877\n",
      "test loss is 0.00032431720528648645\n",
      "Batch: 19900,train loss is: 0.00023573011103196907\n",
      "test loss is 0.0002760245967753956\n",
      "Batch: 20000,train loss is: 0.00022721439851320523\n",
      "test loss is 0.00028022318419171105\n",
      "Batch: 20100,train loss is: 0.00018834051991637596\n",
      "test loss is 0.0002623165549017256\n",
      "Batch: 20200,train loss is: 0.0003561846184176766\n",
      "test loss is 0.0002827335385688572\n",
      "Batch: 20300,train loss is: 0.0003700005277111722\n",
      "test loss is 0.00028366699372157304\n",
      "Batch: 20400,train loss is: 0.00016893650978107912\n",
      "test loss is 0.0002677924745868892\n",
      "Batch: 20500,train loss is: 0.0003140041576693033\n",
      "test loss is 0.00028312113984837474\n",
      "Batch: 20600,train loss is: 0.00022482701557635009\n",
      "test loss is 0.0002975543590104054\n",
      "Batch: 20700,train loss is: 0.0004896102697530805\n",
      "test loss is 0.0002791843776866696\n",
      "Batch: 20800,train loss is: 0.00024351088334116877\n",
      "test loss is 0.00026875637343577095\n",
      "Batch: 20900,train loss is: 0.0002692474059221903\n",
      "test loss is 0.00025750831232329\n",
      "Batch: 21000,train loss is: 0.00025220173800560093\n",
      "test loss is 0.00026320546447082977\n",
      "Batch: 21100,train loss is: 0.0005320736565788549\n",
      "test loss is 0.0002808736165936705\n",
      "Batch: 21200,train loss is: 0.00019230322200192905\n",
      "test loss is 0.0002699685859769396\n",
      "Batch: 21300,train loss is: 0.00024436623778712927\n",
      "test loss is 0.0003118469285437677\n",
      "Batch: 21400,train loss is: 0.00018749152453837478\n",
      "test loss is 0.0002615475541916853\n",
      "Batch: 21500,train loss is: 0.0002569129833775016\n",
      "test loss is 0.0002526549808468229\n",
      "Batch: 21600,train loss is: 0.00024336924627700753\n",
      "test loss is 0.00026660642319441675\n",
      "Batch: 21700,train loss is: 0.00022010670388810353\n",
      "test loss is 0.0002547607239680169\n",
      "Batch: 21800,train loss is: 0.00016114943352150014\n",
      "test loss is 0.0002659744706867335\n",
      "Batch: 21900,train loss is: 0.000283906537778402\n",
      "test loss is 0.0002831440395750843\n",
      "Batch: 22000,train loss is: 0.0002512907859375561\n",
      "test loss is 0.00026773485511162343\n",
      "Batch: 22100,train loss is: 0.00038408696336659435\n",
      "test loss is 0.0002930130920198042\n",
      "Batch: 22200,train loss is: 0.0002504876743297341\n",
      "test loss is 0.0002981158789755691\n",
      "Batch: 22300,train loss is: 0.00019956937819081018\n",
      "test loss is 0.0003010984313286189\n",
      "Batch: 22400,train loss is: 0.000331298011414029\n",
      "test loss is 0.00027363119792656856\n",
      "Batch: 22500,train loss is: 0.0004317433735979727\n",
      "test loss is 0.0003867785588052099\n",
      "Batch: 22600,train loss is: 0.0002289423501885233\n",
      "test loss is 0.0002779673499503986\n",
      "Batch: 22700,train loss is: 0.00016422998972506005\n",
      "test loss is 0.00028809864521427543\n",
      "Batch: 22800,train loss is: 0.0001876174857165004\n",
      "test loss is 0.0002660042063670621\n",
      "Batch: 22900,train loss is: 0.00019152315851581978\n",
      "test loss is 0.000245226220609975\n",
      "Batch: 23000,train loss is: 0.00023516490620323469\n",
      "test loss is 0.0002818950973652194\n",
      "Batch: 23100,train loss is: 0.0002924179550150811\n",
      "test loss is 0.00027534793392543065\n",
      "Batch: 23200,train loss is: 0.0003103480869283655\n",
      "test loss is 0.0002528861375422845\n",
      "Batch: 23300,train loss is: 0.0002572740644140888\n",
      "test loss is 0.00025149204460630294\n",
      "Batch: 23400,train loss is: 0.0005101355123500694\n",
      "test loss is 0.0002697138559875227\n",
      "Batch: 23500,train loss is: 0.0002075605664074533\n",
      "test loss is 0.000293541677708664\n",
      "Batch: 23600,train loss is: 0.0002513156833910395\n",
      "test loss is 0.0002593886960538098\n",
      "Batch: 23700,train loss is: 0.00019021289201625447\n",
      "test loss is 0.0002475891230465886\n",
      "Batch: 23800,train loss is: 0.00023794155929105507\n",
      "test loss is 0.0002632232293078355\n",
      "Batch: 23900,train loss is: 0.0002809848440034609\n",
      "test loss is 0.0002708816664565058\n",
      "Batch: 24000,train loss is: 0.0002605351133234691\n",
      "test loss is 0.0002832523044349367\n",
      "Batch: 24100,train loss is: 0.0002815136234705196\n",
      "test loss is 0.000322074928419515\n",
      "Batch: 24200,train loss is: 0.00020033242695221296\n",
      "test loss is 0.0002740617400714767\n",
      "Batch: 24300,train loss is: 0.00025755492590277156\n",
      "test loss is 0.0002952356636142038\n",
      "Batch: 24400,train loss is: 0.0002360454539771666\n",
      "test loss is 0.00028213159057511103\n",
      "Batch: 24500,train loss is: 0.00025430385252186294\n",
      "test loss is 0.00036718745703745806\n",
      "Batch: 24600,train loss is: 0.00022104389766714448\n",
      "test loss is 0.00026740502325977157\n",
      "Batch: 24700,train loss is: 0.00013864999203228278\n",
      "test loss is 0.00025279472294748614\n",
      "Batch: 24800,train loss is: 0.00023678993445745233\n",
      "test loss is 0.00027370316989820155\n",
      "Batch: 24900,train loss is: 0.0003361316532453964\n",
      "test loss is 0.00030178991984839294\n",
      "Batch: 25000,train loss is: 0.0002268825254201506\n",
      "test loss is 0.00027591069931903197\n",
      "Batch: 25100,train loss is: 0.0003235357938596331\n",
      "test loss is 0.000266681510148622\n",
      "Batch: 25200,train loss is: 0.0002854327669342626\n",
      "test loss is 0.0003006007512634531\n",
      "Batch: 25300,train loss is: 0.00034188014051418706\n",
      "test loss is 0.00026409745274049573\n",
      "Batch: 25400,train loss is: 0.00045225581792941537\n",
      "test loss is 0.0003132817988737477\n",
      "Batch: 25500,train loss is: 0.00019693668253670666\n",
      "test loss is 0.00025465781301685277\n",
      "Batch: 25600,train loss is: 0.0002925131389015825\n",
      "test loss is 0.0002834184372950295\n",
      "Batch: 25700,train loss is: 0.00033445432092042335\n",
      "test loss is 0.00036335994331699955\n",
      "Batch: 25800,train loss is: 0.00017578879538145188\n",
      "test loss is 0.0002962073374448112\n",
      "Batch: 25900,train loss is: 0.00021637555567216024\n",
      "test loss is 0.0002729097768147028\n",
      "Batch: 26000,train loss is: 0.0002889589492682903\n",
      "test loss is 0.0002958830256843263\n",
      "Batch: 26100,train loss is: 0.00024055759968605935\n",
      "test loss is 0.0002789874569833466\n",
      "Batch: 26200,train loss is: 0.00035805314151651443\n",
      "test loss is 0.0002930813052790246\n",
      "Batch: 26300,train loss is: 0.00022127257870897923\n",
      "test loss is 0.00026265014249293153\n",
      "Batch: 26400,train loss is: 0.0002409422963256821\n",
      "test loss is 0.0002598200955038574\n",
      "Batch: 26500,train loss is: 0.00021473002070429432\n",
      "test loss is 0.00026112424452614315\n",
      "Batch: 26600,train loss is: 0.0002372983385791161\n",
      "test loss is 0.0002598100987462466\n",
      "Batch: 26700,train loss is: 0.0002903624752440978\n",
      "test loss is 0.00026329628763319574\n",
      "Batch: 26800,train loss is: 0.0005093855557185742\n",
      "test loss is 0.00026140765799614256\n",
      "Batch: 26900,train loss is: 0.000344194342493132\n",
      "test loss is 0.0002806788465268495\n",
      "Batch: 27000,train loss is: 0.0003461546905131186\n",
      "test loss is 0.00024940122151943843\n",
      "Batch: 27100,train loss is: 0.0001751716704792166\n",
      "test loss is 0.0003185141052515663\n",
      "Batch: 27200,train loss is: 0.00028510349817365417\n",
      "test loss is 0.0002845368197429524\n",
      "Batch: 27300,train loss is: 0.0001965031646460301\n",
      "test loss is 0.00027148606783206124\n",
      "Batch: 27400,train loss is: 0.00021700462266016258\n",
      "test loss is 0.00025806015501793795\n",
      "Batch: 27500,train loss is: 0.0003015790555783782\n",
      "test loss is 0.00025744258879060344\n",
      "Batch: 27600,train loss is: 0.0001949836459648167\n",
      "test loss is 0.0003004080844098227\n",
      "Batch: 27700,train loss is: 0.00021316883481896262\n",
      "test loss is 0.00030434251579993004\n",
      "Batch: 27800,train loss is: 0.00022551098970989206\n",
      "test loss is 0.00028436929490891264\n",
      "Batch: 27900,train loss is: 0.00017531614002334195\n",
      "test loss is 0.0002645323382610114\n",
      "Batch: 28000,train loss is: 0.00024069294701419062\n",
      "test loss is 0.00029207357779859397\n",
      "Batch: 28100,train loss is: 0.0001596508281432952\n",
      "test loss is 0.0002627140458233173\n",
      "Batch: 28200,train loss is: 0.0003230186944599316\n",
      "test loss is 0.0002967023497389672\n",
      "Batch: 28300,train loss is: 0.0002386797111121241\n",
      "test loss is 0.0003404838479477054\n",
      "Batch: 28400,train loss is: 0.0002577160081286654\n",
      "test loss is 0.0003529896734294395\n",
      "Batch: 28500,train loss is: 0.00028412916216943656\n",
      "test loss is 0.00036142385905155183\n",
      "Batch: 28600,train loss is: 0.0002615174643169697\n",
      "test loss is 0.00027173432567591623\n",
      "Batch: 28700,train loss is: 0.00018718996403419022\n",
      "test loss is 0.0002529325152170663\n",
      "Batch: 28800,train loss is: 0.0001814393263976429\n",
      "test loss is 0.0002991632424770631\n",
      "Batch: 28900,train loss is: 0.00025380377644662556\n",
      "test loss is 0.0002824826453870772\n",
      "Batch: 29000,train loss is: 0.00015127113098660375\n",
      "test loss is 0.0002648363852976482\n",
      "Batch: 29100,train loss is: 0.00034059426745577936\n",
      "test loss is 0.0002561513029603089\n",
      "Batch: 29200,train loss is: 0.00039934228623585003\n",
      "test loss is 0.0002619950123515177\n",
      "Batch: 29300,train loss is: 0.00017463315036125014\n",
      "test loss is 0.00027332927635409623\n",
      "Batch: 29400,train loss is: 0.0002453801823963649\n",
      "test loss is 0.00028192312246661297\n",
      "Batch: 29500,train loss is: 0.00022751835737866723\n",
      "test loss is 0.00025014776617485234\n",
      "Batch: 29600,train loss is: 0.00021936641514746003\n",
      "test loss is 0.0002614070212953921\n",
      "Batch: 29700,train loss is: 0.00040656658811664173\n",
      "test loss is 0.00027920170407834237\n",
      "Batch: 29800,train loss is: 0.00022713555923944842\n",
      "test loss is 0.00025447842472717215\n",
      "Batch: 29900,train loss is: 0.0003688041879980607\n",
      "test loss is 0.0002659175744973658\n",
      "Batch: 30000,train loss is: 0.00025287429814218655\n",
      "test loss is 0.00036063715393984614\n",
      "Batch: 30100,train loss is: 0.0004296550084986401\n",
      "test loss is 0.0002993961443335825\n",
      "Batch: 30200,train loss is: 0.0001370667929956066\n",
      "test loss is 0.0002766433043859379\n",
      "Batch: 30300,train loss is: 0.0002182198883982042\n",
      "test loss is 0.0002642503166529511\n",
      "Batch: 30400,train loss is: 0.0002446298037368258\n",
      "test loss is 0.0002824890246270756\n",
      "Batch: 30500,train loss is: 0.0003597989807438837\n",
      "test loss is 0.00032561488613720555\n",
      "Batch: 30600,train loss is: 0.0002763476833661956\n",
      "test loss is 0.000276629012400849\n",
      "Batch: 30700,train loss is: 0.00021356810515667815\n",
      "test loss is 0.0002614487663490177\n",
      "Batch: 30800,train loss is: 0.00026660944490011844\n",
      "test loss is 0.0002529941124013198\n",
      "Batch: 30900,train loss is: 0.00025103762838973886\n",
      "test loss is 0.0003008997583185779\n",
      "Batch: 31000,train loss is: 0.00029506309058558453\n",
      "test loss is 0.00027146101126348284\n",
      "Batch: 31100,train loss is: 0.00021552622250677847\n",
      "test loss is 0.0002635721303781599\n",
      "Batch: 31200,train loss is: 0.0002502524386860445\n",
      "test loss is 0.00029818439915328855\n",
      "Batch: 31300,train loss is: 0.00019166947594338003\n",
      "test loss is 0.0002745877257387285\n",
      "Batch: 31400,train loss is: 0.0001820377839732392\n",
      "test loss is 0.0002703255212285372\n",
      "Batch: 31500,train loss is: 0.000270733795480586\n",
      "test loss is 0.00026653771049622247\n",
      "Batch: 31600,train loss is: 0.00024675700070370777\n",
      "test loss is 0.0002612548494936933\n",
      "Batch: 31700,train loss is: 0.0002228812619556617\n",
      "test loss is 0.00025510402977004106\n",
      "Batch: 31800,train loss is: 0.0003444463106040472\n",
      "test loss is 0.000369575174977276\n",
      "Batch: 31900,train loss is: 0.0002487261238447284\n",
      "test loss is 0.00025903509183733533\n",
      "Batch: 32000,train loss is: 0.00021183424029065683\n",
      "test loss is 0.000272706692653089\n",
      "Batch: 32100,train loss is: 0.00028760417602307015\n",
      "test loss is 0.00025819671245458077\n",
      "Batch: 32200,train loss is: 0.000257905445046022\n",
      "test loss is 0.00026690599470216363\n",
      "Batch: 32300,train loss is: 0.0006753149470177393\n",
      "test loss is 0.0002617263309954317\n",
      "Batch: 32400,train loss is: 0.0004079399174178858\n",
      "test loss is 0.00031893617506656153\n",
      "Batch: 32500,train loss is: 0.00017252986551903637\n",
      "test loss is 0.0002727595547368595\n",
      "Batch: 32600,train loss is: 0.0002724061022384988\n",
      "test loss is 0.00028710477043851257\n",
      "Batch: 32700,train loss is: 0.0003055067826416198\n",
      "test loss is 0.0002862409742818471\n",
      "Batch: 32800,train loss is: 0.00039949349632969274\n",
      "test loss is 0.00029877329954503516\n",
      "Batch: 32900,train loss is: 0.00019004167266019215\n",
      "test loss is 0.00026379987267815576\n",
      "Batch: 33000,train loss is: 0.0002434422267385056\n",
      "test loss is 0.00025318290958312634\n",
      "Batch: 33100,train loss is: 0.00018799507957342643\n",
      "test loss is 0.00029500720886798916\n",
      "Batch: 33200,train loss is: 0.000214603422570646\n",
      "test loss is 0.0003084777303465447\n",
      "Batch: 33300,train loss is: 0.00018729683799517725\n",
      "test loss is 0.0002828624501356694\n",
      "Batch: 33400,train loss is: 0.00021482179701357045\n",
      "test loss is 0.00026832480659428313\n",
      "Batch: 33500,train loss is: 0.0003328583122621441\n",
      "test loss is 0.0003536389737446329\n",
      "Batch: 33600,train loss is: 0.0006809739837257296\n",
      "test loss is 0.0002908228386732709\n",
      "Batch: 33700,train loss is: 0.00030094362573179664\n",
      "test loss is 0.00029500320865595206\n",
      "Batch: 33800,train loss is: 0.00021696231959893823\n",
      "test loss is 0.000262947690048542\n",
      "Batch: 33900,train loss is: 0.00023962672727341507\n",
      "test loss is 0.00032493278186192015\n",
      "Batch: 34000,train loss is: 0.00021541807856871622\n",
      "test loss is 0.00028396619795521876\n",
      "Batch: 34100,train loss is: 0.00021397181128437795\n",
      "test loss is 0.00025270423019935803\n",
      "Batch: 34200,train loss is: 0.00021606209652454592\n",
      "test loss is 0.00027482386019041493\n",
      "Batch: 34300,train loss is: 0.00019930708196179707\n",
      "test loss is 0.0002548836757402353\n",
      "Batch: 34400,train loss is: 0.0003654403826165943\n",
      "test loss is 0.0002876609977491387\n",
      "Batch: 34500,train loss is: 0.00028064497614938965\n",
      "test loss is 0.00027912898154583605\n",
      "Batch: 34600,train loss is: 0.0002490159605125463\n",
      "test loss is 0.0002633855856736523\n",
      "Batch: 34700,train loss is: 0.0002866877275411686\n",
      "test loss is 0.00026920427881902054\n",
      "Batch: 34800,train loss is: 0.00024358127258572362\n",
      "test loss is 0.00026731622673815085\n",
      "Batch: 34900,train loss is: 0.0002697123155194225\n",
      "test loss is 0.00025896479420559396\n",
      "Batch: 35000,train loss is: 0.00021034261415245247\n",
      "test loss is 0.0002729340663300298\n",
      "Batch: 35100,train loss is: 0.00023192742048434937\n",
      "test loss is 0.00028645847693500344\n",
      "Batch: 35200,train loss is: 0.0003761012791862785\n",
      "test loss is 0.00024813539457392537\n",
      "Batch: 35300,train loss is: 0.00031304492012878095\n",
      "test loss is 0.0002655836597917113\n",
      "Batch: 35400,train loss is: 0.000230488535414784\n",
      "test loss is 0.00027674186426681274\n",
      "Batch: 35500,train loss is: 0.00035716394063501565\n",
      "test loss is 0.00029178394527765817\n",
      "Batch: 35600,train loss is: 0.00016256171619309726\n",
      "test loss is 0.0002900972283157709\n",
      "Batch: 35700,train loss is: 0.0003960523751617613\n",
      "test loss is 0.00035865125887331745\n",
      "Batch: 35800,train loss is: 0.00015563985047765781\n",
      "test loss is 0.00026965307966647276\n",
      "Batch: 35900,train loss is: 0.0003403647825470392\n",
      "test loss is 0.0002651320121113513\n",
      "Batch: 36000,train loss is: 0.00021271743522733428\n",
      "test loss is 0.00026929833406636876\n",
      "Batch: 36100,train loss is: 0.00023303084008762139\n",
      "test loss is 0.00028438832881463313\n",
      "Batch: 36200,train loss is: 0.0002544222288635854\n",
      "test loss is 0.0003044379691202628\n",
      "Batch: 36300,train loss is: 0.00027278530375599843\n",
      "test loss is 0.00030132483792580275\n",
      "Batch: 36400,train loss is: 0.0002851458826771965\n",
      "test loss is 0.000326702483886067\n",
      "Batch: 36500,train loss is: 0.0005806189827762488\n",
      "test loss is 0.0003628127025120256\n",
      "Batch: 36600,train loss is: 0.00023380179144124943\n",
      "test loss is 0.00026778679041105735\n",
      "Batch: 36700,train loss is: 0.00026146509002732403\n",
      "test loss is 0.00025743899711101195\n",
      "Batch: 36800,train loss is: 0.0001786466809145543\n",
      "test loss is 0.00026734194432010317\n",
      "Batch: 36900,train loss is: 0.0001699685530156038\n",
      "test loss is 0.00025264986149888875\n",
      "Batch: 37000,train loss is: 0.00017275724427335052\n",
      "test loss is 0.0002480675865192256\n",
      "Batch: 37100,train loss is: 0.0002109114094830379\n",
      "test loss is 0.00027625077862101675\n",
      "Batch: 37200,train loss is: 0.00017021637427677933\n",
      "test loss is 0.0002756633473069765\n",
      "Batch: 37300,train loss is: 0.00025334637314140254\n",
      "test loss is 0.00029064261763006496\n",
      "Batch: 37400,train loss is: 0.000325418885768295\n",
      "test loss is 0.0003004753584652004\n",
      "Batch: 37500,train loss is: 0.0002203241112609787\n",
      "test loss is 0.0002807060359182609\n",
      "Batch: 37600,train loss is: 0.00019059720017745264\n",
      "test loss is 0.00024959496388047555\n",
      "Batch: 37700,train loss is: 0.0002211684307197893\n",
      "test loss is 0.00031867407822905796\n",
      "Batch: 37800,train loss is: 0.0003209530745465383\n",
      "test loss is 0.0002805647523464621\n",
      "Batch: 37900,train loss is: 0.00015588352954512037\n",
      "test loss is 0.000267544908251016\n",
      "Batch: 38000,train loss is: 0.0003144777631491787\n",
      "test loss is 0.00029290120026240375\n",
      "Batch: 38100,train loss is: 0.0001930898467532795\n",
      "test loss is 0.0002565065774498453\n",
      "Batch: 38200,train loss is: 0.0001483689205800254\n",
      "test loss is 0.00028029217406935574\n",
      "Batch: 38300,train loss is: 0.00023875025974991215\n",
      "test loss is 0.0002700752622895349\n",
      "Batch: 38400,train loss is: 0.0002119824966977837\n",
      "test loss is 0.0002775893580575378\n",
      "Batch: 38500,train loss is: 0.00019111010350104007\n",
      "test loss is 0.0002545329355065155\n",
      "Batch: 38600,train loss is: 0.00022705006745916961\n",
      "test loss is 0.00025512690872630004\n",
      "Batch: 38700,train loss is: 0.00022739659745776573\n",
      "test loss is 0.0002686626399334659\n",
      "Batch: 38800,train loss is: 0.00026777060806040635\n",
      "test loss is 0.0003514409763486467\n",
      "Batch: 38900,train loss is: 0.0003008397175677226\n",
      "test loss is 0.0002918431167344235\n",
      "Batch: 39000,train loss is: 0.0004752664855382662\n",
      "test loss is 0.00032438378129559644\n",
      "Batch: 39100,train loss is: 0.00030771909944977017\n",
      "test loss is 0.00026575459238637443\n",
      "Batch: 39200,train loss is: 0.00024800232186680625\n",
      "test loss is 0.0002679228740035019\n",
      "Batch: 39300,train loss is: 0.0002673038145413382\n",
      "test loss is 0.00032477620372144177\n",
      "Batch: 39400,train loss is: 0.0002816324080912065\n",
      "test loss is 0.000298585189856835\n",
      "Batch: 39500,train loss is: 0.00024065584685712568\n",
      "test loss is 0.00028930229951243294\n",
      "Batch: 39600,train loss is: 0.00027387678319908566\n",
      "test loss is 0.0003008112584568978\n",
      "Batch: 39700,train loss is: 0.0003035124185078547\n",
      "test loss is 0.000278574645483335\n",
      "Batch: 39800,train loss is: 0.00032305675711011994\n",
      "test loss is 0.00025236627897983214\n",
      "Batch: 39900,train loss is: 0.00025883038471305095\n",
      "test loss is 0.0002756261261321646\n",
      "Batch: 40000,train loss is: 0.00023964031345897064\n",
      "test loss is 0.00025995289658191893\n",
      "Batch: 40100,train loss is: 0.00030049449505126035\n",
      "test loss is 0.00026303462931627535\n",
      "Batch: 40200,train loss is: 0.00039488567850389465\n",
      "test loss is 0.00025881941706291966\n",
      "Batch: 40300,train loss is: 0.00018952939658263687\n",
      "test loss is 0.0002494786113871508\n",
      "Batch: 40400,train loss is: 0.00020087375734328535\n",
      "test loss is 0.0002493602560904725\n",
      "Batch: 40500,train loss is: 0.00022284417923490365\n",
      "test loss is 0.00026213957289565186\n",
      "Batch: 40600,train loss is: 0.000297312160498573\n",
      "test loss is 0.00027519307113868383\n",
      "Batch: 40700,train loss is: 0.0001883357680299728\n",
      "test loss is 0.00025565498582508385\n",
      "Batch: 40800,train loss is: 0.00036466697996944374\n",
      "test loss is 0.00037657252911255757\n",
      "Batch: 40900,train loss is: 0.00025289442426929334\n",
      "test loss is 0.0002824205372077566\n",
      "Batch: 41000,train loss is: 0.00023468560735196716\n",
      "test loss is 0.000271809341833136\n",
      "Batch: 41100,train loss is: 0.00043511104914445046\n",
      "test loss is 0.0002859132198011524\n",
      "Batch: 41200,train loss is: 0.0004211337584610151\n",
      "test loss is 0.0002949359516942334\n",
      "Batch: 41300,train loss is: 0.00037329914714550856\n",
      "test loss is 0.00026487981920269183\n",
      "Batch: 41400,train loss is: 0.00020195043198782204\n",
      "test loss is 0.0002705905636005027\n",
      "Batch: 41500,train loss is: 0.0003214522564693466\n",
      "test loss is 0.0002793660618618405\n",
      "Batch: 41600,train loss is: 0.00029004977686790476\n",
      "test loss is 0.0003036349163253429\n",
      "Batch: 41700,train loss is: 0.00016195914603620075\n",
      "test loss is 0.00028520464954630504\n",
      "Batch: 41800,train loss is: 0.00039423933459419665\n",
      "test loss is 0.0002819606688290144\n",
      "Batch: 41900,train loss is: 0.00015599220177673321\n",
      "test loss is 0.00028206070691379166\n",
      "Batch: 42000,train loss is: 0.00029533928015716\n",
      "test loss is 0.0003272591865781388\n",
      "Batch: 42100,train loss is: 0.0002426931806662793\n",
      "test loss is 0.00029327179285479533\n",
      "Batch: 42200,train loss is: 0.00021442457281878923\n",
      "test loss is 0.0002827164128497702\n",
      "Batch: 42300,train loss is: 0.0002690590678250716\n",
      "test loss is 0.00041714341692162355\n",
      "Batch: 42400,train loss is: 0.0001412428946574526\n",
      "test loss is 0.00027019063738322656\n",
      "Batch: 42500,train loss is: 0.0001898564034110592\n",
      "test loss is 0.0002628476122415152\n",
      "Batch: 42600,train loss is: 0.0003011053510545942\n",
      "test loss is 0.0002844897848754706\n",
      "Batch: 42700,train loss is: 0.0002828611993863122\n",
      "test loss is 0.0002867112526066098\n",
      "Batch: 42800,train loss is: 0.00026629223594224144\n",
      "test loss is 0.0002765182442999071\n",
      "Batch: 42900,train loss is: 0.0001981574284726916\n",
      "test loss is 0.0002563353305155992\n",
      "Batch: 43000,train loss is: 0.00023890532739060576\n",
      "test loss is 0.00026751237885367957\n",
      "Batch: 43100,train loss is: 0.0001838545859611319\n",
      "test loss is 0.0003103505342876492\n",
      "Batch: 43200,train loss is: 0.00041407894484960157\n",
      "test loss is 0.0002810334655906166\n",
      "Batch: 43300,train loss is: 0.0004104538901355212\n",
      "test loss is 0.000314782406800187\n",
      "Batch: 43400,train loss is: 0.0002761294754757967\n",
      "test loss is 0.0002678192632549942\n",
      "Batch: 43500,train loss is: 0.00024140809192625253\n",
      "test loss is 0.00029983586431699285\n",
      "Batch: 43600,train loss is: 0.00020656082180247205\n",
      "test loss is 0.0002462065990737625\n",
      "Batch: 43700,train loss is: 0.00011675828377046716\n",
      "test loss is 0.0002477869774223543\n",
      "Batch: 43800,train loss is: 0.00027254174214053696\n",
      "test loss is 0.000294237863921165\n",
      "Batch: 43900,train loss is: 0.0001712849958611159\n",
      "test loss is 0.0002858415205583661\n",
      "Batch: 44000,train loss is: 0.0003263057908202435\n",
      "test loss is 0.0003024136767994444\n",
      "Batch: 44100,train loss is: 0.00030005287805091136\n",
      "test loss is 0.00029263885121300905\n",
      "Batch: 44200,train loss is: 0.00022884022293725406\n",
      "test loss is 0.00024519724208159555\n",
      "Batch: 44300,train loss is: 0.00025991598757332725\n",
      "test loss is 0.0003333070392219723\n",
      "Batch: 44400,train loss is: 0.0002612593932296263\n",
      "test loss is 0.000278187550223936\n",
      "Batch: 44500,train loss is: 0.0003501035248540846\n",
      "test loss is 0.0002951761755262364\n",
      "Batch: 44600,train loss is: 0.00023059568802300396\n",
      "test loss is 0.00028237318945656555\n",
      "Batch: 44700,train loss is: 0.00018106989879059713\n",
      "test loss is 0.0002675307192113436\n",
      "Batch: 44800,train loss is: 0.0002289706800810596\n",
      "test loss is 0.00037835814628100627\n",
      "Batch: 44900,train loss is: 0.0002451426828112\n",
      "test loss is 0.00025781947910442606\n",
      "Batch: 45000,train loss is: 0.00026488034315380723\n",
      "test loss is 0.00029075874311962417\n",
      "Batch: 45100,train loss is: 0.00030737901343024605\n",
      "test loss is 0.00032448962894979786\n",
      "Batch: 45200,train loss is: 0.00021183659354827972\n",
      "test loss is 0.0003316372127982032\n",
      "Batch: 45300,train loss is: 0.00025584925022075194\n",
      "test loss is 0.0002774022486800453\n",
      "Batch: 45400,train loss is: 0.0003450295473476744\n",
      "test loss is 0.0002753863529400288\n",
      "Batch: 45500,train loss is: 0.0002791156612288818\n",
      "test loss is 0.00025965520253603424\n",
      "Batch: 45600,train loss is: 0.00031514471047977837\n",
      "test loss is 0.0002729528763526985\n",
      "Batch: 45700,train loss is: 0.00031655424275100976\n",
      "test loss is 0.00027418929420660416\n",
      "Batch: 45800,train loss is: 0.00044944012981319606\n",
      "test loss is 0.000254303507602325\n",
      "Batch: 45900,train loss is: 0.0002100438069832157\n",
      "test loss is 0.0002507977145792491\n",
      "Batch: 46000,train loss is: 0.00033834791069129046\n",
      "test loss is 0.00028208325617619097\n",
      "Batch: 46100,train loss is: 0.0002341037262451531\n",
      "test loss is 0.00028110182872530286\n",
      "Batch: 46200,train loss is: 0.0001540221406387589\n",
      "test loss is 0.0002623908542331811\n",
      "Batch: 46300,train loss is: 0.0004359935101228514\n",
      "test loss is 0.0002594457832773927\n",
      "Batch: 46400,train loss is: 0.0002455031462520199\n",
      "test loss is 0.00028337409626912004\n",
      "Batch: 46500,train loss is: 0.0004266372869395146\n",
      "test loss is 0.0002934453198958166\n",
      "Batch: 46600,train loss is: 0.00034836731412674\n",
      "test loss is 0.00025151082837103576\n",
      "Batch: 46700,train loss is: 0.0002088675032438604\n",
      "test loss is 0.00028540546111127234\n",
      "-----------------------Epoch: 8----------------------------------\n",
      "Batch: 0,train loss is: 0.0003857529962946017\n",
      "test loss is 0.00025005594680858965\n",
      "Batch: 100,train loss is: 0.0002565801683310778\n",
      "test loss is 0.00025602688738794965\n",
      "Batch: 200,train loss is: 0.00039387687569622054\n",
      "test loss is 0.0002696530737027737\n",
      "Batch: 300,train loss is: 0.0002368474650683947\n",
      "test loss is 0.00030371567590807785\n",
      "Batch: 400,train loss is: 0.00014518159613320576\n",
      "test loss is 0.0002490362258571553\n",
      "Batch: 500,train loss is: 0.00022495169372628186\n",
      "test loss is 0.0003086322239556818\n",
      "Batch: 600,train loss is: 0.0002852247998927309\n",
      "test loss is 0.00031253328374037547\n",
      "Batch: 700,train loss is: 0.00023904268417463954\n",
      "test loss is 0.000320555578967142\n",
      "Batch: 800,train loss is: 0.00020544188876878993\n",
      "test loss is 0.0002651304213181244\n",
      "Batch: 900,train loss is: 0.00018177982068969968\n",
      "test loss is 0.00025752211695980994\n",
      "Batch: 1000,train loss is: 0.0003302765549834475\n",
      "test loss is 0.00027143927675013957\n",
      "Batch: 1100,train loss is: 0.00025925084872576755\n",
      "test loss is 0.0002499258685860814\n",
      "Batch: 1200,train loss is: 0.00019625325272801757\n",
      "test loss is 0.0002553784388774579\n",
      "Batch: 1300,train loss is: 0.00020279963175454997\n",
      "test loss is 0.00032380788699989266\n",
      "Batch: 1400,train loss is: 0.000276421439288304\n",
      "test loss is 0.0002726341560637328\n",
      "Batch: 1500,train loss is: 0.00020291814540820345\n",
      "test loss is 0.0002912942409759884\n",
      "Batch: 1600,train loss is: 0.0002006439736856985\n",
      "test loss is 0.00026809880095188986\n",
      "Batch: 1700,train loss is: 0.00026405800867872095\n",
      "test loss is 0.0002904206222057537\n",
      "Batch: 1800,train loss is: 0.00033484290834655793\n",
      "test loss is 0.00031172797608316447\n",
      "Batch: 1900,train loss is: 0.0002459022743545138\n",
      "test loss is 0.00031063744018248454\n",
      "Batch: 2000,train loss is: 0.0002759970037593337\n",
      "test loss is 0.00024244873222667868\n",
      "Batch: 2100,train loss is: 0.00018085579986924316\n",
      "test loss is 0.000276190139570607\n",
      "Batch: 2200,train loss is: 0.00019991077853119175\n",
      "test loss is 0.0002776379641464709\n",
      "Batch: 2300,train loss is: 0.0003023700523820002\n",
      "test loss is 0.0002704522712922974\n",
      "Batch: 2400,train loss is: 0.00021056928073608964\n",
      "test loss is 0.00026377163725724496\n",
      "Batch: 2500,train loss is: 0.00027076228902926734\n",
      "test loss is 0.0002712088545972177\n",
      "Batch: 2600,train loss is: 0.0003188172635269796\n",
      "test loss is 0.00034724261592849803\n",
      "Batch: 2700,train loss is: 0.0002671347301342713\n",
      "test loss is 0.0002934830460402206\n",
      "Batch: 2800,train loss is: 0.00043397967043174823\n",
      "test loss is 0.00036005925161975517\n",
      "Batch: 2900,train loss is: 0.0001921353741202442\n",
      "test loss is 0.0002425740958602894\n",
      "Batch: 3000,train loss is: 0.0002823792216181837\n",
      "test loss is 0.0002632417866898546\n",
      "Batch: 3100,train loss is: 0.0006466747453688569\n",
      "test loss is 0.00029190288046889\n",
      "Batch: 3200,train loss is: 0.00036769604393011685\n",
      "test loss is 0.00026649949062710964\n",
      "Batch: 3300,train loss is: 0.00028911702448717463\n",
      "test loss is 0.00026080420601983303\n",
      "Batch: 3400,train loss is: 0.0001909225189169908\n",
      "test loss is 0.0002840899041990883\n",
      "Batch: 3500,train loss is: 0.00037129187102496065\n",
      "test loss is 0.0003146410277423304\n",
      "Batch: 3600,train loss is: 0.00021323550676320203\n",
      "test loss is 0.0002691663812131172\n",
      "Batch: 3700,train loss is: 0.000442662802606816\n",
      "test loss is 0.00026183148615568317\n",
      "Batch: 3800,train loss is: 0.0003525454970659991\n",
      "test loss is 0.00029395836190218523\n",
      "Batch: 3900,train loss is: 0.0002581202821725826\n",
      "test loss is 0.0003293986290084673\n",
      "Batch: 4000,train loss is: 0.00027682874041898525\n",
      "test loss is 0.0002684676283948114\n",
      "Batch: 4100,train loss is: 0.0003541592446958632\n",
      "test loss is 0.00026473786674124667\n",
      "Batch: 4200,train loss is: 0.00035363310403825957\n",
      "test loss is 0.0002500992440566211\n",
      "Batch: 4300,train loss is: 0.00034100062484442213\n",
      "test loss is 0.0003254984757151635\n",
      "Batch: 4400,train loss is: 0.0003309206617007186\n",
      "test loss is 0.00027955671899293326\n",
      "Batch: 4500,train loss is: 0.00020208830272386402\n",
      "test loss is 0.00027400032399505907\n",
      "Batch: 4600,train loss is: 0.0002721153944791251\n",
      "test loss is 0.00023875082258365893\n",
      "Batch: 4700,train loss is: 0.00021030680428037218\n",
      "test loss is 0.00027974790841272384\n",
      "Batch: 4800,train loss is: 0.00023652846919807726\n",
      "test loss is 0.0002886310989499052\n",
      "Batch: 4900,train loss is: 0.00030703948408472147\n",
      "test loss is 0.0002837100096574084\n",
      "Batch: 5000,train loss is: 0.0001911845635681876\n",
      "test loss is 0.00025399558117073764\n",
      "Batch: 5100,train loss is: 0.0003695125700107762\n",
      "test loss is 0.0002970058710638652\n",
      "Batch: 5200,train loss is: 0.00015641220012776736\n",
      "test loss is 0.0002487866539078508\n",
      "Batch: 5300,train loss is: 0.00030844241787161245\n",
      "test loss is 0.0002882089101927184\n",
      "Batch: 5400,train loss is: 0.00021150349740758044\n",
      "test loss is 0.0002453196312030839\n",
      "Batch: 5500,train loss is: 0.0001906383696678853\n",
      "test loss is 0.00029681492319486957\n",
      "Batch: 5600,train loss is: 0.001072547739019214\n",
      "test loss is 0.00029412338443030863\n",
      "Batch: 5700,train loss is: 0.00031520000764403045\n",
      "test loss is 0.00026480984096821154\n",
      "Batch: 5800,train loss is: 0.0002776007491580648\n",
      "test loss is 0.0002681242448403994\n",
      "Batch: 5900,train loss is: 0.00029681591474204445\n",
      "test loss is 0.0002678216372059146\n",
      "Batch: 6000,train loss is: 0.00031693468487118185\n",
      "test loss is 0.00032508418838238565\n",
      "Batch: 6100,train loss is: 0.00016335662174024663\n",
      "test loss is 0.0002601321750000666\n",
      "Batch: 6200,train loss is: 0.0003502734937998422\n",
      "test loss is 0.0003471129582558449\n",
      "Batch: 6300,train loss is: 0.00030618379993163603\n",
      "test loss is 0.00027195982001180645\n",
      "Batch: 6400,train loss is: 0.00014520217065250367\n",
      "test loss is 0.0002542910223345196\n",
      "Batch: 6500,train loss is: 0.0003277391145899375\n",
      "test loss is 0.0002675521765110398\n",
      "Batch: 6600,train loss is: 0.00022013949349908903\n",
      "test loss is 0.0002703940279143931\n",
      "Batch: 6700,train loss is: 0.00019353524392614545\n",
      "test loss is 0.0002476938414195482\n",
      "Batch: 6800,train loss is: 0.0003380247025178606\n",
      "test loss is 0.0002828695965053323\n",
      "Batch: 6900,train loss is: 0.0002576843253849977\n",
      "test loss is 0.0003088568065925936\n",
      "Batch: 7000,train loss is: 0.00017772164994879323\n",
      "test loss is 0.0002710906596425913\n",
      "Batch: 7100,train loss is: 0.0001806327751879373\n",
      "test loss is 0.00025135067409712806\n",
      "Batch: 7200,train loss is: 0.00011748746706473482\n",
      "test loss is 0.0002608032453811218\n",
      "Batch: 7300,train loss is: 0.000267392918665421\n",
      "test loss is 0.00032451780746508766\n",
      "Batch: 7400,train loss is: 0.0002868269254371552\n",
      "test loss is 0.0002502484090607595\n",
      "Batch: 7500,train loss is: 0.00019335211688619917\n",
      "test loss is 0.00025763939133610083\n",
      "Batch: 7600,train loss is: 0.00025107766511607864\n",
      "test loss is 0.00037210981230316573\n",
      "Batch: 7700,train loss is: 0.00037956875079211874\n",
      "test loss is 0.00025787653481815757\n",
      "Batch: 7800,train loss is: 0.0003189233922077334\n",
      "test loss is 0.0003159944452261018\n",
      "Batch: 7900,train loss is: 0.00023893514032044923\n",
      "test loss is 0.0002810595870614595\n",
      "Batch: 8000,train loss is: 0.0002762905140710773\n",
      "test loss is 0.00027156071546074247\n",
      "Batch: 8100,train loss is: 0.00015810505321942774\n",
      "test loss is 0.0002625771662779755\n",
      "Batch: 8200,train loss is: 0.0004826916841330231\n",
      "test loss is 0.0002606489173052976\n",
      "Batch: 8300,train loss is: 0.00022915707851142754\n",
      "test loss is 0.00028555910443860296\n",
      "Batch: 8400,train loss is: 0.0001729501567205955\n",
      "test loss is 0.0002575790584982635\n",
      "Batch: 8500,train loss is: 0.00039132515567103963\n",
      "test loss is 0.000266894852858463\n",
      "Batch: 8600,train loss is: 0.0002266273809267414\n",
      "test loss is 0.00028904473982456984\n",
      "Batch: 8700,train loss is: 0.0002262345376600411\n",
      "test loss is 0.0002627801672952799\n",
      "Batch: 8800,train loss is: 0.00034889131991741254\n",
      "test loss is 0.0002570202536486917\n",
      "Batch: 8900,train loss is: 0.00023336154057108807\n",
      "test loss is 0.0002500271876724774\n",
      "Batch: 9000,train loss is: 0.0002479949304708498\n",
      "test loss is 0.0002711901335168433\n",
      "Batch: 9100,train loss is: 0.00023614610681612546\n",
      "test loss is 0.0004140995445165165\n",
      "Batch: 9200,train loss is: 0.0005129988736600059\n",
      "test loss is 0.00026925326967601986\n",
      "Batch: 9300,train loss is: 0.00032379388679595666\n",
      "test loss is 0.0002474804423433291\n",
      "Batch: 9400,train loss is: 0.00020057919622379678\n",
      "test loss is 0.0002654334468457188\n",
      "Batch: 9500,train loss is: 0.00020139074158619268\n",
      "test loss is 0.0002786734489851996\n",
      "Batch: 9600,train loss is: 0.00020909352602723708\n",
      "test loss is 0.0002519962787721712\n",
      "Batch: 9700,train loss is: 0.0003972094737116437\n",
      "test loss is 0.00026309742245807525\n",
      "Batch: 9800,train loss is: 0.00010012766292784281\n",
      "test loss is 0.00025150041028474275\n",
      "Batch: 9900,train loss is: 0.00041170685661691615\n",
      "test loss is 0.000361535680356708\n",
      "Batch: 10000,train loss is: 0.00027672428175384216\n",
      "test loss is 0.00032574984720914633\n",
      "Batch: 10100,train loss is: 0.00019461120415152365\n",
      "test loss is 0.0002497750747057237\n",
      "Batch: 10200,train loss is: 0.00020655778951379536\n",
      "test loss is 0.0002631466378308079\n",
      "Batch: 10300,train loss is: 0.00021324163605435884\n",
      "test loss is 0.00027014723119432333\n",
      "Batch: 10400,train loss is: 0.00023661571767815854\n",
      "test loss is 0.0002573733117622163\n",
      "Batch: 10500,train loss is: 0.00031076055649908584\n",
      "test loss is 0.0002802846622019074\n",
      "Batch: 10600,train loss is: 0.000251291185003469\n",
      "test loss is 0.0002592956439583222\n",
      "Batch: 10700,train loss is: 0.00036206940741292655\n",
      "test loss is 0.0002479771973033461\n",
      "Batch: 10800,train loss is: 0.00022052595822100033\n",
      "test loss is 0.0002748396084034572\n",
      "Batch: 10900,train loss is: 0.0004424082126348565\n",
      "test loss is 0.0002719150128970911\n",
      "Batch: 11000,train loss is: 0.00030720406690792283\n",
      "test loss is 0.00030576666498334596\n",
      "Batch: 11100,train loss is: 0.00025129328755910043\n",
      "test loss is 0.00025116845676210076\n",
      "Batch: 11200,train loss is: 0.0006475750687630847\n",
      "test loss is 0.00043098584557111106\n",
      "Batch: 11300,train loss is: 0.0002958385429678778\n",
      "test loss is 0.0002551290471456161\n",
      "Batch: 11400,train loss is: 0.0003100591603286629\n",
      "test loss is 0.0002976541440990156\n",
      "Batch: 11500,train loss is: 0.0002200802844745516\n",
      "test loss is 0.0003162955175543025\n",
      "Batch: 11600,train loss is: 0.00033506165331203527\n",
      "test loss is 0.0002532318202882222\n",
      "Batch: 11700,train loss is: 0.0003463402195506663\n",
      "test loss is 0.00032952257447920423\n",
      "Batch: 11800,train loss is: 0.0004120363057674969\n",
      "test loss is 0.00030186766880972406\n",
      "Batch: 11900,train loss is: 0.00018982004744952195\n",
      "test loss is 0.0002523649954759283\n",
      "Batch: 12000,train loss is: 0.00034024957508776174\n",
      "test loss is 0.00026461668283495526\n",
      "Batch: 12100,train loss is: 0.0003964892531253097\n",
      "test loss is 0.00037645908965287225\n",
      "Batch: 12200,train loss is: 0.00021264681221513308\n",
      "test loss is 0.00027147722212802745\n",
      "Batch: 12300,train loss is: 0.000174693074070147\n",
      "test loss is 0.00030531192926969066\n",
      "Batch: 12400,train loss is: 0.0002992511329981209\n",
      "test loss is 0.0002914640436194391\n",
      "Batch: 12500,train loss is: 0.0003259204263389429\n",
      "test loss is 0.00029559120931232235\n",
      "Batch: 12600,train loss is: 0.00023334328413524485\n",
      "test loss is 0.0002628779884577448\n",
      "Batch: 12700,train loss is: 0.0003759662434955314\n",
      "test loss is 0.00027911802460905673\n",
      "Batch: 12800,train loss is: 0.00022837131712007816\n",
      "test loss is 0.00024112527574219708\n",
      "Batch: 12900,train loss is: 0.00047832333759928675\n",
      "test loss is 0.000381976897627115\n",
      "Batch: 13000,train loss is: 0.00031567498519121593\n",
      "test loss is 0.00023821305572877002\n",
      "Batch: 13100,train loss is: 0.000222352463816409\n",
      "test loss is 0.00028929970573512683\n",
      "Batch: 13200,train loss is: 0.000350050506054197\n",
      "test loss is 0.000250043621544252\n",
      "Batch: 13300,train loss is: 0.00018851656278016086\n",
      "test loss is 0.0002574807964288311\n",
      "Batch: 13400,train loss is: 0.00023779159579559943\n",
      "test loss is 0.00030020459151299504\n",
      "Batch: 13500,train loss is: 0.00031464437322149894\n",
      "test loss is 0.00028154468519129026\n",
      "Batch: 13600,train loss is: 0.00019791356840677857\n",
      "test loss is 0.0002377054471339098\n",
      "Batch: 13700,train loss is: 0.0001554020337834177\n",
      "test loss is 0.000254204302635116\n",
      "Batch: 13800,train loss is: 0.00020917444385632475\n",
      "test loss is 0.00024838034323543587\n",
      "Batch: 13900,train loss is: 0.00023450792675759163\n",
      "test loss is 0.0002527381338316117\n",
      "Batch: 14000,train loss is: 0.0002037652076808904\n",
      "test loss is 0.0002761307080323078\n",
      "Batch: 14100,train loss is: 0.0005764171844997431\n",
      "test loss is 0.00032309993589446726\n",
      "Batch: 14200,train loss is: 0.0002542794013360941\n",
      "test loss is 0.00033517553159919484\n",
      "Batch: 14300,train loss is: 0.00020739015631852838\n",
      "test loss is 0.0002742752689793413\n",
      "Batch: 14400,train loss is: 0.0001749701588501703\n",
      "test loss is 0.00026596088350789204\n",
      "Batch: 14500,train loss is: 0.0002142085998567441\n",
      "test loss is 0.0002676804093017566\n",
      "Batch: 14600,train loss is: 0.00031681614110185447\n",
      "test loss is 0.0002848110547863717\n",
      "Batch: 14700,train loss is: 0.0002897129123371172\n",
      "test loss is 0.00031979646138827455\n",
      "Batch: 14800,train loss is: 0.00031123610496803974\n",
      "test loss is 0.00026545335974443814\n",
      "Batch: 14900,train loss is: 0.00031702250921026263\n",
      "test loss is 0.00027903900243953\n",
      "Batch: 15000,train loss is: 0.0003216042974817122\n",
      "test loss is 0.00028676078079202527\n",
      "Batch: 15100,train loss is: 0.00031629570649696235\n",
      "test loss is 0.0003177372959281339\n",
      "Batch: 15200,train loss is: 0.00021782837685864974\n",
      "test loss is 0.00027308700662798964\n",
      "Batch: 15300,train loss is: 0.00027162618232833295\n",
      "test loss is 0.0002751085480791819\n",
      "Batch: 15400,train loss is: 0.00024301701445448833\n",
      "test loss is 0.00031612543290684476\n",
      "Batch: 15500,train loss is: 0.00013249533983863412\n",
      "test loss is 0.0002990013213950381\n",
      "Batch: 15600,train loss is: 0.00017532434313371798\n",
      "test loss is 0.0002486255523072891\n",
      "Batch: 15700,train loss is: 0.00038269090610426864\n",
      "test loss is 0.00024991424872200876\n",
      "Batch: 15800,train loss is: 0.0002022045570661435\n",
      "test loss is 0.0002626658525599034\n",
      "Batch: 15900,train loss is: 0.00016021078434295111\n",
      "test loss is 0.00027026208435548403\n",
      "Batch: 16000,train loss is: 0.00023256142108897375\n",
      "test loss is 0.00026770829275833485\n",
      "Batch: 16100,train loss is: 0.00021143182311906446\n",
      "test loss is 0.0002633417374972907\n",
      "Batch: 16200,train loss is: 0.0004498299526248429\n",
      "test loss is 0.0002917490941445976\n",
      "Batch: 16300,train loss is: 0.00016147480583971125\n",
      "test loss is 0.0002609377243330092\n",
      "Batch: 16400,train loss is: 0.00027212425813415977\n",
      "test loss is 0.00026872493180613396\n",
      "Batch: 16500,train loss is: 0.0003409444291052234\n",
      "test loss is 0.00027211153021265716\n",
      "Batch: 16600,train loss is: 0.0002888992064560925\n",
      "test loss is 0.00025455856776358224\n",
      "Batch: 16700,train loss is: 0.00027458461691040733\n",
      "test loss is 0.00034154625096557143\n",
      "Batch: 16800,train loss is: 0.0002730518968431326\n",
      "test loss is 0.00029712802045650755\n",
      "Batch: 16900,train loss is: 0.0003932915050823253\n",
      "test loss is 0.00032566411146177967\n",
      "Batch: 17000,train loss is: 0.0002889391033844529\n",
      "test loss is 0.00025953584526087483\n",
      "Batch: 17100,train loss is: 0.00041623065095228726\n",
      "test loss is 0.0002962600168847286\n",
      "Batch: 17200,train loss is: 0.0002506292982311462\n",
      "test loss is 0.00027488995591647353\n",
      "Batch: 17300,train loss is: 0.00030179641530170666\n",
      "test loss is 0.0002947744003699849\n",
      "Batch: 17400,train loss is: 0.00017600577890111707\n",
      "test loss is 0.0002490511155688811\n",
      "Batch: 17500,train loss is: 0.00019159539458953495\n",
      "test loss is 0.0002965969081043097\n",
      "Batch: 17600,train loss is: 0.00015764059835009805\n",
      "test loss is 0.0002835172262655381\n",
      "Batch: 17700,train loss is: 0.00040670902838859865\n",
      "test loss is 0.0003024319146577688\n",
      "Batch: 17800,train loss is: 0.0004248722722255312\n",
      "test loss is 0.00028295313249746603\n",
      "Batch: 17900,train loss is: 0.0001973560262388858\n",
      "test loss is 0.0003007021831471593\n",
      "Batch: 18000,train loss is: 0.00014496834551434612\n",
      "test loss is 0.0002592041081497086\n",
      "Batch: 18100,train loss is: 0.0002806640161388045\n",
      "test loss is 0.00026416906455911\n",
      "Batch: 18200,train loss is: 0.0002312748385640536\n",
      "test loss is 0.00030942724242353755\n",
      "Batch: 18300,train loss is: 0.0002340969953725957\n",
      "test loss is 0.0005057751436610519\n",
      "Batch: 18400,train loss is: 0.00023283405616924752\n",
      "test loss is 0.0002932888421255808\n",
      "Batch: 18500,train loss is: 0.00019241264036743842\n",
      "test loss is 0.00026491437324382196\n",
      "Batch: 18600,train loss is: 0.000223324526365653\n",
      "test loss is 0.00025758135710598676\n",
      "Batch: 18700,train loss is: 0.0002726389959873333\n",
      "test loss is 0.00024723219983267727\n",
      "Batch: 18800,train loss is: 0.0002150437432230459\n",
      "test loss is 0.000269137243733429\n",
      "Batch: 18900,train loss is: 0.0003116506374528323\n",
      "test loss is 0.0002699483338008932\n",
      "Batch: 19000,train loss is: 0.0002453509613551533\n",
      "test loss is 0.00027291109099627977\n",
      "Batch: 19100,train loss is: 0.000422039634683353\n",
      "test loss is 0.0002720582731270227\n",
      "Batch: 19200,train loss is: 0.0004927336941518592\n",
      "test loss is 0.0002886779690818574\n",
      "Batch: 19300,train loss is: 0.00016059444384482468\n",
      "test loss is 0.0002473769598380514\n",
      "Batch: 19400,train loss is: 0.0003038767213858497\n",
      "test loss is 0.0002993133653744105\n",
      "Batch: 19500,train loss is: 0.0002830112827890449\n",
      "test loss is 0.00027357454650383156\n",
      "Batch: 19600,train loss is: 0.00019947589594307555\n",
      "test loss is 0.00024569222782929795\n",
      "Batch: 19700,train loss is: 0.0001781020378148724\n",
      "test loss is 0.00026833813153595446\n",
      "Batch: 19800,train loss is: 0.00023181358652160948\n",
      "test loss is 0.0003481537908503763\n",
      "Batch: 19900,train loss is: 0.0001542967422271143\n",
      "test loss is 0.0002483218084412637\n",
      "Batch: 20000,train loss is: 0.000175015031833548\n",
      "test loss is 0.00032290826752579547\n",
      "Batch: 20100,train loss is: 0.0002989431932480354\n",
      "test loss is 0.0003086570135589518\n",
      "Batch: 20200,train loss is: 0.0003165397067696182\n",
      "test loss is 0.0002765482555394323\n",
      "Batch: 20300,train loss is: 0.0005374397962139608\n",
      "test loss is 0.00026730230676517513\n",
      "Batch: 20400,train loss is: 0.00013507037183110206\n",
      "test loss is 0.0002876191557055115\n",
      "Batch: 20500,train loss is: 0.00018192485420032278\n",
      "test loss is 0.0002763418880242278\n",
      "Batch: 20600,train loss is: 0.0002807776054606868\n",
      "test loss is 0.0002664699292493491\n",
      "Batch: 20700,train loss is: 0.00030991999893977885\n",
      "test loss is 0.0002589358658944521\n",
      "Batch: 20800,train loss is: 0.00021511640955673773\n",
      "test loss is 0.00028543831431449793\n",
      "Batch: 20900,train loss is: 0.00025791048984692043\n",
      "test loss is 0.00025652681514719224\n",
      "Batch: 21000,train loss is: 0.00021322022018285607\n",
      "test loss is 0.0002697686482029785\n",
      "Batch: 21100,train loss is: 0.00024284970298605593\n",
      "test loss is 0.00026408157289085487\n",
      "Batch: 21200,train loss is: 0.0005425039313272293\n",
      "test loss is 0.00025459661239434525\n",
      "Batch: 21300,train loss is: 0.0003589861642571279\n",
      "test loss is 0.0003802378188846359\n",
      "Batch: 21400,train loss is: 0.00025349089195820254\n",
      "test loss is 0.00026858324502146473\n",
      "Batch: 21500,train loss is: 0.0002594102199543429\n",
      "test loss is 0.000298565768246627\n",
      "Batch: 21600,train loss is: 0.00026537288219168336\n",
      "test loss is 0.0002841249583225793\n",
      "Batch: 21700,train loss is: 0.00024595553891296353\n",
      "test loss is 0.00024446388367323343\n",
      "Batch: 21800,train loss is: 0.0002202575611721688\n",
      "test loss is 0.00025581576226074265\n",
      "Batch: 21900,train loss is: 0.00020802813343711723\n",
      "test loss is 0.0002472366951697737\n",
      "Batch: 22000,train loss is: 0.0003218342977813042\n",
      "test loss is 0.0003002283230085153\n",
      "Batch: 22100,train loss is: 0.00025402834373990823\n",
      "test loss is 0.0002491445403135462\n",
      "Batch: 22200,train loss is: 0.00033713474576830833\n",
      "test loss is 0.00026003345225808716\n",
      "Batch: 22300,train loss is: 0.000350952763637956\n",
      "test loss is 0.00024692018235902087\n",
      "Batch: 22400,train loss is: 0.00023531235137042866\n",
      "test loss is 0.00028994968567362596\n",
      "Batch: 22500,train loss is: 0.0003419103103130428\n",
      "test loss is 0.0003213954403267795\n",
      "Batch: 22600,train loss is: 0.00023724494108566112\n",
      "test loss is 0.00025356094431512485\n",
      "Batch: 22700,train loss is: 0.0009573313460609818\n",
      "test loss is 0.0002788828436773732\n",
      "Batch: 22800,train loss is: 0.000256864535483203\n",
      "test loss is 0.0002743124881670274\n",
      "Batch: 22900,train loss is: 0.0002583533632359176\n",
      "test loss is 0.0003040287497349127\n",
      "Batch: 23000,train loss is: 0.00029991986179370275\n",
      "test loss is 0.0002497037499711984\n",
      "Batch: 23100,train loss is: 0.0002464939344981278\n",
      "test loss is 0.0002757326149478173\n",
      "Batch: 23200,train loss is: 0.0002498231554549484\n",
      "test loss is 0.0002599622024369151\n",
      "Batch: 23300,train loss is: 0.0005231928580628784\n",
      "test loss is 0.0002626162467022392\n",
      "Batch: 23400,train loss is: 0.00030548426843551586\n",
      "test loss is 0.0002754868346345436\n",
      "Batch: 23500,train loss is: 0.00024007450733507673\n",
      "test loss is 0.0002749373680218261\n",
      "Batch: 23600,train loss is: 0.0002961410351612851\n",
      "test loss is 0.00036692951093203805\n",
      "Batch: 23700,train loss is: 0.0002373322452306732\n",
      "test loss is 0.0002607967591264609\n",
      "Batch: 23800,train loss is: 0.00017577825561806108\n",
      "test loss is 0.00025845334972622456\n",
      "Batch: 23900,train loss is: 0.00017347515911211945\n",
      "test loss is 0.0002522171769881783\n",
      "Batch: 24000,train loss is: 0.0003383709385451189\n",
      "test loss is 0.00026165127837447663\n",
      "Batch: 24100,train loss is: 0.00022354334360860103\n",
      "test loss is 0.000282238953905283\n",
      "Batch: 24200,train loss is: 0.00018016523659733968\n",
      "test loss is 0.0002723348536790014\n",
      "Batch: 24300,train loss is: 0.0003741604839874388\n",
      "test loss is 0.00028666652890336775\n",
      "Batch: 24400,train loss is: 0.000245525920703189\n",
      "test loss is 0.0002607084835319467\n",
      "Batch: 24500,train loss is: 0.00014125896121947287\n",
      "test loss is 0.0002544931966317324\n",
      "Batch: 24600,train loss is: 0.000423448973577964\n",
      "test loss is 0.00024099487233477868\n",
      "Batch: 24700,train loss is: 0.00024668657883405694\n",
      "test loss is 0.0002671231517852516\n",
      "Batch: 24800,train loss is: 0.00035303467985329997\n",
      "test loss is 0.0003130491223355945\n",
      "Batch: 24900,train loss is: 0.0002775613684510814\n",
      "test loss is 0.00034114956486393645\n",
      "Batch: 25000,train loss is: 0.0002645036103502206\n",
      "test loss is 0.00028826321295811623\n",
      "Batch: 25100,train loss is: 0.00021289830709420358\n",
      "test loss is 0.00024868395571219973\n",
      "Batch: 25200,train loss is: 0.0001998260078654234\n",
      "test loss is 0.000294286020775219\n",
      "Batch: 25300,train loss is: 0.00018252759931352224\n",
      "test loss is 0.0002953963665509641\n",
      "Batch: 25400,train loss is: 0.0002631681401693299\n",
      "test loss is 0.00030398687929221544\n",
      "Batch: 25500,train loss is: 0.00027443334904311866\n",
      "test loss is 0.00025875140621214795\n",
      "Batch: 25600,train loss is: 0.00025473169655804986\n",
      "test loss is 0.0002473242080280063\n",
      "Batch: 25700,train loss is: 0.0009508838002932516\n",
      "test loss is 0.0002717630297264693\n",
      "Batch: 25800,train loss is: 0.00016593978267980317\n",
      "test loss is 0.00027814176226735463\n",
      "Batch: 25900,train loss is: 0.00024798683674261\n",
      "test loss is 0.0002778309697241811\n",
      "Batch: 26000,train loss is: 0.00026244390361803934\n",
      "test loss is 0.00024312335171614323\n",
      "Batch: 26100,train loss is: 0.00019686157018892472\n",
      "test loss is 0.0002840153606062967\n",
      "Batch: 26200,train loss is: 0.0002840655303688589\n",
      "test loss is 0.0002558956846177757\n",
      "Batch: 26300,train loss is: 0.0003130323317179923\n",
      "test loss is 0.0003128987746439008\n",
      "Batch: 26400,train loss is: 0.0002882947895912629\n",
      "test loss is 0.00025289946757562525\n",
      "Batch: 26500,train loss is: 0.00020017898955687255\n",
      "test loss is 0.00024314351990945457\n",
      "Batch: 26600,train loss is: 0.00020507096217931044\n",
      "test loss is 0.00038784400639898354\n",
      "Batch: 26700,train loss is: 0.0002211650039644942\n",
      "test loss is 0.00026148207745226574\n",
      "Batch: 26800,train loss is: 0.00017176630114909617\n",
      "test loss is 0.000298377114765369\n",
      "Batch: 26900,train loss is: 0.0002978663203070244\n",
      "test loss is 0.0003089458126518774\n",
      "Batch: 27000,train loss is: 0.00018845982928757462\n",
      "test loss is 0.00028875563726356545\n",
      "Batch: 27100,train loss is: 0.0001904854165805944\n",
      "test loss is 0.0002924302298973103\n",
      "Batch: 27200,train loss is: 0.00019114369283422774\n",
      "test loss is 0.0002691251423996087\n",
      "Batch: 27300,train loss is: 0.00018115385213312348\n",
      "test loss is 0.0002642382792338792\n",
      "Batch: 27400,train loss is: 0.0002854237355177088\n",
      "test loss is 0.00024926632007480086\n",
      "Batch: 27500,train loss is: 0.00022795412352622418\n",
      "test loss is 0.00027007881459611207\n",
      "Batch: 27600,train loss is: 0.00027740936040538723\n",
      "test loss is 0.0003239713477332994\n",
      "Batch: 27700,train loss is: 0.0002699010517493404\n",
      "test loss is 0.0003124364344372569\n",
      "Batch: 27800,train loss is: 0.00022942958900629801\n",
      "test loss is 0.0002882262802786721\n",
      "Batch: 27900,train loss is: 0.00021192786306865001\n",
      "test loss is 0.00027842377185056683\n",
      "Batch: 28000,train loss is: 0.00016582705297373617\n",
      "test loss is 0.00024252648006333169\n",
      "Batch: 28100,train loss is: 0.00046065685667948195\n",
      "test loss is 0.0002758627428385784\n",
      "Batch: 28200,train loss is: 0.00027412982454563096\n",
      "test loss is 0.00030566940846885765\n",
      "Batch: 28300,train loss is: 0.0003143344800357958\n",
      "test loss is 0.00025310209537261416\n",
      "Batch: 28400,train loss is: 0.0002536601505653816\n",
      "test loss is 0.0002592021279035261\n",
      "Batch: 28500,train loss is: 0.0002525011449176372\n",
      "test loss is 0.0002566247641975069\n",
      "Batch: 28600,train loss is: 0.0003570272260495216\n",
      "test loss is 0.00027671101822091546\n",
      "Batch: 28700,train loss is: 0.0002660903808236273\n",
      "test loss is 0.0002854423270447188\n",
      "Batch: 28800,train loss is: 0.0002842979638550627\n",
      "test loss is 0.00027317993722486306\n",
      "Batch: 28900,train loss is: 0.00027202158094011763\n",
      "test loss is 0.0002647432241768979\n",
      "Batch: 29000,train loss is: 0.00028601052382935056\n",
      "test loss is 0.00031569749302496356\n",
      "Batch: 29100,train loss is: 0.0002624812499237307\n",
      "test loss is 0.0002676534518750033\n",
      "Batch: 29200,train loss is: 0.00019940822298701664\n",
      "test loss is 0.0002463586454747719\n",
      "Batch: 29300,train loss is: 0.00017465513914973285\n",
      "test loss is 0.00026257024985444443\n",
      "Batch: 29400,train loss is: 0.0002072818929393549\n",
      "test loss is 0.0002783146107449698\n",
      "Batch: 29500,train loss is: 0.00021299800730103\n",
      "test loss is 0.00027494369068591855\n",
      "Batch: 29600,train loss is: 0.0002474637489518046\n",
      "test loss is 0.00026083462733367203\n",
      "Batch: 29700,train loss is: 0.00028154068455629765\n",
      "test loss is 0.0002698053833300335\n",
      "Batch: 29800,train loss is: 0.00026605498051302277\n",
      "test loss is 0.00026981231649134965\n",
      "Batch: 29900,train loss is: 0.00017860261462231236\n",
      "test loss is 0.0003045536170150429\n",
      "Batch: 30000,train loss is: 0.0002022190910156259\n",
      "test loss is 0.00028061594576718593\n",
      "Batch: 30100,train loss is: 0.0002129847910847526\n",
      "test loss is 0.00024915400615701573\n",
      "Batch: 30200,train loss is: 0.00020093457733672454\n",
      "test loss is 0.00025505899028847157\n",
      "Batch: 30300,train loss is: 0.0002355301385663549\n",
      "test loss is 0.00024883379516056945\n",
      "Batch: 30400,train loss is: 0.0002753973460189853\n",
      "test loss is 0.00027260548698493364\n",
      "Batch: 30500,train loss is: 0.00028064452720522645\n",
      "test loss is 0.00025210829270072663\n",
      "Batch: 30600,train loss is: 0.00036977009733557363\n",
      "test loss is 0.0002605954698583472\n",
      "Batch: 30700,train loss is: 0.0003252416091354106\n",
      "test loss is 0.00026683331107871824\n",
      "Batch: 30800,train loss is: 0.0003319341078430033\n",
      "test loss is 0.00033852746324700877\n",
      "Batch: 30900,train loss is: 0.00025555346253862833\n",
      "test loss is 0.0002921985822858124\n",
      "Batch: 31000,train loss is: 0.00022942269756820493\n",
      "test loss is 0.0002478533120109414\n",
      "Batch: 31100,train loss is: 0.00018825405321936815\n",
      "test loss is 0.0002571898194326447\n",
      "Batch: 31200,train loss is: 0.0003320278939437665\n",
      "test loss is 0.0002839961802095574\n",
      "Batch: 31300,train loss is: 0.0001578229990917459\n",
      "test loss is 0.000286180400235749\n",
      "Batch: 31400,train loss is: 0.00022225563119923598\n",
      "test loss is 0.00027900689763879234\n",
      "Batch: 31500,train loss is: 0.00029855304121965845\n",
      "test loss is 0.0002497743035563869\n",
      "Batch: 31600,train loss is: 0.0002533323793681131\n",
      "test loss is 0.0003013245215166334\n",
      "Batch: 31700,train loss is: 0.00025205506364316187\n",
      "test loss is 0.0002828158700340307\n",
      "Batch: 31800,train loss is: 0.00020651385347585143\n",
      "test loss is 0.0003205187437768197\n",
      "Batch: 31900,train loss is: 0.00022429280328646088\n",
      "test loss is 0.0002734392681178401\n",
      "Batch: 32000,train loss is: 0.00020015061069712504\n",
      "test loss is 0.00029539731687532305\n",
      "Batch: 32100,train loss is: 0.00022443323931360277\n",
      "test loss is 0.00029760568195241847\n",
      "Batch: 32200,train loss is: 0.0002999802494845386\n",
      "test loss is 0.00031120006649060114\n",
      "Batch: 32300,train loss is: 0.0002182865778854694\n",
      "test loss is 0.0003118145455456952\n",
      "Batch: 32400,train loss is: 0.0003029324827872844\n",
      "test loss is 0.00034418592633025843\n",
      "Batch: 32500,train loss is: 0.00017790950888151212\n",
      "test loss is 0.00029510474686345495\n",
      "Batch: 32600,train loss is: 0.00019418472351665771\n",
      "test loss is 0.0003047751669378511\n",
      "Batch: 32700,train loss is: 0.0002805240207656793\n",
      "test loss is 0.0003010078061180891\n",
      "Batch: 32800,train loss is: 0.0002845521701653151\n",
      "test loss is 0.0002907395453861844\n",
      "Batch: 32900,train loss is: 0.000956937389500023\n",
      "test loss is 0.0003095406888282752\n",
      "Batch: 33000,train loss is: 0.0001472712305453453\n",
      "test loss is 0.0002604301743903572\n",
      "Batch: 33100,train loss is: 0.0002450893913412648\n",
      "test loss is 0.00025802873263228973\n",
      "Batch: 33200,train loss is: 0.0002128521144516312\n",
      "test loss is 0.00027532625156235813\n",
      "Batch: 33300,train loss is: 0.00019511758694404601\n",
      "test loss is 0.0002651035783426994\n",
      "Batch: 33400,train loss is: 0.000256515017657034\n",
      "test loss is 0.00024479125648470837\n",
      "Batch: 33500,train loss is: 0.00022653548620168382\n",
      "test loss is 0.0002603006168341012\n",
      "Batch: 33600,train loss is: 0.000157524750366357\n",
      "test loss is 0.0002770031001536442\n",
      "Batch: 33700,train loss is: 0.00021072510163447835\n",
      "test loss is 0.0002637154268908897\n",
      "Batch: 33800,train loss is: 0.00033611187623940975\n",
      "test loss is 0.0003362062673119804\n",
      "Batch: 33900,train loss is: 0.00021651274119444667\n",
      "test loss is 0.0002897053727630706\n",
      "Batch: 34000,train loss is: 0.0002029819783096827\n",
      "test loss is 0.0002364428619729216\n",
      "Batch: 34100,train loss is: 0.0002898787201393823\n",
      "test loss is 0.0002803554204934045\n",
      "Batch: 34200,train loss is: 0.00026305215083131093\n",
      "test loss is 0.00029556329864382857\n",
      "Batch: 34300,train loss is: 0.00014448496951282333\n",
      "test loss is 0.0002486431534915157\n",
      "Batch: 34400,train loss is: 0.00024150462059319432\n",
      "test loss is 0.0002630222111862646\n",
      "Batch: 34500,train loss is: 0.00031040170858967063\n",
      "test loss is 0.00030489799895015967\n",
      "Batch: 34600,train loss is: 0.0002606147662895404\n",
      "test loss is 0.00026225968542751367\n",
      "Batch: 34700,train loss is: 0.000195865922285332\n",
      "test loss is 0.0002524232759597139\n",
      "Batch: 34800,train loss is: 0.0004640350172811096\n",
      "test loss is 0.00033093965129471573\n",
      "Batch: 34900,train loss is: 0.00024280204209257465\n",
      "test loss is 0.00025306982725067126\n",
      "Batch: 35000,train loss is: 0.0003767261213246422\n",
      "test loss is 0.0003709796489555524\n",
      "Batch: 35100,train loss is: 0.0002716093041363284\n",
      "test loss is 0.00027989203498030165\n",
      "Batch: 35200,train loss is: 0.0002952226734123135\n",
      "test loss is 0.00027060168112927525\n",
      "Batch: 35300,train loss is: 0.000202978343731657\n",
      "test loss is 0.00024663724517850956\n",
      "Batch: 35400,train loss is: 0.00021965317883566293\n",
      "test loss is 0.0002559573719897175\n",
      "Batch: 35500,train loss is: 0.0002473649125493642\n",
      "test loss is 0.00024410200787179176\n",
      "Batch: 35600,train loss is: 0.0005815735165241671\n",
      "test loss is 0.00030260105249123385\n",
      "Batch: 35700,train loss is: 0.00022515853181611696\n",
      "test loss is 0.00028054824127843114\n",
      "Batch: 35800,train loss is: 0.0002154061276093003\n",
      "test loss is 0.00028386186931801476\n",
      "Batch: 35900,train loss is: 0.00018439583572525952\n",
      "test loss is 0.0002822395738508531\n",
      "Batch: 36000,train loss is: 0.0002111666538497823\n",
      "test loss is 0.0002675458645636367\n",
      "Batch: 36100,train loss is: 0.00030899122616823\n",
      "test loss is 0.0002532093953897762\n",
      "Batch: 36200,train loss is: 0.00027538322159814465\n",
      "test loss is 0.00025569014772981274\n",
      "Batch: 36300,train loss is: 0.00015632180698293383\n",
      "test loss is 0.0002555547032016682\n",
      "Batch: 36400,train loss is: 0.0002346061944914234\n",
      "test loss is 0.00025459151766689614\n",
      "Batch: 36500,train loss is: 0.00012871047273025385\n",
      "test loss is 0.0002976587461708536\n",
      "Batch: 36600,train loss is: 0.0002339857555323589\n",
      "test loss is 0.0002502396259024464\n",
      "Batch: 36700,train loss is: 0.00024211629522930396\n",
      "test loss is 0.000270392154551485\n",
      "Batch: 36800,train loss is: 0.0002966918256459678\n",
      "test loss is 0.0002846522262921847\n",
      "Batch: 36900,train loss is: 0.00033638975215767835\n",
      "test loss is 0.00031837242190862575\n",
      "Batch: 37000,train loss is: 0.00022236014265998566\n",
      "test loss is 0.00027209671932518844\n",
      "Batch: 37100,train loss is: 0.0001477394591699298\n",
      "test loss is 0.0003446296013039348\n",
      "Batch: 37200,train loss is: 0.000328736351187554\n",
      "test loss is 0.0002499648696776779\n",
      "Batch: 37300,train loss is: 0.00026536697912200435\n",
      "test loss is 0.00030228179931928177\n",
      "Batch: 37400,train loss is: 0.00031715512664648156\n",
      "test loss is 0.00030569177379067034\n",
      "Batch: 37500,train loss is: 0.00022168184126145922\n",
      "test loss is 0.0002798084797673145\n",
      "Batch: 37600,train loss is: 0.0003204278280161368\n",
      "test loss is 0.0002552672824874187\n",
      "Batch: 37700,train loss is: 0.00024196185374618075\n",
      "test loss is 0.00024706829609006395\n",
      "Batch: 37800,train loss is: 0.000294179074748563\n",
      "test loss is 0.0002666203127729604\n",
      "Batch: 37900,train loss is: 0.00024955158719574676\n",
      "test loss is 0.0002492291129967957\n",
      "Batch: 38000,train loss is: 0.00026986319669823817\n",
      "test loss is 0.0002835168301776278\n",
      "Batch: 38100,train loss is: 0.00020025376532727772\n",
      "test loss is 0.0003008728896669531\n",
      "Batch: 38200,train loss is: 0.00019067378599457388\n",
      "test loss is 0.0002727682387457954\n",
      "Batch: 38300,train loss is: 0.0003516420325250455\n",
      "test loss is 0.0002416309426673635\n",
      "Batch: 38400,train loss is: 0.0002822046191025992\n",
      "test loss is 0.0002714944263188293\n",
      "Batch: 38500,train loss is: 0.0002656275800701661\n",
      "test loss is 0.00026593627070547907\n",
      "Batch: 38600,train loss is: 0.00014887253298412497\n",
      "test loss is 0.0002342245242964985\n",
      "Batch: 38700,train loss is: 0.0002536571780325642\n",
      "test loss is 0.0003105812217321083\n",
      "Batch: 38800,train loss is: 0.00019557095341168033\n",
      "test loss is 0.00028080314485362984\n",
      "Batch: 38900,train loss is: 0.00014647921277934246\n",
      "test loss is 0.00026182959651084033\n",
      "Batch: 39000,train loss is: 0.00031458284452357736\n",
      "test loss is 0.00024983057406458627\n",
      "Batch: 39100,train loss is: 0.00015221925831906022\n",
      "test loss is 0.0003077086292638794\n",
      "Batch: 39200,train loss is: 0.00035818406147768926\n",
      "test loss is 0.0003490240694019349\n",
      "Batch: 39300,train loss is: 0.0002570000032646629\n",
      "test loss is 0.0002472206418977942\n",
      "Batch: 39400,train loss is: 0.00039237144239487776\n",
      "test loss is 0.00028989160313328417\n",
      "Batch: 39500,train loss is: 0.00035198102007873524\n",
      "test loss is 0.00025405092263244974\n",
      "Batch: 39600,train loss is: 0.0003915397469580259\n",
      "test loss is 0.00037906297765336066\n",
      "Batch: 39700,train loss is: 0.0002963427002261478\n",
      "test loss is 0.00024603083852921633\n",
      "Batch: 39800,train loss is: 0.00023071058929146439\n",
      "test loss is 0.000280896255242293\n",
      "Batch: 39900,train loss is: 0.00033171645291197915\n",
      "test loss is 0.0003903652620625254\n",
      "Batch: 40000,train loss is: 0.0002745752141420349\n",
      "test loss is 0.0003428859819524828\n",
      "Batch: 40100,train loss is: 0.00022780117999075643\n",
      "test loss is 0.000338649766740797\n",
      "Batch: 40200,train loss is: 0.00015694538750524772\n",
      "test loss is 0.0002636701000622409\n",
      "Batch: 40300,train loss is: 0.0001558870263179973\n",
      "test loss is 0.00023863721130062225\n",
      "Batch: 40400,train loss is: 0.0002659565755097457\n",
      "test loss is 0.00027126549272342714\n",
      "Batch: 40500,train loss is: 0.00023628402193525322\n",
      "test loss is 0.00025056248650390586\n",
      "Batch: 40600,train loss is: 0.0002832829344333648\n",
      "test loss is 0.0002991664692393129\n",
      "Batch: 40700,train loss is: 0.00014033409799692575\n",
      "test loss is 0.00027172410245598775\n",
      "Batch: 40800,train loss is: 0.0001895857903757092\n",
      "test loss is 0.00026376672932328614\n",
      "Batch: 40900,train loss is: 0.0004000531328665698\n",
      "test loss is 0.0003049531115402957\n",
      "Batch: 41000,train loss is: 0.0002525168961225394\n",
      "test loss is 0.00026980187984533424\n",
      "Batch: 41100,train loss is: 0.00019199336692042046\n",
      "test loss is 0.00026441532629785716\n",
      "Batch: 41200,train loss is: 0.00025934380978233763\n",
      "test loss is 0.000291572422435125\n",
      "Batch: 41300,train loss is: 0.00030391154456013984\n",
      "test loss is 0.00033323301560127434\n",
      "Batch: 41400,train loss is: 0.0003168123510234986\n",
      "test loss is 0.00028644472410216566\n",
      "Batch: 41500,train loss is: 0.0002757376134042263\n",
      "test loss is 0.00027915921344283016\n",
      "Batch: 41600,train loss is: 0.000230463504160627\n",
      "test loss is 0.00034189187961396585\n",
      "Batch: 41700,train loss is: 0.00027499859118779505\n",
      "test loss is 0.00027614193596221353\n",
      "Batch: 41800,train loss is: 0.00023250264668582226\n",
      "test loss is 0.00033535305011959244\n",
      "Batch: 41900,train loss is: 0.0002522428775986548\n",
      "test loss is 0.0002645939646837259\n",
      "Batch: 42000,train loss is: 0.00023490797296453527\n",
      "test loss is 0.00028673406690165373\n",
      "Batch: 42100,train loss is: 0.000265976972294094\n",
      "test loss is 0.0003117362812099713\n",
      "Batch: 42200,train loss is: 0.00034028591064794525\n",
      "test loss is 0.00024759848283046384\n",
      "Batch: 42300,train loss is: 0.00032145079307281985\n",
      "test loss is 0.000282465382487157\n",
      "Batch: 42400,train loss is: 0.00024102765816973206\n",
      "test loss is 0.00032161989477237895\n",
      "Batch: 42500,train loss is: 0.0002424518607562494\n",
      "test loss is 0.0002795589120361741\n",
      "Batch: 42600,train loss is: 0.00028784255013604865\n",
      "test loss is 0.0002866961828135862\n",
      "Batch: 42700,train loss is: 0.000279619042978062\n",
      "test loss is 0.0002470547820408022\n",
      "Batch: 42800,train loss is: 0.00022159262269736194\n",
      "test loss is 0.0002652589940253188\n",
      "Batch: 42900,train loss is: 0.0002326865674418891\n",
      "test loss is 0.0003093387468705224\n",
      "Batch: 43000,train loss is: 0.00015864187058778784\n",
      "test loss is 0.0002479783013311323\n",
      "Batch: 43100,train loss is: 0.00017510001547171237\n",
      "test loss is 0.0002536330643907935\n",
      "Batch: 43200,train loss is: 0.0001817653670086774\n",
      "test loss is 0.0002398005038605814\n",
      "Batch: 43300,train loss is: 0.0005015775141635347\n",
      "test loss is 0.00028902397782838105\n",
      "Batch: 43400,train loss is: 0.00020109788261752753\n",
      "test loss is 0.0002682521801488688\n",
      "Batch: 43500,train loss is: 0.0002598865189453572\n",
      "test loss is 0.00027801121873854485\n",
      "Batch: 43600,train loss is: 0.00016495965473700675\n",
      "test loss is 0.00024550202406753397\n",
      "Batch: 43700,train loss is: 0.0002919193862511671\n",
      "test loss is 0.0002880932532132847\n",
      "Batch: 43800,train loss is: 0.0003441248859922981\n",
      "test loss is 0.00027004828639444055\n",
      "Batch: 43900,train loss is: 0.00017160035355732507\n",
      "test loss is 0.0002784460833022784\n",
      "Batch: 44000,train loss is: 0.00032870280930973094\n",
      "test loss is 0.0004049834506615412\n",
      "Batch: 44100,train loss is: 0.00033349813660872823\n",
      "test loss is 0.00024801914421108837\n",
      "Batch: 44200,train loss is: 0.00036967364727346106\n",
      "test loss is 0.0003190495583894644\n",
      "Batch: 44300,train loss is: 0.0002172562294312152\n",
      "test loss is 0.0002439445026242978\n",
      "Batch: 44400,train loss is: 0.00019270700923380314\n",
      "test loss is 0.00024880352665149414\n",
      "Batch: 44500,train loss is: 0.00029613234801111586\n",
      "test loss is 0.00025228907085628335\n",
      "Batch: 44600,train loss is: 0.00030940478390344806\n",
      "test loss is 0.00026419551640127195\n",
      "Batch: 44700,train loss is: 0.00020274601619805867\n",
      "test loss is 0.00023737684676434096\n",
      "Batch: 44800,train loss is: 0.0002656383079500804\n",
      "test loss is 0.00023736142213112604\n",
      "Batch: 44900,train loss is: 0.00037002523272554474\n",
      "test loss is 0.00033318355921514427\n",
      "Batch: 45000,train loss is: 0.00020800652686266973\n",
      "test loss is 0.0002635847035120659\n",
      "Batch: 45100,train loss is: 0.00018440687330216124\n",
      "test loss is 0.00031453990408574205\n",
      "Batch: 45200,train loss is: 0.00026716385662645754\n",
      "test loss is 0.00027781390492240173\n",
      "Batch: 45300,train loss is: 0.00029598016813424106\n",
      "test loss is 0.00027856745471689117\n",
      "Batch: 45400,train loss is: 0.00025907585647007444\n",
      "test loss is 0.00024741286724312765\n",
      "Batch: 45500,train loss is: 0.00022926150287984727\n",
      "test loss is 0.00024996868201732395\n",
      "Batch: 45600,train loss is: 0.00038857877581216526\n",
      "test loss is 0.00025685802803123803\n",
      "Batch: 45700,train loss is: 0.00030300072421676966\n",
      "test loss is 0.00030921320693119334\n",
      "Batch: 45800,train loss is: 0.0005158341342970837\n",
      "test loss is 0.00031171208912638673\n",
      "Batch: 45900,train loss is: 0.00018712062894278797\n",
      "test loss is 0.0002711827265935886\n",
      "Batch: 46000,train loss is: 0.001297758214280736\n",
      "test loss is 0.0003025877877512312\n",
      "Batch: 46100,train loss is: 0.00024872908390443423\n",
      "test loss is 0.00029607011771386166\n",
      "Batch: 46200,train loss is: 0.00021285468278956898\n",
      "test loss is 0.000249805845447277\n",
      "Batch: 46300,train loss is: 0.0002474042265526056\n",
      "test loss is 0.0002916916806339475\n",
      "Batch: 46400,train loss is: 0.00030109909926823977\n",
      "test loss is 0.0002834648155353759\n",
      "Batch: 46500,train loss is: 0.0002236464921037323\n",
      "test loss is 0.0002805546661471572\n",
      "Batch: 46600,train loss is: 0.00015565153961748988\n",
      "test loss is 0.00026988575333816814\n",
      "Batch: 46700,train loss is: 0.0003073862785665345\n",
      "test loss is 0.0002633129410091169\n",
      "-----------------------Epoch: 9----------------------------------\n",
      "Batch: 0,train loss is: 0.00028921975308484585\n",
      "test loss is 0.0002737001993831768\n",
      "Batch: 100,train loss is: 0.0002139655641205714\n",
      "test loss is 0.0003091844700562369\n",
      "Batch: 200,train loss is: 0.0003379059400103173\n",
      "test loss is 0.0003127989232632551\n",
      "Batch: 300,train loss is: 0.0002052153721545987\n",
      "test loss is 0.00026481207183705753\n",
      "Batch: 400,train loss is: 0.00030663104157980093\n",
      "test loss is 0.0002634938172907078\n",
      "Batch: 500,train loss is: 0.00026165913988050006\n",
      "test loss is 0.0003014299437668413\n",
      "Batch: 600,train loss is: 0.00018579425720163513\n",
      "test loss is 0.00023876496302143913\n",
      "Batch: 700,train loss is: 0.00017721824383842125\n",
      "test loss is 0.00026281899377382696\n",
      "Batch: 800,train loss is: 0.00048341359368752154\n",
      "test loss is 0.00031892206895094236\n",
      "Batch: 900,train loss is: 0.0007247854062393355\n",
      "test loss is 0.00025759850827997035\n",
      "Batch: 1000,train loss is: 0.00021879604932035048\n",
      "test loss is 0.0002777921105353292\n",
      "Batch: 1100,train loss is: 0.00020100183559312577\n",
      "test loss is 0.00026853049764381077\n",
      "Batch: 1200,train loss is: 0.00016772494149637387\n",
      "test loss is 0.00025386311390690854\n",
      "Batch: 1300,train loss is: 0.0004676360715240843\n",
      "test loss is 0.00029221248686915575\n",
      "Batch: 1400,train loss is: 0.0002643920292653984\n",
      "test loss is 0.00026304037295120765\n",
      "Batch: 1500,train loss is: 0.00012665227318497594\n",
      "test loss is 0.0002443276924975891\n",
      "Batch: 1600,train loss is: 0.00027189285042025443\n",
      "test loss is 0.00026628530586145227\n",
      "Batch: 1700,train loss is: 0.000173425920590226\n",
      "test loss is 0.00025804917601244447\n",
      "Batch: 1800,train loss is: 0.0004024816249229139\n",
      "test loss is 0.0002946338161881375\n",
      "Batch: 1900,train loss is: 0.0002936072038548939\n",
      "test loss is 0.0002836562805746454\n",
      "Batch: 2000,train loss is: 0.0002994623868540036\n",
      "test loss is 0.00029203571567178513\n",
      "Batch: 2100,train loss is: 0.00036544215259584915\n",
      "test loss is 0.0003569048560798978\n",
      "Batch: 2200,train loss is: 0.0002572835154391783\n",
      "test loss is 0.00025559530483548943\n",
      "Batch: 2300,train loss is: 0.0002206681667658458\n",
      "test loss is 0.0002611560620620781\n",
      "Batch: 2400,train loss is: 0.00018332654594497557\n",
      "test loss is 0.00025037317769489853\n",
      "Batch: 2500,train loss is: 0.0004187824424476029\n",
      "test loss is 0.00032213391167753316\n",
      "Batch: 2600,train loss is: 0.00013104906861465676\n",
      "test loss is 0.0002554523497493224\n",
      "Batch: 2700,train loss is: 0.0003272960901262793\n",
      "test loss is 0.00027446825801634737\n",
      "Batch: 2800,train loss is: 0.00030452191875268513\n",
      "test loss is 0.00025426778296060187\n",
      "Batch: 2900,train loss is: 0.0003052971437798659\n",
      "test loss is 0.00025595791196518107\n",
      "Batch: 3000,train loss is: 0.0003334381288703807\n",
      "test loss is 0.0003122152887941823\n",
      "Batch: 3100,train loss is: 0.0005855886549932027\n",
      "test loss is 0.0002949621515905185\n",
      "Batch: 3200,train loss is: 0.00024607344799172306\n",
      "test loss is 0.0002446759050836885\n",
      "Batch: 3300,train loss is: 0.0003764028806377176\n",
      "test loss is 0.00025866519961771536\n",
      "Batch: 3400,train loss is: 0.00017322328502192231\n",
      "test loss is 0.00026819131406961356\n",
      "Batch: 3500,train loss is: 0.0004892709622416366\n",
      "test loss is 0.0002651751433440285\n",
      "Batch: 3600,train loss is: 0.0002878912325581174\n",
      "test loss is 0.0002552250930515052\n",
      "Batch: 3700,train loss is: 0.00037089252333250705\n",
      "test loss is 0.00025931497910598164\n",
      "Batch: 3800,train loss is: 0.00021563253215478055\n",
      "test loss is 0.00025483833354327467\n",
      "Batch: 3900,train loss is: 0.00023612086311439366\n",
      "test loss is 0.0002599122278208989\n",
      "Batch: 4000,train loss is: 0.00031826035719732945\n",
      "test loss is 0.0002475438181019472\n",
      "Batch: 4100,train loss is: 0.00020769364819547516\n",
      "test loss is 0.00027630220412049684\n",
      "Batch: 4200,train loss is: 0.0003043257340675621\n",
      "test loss is 0.0002735804088490866\n",
      "Batch: 4300,train loss is: 0.00023301971035145995\n",
      "test loss is 0.0004167043686293047\n",
      "Batch: 4400,train loss is: 0.00024724343238386137\n",
      "test loss is 0.0002410640011340502\n",
      "Batch: 4500,train loss is: 0.00017369014485838944\n",
      "test loss is 0.00035913024190859687\n",
      "Batch: 4600,train loss is: 0.00034990329393377135\n",
      "test loss is 0.0002493208509675153\n",
      "Batch: 4700,train loss is: 0.0003140436720562036\n",
      "test loss is 0.0002586263656009442\n",
      "Batch: 4800,train loss is: 0.0002133847283353208\n",
      "test loss is 0.0002664879614611346\n",
      "Batch: 4900,train loss is: 0.0002359717279251418\n",
      "test loss is 0.00030441456688591893\n",
      "Batch: 5000,train loss is: 0.00035132657583932127\n",
      "test loss is 0.00026460884908283486\n",
      "Batch: 5100,train loss is: 0.00012653337746075717\n",
      "test loss is 0.0002358138521734335\n",
      "Batch: 5200,train loss is: 0.0004080127431160465\n",
      "test loss is 0.0003006940477931205\n",
      "Batch: 5300,train loss is: 0.0002133550316673445\n",
      "test loss is 0.0003098981093565557\n",
      "Batch: 5400,train loss is: 0.0003034792791290631\n",
      "test loss is 0.00027491999127802824\n",
      "Batch: 5500,train loss is: 0.000253810056882184\n",
      "test loss is 0.00024356240862399758\n",
      "Batch: 5600,train loss is: 0.00012049647318024692\n",
      "test loss is 0.00024147205448323602\n",
      "Batch: 5700,train loss is: 0.00042921153869631486\n",
      "test loss is 0.00027656488200050256\n",
      "Batch: 5800,train loss is: 0.0003397135579711495\n",
      "test loss is 0.0002765650893626206\n",
      "Batch: 5900,train loss is: 0.0002936634491388285\n",
      "test loss is 0.00027451755423962815\n",
      "Batch: 6000,train loss is: 0.00021954605697649957\n",
      "test loss is 0.00026780778439212076\n",
      "Batch: 6100,train loss is: 0.00024767227192857503\n",
      "test loss is 0.000340327860868935\n",
      "Batch: 6200,train loss is: 0.000327306002424988\n",
      "test loss is 0.00024198496836815923\n",
      "Batch: 6300,train loss is: 0.00019927818048524608\n",
      "test loss is 0.00027536174213223285\n",
      "Batch: 6400,train loss is: 0.00023410865493263242\n",
      "test loss is 0.0002542705661869266\n",
      "Batch: 6500,train loss is: 0.0001471028960178352\n",
      "test loss is 0.0002626433129799961\n",
      "Batch: 6600,train loss is: 0.00023335091277933498\n",
      "test loss is 0.00026129033246048315\n",
      "Batch: 6700,train loss is: 0.00029426338974640714\n",
      "test loss is 0.00027081014950237073\n",
      "Batch: 6800,train loss is: 0.00035573513992045335\n",
      "test loss is 0.0002633780110842984\n",
      "Batch: 6900,train loss is: 0.000370274185265208\n",
      "test loss is 0.00026380923918707693\n",
      "Batch: 7000,train loss is: 0.00030143126463047574\n",
      "test loss is 0.00029191320878177126\n",
      "Batch: 7100,train loss is: 0.0003678857513844521\n",
      "test loss is 0.00034645071317876273\n",
      "Batch: 7200,train loss is: 0.0002803335389431963\n",
      "test loss is 0.0002477038666087169\n",
      "Batch: 7300,train loss is: 0.0003081910333550339\n",
      "test loss is 0.0002628781905082302\n",
      "Batch: 7400,train loss is: 0.0003592900996484673\n",
      "test loss is 0.0002669420545031181\n",
      "Batch: 7500,train loss is: 0.00039167594471950084\n",
      "test loss is 0.00026588936601448386\n",
      "Batch: 7600,train loss is: 0.0002261689917215976\n",
      "test loss is 0.0003330243193104936\n",
      "Batch: 7700,train loss is: 0.00019354621387342107\n",
      "test loss is 0.00028472210444166924\n",
      "Batch: 7800,train loss is: 0.0003833163461508806\n",
      "test loss is 0.00034385367952706084\n",
      "Batch: 7900,train loss is: 0.00023984766383100583\n",
      "test loss is 0.0002767373198596433\n",
      "Batch: 8000,train loss is: 0.0002429920304403014\n",
      "test loss is 0.0002455794660628424\n",
      "Batch: 8100,train loss is: 0.0002575263568243578\n",
      "test loss is 0.0002517880945464515\n",
      "Batch: 8200,train loss is: 0.00023951730916754645\n",
      "test loss is 0.00024261766634843738\n",
      "Batch: 8300,train loss is: 0.00017338167614139183\n",
      "test loss is 0.0002570098188822148\n",
      "Batch: 8400,train loss is: 0.0004817843214658173\n",
      "test loss is 0.0005101795598466016\n",
      "Batch: 8500,train loss is: 0.00021597395431111573\n",
      "test loss is 0.0002510125437109232\n",
      "Batch: 8600,train loss is: 0.00021938099796755883\n",
      "test loss is 0.0003180690229752052\n",
      "Batch: 8700,train loss is: 0.00019616763863479478\n",
      "test loss is 0.0002668460631754474\n",
      "Batch: 8800,train loss is: 0.00012488958945029157\n",
      "test loss is 0.0002380490330851887\n",
      "Batch: 8900,train loss is: 0.00019445039997695098\n",
      "test loss is 0.00029181678468729564\n",
      "Batch: 9000,train loss is: 0.00021790420289651517\n",
      "test loss is 0.00027793454339950814\n",
      "Batch: 9100,train loss is: 0.00039728107241773444\n",
      "test loss is 0.00031467326757860843\n",
      "Batch: 9200,train loss is: 0.0002499769813466403\n",
      "test loss is 0.00026202794673199544\n",
      "Batch: 9300,train loss is: 0.00021769480282585085\n",
      "test loss is 0.00025200453829368194\n",
      "Batch: 9400,train loss is: 0.00019623471399841157\n",
      "test loss is 0.00023446032688351874\n",
      "Batch: 9500,train loss is: 0.00022155836174737325\n",
      "test loss is 0.0003341505457152023\n",
      "Batch: 9600,train loss is: 0.00024295821803770008\n",
      "test loss is 0.00032925040930457006\n",
      "Batch: 9700,train loss is: 0.00026349677718621176\n",
      "test loss is 0.0002482534792812868\n",
      "Batch: 9800,train loss is: 0.00023644918464594628\n",
      "test loss is 0.0003031647595487383\n",
      "Batch: 9900,train loss is: 0.00018850268067885736\n",
      "test loss is 0.0002389564244703405\n",
      "Batch: 10000,train loss is: 0.00031393242041453614\n",
      "test loss is 0.00024960256084732976\n",
      "Batch: 10100,train loss is: 0.00019167551267724027\n",
      "test loss is 0.00025386868954829246\n",
      "Batch: 10200,train loss is: 0.0001621270663223914\n",
      "test loss is 0.000264507911915181\n",
      "Batch: 10300,train loss is: 0.000255124121885552\n",
      "test loss is 0.00028578765223507554\n",
      "Batch: 10400,train loss is: 0.00011650363486894986\n",
      "test loss is 0.0002451786335136655\n",
      "Batch: 10500,train loss is: 0.00037378253667871386\n",
      "test loss is 0.00033874231852652965\n",
      "Batch: 10600,train loss is: 0.00017435113952265265\n",
      "test loss is 0.0002547909086857564\n",
      "Batch: 10700,train loss is: 0.00019702414627940305\n",
      "test loss is 0.00027218771510057014\n",
      "Batch: 10800,train loss is: 0.000350128091114602\n",
      "test loss is 0.0002874250390788864\n",
      "Batch: 10900,train loss is: 0.00022458002857791658\n",
      "test loss is 0.0002507940173325704\n",
      "Batch: 11000,train loss is: 0.00022669500390886323\n",
      "test loss is 0.0002681109893332717\n",
      "Batch: 11100,train loss is: 0.00045438887873158117\n",
      "test loss is 0.00027239647571042185\n",
      "Batch: 11200,train loss is: 0.00027284578238134474\n",
      "test loss is 0.00025281489764013564\n",
      "Batch: 11300,train loss is: 0.0001953319744010853\n",
      "test loss is 0.0002653921600192932\n",
      "Batch: 11400,train loss is: 0.00025258749720410696\n",
      "test loss is 0.0002649221059854368\n",
      "Batch: 11500,train loss is: 0.00018497084102754962\n",
      "test loss is 0.00025008955292443356\n",
      "Batch: 11600,train loss is: 0.00020521955322357622\n",
      "test loss is 0.00027542830045461413\n",
      "Batch: 11700,train loss is: 0.00032704177235008566\n",
      "test loss is 0.00027073750783777846\n",
      "Batch: 11800,train loss is: 0.0004352768008264268\n",
      "test loss is 0.00031429326821084616\n",
      "Batch: 11900,train loss is: 0.00026122119203943986\n",
      "test loss is 0.0003083849182935855\n",
      "Batch: 12000,train loss is: 0.0002470864497048607\n",
      "test loss is 0.0003010323591661863\n",
      "Batch: 12100,train loss is: 0.00021560622710104584\n",
      "test loss is 0.0002554748066374156\n",
      "Batch: 12200,train loss is: 0.00033359859715854787\n",
      "test loss is 0.0004320695757026694\n",
      "Batch: 12300,train loss is: 0.00025237084190246986\n",
      "test loss is 0.0002597946867902966\n",
      "Batch: 12400,train loss is: 0.00038330080424996225\n",
      "test loss is 0.00026984996360048815\n",
      "Batch: 12500,train loss is: 0.00019227317797519585\n",
      "test loss is 0.0002762302391989783\n",
      "Batch: 12600,train loss is: 0.00024600804974316866\n",
      "test loss is 0.0002993134856306004\n",
      "Batch: 12700,train loss is: 0.0003001693540346061\n",
      "test loss is 0.0003170582070092472\n",
      "Batch: 12800,train loss is: 0.00020277845692108487\n",
      "test loss is 0.0002540577858014766\n",
      "Batch: 12900,train loss is: 0.000285682502413734\n",
      "test loss is 0.00024086403237864954\n",
      "Batch: 13000,train loss is: 0.00027846453022972663\n",
      "test loss is 0.00029018049821600113\n",
      "Batch: 13100,train loss is: 0.0003863983002436159\n",
      "test loss is 0.00030971004405279525\n",
      "Batch: 13200,train loss is: 0.00036189515903573694\n",
      "test loss is 0.00027558390316533455\n",
      "Batch: 13300,train loss is: 0.0002070236085514463\n",
      "test loss is 0.0002637287151416706\n",
      "Batch: 13400,train loss is: 0.00026843916801961714\n",
      "test loss is 0.0002591799610000184\n",
      "Batch: 13500,train loss is: 0.00015525513422083234\n",
      "test loss is 0.00025275393663133324\n",
      "Batch: 13600,train loss is: 0.0003282836829999683\n",
      "test loss is 0.0002888482752985524\n",
      "Batch: 13700,train loss is: 0.00022949439681944726\n",
      "test loss is 0.00027517586239712865\n",
      "Batch: 13800,train loss is: 0.0002510751309952237\n",
      "test loss is 0.00032407470296887597\n",
      "Batch: 13900,train loss is: 0.00014015300445043848\n",
      "test loss is 0.00027539603238356514\n",
      "Batch: 14000,train loss is: 0.00016778764936656863\n",
      "test loss is 0.0003278463465429442\n",
      "Batch: 14100,train loss is: 0.00026480027047618364\n",
      "test loss is 0.00024396310431131746\n",
      "Batch: 14200,train loss is: 0.0002007985666169905\n",
      "test loss is 0.0002665475352616231\n",
      "Batch: 14300,train loss is: 0.0004010746743542493\n",
      "test loss is 0.0002737615801347205\n",
      "Batch: 14400,train loss is: 0.0004491104056841452\n",
      "test loss is 0.000259752672530872\n",
      "Batch: 14500,train loss is: 0.00022165502448730597\n",
      "test loss is 0.00026630746112741096\n",
      "Batch: 14600,train loss is: 0.00025730690323538005\n",
      "test loss is 0.0002755221923539262\n",
      "Batch: 14700,train loss is: 0.00022895693129946424\n",
      "test loss is 0.00023772498491964378\n",
      "Batch: 14800,train loss is: 0.00024375224111576374\n",
      "test loss is 0.00023644623929600222\n",
      "Batch: 14900,train loss is: 0.00020119228239807505\n",
      "test loss is 0.0002677405138362991\n",
      "Batch: 15000,train loss is: 0.00026377695872763934\n",
      "test loss is 0.00028726579017666524\n",
      "Batch: 15100,train loss is: 0.00028007637498199937\n",
      "test loss is 0.0002495943455576611\n",
      "Batch: 15200,train loss is: 0.0001858349969288459\n",
      "test loss is 0.0002908944765267263\n",
      "Batch: 15300,train loss is: 0.00028317863763176307\n",
      "test loss is 0.0002538835930203958\n",
      "Batch: 15400,train loss is: 0.00035402915474808973\n",
      "test loss is 0.00026921352505797634\n",
      "Batch: 15500,train loss is: 0.00036778829964159883\n",
      "test loss is 0.0002573306549903141\n",
      "Batch: 15600,train loss is: 0.00024725220572205473\n",
      "test loss is 0.00025333738173350854\n",
      "Batch: 15700,train loss is: 0.00020168405916928112\n",
      "test loss is 0.0002447000579222944\n",
      "Batch: 15800,train loss is: 0.0003798170927813621\n",
      "test loss is 0.00031444515995101977\n",
      "Batch: 15900,train loss is: 0.0002159831060250959\n",
      "test loss is 0.00031228635710480113\n",
      "Batch: 16000,train loss is: 0.0003695568612041859\n",
      "test loss is 0.00031953240268556716\n",
      "Batch: 16100,train loss is: 0.0003129023357134855\n",
      "test loss is 0.00026288349985630803\n",
      "Batch: 16200,train loss is: 0.0002575976403450624\n",
      "test loss is 0.00029559164334804405\n",
      "Batch: 16300,train loss is: 0.0002101656834147469\n",
      "test loss is 0.00026165230130266876\n",
      "Batch: 16400,train loss is: 0.0001985661954814592\n",
      "test loss is 0.0002698364889480107\n",
      "Batch: 16500,train loss is: 0.00023798631949196897\n",
      "test loss is 0.0002997513725208825\n",
      "Batch: 16600,train loss is: 0.00019620269211009653\n",
      "test loss is 0.00027700875885944493\n",
      "Batch: 16700,train loss is: 0.00045537682047286213\n",
      "test loss is 0.00026677712345814117\n",
      "Batch: 16800,train loss is: 0.00021297440316244714\n",
      "test loss is 0.00028396502860150876\n",
      "Batch: 16900,train loss is: 0.00022381375156649416\n",
      "test loss is 0.00030131125532283076\n",
      "Batch: 17000,train loss is: 0.000192911220151683\n",
      "test loss is 0.00025954717780652054\n",
      "Batch: 17100,train loss is: 0.0002267315770632134\n",
      "test loss is 0.0002887990314896489\n",
      "Batch: 17200,train loss is: 0.00023709331897717675\n",
      "test loss is 0.0002657057455825558\n",
      "Batch: 17300,train loss is: 0.00023934081491010743\n",
      "test loss is 0.0002462494638345374\n",
      "Batch: 17400,train loss is: 0.00045219671546433543\n",
      "test loss is 0.0002994886339456987\n",
      "Batch: 17500,train loss is: 0.0002729284312340475\n",
      "test loss is 0.00024841212817544687\n",
      "Batch: 17600,train loss is: 0.00018796652779483683\n",
      "test loss is 0.00025404235661739984\n",
      "Batch: 17700,train loss is: 0.00020124323920076893\n",
      "test loss is 0.0002677995505620872\n",
      "Batch: 17800,train loss is: 0.0002950358845589638\n",
      "test loss is 0.00034394932064809626\n",
      "Batch: 17900,train loss is: 0.0003540903256878875\n",
      "test loss is 0.000324582032845769\n",
      "Batch: 18000,train loss is: 0.0004558718886526395\n",
      "test loss is 0.0002779548844035461\n",
      "Batch: 18100,train loss is: 0.00025194562961833257\n",
      "test loss is 0.000312878390554566\n",
      "Batch: 18200,train loss is: 0.00023580904402211315\n",
      "test loss is 0.00023751417458221517\n",
      "Batch: 18300,train loss is: 0.0003394682168839353\n",
      "test loss is 0.0003407460151071697\n",
      "Batch: 18400,train loss is: 0.0002009456394174152\n",
      "test loss is 0.00027284051455437485\n",
      "Batch: 18500,train loss is: 0.00019749090154604432\n",
      "test loss is 0.0003283996870678842\n",
      "Batch: 18600,train loss is: 0.0003483212199800251\n",
      "test loss is 0.00025326779363705055\n",
      "Batch: 18700,train loss is: 0.00021114701158629574\n",
      "test loss is 0.0002871062035136729\n",
      "Batch: 18800,train loss is: 0.00023088161217313274\n",
      "test loss is 0.00027006785141508945\n",
      "Batch: 18900,train loss is: 0.00024403431468103282\n",
      "test loss is 0.00033043192656734564\n",
      "Batch: 19000,train loss is: 0.00042959500210772953\n",
      "test loss is 0.00035874109257877997\n",
      "Batch: 19100,train loss is: 0.00033851755663235933\n",
      "test loss is 0.00035094545686968745\n",
      "Batch: 19200,train loss is: 0.0003733635143351467\n",
      "test loss is 0.00024345749224781224\n",
      "Batch: 19300,train loss is: 0.00027506965758510363\n",
      "test loss is 0.00024522543059371066\n",
      "Batch: 19400,train loss is: 0.00030458589954996117\n",
      "test loss is 0.0002468014737279458\n",
      "Batch: 19500,train loss is: 0.00018016691274468945\n",
      "test loss is 0.00023765073367148674\n",
      "Batch: 19600,train loss is: 0.0006210656259097705\n",
      "test loss is 0.00025663837774951917\n",
      "Batch: 19700,train loss is: 0.00024743454572497686\n",
      "test loss is 0.00024385610708565965\n",
      "Batch: 19800,train loss is: 0.00015985055494785466\n",
      "test loss is 0.0002577648610864652\n",
      "Batch: 19900,train loss is: 0.00021509558887424537\n",
      "test loss is 0.00024900109229217374\n",
      "Batch: 20000,train loss is: 0.0001722496049162154\n",
      "test loss is 0.0003276094851575154\n",
      "Batch: 20100,train loss is: 0.0002143041163985302\n",
      "test loss is 0.0002733899271703349\n",
      "Batch: 20200,train loss is: 0.00023920898120469625\n",
      "test loss is 0.00026500891995019275\n",
      "Batch: 20300,train loss is: 0.00030000638527037947\n",
      "test loss is 0.0002988550717678764\n",
      "Batch: 20400,train loss is: 0.00024594074573301624\n",
      "test loss is 0.0002685498145320184\n",
      "Batch: 20500,train loss is: 0.00022797671600575654\n",
      "test loss is 0.000260204253935774\n",
      "Batch: 20600,train loss is: 0.00038583605489575696\n",
      "test loss is 0.0002723566133965146\n",
      "Batch: 20700,train loss is: 0.00021330865807020868\n",
      "test loss is 0.0002604534752121802\n",
      "Batch: 20800,train loss is: 0.0002946482243098431\n",
      "test loss is 0.0003145561513061315\n",
      "Batch: 20900,train loss is: 0.00012273300125869365\n",
      "test loss is 0.00026875489774766707\n",
      "Batch: 21000,train loss is: 0.0003093834588342765\n",
      "test loss is 0.0002673562734982124\n",
      "Batch: 21100,train loss is: 0.00014301216475263545\n",
      "test loss is 0.00024777526848705295\n",
      "Batch: 21200,train loss is: 0.0001928355924314353\n",
      "test loss is 0.0002813793946569613\n",
      "Batch: 21300,train loss is: 0.0003016585902963178\n",
      "test loss is 0.00032676298590035406\n",
      "Batch: 21400,train loss is: 0.0002577424927004629\n",
      "test loss is 0.0002507684462265845\n",
      "Batch: 21500,train loss is: 0.0002390243441810591\n",
      "test loss is 0.00036230887697504823\n",
      "Batch: 21600,train loss is: 0.00027991085776456556\n",
      "test loss is 0.0003233799961853616\n",
      "Batch: 21700,train loss is: 0.00020498454787462776\n",
      "test loss is 0.0002628574464308923\n",
      "Batch: 21800,train loss is: 0.00027347854876863856\n",
      "test loss is 0.0002502582086593237\n",
      "Batch: 21900,train loss is: 0.0002439030176733177\n",
      "test loss is 0.0002629582476026308\n",
      "Batch: 22000,train loss is: 0.00023454748566211454\n",
      "test loss is 0.00025448575252758667\n",
      "Batch: 22100,train loss is: 0.00029769756154467223\n",
      "test loss is 0.00025056806569548297\n",
      "Batch: 22200,train loss is: 0.00017498533181051207\n",
      "test loss is 0.00025919226556873003\n",
      "Batch: 22300,train loss is: 0.00029890246128591526\n",
      "test loss is 0.00030164473713334113\n",
      "Batch: 22400,train loss is: 0.0002647964515449359\n",
      "test loss is 0.0003399836874946115\n",
      "Batch: 22500,train loss is: 0.0004309883339462819\n",
      "test loss is 0.0003328068850427482\n",
      "Batch: 22600,train loss is: 0.0003471909704154903\n",
      "test loss is 0.0002721294091308069\n",
      "Batch: 22700,train loss is: 0.00046912516854616274\n",
      "test loss is 0.0004133178697022602\n",
      "Batch: 22800,train loss is: 0.00022870649145527515\n",
      "test loss is 0.00024482809170779424\n",
      "Batch: 22900,train loss is: 0.00031712277781635774\n",
      "test loss is 0.0002832738872622343\n",
      "Batch: 23000,train loss is: 0.0002902669668200702\n",
      "test loss is 0.0002646493123403052\n",
      "Batch: 23100,train loss is: 0.0003575959751867357\n",
      "test loss is 0.00028216729384913355\n",
      "Batch: 23200,train loss is: 0.0002013366047253095\n",
      "test loss is 0.00023373633250507562\n",
      "Batch: 23300,train loss is: 0.0002454196971740135\n",
      "test loss is 0.0002583024911617897\n",
      "Batch: 23400,train loss is: 0.00022366005312879017\n",
      "test loss is 0.00029306712636224005\n",
      "Batch: 23500,train loss is: 0.00027925462131876233\n",
      "test loss is 0.0002443930357379039\n",
      "Batch: 23600,train loss is: 0.000309252935001347\n",
      "test loss is 0.0002540392678012402\n",
      "Batch: 23700,train loss is: 0.0006467611217006134\n",
      "test loss is 0.0004343351320172549\n",
      "Batch: 23800,train loss is: 0.00031359268915261715\n",
      "test loss is 0.00031142549242517333\n",
      "Batch: 23900,train loss is: 0.0002226754304998687\n",
      "test loss is 0.00026156354937590146\n",
      "Batch: 24000,train loss is: 0.000239477030668528\n",
      "test loss is 0.0002727113415556621\n",
      "Batch: 24100,train loss is: 0.00041665952419625145\n",
      "test loss is 0.0002639322972131557\n",
      "Batch: 24200,train loss is: 0.0001397344568731672\n",
      "test loss is 0.00024051583692990026\n",
      "Batch: 24300,train loss is: 0.00025499683120215757\n",
      "test loss is 0.00024822214439417905\n",
      "Batch: 24400,train loss is: 0.00021245220696800842\n",
      "test loss is 0.0002802170865951939\n",
      "Batch: 24500,train loss is: 0.0002281094446500977\n",
      "test loss is 0.0002502518844847936\n",
      "Batch: 24600,train loss is: 0.00022438558269943326\n",
      "test loss is 0.0002576390539353267\n",
      "Batch: 24700,train loss is: 0.0002400259991419525\n",
      "test loss is 0.0002848168416378431\n",
      "Batch: 24800,train loss is: 0.00022195037680187134\n",
      "test loss is 0.0002470188246494444\n",
      "Batch: 24900,train loss is: 0.00020597148719906744\n",
      "test loss is 0.0002400315376197755\n",
      "Batch: 25000,train loss is: 0.00018744445122629455\n",
      "test loss is 0.0002791925223528147\n",
      "Batch: 25100,train loss is: 0.00024694281288567885\n",
      "test loss is 0.0002426632418515537\n",
      "Batch: 25200,train loss is: 0.0002557878994878482\n",
      "test loss is 0.0002731331549628712\n",
      "Batch: 25300,train loss is: 0.000220087507047872\n",
      "test loss is 0.0002499691310191989\n",
      "Batch: 25400,train loss is: 0.00027429699996668627\n",
      "test loss is 0.00029509326379381423\n",
      "Batch: 25500,train loss is: 0.00019201548203195382\n",
      "test loss is 0.00023385905854814726\n",
      "Batch: 25600,train loss is: 0.000191821623995488\n",
      "test loss is 0.0003114070724552075\n",
      "Batch: 25700,train loss is: 0.00020186469471699216\n",
      "test loss is 0.0002771960580611417\n",
      "Batch: 25800,train loss is: 0.00020195041467694846\n",
      "test loss is 0.0002781699006047943\n",
      "Batch: 25900,train loss is: 0.00027333480388312193\n",
      "test loss is 0.00031991384607304123\n",
      "Batch: 26000,train loss is: 0.000221041848027464\n",
      "test loss is 0.00029459384043938747\n",
      "Batch: 26100,train loss is: 0.00036576419002486236\n",
      "test loss is 0.00026758946569563986\n",
      "Batch: 26200,train loss is: 0.00024224414899098702\n",
      "test loss is 0.00026965511781890333\n",
      "Batch: 26300,train loss is: 0.00023100331727849902\n",
      "test loss is 0.0002586938273381198\n",
      "Batch: 26400,train loss is: 0.0003145959847615418\n",
      "test loss is 0.000262493517510833\n",
      "Batch: 26500,train loss is: 0.00021168280624446736\n",
      "test loss is 0.000291592398437685\n",
      "Batch: 26600,train loss is: 0.0002601243241460245\n",
      "test loss is 0.00030344791072709127\n",
      "Batch: 26700,train loss is: 0.0002874249577499574\n",
      "test loss is 0.00029230782317058823\n",
      "Batch: 26800,train loss is: 0.0002809742584520456\n",
      "test loss is 0.0002646212972413203\n",
      "Batch: 26900,train loss is: 0.00030512185714588194\n",
      "test loss is 0.00023417174129343606\n",
      "Batch: 27000,train loss is: 0.00013732336096382866\n",
      "test loss is 0.0003345417646270713\n",
      "Batch: 27100,train loss is: 0.00020000180588902082\n",
      "test loss is 0.0002686950444570198\n",
      "Batch: 27200,train loss is: 0.0005732973618349532\n",
      "test loss is 0.0002774272567823435\n",
      "Batch: 27300,train loss is: 0.0002926770442859087\n",
      "test loss is 0.00031852013751735404\n",
      "Batch: 27400,train loss is: 0.00024711702482862845\n",
      "test loss is 0.0003058587612259701\n",
      "Batch: 27500,train loss is: 0.00031104161025169376\n",
      "test loss is 0.000258349179138578\n",
      "Batch: 27600,train loss is: 0.00045145176399508677\n",
      "test loss is 0.0002471654389392129\n",
      "Batch: 27700,train loss is: 0.00029125051133307425\n",
      "test loss is 0.00027190780509072905\n",
      "Batch: 27800,train loss is: 0.00016213110495607257\n",
      "test loss is 0.00025411474328533405\n",
      "Batch: 27900,train loss is: 0.00024784945509680943\n",
      "test loss is 0.00027802851371962726\n",
      "Batch: 28000,train loss is: 0.00021027281281389075\n",
      "test loss is 0.0002412944784867542\n",
      "Batch: 28100,train loss is: 0.0002417771106710674\n",
      "test loss is 0.00026648475030741173\n",
      "Batch: 28200,train loss is: 0.00026221781905224543\n",
      "test loss is 0.00029503566374170203\n",
      "Batch: 28300,train loss is: 0.00019283505787567624\n",
      "test loss is 0.0002710653251053448\n",
      "Batch: 28400,train loss is: 0.00036081747515325733\n",
      "test loss is 0.0002780864555910712\n",
      "Batch: 28500,train loss is: 0.00025792592907147925\n",
      "test loss is 0.0002868615123580678\n",
      "Batch: 28600,train loss is: 0.00019465566337504423\n",
      "test loss is 0.0002887127945411783\n",
      "Batch: 28700,train loss is: 0.00020394939853616754\n",
      "test loss is 0.0002399465137027011\n",
      "Batch: 28800,train loss is: 0.00026156517995378143\n",
      "test loss is 0.00023818805899442704\n",
      "Batch: 28900,train loss is: 0.0006097176552934547\n",
      "test loss is 0.00040422890955158424\n",
      "Batch: 29000,train loss is: 0.0003283855062033883\n",
      "test loss is 0.0003352821988274217\n",
      "Batch: 29100,train loss is: 0.0003384581941599202\n",
      "test loss is 0.00025190097018042666\n",
      "Batch: 29200,train loss is: 0.00018145300102269973\n",
      "test loss is 0.00025122686375876385\n",
      "Batch: 29300,train loss is: 0.0002559212544553926\n",
      "test loss is 0.00024237581534631376\n",
      "Batch: 29400,train loss is: 0.00022796075660984003\n",
      "test loss is 0.0003067142629330413\n",
      "Batch: 29500,train loss is: 0.0002619002498407698\n",
      "test loss is 0.00030389109192183867\n",
      "Batch: 29600,train loss is: 0.00018815400293042034\n",
      "test loss is 0.00023463123238187642\n",
      "Batch: 29700,train loss is: 0.0003128679573169437\n",
      "test loss is 0.0002691142867830157\n",
      "Batch: 29800,train loss is: 0.0003419538292205896\n",
      "test loss is 0.0002512846764801367\n",
      "Batch: 29900,train loss is: 0.0004366541300398517\n",
      "test loss is 0.0002760034279491206\n",
      "Batch: 30000,train loss is: 0.00037341591926419514\n",
      "test loss is 0.0002833357361160377\n",
      "Batch: 30100,train loss is: 0.0002227901299020075\n",
      "test loss is 0.0002670879507307005\n",
      "Batch: 30200,train loss is: 0.00024031233295388374\n",
      "test loss is 0.0002832894677180345\n",
      "Batch: 30300,train loss is: 0.00016975968671742396\n",
      "test loss is 0.0002986958194746314\n",
      "Batch: 30400,train loss is: 0.0002473763414698639\n",
      "test loss is 0.00028898305116018517\n",
      "Batch: 30500,train loss is: 0.00021808430469201913\n",
      "test loss is 0.0002450007365825391\n",
      "Batch: 30600,train loss is: 0.00022598955238886258\n",
      "test loss is 0.0002663009606044238\n",
      "Batch: 30700,train loss is: 0.00020446159474020535\n",
      "test loss is 0.0002651257250037802\n",
      "Batch: 30800,train loss is: 0.0003477374198973715\n",
      "test loss is 0.0002651128639222291\n",
      "Batch: 30900,train loss is: 0.00023375089149819337\n",
      "test loss is 0.00025448651075740556\n",
      "Batch: 31000,train loss is: 0.0005457558711217203\n",
      "test loss is 0.000375370245959575\n",
      "Batch: 31100,train loss is: 0.00017847748804615334\n",
      "test loss is 0.0002929677057332228\n",
      "Batch: 31200,train loss is: 0.0002586109709045257\n",
      "test loss is 0.00030234902874238053\n",
      "Batch: 31300,train loss is: 0.00039707819360316815\n",
      "test loss is 0.0002705109253201816\n",
      "Batch: 31400,train loss is: 0.0002903650360532556\n",
      "test loss is 0.00025997284147523175\n",
      "Batch: 31500,train loss is: 0.00022395615529999913\n",
      "test loss is 0.0002857128782642523\n",
      "Batch: 31600,train loss is: 0.00032038568037643896\n",
      "test loss is 0.00028690018585139724\n",
      "Batch: 31700,train loss is: 0.0006388401591758167\n",
      "test loss is 0.000244355007857\n",
      "Batch: 31800,train loss is: 0.0005265488273788871\n",
      "test loss is 0.0002690655490446644\n",
      "Batch: 31900,train loss is: 0.00022804692422947342\n",
      "test loss is 0.000252869562955424\n",
      "Batch: 32000,train loss is: 0.00020744389160648756\n",
      "test loss is 0.0002840974235943778\n",
      "Batch: 32100,train loss is: 0.000314233132158113\n",
      "test loss is 0.00026220625551489274\n",
      "Batch: 32200,train loss is: 0.00022641065292262742\n",
      "test loss is 0.00024125541740164076\n",
      "Batch: 32300,train loss is: 0.00020512563317534778\n",
      "test loss is 0.00027017646712750715\n",
      "Batch: 32400,train loss is: 0.00027525765075175706\n",
      "test loss is 0.0002833220162123403\n",
      "Batch: 32500,train loss is: 0.00018232846213580233\n",
      "test loss is 0.00026303335296025523\n",
      "Batch: 32600,train loss is: 0.0003597021927927605\n",
      "test loss is 0.0002936071694218619\n",
      "Batch: 32700,train loss is: 0.0002275822144353634\n",
      "test loss is 0.0002704355357887699\n",
      "Batch: 32800,train loss is: 0.00025715548371301204\n",
      "test loss is 0.00026739294292735054\n",
      "Batch: 32900,train loss is: 0.0001868476751199801\n",
      "test loss is 0.0002639543691605032\n",
      "Batch: 33000,train loss is: 0.00028648409478823134\n",
      "test loss is 0.00031502815980504953\n",
      "Batch: 33100,train loss is: 0.0002866106235726097\n",
      "test loss is 0.0002570267353364444\n",
      "Batch: 33200,train loss is: 0.0002511826727757965\n",
      "test loss is 0.00024165067143213287\n",
      "Batch: 33300,train loss is: 0.00025430987999858163\n",
      "test loss is 0.0002565363802799275\n",
      "Batch: 33400,train loss is: 0.00034093784135048026\n",
      "test loss is 0.0002955753226429673\n",
      "Batch: 33500,train loss is: 0.00020311980751778035\n",
      "test loss is 0.00029423542649560625\n",
      "Batch: 33600,train loss is: 0.0001793538041764512\n",
      "test loss is 0.00025920159161566404\n",
      "Batch: 33700,train loss is: 0.00019259843913032982\n",
      "test loss is 0.00026166409628685435\n",
      "Batch: 33800,train loss is: 0.00023153322621751759\n",
      "test loss is 0.00027977269595476205\n",
      "Batch: 33900,train loss is: 0.0002512164198632089\n",
      "test loss is 0.00025821710087392703\n",
      "Batch: 34000,train loss is: 0.00019951554991413093\n",
      "test loss is 0.00032868632421804024\n",
      "Batch: 34100,train loss is: 0.00026432525299408406\n",
      "test loss is 0.00037100603337991055\n",
      "Batch: 34200,train loss is: 0.00038528061996824503\n",
      "test loss is 0.00027652933338980163\n",
      "Batch: 34300,train loss is: 0.0001722204478329793\n",
      "test loss is 0.0003103240291934587\n",
      "Batch: 34400,train loss is: 0.00028531087789173933\n",
      "test loss is 0.000262398022571071\n",
      "Batch: 34500,train loss is: 0.00023391067802621345\n",
      "test loss is 0.00026591520550014504\n",
      "Batch: 34600,train loss is: 0.0003068759522406627\n",
      "test loss is 0.0002849624646457173\n",
      "Batch: 34700,train loss is: 0.00028587033538863085\n",
      "test loss is 0.00033343642817828465\n",
      "Batch: 34800,train loss is: 0.00025999784252144024\n",
      "test loss is 0.0002529304096744203\n",
      "Batch: 34900,train loss is: 0.00028606923742018585\n",
      "test loss is 0.0002482102874349187\n",
      "Batch: 35000,train loss is: 0.00019888749201812405\n",
      "test loss is 0.0002521444212664189\n",
      "Batch: 35100,train loss is: 0.0002064085567934123\n",
      "test loss is 0.00025496911795166196\n",
      "Batch: 35200,train loss is: 0.00015129823956019998\n",
      "test loss is 0.00026211986440742064\n",
      "Batch: 35300,train loss is: 0.0004226891419755667\n",
      "test loss is 0.00027577949581374193\n",
      "Batch: 35400,train loss is: 0.00015454885076848058\n",
      "test loss is 0.0002556387262389193\n",
      "Batch: 35500,train loss is: 0.0002408093558676789\n",
      "test loss is 0.0002603586725142596\n",
      "Batch: 35600,train loss is: 0.0001886435220384622\n",
      "test loss is 0.00030005789110929877\n",
      "Batch: 35700,train loss is: 0.00023996740063314656\n",
      "test loss is 0.0003053443244595557\n",
      "Batch: 35800,train loss is: 0.0002028615562828642\n",
      "test loss is 0.00025964711253530936\n",
      "Batch: 35900,train loss is: 0.00022927355286107484\n",
      "test loss is 0.00024818152773712444\n",
      "Batch: 36000,train loss is: 0.00020749469696638946\n",
      "test loss is 0.0002562189907147407\n",
      "Batch: 36100,train loss is: 0.0002988134764965692\n",
      "test loss is 0.00032210160310638215\n",
      "Batch: 36200,train loss is: 0.0005193097522819868\n",
      "test loss is 0.0002503387755544396\n",
      "Batch: 36300,train loss is: 0.00032686890798801964\n",
      "test loss is 0.0002894229619312092\n",
      "Batch: 36400,train loss is: 0.00021421147249305834\n",
      "test loss is 0.00024863392095907506\n",
      "Batch: 36500,train loss is: 0.0002101274819648569\n",
      "test loss is 0.0002594618765144927\n",
      "Batch: 36600,train loss is: 0.0003600757391346076\n",
      "test loss is 0.0003384078211933141\n",
      "Batch: 36700,train loss is: 0.00030966350701734657\n",
      "test loss is 0.00023756789543208552\n",
      "Batch: 36800,train loss is: 0.00020959060084742856\n",
      "test loss is 0.00028204436776601443\n",
      "Batch: 36900,train loss is: 0.0002733551413230317\n",
      "test loss is 0.0003794535000294103\n",
      "Batch: 37000,train loss is: 0.0002187195616641663\n",
      "test loss is 0.00030617696757355423\n",
      "Batch: 37100,train loss is: 0.0002467442598678164\n",
      "test loss is 0.00025953030077715156\n",
      "Batch: 37200,train loss is: 0.0001981371149873379\n",
      "test loss is 0.00025989717758336905\n",
      "Batch: 37300,train loss is: 0.0004190846525900161\n",
      "test loss is 0.00031769927271702757\n",
      "Batch: 37400,train loss is: 0.0003282189399452509\n",
      "test loss is 0.000349507554045583\n",
      "Batch: 37500,train loss is: 0.0002070450900960052\n",
      "test loss is 0.00026401094764606206\n",
      "Batch: 37600,train loss is: 0.00031868635084102343\n",
      "test loss is 0.00027662041291730516\n",
      "Batch: 37700,train loss is: 0.00033102073271243013\n",
      "test loss is 0.00030040639678646856\n",
      "Batch: 37800,train loss is: 0.0002275313018527455\n",
      "test loss is 0.0002525148317606448\n",
      "Batch: 37900,train loss is: 0.00025060221689131925\n",
      "test loss is 0.00028374110652718487\n",
      "Batch: 38000,train loss is: 0.0002621090401727225\n",
      "test loss is 0.000283514359460726\n",
      "Batch: 38100,train loss is: 0.00020430727772646252\n",
      "test loss is 0.00025790825390556145\n",
      "Batch: 38200,train loss is: 0.0003070508048601364\n",
      "test loss is 0.00025769379505250644\n",
      "Batch: 38300,train loss is: 0.0002220344039751452\n",
      "test loss is 0.00026195769870132775\n",
      "Batch: 38400,train loss is: 0.00028031115324853376\n",
      "test loss is 0.0002382506482021594\n",
      "Batch: 38500,train loss is: 0.0002915906504525319\n",
      "test loss is 0.00031348987241049836\n",
      "Batch: 38600,train loss is: 0.0003119892824429921\n",
      "test loss is 0.0003284944025311187\n",
      "Batch: 38700,train loss is: 0.0003700828513621691\n",
      "test loss is 0.00029359123687979687\n",
      "Batch: 38800,train loss is: 0.00022950909523613975\n",
      "test loss is 0.00025103466666024006\n",
      "Batch: 38900,train loss is: 7.867914943639137e-05\n",
      "test loss is 0.0002500122176861987\n",
      "Batch: 39000,train loss is: 0.00027048778160049974\n",
      "test loss is 0.0002507516476970477\n",
      "Batch: 39100,train loss is: 0.00023665516669015092\n",
      "test loss is 0.00025846553567324625\n",
      "Batch: 39200,train loss is: 0.0003495097020074673\n",
      "test loss is 0.00028417029904026985\n",
      "Batch: 39300,train loss is: 0.0004484996364683588\n",
      "test loss is 0.0003054327532293045\n",
      "Batch: 39400,train loss is: 0.00014145123562404657\n",
      "test loss is 0.00026817235500511597\n",
      "Batch: 39500,train loss is: 0.00017913106091355457\n",
      "test loss is 0.00025648877936169777\n",
      "Batch: 39600,train loss is: 0.0002765164857283552\n",
      "test loss is 0.0002513804185637978\n",
      "Batch: 39700,train loss is: 0.00022378834843468984\n",
      "test loss is 0.00025581861269173534\n",
      "Batch: 39800,train loss is: 0.0003150480531675649\n",
      "test loss is 0.00027998095969484517\n",
      "Batch: 39900,train loss is: 0.00038206780886958073\n",
      "test loss is 0.00025202880650350494\n",
      "Batch: 40000,train loss is: 0.00019571377950097074\n",
      "test loss is 0.00027400740488455193\n",
      "Batch: 40100,train loss is: 0.0002559266442830668\n",
      "test loss is 0.00027890809209903935\n",
      "Batch: 40200,train loss is: 0.00029089514501960895\n",
      "test loss is 0.0002639150761307725\n",
      "Batch: 40300,train loss is: 0.00020909296064130676\n",
      "test loss is 0.00038072871333145905\n",
      "Batch: 40400,train loss is: 0.0004373281107742936\n",
      "test loss is 0.0003200266117500662\n",
      "Batch: 40500,train loss is: 0.00026806343377031976\n",
      "test loss is 0.00028304894715801276\n",
      "Batch: 40600,train loss is: 0.0001951196697178457\n",
      "test loss is 0.00024385894622032454\n",
      "Batch: 40700,train loss is: 0.0002540779117455091\n",
      "test loss is 0.0002672410944704006\n",
      "Batch: 40800,train loss is: 0.0002352101498140451\n",
      "test loss is 0.00025485041204207834\n",
      "Batch: 40900,train loss is: 0.0002493438377762922\n",
      "test loss is 0.0002948010626714376\n",
      "Batch: 41000,train loss is: 0.00040794439950653916\n",
      "test loss is 0.0002539904388509988\n",
      "Batch: 41100,train loss is: 0.00020979528711640065\n",
      "test loss is 0.0002463015787827164\n",
      "Batch: 41200,train loss is: 0.0003069012978807405\n",
      "test loss is 0.0002475405328171065\n",
      "Batch: 41300,train loss is: 0.0002738009641896545\n",
      "test loss is 0.0002652881349448874\n",
      "Batch: 41400,train loss is: 0.00024964687250067703\n",
      "test loss is 0.0002617080935294403\n",
      "Batch: 41500,train loss is: 0.00042703601187688424\n",
      "test loss is 0.000244848709913752\n",
      "Batch: 41600,train loss is: 0.0002805869814631012\n",
      "test loss is 0.00023745422695893717\n",
      "Batch: 41700,train loss is: 0.000357167164690941\n",
      "test loss is 0.00023215889531656497\n",
      "Batch: 41800,train loss is: 0.000248513555535776\n",
      "test loss is 0.0002700961860367599\n",
      "Batch: 41900,train loss is: 0.0003737087224633159\n",
      "test loss is 0.00025901451967385555\n",
      "Batch: 42000,train loss is: 0.00014765029394712493\n",
      "test loss is 0.0002373029205689217\n",
      "Batch: 42100,train loss is: 0.00026485216669253195\n",
      "test loss is 0.0002589088645311717\n",
      "Batch: 42200,train loss is: 0.00022002435838973367\n",
      "test loss is 0.00027989983888547694\n",
      "Batch: 42300,train loss is: 0.00028976367968555953\n",
      "test loss is 0.0002798786718061388\n",
      "Batch: 42400,train loss is: 0.00020380414537429222\n",
      "test loss is 0.0002575140071074277\n",
      "Batch: 42500,train loss is: 0.0001870860039948518\n",
      "test loss is 0.0002639815432226752\n",
      "Batch: 42600,train loss is: 0.0002523856800839077\n",
      "test loss is 0.0002780367731435183\n",
      "Batch: 42700,train loss is: 0.000250856773099611\n",
      "test loss is 0.0002920887517000796\n",
      "Batch: 42800,train loss is: 0.0003260735579504233\n",
      "test loss is 0.000276200431542844\n",
      "Batch: 42900,train loss is: 0.0002427387624430693\n",
      "test loss is 0.0002580556368339604\n",
      "Batch: 43000,train loss is: 0.0005077935437197386\n",
      "test loss is 0.00024344991643679065\n",
      "Batch: 43100,train loss is: 0.00019974888326683142\n",
      "test loss is 0.000299747021311635\n",
      "Batch: 43200,train loss is: 0.00031629128915527043\n",
      "test loss is 0.00032821647710603777\n",
      "Batch: 43300,train loss is: 0.00032997945764016306\n",
      "test loss is 0.00026215571170707416\n",
      "Batch: 43400,train loss is: 0.00048053715304851034\n",
      "test loss is 0.00032076166058350535\n",
      "Batch: 43500,train loss is: 0.0004328752854073445\n",
      "test loss is 0.0003145737816667092\n",
      "Batch: 43600,train loss is: 0.00033613298225895164\n",
      "test loss is 0.00025528411264098485\n",
      "Batch: 43700,train loss is: 0.00019781617186969195\n",
      "test loss is 0.0002967184081001747\n",
      "Batch: 43800,train loss is: 0.00017553594752569847\n",
      "test loss is 0.0002838493424282119\n",
      "Batch: 43900,train loss is: 0.00015237683501492745\n",
      "test loss is 0.0002557034683088706\n",
      "Batch: 44000,train loss is: 0.00038474164719160035\n",
      "test loss is 0.0002488858274124126\n",
      "Batch: 44100,train loss is: 0.0002538938074740788\n",
      "test loss is 0.00023533465931962433\n",
      "Batch: 44200,train loss is: 0.000197747105507923\n",
      "test loss is 0.0002494601518480345\n",
      "Batch: 44300,train loss is: 0.00044283235502812527\n",
      "test loss is 0.00031793435696597855\n",
      "Batch: 44400,train loss is: 0.00019537802829069348\n",
      "test loss is 0.00024914050149133085\n",
      "Batch: 44500,train loss is: 0.0002496212929689079\n",
      "test loss is 0.00026593548351438247\n",
      "Batch: 44600,train loss is: 0.00011532192938003294\n",
      "test loss is 0.0002853903411008899\n",
      "Batch: 44700,train loss is: 0.0002607820372267942\n",
      "test loss is 0.0002728239816463493\n",
      "Batch: 44800,train loss is: 0.00019873062020019571\n",
      "test loss is 0.00025564549418427774\n",
      "Batch: 44900,train loss is: 0.0002638328754965858\n",
      "test loss is 0.00026152801438620774\n",
      "Batch: 45000,train loss is: 0.00024618133780105394\n",
      "test loss is 0.000539596296176557\n",
      "Batch: 45100,train loss is: 0.0002001201633074873\n",
      "test loss is 0.0002935069418471283\n",
      "Batch: 45200,train loss is: 0.0003639839888763084\n",
      "test loss is 0.000275344165719087\n",
      "Batch: 45300,train loss is: 0.00015028473420185154\n",
      "test loss is 0.00026997411258853217\n",
      "Batch: 45400,train loss is: 0.0001922083223708665\n",
      "test loss is 0.0002380558640603547\n",
      "Batch: 45500,train loss is: 0.00017110712416200721\n",
      "test loss is 0.0002454418428769906\n",
      "Batch: 45600,train loss is: 0.0002633366433757402\n",
      "test loss is 0.0002937111047359506\n",
      "Batch: 45700,train loss is: 0.0007171323051154161\n",
      "test loss is 0.0002663392722297535\n",
      "Batch: 45800,train loss is: 0.0002092102849579767\n",
      "test loss is 0.00030234713957421824\n",
      "Batch: 45900,train loss is: 0.0002987099915159185\n",
      "test loss is 0.00026133419348174284\n",
      "Batch: 46000,train loss is: 0.00027129313599718767\n",
      "test loss is 0.00025235355455112944\n",
      "Batch: 46100,train loss is: 0.0002979668537064109\n",
      "test loss is 0.00026619425403045187\n",
      "Batch: 46200,train loss is: 0.00040846921580838175\n",
      "test loss is 0.0002548124693734323\n",
      "Batch: 46300,train loss is: 0.0002638993613967494\n",
      "test loss is 0.0002940920426012416\n",
      "Batch: 46400,train loss is: 0.0002557253971582565\n",
      "test loss is 0.00026542832483550957\n",
      "Batch: 46500,train loss is: 0.00024031220955726955\n",
      "test loss is 0.00027385628632394334\n",
      "Batch: 46600,train loss is: 0.00020343433768872798\n",
      "test loss is 0.0003061236390136586\n",
      "Batch: 46700,train loss is: 0.00022828719814482453\n",
      "test loss is 0.00024447334901187093\n",
      "-----------------------Epoch: 10----------------------------------\n",
      "Batch: 0,train loss is: 0.00024321987044492523\n",
      "test loss is 0.0004654024181301792\n",
      "Batch: 100,train loss is: 0.0002694494871638539\n",
      "test loss is 0.0002368041284554989\n",
      "Batch: 200,train loss is: 0.00031796603616851516\n",
      "test loss is 0.00023481818620604041\n",
      "Batch: 300,train loss is: 0.00038850326840229887\n",
      "test loss is 0.0002978046726428776\n",
      "Batch: 400,train loss is: 0.0002649172355914792\n",
      "test loss is 0.000269102849983247\n",
      "Batch: 500,train loss is: 0.0002165528615910662\n",
      "test loss is 0.0003167121128515676\n",
      "Batch: 600,train loss is: 0.00040859317725043184\n",
      "test loss is 0.0002725309103978904\n",
      "Batch: 700,train loss is: 0.00016267053074671704\n",
      "test loss is 0.00026147989220800285\n",
      "Batch: 800,train loss is: 0.0002976067448296245\n",
      "test loss is 0.0002565661741949539\n",
      "Batch: 900,train loss is: 0.0003445075218810213\n",
      "test loss is 0.0002598172157971355\n",
      "Batch: 1000,train loss is: 0.0002440139205647117\n",
      "test loss is 0.00024026683069280634\n",
      "Batch: 1100,train loss is: 0.0002804687375067898\n",
      "test loss is 0.0002508392135130232\n",
      "Batch: 1200,train loss is: 0.00018261940489500811\n",
      "test loss is 0.00026091856057681404\n",
      "Batch: 1300,train loss is: 0.0002088686567658953\n",
      "test loss is 0.0002670125037422468\n",
      "Batch: 1400,train loss is: 0.00024992198841248414\n",
      "test loss is 0.00030304494820973033\n",
      "Batch: 1500,train loss is: 0.0002421482796158879\n",
      "test loss is 0.00027554984309366117\n",
      "Batch: 1600,train loss is: 0.00027477524863781953\n",
      "test loss is 0.0002685477828592073\n",
      "Batch: 1700,train loss is: 0.0004072702293477039\n",
      "test loss is 0.00028834607964405327\n",
      "Batch: 1800,train loss is: 0.00022425773137119844\n",
      "test loss is 0.0003153840598882655\n",
      "Batch: 1900,train loss is: 0.0005058436399104492\n",
      "test loss is 0.00024094464155334773\n",
      "Batch: 2000,train loss is: 0.00032383225710864007\n",
      "test loss is 0.0002558476984448093\n",
      "Batch: 2100,train loss is: 0.0002190500172106331\n",
      "test loss is 0.000315905001371691\n",
      "Batch: 2200,train loss is: 0.00018478737869270852\n",
      "test loss is 0.00026574504970092926\n",
      "Batch: 2300,train loss is: 0.0003465173413445293\n",
      "test loss is 0.000254444685649525\n",
      "Batch: 2400,train loss is: 0.00021190796198633646\n",
      "test loss is 0.000293608607013895\n",
      "Batch: 2500,train loss is: 0.00022430603071913645\n",
      "test loss is 0.00025600777432711565\n",
      "Batch: 2600,train loss is: 0.00024096510129947683\n",
      "test loss is 0.0002637649241280763\n",
      "Batch: 2700,train loss is: 0.0002317503253087538\n",
      "test loss is 0.0002779311334974215\n",
      "Batch: 2800,train loss is: 0.00025322650614639753\n",
      "test loss is 0.00032602650917737256\n",
      "Batch: 2900,train loss is: 0.00017104612958889252\n",
      "test loss is 0.00024859145553972307\n",
      "Batch: 3000,train loss is: 0.0004200903465326129\n",
      "test loss is 0.00031726802578152687\n",
      "Batch: 3100,train loss is: 0.0001675733241245279\n",
      "test loss is 0.00025556319672868054\n",
      "Batch: 3200,train loss is: 0.00021088114945346783\n",
      "test loss is 0.0002955756764950749\n",
      "Batch: 3300,train loss is: 0.00016910430807985476\n",
      "test loss is 0.0002773987950015046\n",
      "Batch: 3400,train loss is: 0.00027927130653116286\n",
      "test loss is 0.000358239292829791\n",
      "Batch: 3500,train loss is: 0.00010838818325502087\n",
      "test loss is 0.0002680082813851793\n",
      "Batch: 3600,train loss is: 0.00019412064654530665\n",
      "test loss is 0.00023085288882505027\n",
      "Batch: 3700,train loss is: 0.00022599249667943887\n",
      "test loss is 0.00038677202524918823\n",
      "Batch: 3800,train loss is: 0.00026237839187941885\n",
      "test loss is 0.00023647312207524245\n",
      "Batch: 3900,train loss is: 0.0003077673738554029\n",
      "test loss is 0.0002688259119883092\n",
      "Batch: 4000,train loss is: 0.00016758937237767475\n",
      "test loss is 0.0003183393403186218\n",
      "Batch: 4100,train loss is: 0.0002911107275471028\n",
      "test loss is 0.00024383887041058122\n",
      "Batch: 4200,train loss is: 0.0002189126866276594\n",
      "test loss is 0.00026018731708418757\n",
      "Batch: 4300,train loss is: 0.0002521056163972628\n",
      "test loss is 0.0002644312153848592\n",
      "Batch: 4400,train loss is: 0.0002937004405964006\n",
      "test loss is 0.00025427476333282346\n",
      "Batch: 4500,train loss is: 0.00023329675297570238\n",
      "test loss is 0.00024981767691983366\n",
      "Batch: 4600,train loss is: 0.00023703563822341473\n",
      "test loss is 0.000259450197380862\n",
      "Batch: 4700,train loss is: 0.0002112607944868342\n",
      "test loss is 0.0002584816150489478\n",
      "Batch: 4800,train loss is: 0.0003625800638581733\n",
      "test loss is 0.00033186424560526867\n",
      "Batch: 4900,train loss is: 0.00020988906327237984\n",
      "test loss is 0.00035366926698839827\n",
      "Batch: 5000,train loss is: 0.0002999250746755722\n",
      "test loss is 0.0002702118862702781\n",
      "Batch: 5100,train loss is: 0.00021194902997470166\n",
      "test loss is 0.00024800917944282935\n",
      "Batch: 5200,train loss is: 0.00020590381755029025\n",
      "test loss is 0.00023618479343312529\n",
      "Batch: 5300,train loss is: 0.0001967195491005476\n",
      "test loss is 0.0002871069164852581\n",
      "Batch: 5400,train loss is: 0.00022076248597641297\n",
      "test loss is 0.00027896256966149165\n",
      "Batch: 5500,train loss is: 0.0002469930770433152\n",
      "test loss is 0.00024332421846610513\n",
      "Batch: 5600,train loss is: 0.00024563396252319237\n",
      "test loss is 0.0002561442998928276\n",
      "Batch: 5700,train loss is: 0.00022114734560477406\n",
      "test loss is 0.00025367699944631156\n",
      "Batch: 5800,train loss is: 0.0006177032920090469\n",
      "test loss is 0.000275996399797561\n",
      "Batch: 5900,train loss is: 0.0003018211021756219\n",
      "test loss is 0.000286766253098919\n",
      "Batch: 6000,train loss is: 0.00025648640583872065\n",
      "test loss is 0.0002589604414845813\n",
      "Batch: 6100,train loss is: 0.0002553934740363472\n",
      "test loss is 0.00029418027761026107\n",
      "Batch: 6200,train loss is: 0.0002255132505286158\n",
      "test loss is 0.0003163195509465783\n",
      "Batch: 6300,train loss is: 0.00025791545102782164\n",
      "test loss is 0.0002672578287522577\n",
      "Batch: 6400,train loss is: 0.00021274959788774057\n",
      "test loss is 0.00023875010860070675\n",
      "Batch: 6500,train loss is: 0.00027222876773236256\n",
      "test loss is 0.0002540036898793487\n",
      "Batch: 6600,train loss is: 0.00032667900265103897\n",
      "test loss is 0.00040935530990647063\n",
      "Batch: 6700,train loss is: 0.0003374431464008519\n",
      "test loss is 0.0002557941458043777\n",
      "Batch: 6800,train loss is: 0.00019411154056582782\n",
      "test loss is 0.0002838432037425791\n",
      "Batch: 6900,train loss is: 0.000256603699464727\n",
      "test loss is 0.00025663318450690854\n",
      "Batch: 7000,train loss is: 0.000276030577245877\n",
      "test loss is 0.0002527400134356553\n",
      "Batch: 7100,train loss is: 0.0003324171032858658\n",
      "test loss is 0.0002661738477334757\n",
      "Batch: 7200,train loss is: 0.0003840554996979469\n",
      "test loss is 0.00025058335062119416\n",
      "Batch: 7300,train loss is: 0.0005395215219144137\n",
      "test loss is 0.00034505049755180917\n",
      "Batch: 7400,train loss is: 0.00020340072735171\n",
      "test loss is 0.0002562098143418623\n",
      "Batch: 7500,train loss is: 0.00022252071284315156\n",
      "test loss is 0.00026300996612602833\n",
      "Batch: 7600,train loss is: 0.0003781575981524331\n",
      "test loss is 0.00027370510860574534\n",
      "Batch: 7700,train loss is: 0.00025214493186844107\n",
      "test loss is 0.0002478162213393114\n",
      "Batch: 7800,train loss is: 0.00031086461760998426\n",
      "test loss is 0.0002499072029665612\n",
      "Batch: 7900,train loss is: 0.00022411250049014935\n",
      "test loss is 0.00024446512348065346\n",
      "Batch: 8000,train loss is: 0.0001456884892327466\n",
      "test loss is 0.00024636441455934725\n",
      "Batch: 8100,train loss is: 0.00021428800604679702\n",
      "test loss is 0.00027553429233052596\n",
      "Batch: 8200,train loss is: 0.0004134472536491187\n",
      "test loss is 0.00028665371076651823\n",
      "Batch: 8300,train loss is: 0.0007833001137046749\n",
      "test loss is 0.00040518500836028807\n",
      "Batch: 8400,train loss is: 0.00033732080759041837\n",
      "test loss is 0.0003298812931773136\n",
      "Batch: 8500,train loss is: 0.0006971944003832523\n",
      "test loss is 0.00025513208105038557\n",
      "Batch: 8600,train loss is: 0.0003135653392222671\n",
      "test loss is 0.0002964638122864733\n",
      "Batch: 8700,train loss is: 0.0002824929517079107\n",
      "test loss is 0.00024395230601240552\n",
      "Batch: 8800,train loss is: 0.0002741755590795228\n",
      "test loss is 0.000266941706019717\n",
      "Batch: 8900,train loss is: 0.00022976829425276662\n",
      "test loss is 0.0002637173815911099\n",
      "Batch: 9000,train loss is: 0.0002950723762520288\n",
      "test loss is 0.0002616725188868046\n",
      "Batch: 9100,train loss is: 0.0002764344094542444\n",
      "test loss is 0.00023694157836099254\n",
      "Batch: 9200,train loss is: 0.00022641521580982042\n",
      "test loss is 0.00027680539320617945\n",
      "Batch: 9300,train loss is: 0.0002656624177615368\n",
      "test loss is 0.00026985718552768347\n",
      "Batch: 9400,train loss is: 0.0002248829283147717\n",
      "test loss is 0.000249230171885006\n",
      "Batch: 9500,train loss is: 0.00029882069714148613\n",
      "test loss is 0.0002770354027469703\n",
      "Batch: 9600,train loss is: 0.0002499959609446724\n",
      "test loss is 0.00026453262833174156\n",
      "Batch: 9700,train loss is: 0.00014562694719539665\n",
      "test loss is 0.0002467240527459296\n",
      "Batch: 9800,train loss is: 0.00026288766726569957\n",
      "test loss is 0.0002465834260527901\n",
      "Batch: 9900,train loss is: 0.00018848863671188672\n",
      "test loss is 0.00024984582271055046\n",
      "Batch: 10000,train loss is: 0.0005767168380800965\n",
      "test loss is 0.00028888593645731833\n",
      "Batch: 10100,train loss is: 0.00042838854491993646\n",
      "test loss is 0.00028807410916589424\n",
      "Batch: 10200,train loss is: 0.0002308726349877256\n",
      "test loss is 0.00028452083986916203\n",
      "Batch: 10300,train loss is: 0.00021162119387013137\n",
      "test loss is 0.00028065448862645486\n",
      "Batch: 10400,train loss is: 0.00033865373654013765\n",
      "test loss is 0.00026865397536567033\n",
      "Batch: 10500,train loss is: 0.0002533008095721735\n",
      "test loss is 0.0002589051913454852\n",
      "Batch: 10600,train loss is: 0.00028749091968701735\n",
      "test loss is 0.00025760333538794324\n",
      "Batch: 10700,train loss is: 0.00023416192907266863\n",
      "test loss is 0.0002980890694984669\n",
      "Batch: 10800,train loss is: 0.00022791581006681102\n",
      "test loss is 0.00029222268034337476\n",
      "Batch: 10900,train loss is: 0.0004157887635047404\n",
      "test loss is 0.00027070225676606354\n",
      "Batch: 11000,train loss is: 0.00027602162288257787\n",
      "test loss is 0.0003354144016432141\n",
      "Batch: 11100,train loss is: 0.00014634401789946134\n",
      "test loss is 0.00028749850703364295\n",
      "Batch: 11200,train loss is: 0.00037718910012643595\n",
      "test loss is 0.0002576967269757792\n",
      "Batch: 11300,train loss is: 0.00016547526463794632\n",
      "test loss is 0.0002442885177986538\n",
      "Batch: 11400,train loss is: 0.00012914753417812006\n",
      "test loss is 0.00025139012447570057\n",
      "Batch: 11500,train loss is: 0.0004297051059139588\n",
      "test loss is 0.0003803524351703289\n",
      "Batch: 11600,train loss is: 0.00023218023002259043\n",
      "test loss is 0.0002645947896002748\n",
      "Batch: 11700,train loss is: 0.0001629821465671214\n",
      "test loss is 0.0002705310203962636\n",
      "Batch: 11800,train loss is: 0.00023360653982612475\n",
      "test loss is 0.0002582619709215083\n",
      "Batch: 11900,train loss is: 0.0006318810035470178\n",
      "test loss is 0.00025919642679673213\n",
      "Batch: 12000,train loss is: 0.00033511749571942375\n",
      "test loss is 0.0002984966260329408\n",
      "Batch: 12100,train loss is: 0.00020408148141713787\n",
      "test loss is 0.00024259808203306349\n",
      "Batch: 12200,train loss is: 0.0002845525126249927\n",
      "test loss is 0.00027492306597466014\n",
      "Batch: 12300,train loss is: 0.0001824075964766555\n",
      "test loss is 0.00028627759870571574\n",
      "Batch: 12400,train loss is: 0.0002252635977625295\n",
      "test loss is 0.00024361428063337804\n",
      "Batch: 12500,train loss is: 0.0003095744506411031\n",
      "test loss is 0.0002843241168616639\n",
      "Batch: 12600,train loss is: 0.00023265175309628803\n",
      "test loss is 0.00031563754643003806\n",
      "Batch: 12700,train loss is: 0.00023654317525157755\n",
      "test loss is 0.000290327499609687\n",
      "Batch: 12800,train loss is: 0.0001486479530188027\n",
      "test loss is 0.0002527914425443318\n",
      "Batch: 12900,train loss is: 0.0003061861860024116\n",
      "test loss is 0.000274936756525887\n",
      "Batch: 13000,train loss is: 0.0002423201205187421\n",
      "test loss is 0.0002521956638858005\n",
      "Batch: 13100,train loss is: 0.00021977853670228088\n",
      "test loss is 0.0002563774757538121\n",
      "Batch: 13200,train loss is: 0.00040963777338420625\n",
      "test loss is 0.0003165769589939872\n",
      "Batch: 13300,train loss is: 0.0005596962247273099\n",
      "test loss is 0.00036420667492514166\n",
      "Batch: 13400,train loss is: 0.0001917238155226939\n",
      "test loss is 0.00023241500064380972\n",
      "Batch: 13500,train loss is: 0.00025071578655653907\n",
      "test loss is 0.000239860488921727\n",
      "Batch: 13600,train loss is: 0.0003714060549310442\n",
      "test loss is 0.00028702391493302915\n",
      "Batch: 13700,train loss is: 0.00022111272265164643\n",
      "test loss is 0.00026259086520971994\n",
      "Batch: 13800,train loss is: 0.0003090891097992614\n",
      "test loss is 0.0003284720887289924\n",
      "Batch: 13900,train loss is: 0.0002350312959758765\n",
      "test loss is 0.00023030536433644294\n",
      "Batch: 14000,train loss is: 0.00021782483187210715\n",
      "test loss is 0.00026596943385344727\n",
      "Batch: 14100,train loss is: 0.00033175622994763634\n",
      "test loss is 0.00026070406695304494\n",
      "Batch: 14200,train loss is: 0.0005153453556996935\n",
      "test loss is 0.0002769810667826471\n",
      "Batch: 14300,train loss is: 0.000183567989790558\n",
      "test loss is 0.00025686354262702813\n",
      "Batch: 14400,train loss is: 0.00025025037680848403\n",
      "test loss is 0.00023790963375966437\n",
      "Batch: 14500,train loss is: 0.00044849144749354057\n",
      "test loss is 0.0002749090050326031\n",
      "Batch: 14600,train loss is: 0.0002129439851112706\n",
      "test loss is 0.0002785198273071483\n",
      "Batch: 14700,train loss is: 0.00026486609420593157\n",
      "test loss is 0.0003194270314149347\n",
      "Batch: 14800,train loss is: 0.0001926806442807069\n",
      "test loss is 0.00023454470247220939\n",
      "Batch: 14900,train loss is: 0.00046623186455742473\n",
      "test loss is 0.00023919471180105294\n",
      "Batch: 15000,train loss is: 0.0002694684866378285\n",
      "test loss is 0.00029897408543049685\n",
      "Batch: 15100,train loss is: 0.0002041045620574065\n",
      "test loss is 0.0002483331814082397\n",
      "Batch: 15200,train loss is: 0.0002064104236326581\n",
      "test loss is 0.0002582572191029212\n",
      "Batch: 15300,train loss is: 0.00026134253262535205\n",
      "test loss is 0.00030924870165339026\n",
      "Batch: 15400,train loss is: 0.00020354351978060727\n",
      "test loss is 0.0002624607430823349\n",
      "Batch: 15500,train loss is: 0.00016321011155068444\n",
      "test loss is 0.0002714114725947548\n",
      "Batch: 15600,train loss is: 0.0002539350840391609\n",
      "test loss is 0.0002726040443334767\n",
      "Batch: 15700,train loss is: 0.0002717903199145075\n",
      "test loss is 0.0003413848767849892\n",
      "Batch: 15800,train loss is: 0.00021832431028408607\n",
      "test loss is 0.00024655966557635774\n",
      "Batch: 15900,train loss is: 0.0002060718583746952\n",
      "test loss is 0.0002673524984263664\n",
      "Batch: 16000,train loss is: 0.00027512738195007173\n",
      "test loss is 0.00030486549459161536\n",
      "Batch: 16100,train loss is: 0.00019382586812538197\n",
      "test loss is 0.00025492280308658333\n",
      "Batch: 16200,train loss is: 0.0002611131741423117\n",
      "test loss is 0.00033883275648997815\n",
      "Batch: 16300,train loss is: 0.0002548800021091634\n",
      "test loss is 0.00024128811749083245\n",
      "Batch: 16400,train loss is: 0.0001884187560428347\n",
      "test loss is 0.00026204973156542655\n",
      "Batch: 16500,train loss is: 0.0002174975365979974\n",
      "test loss is 0.000258769190900404\n",
      "Batch: 16600,train loss is: 0.00023688048592710603\n",
      "test loss is 0.0002701596724328554\n",
      "Batch: 16700,train loss is: 0.0001758970524353539\n",
      "test loss is 0.0002559596549865899\n",
      "Batch: 16800,train loss is: 0.0003007878577107856\n",
      "test loss is 0.0002703430812698877\n",
      "Batch: 16900,train loss is: 0.0002002897150245234\n",
      "test loss is 0.000244229495294483\n",
      "Batch: 17000,train loss is: 0.00036504862264961707\n",
      "test loss is 0.0003176987097360465\n",
      "Batch: 17100,train loss is: 0.000193506173088605\n",
      "test loss is 0.000256009146258954\n",
      "Batch: 17200,train loss is: 0.00019297942105110826\n",
      "test loss is 0.0002717566495256666\n",
      "Batch: 17300,train loss is: 0.00025960739674366204\n",
      "test loss is 0.00026272094005658057\n",
      "Batch: 17400,train loss is: 0.00041121216217259924\n",
      "test loss is 0.0002848106792551467\n",
      "Batch: 17500,train loss is: 0.0003090586259303466\n",
      "test loss is 0.00029414017359227166\n",
      "Batch: 17600,train loss is: 0.0002245483532322813\n",
      "test loss is 0.0002496849357310155\n",
      "Batch: 17700,train loss is: 0.0003077050281513531\n",
      "test loss is 0.0003027044024405537\n",
      "Batch: 17800,train loss is: 0.00036435680677919134\n",
      "test loss is 0.00025496384349455364\n",
      "Batch: 17900,train loss is: 0.0002925361520965257\n",
      "test loss is 0.0002664415010621451\n",
      "Batch: 18000,train loss is: 0.000173476073821601\n",
      "test loss is 0.0002839909751474293\n",
      "Batch: 18100,train loss is: 0.00023667816436291457\n",
      "test loss is 0.00023600994773270335\n",
      "Batch: 18200,train loss is: 0.0002024225164149234\n",
      "test loss is 0.00024236769230367669\n",
      "Batch: 18300,train loss is: 0.0002469182026939558\n",
      "test loss is 0.0002738895935460102\n",
      "Batch: 18400,train loss is: 0.0002886371553319308\n",
      "test loss is 0.00025118383243066285\n",
      "Batch: 18500,train loss is: 0.00016819856056385187\n",
      "test loss is 0.000229499599295534\n",
      "Batch: 18600,train loss is: 0.00021754465783676826\n",
      "test loss is 0.0002790814446348715\n",
      "Batch: 18700,train loss is: 0.00019542741808876085\n",
      "test loss is 0.00023366897481348376\n",
      "Batch: 18800,train loss is: 0.00016026859268808056\n",
      "test loss is 0.00027022968069590793\n",
      "Batch: 18900,train loss is: 0.00020882970687096996\n",
      "test loss is 0.000283181503804971\n",
      "Batch: 19000,train loss is: 0.0004989397551025261\n",
      "test loss is 0.00025749697294366055\n",
      "Batch: 19100,train loss is: 0.00026108218302684855\n",
      "test loss is 0.0002835517248599224\n",
      "Batch: 19200,train loss is: 0.00015408981382767525\n",
      "test loss is 0.00026216152139430464\n",
      "Batch: 19300,train loss is: 0.0002773468461015076\n",
      "test loss is 0.00026342348685137875\n",
      "Batch: 19400,train loss is: 0.00019384130888334549\n",
      "test loss is 0.0002407587921380108\n",
      "Batch: 19500,train loss is: 0.00028129433530680524\n",
      "test loss is 0.0003183302114010948\n",
      "Batch: 19600,train loss is: 0.00017037257358254392\n",
      "test loss is 0.00028554221501505693\n",
      "Batch: 19700,train loss is: 0.0002695681760069164\n",
      "test loss is 0.00025176818112311675\n",
      "Batch: 19800,train loss is: 0.00031792436306484617\n",
      "test loss is 0.00027435018013268224\n",
      "Batch: 19900,train loss is: 0.00018191400617654585\n",
      "test loss is 0.0002617255107433662\n",
      "Batch: 20000,train loss is: 0.0002502034765544459\n",
      "test loss is 0.00027185264678587166\n",
      "Batch: 20100,train loss is: 0.00031389091153670814\n",
      "test loss is 0.0002560078007199092\n",
      "Batch: 20200,train loss is: 0.00026016977291313517\n",
      "test loss is 0.00045874614178664064\n",
      "Batch: 20300,train loss is: 0.0001902288378557864\n",
      "test loss is 0.00027965843502081176\n",
      "Batch: 20400,train loss is: 0.00025029607202247924\n",
      "test loss is 0.0002318650197040457\n",
      "Batch: 20500,train loss is: 0.0002507721472197772\n",
      "test loss is 0.000286339785604317\n",
      "Batch: 20600,train loss is: 0.0003370801694089022\n",
      "test loss is 0.0003005384748338379\n",
      "Batch: 20700,train loss is: 0.00028740079613077707\n",
      "test loss is 0.0002715759092981975\n",
      "Batch: 20800,train loss is: 0.00020723033112149915\n",
      "test loss is 0.00026123654162420835\n",
      "Batch: 20900,train loss is: 0.00028403407367435715\n",
      "test loss is 0.00026886500064795724\n",
      "Batch: 21000,train loss is: 0.00023812181433717337\n",
      "test loss is 0.0002565284652258642\n",
      "Batch: 21100,train loss is: 0.00019828828667420596\n",
      "test loss is 0.00032695211064826457\n",
      "Batch: 21200,train loss is: 0.0002396942981922764\n",
      "test loss is 0.00024080445911387046\n",
      "Batch: 21300,train loss is: 0.00020869717404775064\n",
      "test loss is 0.0002486411713548426\n",
      "Batch: 21400,train loss is: 0.0001784604309845308\n",
      "test loss is 0.00027558790391187703\n",
      "Batch: 21500,train loss is: 0.0002947234991855591\n",
      "test loss is 0.0004410556216418732\n",
      "Batch: 21600,train loss is: 0.00011355621994589789\n",
      "test loss is 0.00023584533145193262\n",
      "Batch: 21700,train loss is: 0.00018045254089572936\n",
      "test loss is 0.00024091241230145424\n",
      "Batch: 21800,train loss is: 0.0004478097798156509\n",
      "test loss is 0.0002460013495588492\n",
      "Batch: 21900,train loss is: 0.00019824410733181779\n",
      "test loss is 0.0002614650436162894\n",
      "Batch: 22000,train loss is: 0.00023013578435232364\n",
      "test loss is 0.00030225103461397815\n",
      "Batch: 22100,train loss is: 0.00032949046568330246\n",
      "test loss is 0.0002769589689255429\n",
      "Batch: 22200,train loss is: 0.00034152505004821256\n",
      "test loss is 0.00027278731680431854\n",
      "Batch: 22300,train loss is: 0.0003008884925699003\n",
      "test loss is 0.00023595401908098454\n",
      "Batch: 22400,train loss is: 0.0002683495689110882\n",
      "test loss is 0.00024380371517968295\n",
      "Batch: 22500,train loss is: 0.00024567271664057474\n",
      "test loss is 0.0002749109185395633\n",
      "Batch: 22600,train loss is: 0.0003468577459606265\n",
      "test loss is 0.00023348794481002866\n",
      "Batch: 22700,train loss is: 0.00025835358335760724\n",
      "test loss is 0.00026842246962225405\n",
      "Batch: 22800,train loss is: 0.0002925925329819574\n",
      "test loss is 0.00022679889667684013\n",
      "Batch: 22900,train loss is: 0.0003611613438098772\n",
      "test loss is 0.0003060262232959637\n",
      "Batch: 23000,train loss is: 0.00031291283513591316\n",
      "test loss is 0.0002867945115314878\n",
      "Batch: 23100,train loss is: 0.0003469442167954459\n",
      "test loss is 0.000246828592433075\n",
      "Batch: 23200,train loss is: 0.0002918805492799129\n",
      "test loss is 0.00027806206272932047\n",
      "Batch: 23300,train loss is: 0.0002573465541027137\n",
      "test loss is 0.00027914967703463595\n",
      "Batch: 23400,train loss is: 0.00032412808381637016\n",
      "test loss is 0.00025737410356787324\n",
      "Batch: 23500,train loss is: 0.0001781593818264312\n",
      "test loss is 0.0002478423850430218\n",
      "Batch: 23600,train loss is: 0.0003280630080294559\n",
      "test loss is 0.00027364481119824516\n",
      "Batch: 23700,train loss is: 0.00022569104642640388\n",
      "test loss is 0.00023955161623352114\n",
      "Batch: 23800,train loss is: 0.0001510518760624783\n",
      "test loss is 0.0002670590983031689\n",
      "Batch: 23900,train loss is: 0.00021328690700897127\n",
      "test loss is 0.0002890656566808537\n",
      "Batch: 24000,train loss is: 0.0002841952904499746\n",
      "test loss is 0.00025336691797779457\n",
      "Batch: 24100,train loss is: 0.0003458603161552239\n",
      "test loss is 0.0004063713946338599\n",
      "Batch: 24200,train loss is: 0.0003409485005902629\n",
      "test loss is 0.0003463438699496928\n",
      "Batch: 24300,train loss is: 0.00019491575539789315\n",
      "test loss is 0.000264061227198985\n",
      "Batch: 24400,train loss is: 0.0001754062488713617\n",
      "test loss is 0.00023778815759561525\n",
      "Batch: 24500,train loss is: 0.0001972990603715319\n",
      "test loss is 0.00027595626118957783\n",
      "Batch: 24600,train loss is: 0.0002905097405739655\n",
      "test loss is 0.00032590895366725473\n",
      "Batch: 24700,train loss is: 0.0002272674813985951\n",
      "test loss is 0.00024815724863897225\n",
      "Batch: 24800,train loss is: 0.00016795482658891087\n",
      "test loss is 0.00027286607847733577\n",
      "Batch: 24900,train loss is: 0.00024234303446071188\n",
      "test loss is 0.00031317015023785594\n",
      "Batch: 25000,train loss is: 0.00027296205210138003\n",
      "test loss is 0.0002383898414335789\n",
      "Batch: 25100,train loss is: 0.00024391285960089705\n",
      "test loss is 0.00023609135740806515\n",
      "Batch: 25200,train loss is: 0.0003022999454597472\n",
      "test loss is 0.0002584955680575604\n",
      "Batch: 25300,train loss is: 0.00026658735816912656\n",
      "test loss is 0.0002611547182889256\n",
      "Batch: 25400,train loss is: 0.0001568096420628672\n",
      "test loss is 0.00026012563929681546\n",
      "Batch: 25500,train loss is: 0.00018421239594443645\n",
      "test loss is 0.00040075453197728267\n",
      "Batch: 25600,train loss is: 0.00021582959921464438\n",
      "test loss is 0.00025390527776381923\n",
      "Batch: 25700,train loss is: 0.00030679478824711015\n",
      "test loss is 0.000253009863026404\n",
      "Batch: 25800,train loss is: 0.00023638448242381172\n",
      "test loss is 0.00025412737792504243\n",
      "Batch: 25900,train loss is: 0.0003476718703142701\n",
      "test loss is 0.0002728762678988475\n",
      "Batch: 26000,train loss is: 0.0003150250443720375\n",
      "test loss is 0.00032916064285091605\n",
      "Batch: 26100,train loss is: 0.00016635021956591363\n",
      "test loss is 0.0002507294311676956\n",
      "Batch: 26200,train loss is: 0.0001529988352028235\n",
      "test loss is 0.00024111595775253274\n",
      "Batch: 26300,train loss is: 0.0003178885972363214\n",
      "test loss is 0.0002751832078417055\n",
      "Batch: 26400,train loss is: 0.0001605317145868308\n",
      "test loss is 0.00023180878951452487\n",
      "Batch: 26500,train loss is: 0.00022006352461233694\n",
      "test loss is 0.00024129660012656867\n",
      "Batch: 26600,train loss is: 0.00029198031951344235\n",
      "test loss is 0.00026739556408016764\n",
      "Batch: 26700,train loss is: 0.0002233144341806386\n",
      "test loss is 0.0002487078346151754\n",
      "Batch: 26800,train loss is: 0.0003273185586192175\n",
      "test loss is 0.0003161651141478641\n",
      "Batch: 26900,train loss is: 0.0002406619124330478\n",
      "test loss is 0.00026700449451931254\n",
      "Batch: 27000,train loss is: 0.0003586454350413391\n",
      "test loss is 0.00026185966442357175\n",
      "Batch: 27100,train loss is: 0.0002493345072253515\n",
      "test loss is 0.0003004265179661061\n",
      "Batch: 27200,train loss is: 0.00022377180085553645\n",
      "test loss is 0.00027358893714033784\n",
      "Batch: 27300,train loss is: 0.0003702119655482959\n",
      "test loss is 0.00029253550780406135\n",
      "Batch: 27400,train loss is: 0.0002515084422686693\n",
      "test loss is 0.0002934482302976522\n",
      "Batch: 27500,train loss is: 0.00018533601814864202\n",
      "test loss is 0.0002668602675280447\n",
      "Batch: 27600,train loss is: 0.00020281308247206215\n",
      "test loss is 0.00027234617327910886\n",
      "Batch: 27700,train loss is: 0.0002138783912157407\n",
      "test loss is 0.00024217165679897915\n",
      "Batch: 27800,train loss is: 0.000267107951343834\n",
      "test loss is 0.00029155781606265724\n",
      "Batch: 27900,train loss is: 0.00016631432733149132\n",
      "test loss is 0.0002708133766116376\n",
      "Batch: 28000,train loss is: 0.00021522518393415633\n",
      "test loss is 0.0002462812476065889\n",
      "Batch: 28100,train loss is: 0.00021026108354905174\n",
      "test loss is 0.00027965320164336605\n",
      "Batch: 28200,train loss is: 0.00039337373256300486\n",
      "test loss is 0.0002569346676649813\n",
      "Batch: 28300,train loss is: 0.00030819913986642703\n",
      "test loss is 0.00026632834098220344\n",
      "Batch: 28400,train loss is: 0.00015656540244749306\n",
      "test loss is 0.000283624288557674\n",
      "Batch: 28500,train loss is: 0.0003622754623524525\n",
      "test loss is 0.0003175998562210833\n",
      "Batch: 28600,train loss is: 0.0004304737694971235\n",
      "test loss is 0.00027331555464691585\n",
      "Batch: 28700,train loss is: 0.0002338907786422802\n",
      "test loss is 0.000241821704882955\n",
      "Batch: 28800,train loss is: 0.00028399267773429024\n",
      "test loss is 0.00027784105965309895\n",
      "Batch: 28900,train loss is: 0.0002886283973076468\n",
      "test loss is 0.0002506391773348766\n",
      "Batch: 29000,train loss is: 0.0001464954854420651\n",
      "test loss is 0.00026399769066774546\n",
      "Batch: 29100,train loss is: 0.00017316786944055634\n",
      "test loss is 0.00031212801718597915\n",
      "Batch: 29200,train loss is: 0.00020057947394937998\n",
      "test loss is 0.00027369151041761514\n",
      "Batch: 29300,train loss is: 0.00028252743568670083\n",
      "test loss is 0.0002838376961543293\n",
      "Batch: 29400,train loss is: 0.0003527236979763742\n",
      "test loss is 0.00026153010191303015\n",
      "Batch: 29500,train loss is: 0.00034582026608916485\n",
      "test loss is 0.00026158600120374144\n",
      "Batch: 29600,train loss is: 0.0002455107424622123\n",
      "test loss is 0.0002920805464891858\n",
      "Batch: 29700,train loss is: 0.00018074264460354487\n",
      "test loss is 0.00027899345832603436\n",
      "Batch: 29800,train loss is: 0.0003352346270300666\n",
      "test loss is 0.0003016251678300796\n",
      "Batch: 29900,train loss is: 0.00021789344508170037\n",
      "test loss is 0.00028107228506339346\n",
      "Batch: 30000,train loss is: 0.00022106160811608992\n",
      "test loss is 0.0002493570296373559\n",
      "Batch: 30100,train loss is: 0.00028344670944223485\n",
      "test loss is 0.0002597401301763132\n",
      "Batch: 30200,train loss is: 0.00024642377940802207\n",
      "test loss is 0.0002549268828374702\n",
      "Batch: 30300,train loss is: 0.00019785472752855776\n",
      "test loss is 0.0003270604091304814\n",
      "Batch: 30400,train loss is: 0.0002055029545280924\n",
      "test loss is 0.00026046176512560076\n",
      "Batch: 30500,train loss is: 0.00017346719175701945\n",
      "test loss is 0.00023903016225347925\n",
      "Batch: 30600,train loss is: 0.00016537271103228934\n",
      "test loss is 0.00025711105114279586\n",
      "Batch: 30700,train loss is: 0.00020292359967195582\n",
      "test loss is 0.0002824966071654379\n",
      "Batch: 30800,train loss is: 0.00030254220131819903\n",
      "test loss is 0.0002711738877690975\n",
      "Batch: 30900,train loss is: 0.00022036237821007894\n",
      "test loss is 0.00023307726995125608\n",
      "Batch: 31000,train loss is: 0.00016147698810805135\n",
      "test loss is 0.0003016781914307027\n",
      "Batch: 31100,train loss is: 0.00020991410466038665\n",
      "test loss is 0.00024146021681324234\n",
      "Batch: 31200,train loss is: 0.00027481715159817873\n",
      "test loss is 0.0002537298273443872\n",
      "Batch: 31300,train loss is: 0.0002801250069532123\n",
      "test loss is 0.00024509556006571236\n",
      "Batch: 31400,train loss is: 0.00019095769778173325\n",
      "test loss is 0.00023906514036684372\n",
      "Batch: 31500,train loss is: 0.00028914039909743865\n",
      "test loss is 0.0002474064388755044\n",
      "Batch: 31600,train loss is: 0.000181899598916362\n",
      "test loss is 0.00023409212697381542\n",
      "Batch: 31700,train loss is: 0.00024851885546779603\n",
      "test loss is 0.0002717641070276352\n",
      "Batch: 31800,train loss is: 0.0002740938586482661\n",
      "test loss is 0.00027000689310907475\n",
      "Batch: 31900,train loss is: 0.00023371369124154442\n",
      "test loss is 0.0002544862422772629\n",
      "Batch: 32000,train loss is: 0.0003401382188953775\n",
      "test loss is 0.0003004601183508582\n",
      "Batch: 32100,train loss is: 0.0003927403813673738\n",
      "test loss is 0.0002449619367791148\n",
      "Batch: 32200,train loss is: 0.0004041906793872017\n",
      "test loss is 0.00023953763284082635\n",
      "Batch: 32300,train loss is: 0.00028005069691276936\n",
      "test loss is 0.000240704946235886\n",
      "Batch: 32400,train loss is: 0.00046119510060904856\n",
      "test loss is 0.0002412942287249204\n",
      "Batch: 32500,train loss is: 0.0002499184468896049\n",
      "test loss is 0.00027354254715810007\n",
      "Batch: 32600,train loss is: 0.0002413759033911348\n",
      "test loss is 0.0002561983019397541\n",
      "Batch: 32700,train loss is: 0.00021551833076321786\n",
      "test loss is 0.00022991487620244784\n",
      "Batch: 32800,train loss is: 0.00030098993846111756\n",
      "test loss is 0.0002745309136637979\n",
      "Batch: 32900,train loss is: 0.00017558056146818816\n",
      "test loss is 0.00024962771938589524\n",
      "Batch: 33000,train loss is: 0.00023394742795984955\n",
      "test loss is 0.0003246198214993534\n",
      "Batch: 33100,train loss is: 0.00018224135626354393\n",
      "test loss is 0.0002619067493573913\n",
      "Batch: 33200,train loss is: 0.00019580065681550627\n",
      "test loss is 0.00026760636540052113\n",
      "Batch: 33300,train loss is: 0.0003520802006691071\n",
      "test loss is 0.0002877794119610854\n",
      "Batch: 33400,train loss is: 0.00017814345374923937\n",
      "test loss is 0.00024275326245868623\n",
      "Batch: 33500,train loss is: 0.00017982472834827512\n",
      "test loss is 0.00023800646822915968\n",
      "Batch: 33600,train loss is: 0.00018458966307697113\n",
      "test loss is 0.00024416872357554346\n",
      "Batch: 33700,train loss is: 0.0002465924103711187\n",
      "test loss is 0.00028159960165621584\n",
      "Batch: 33800,train loss is: 0.00021078690633078925\n",
      "test loss is 0.00028328315145890944\n",
      "Batch: 33900,train loss is: 0.00014492570685084206\n",
      "test loss is 0.000246050904689364\n",
      "Batch: 34000,train loss is: 0.00018628692289292611\n",
      "test loss is 0.000290746611683611\n",
      "Batch: 34100,train loss is: 0.0001224195707902772\n",
      "test loss is 0.00027734529779901057\n",
      "Batch: 34200,train loss is: 0.0002549677738163677\n",
      "test loss is 0.00029028517059302516\n",
      "Batch: 34300,train loss is: 0.0002494188170119612\n",
      "test loss is 0.0002695329550087959\n",
      "Batch: 34400,train loss is: 0.00016669768640448001\n",
      "test loss is 0.00033832467620279307\n",
      "Batch: 34500,train loss is: 0.0002339212927167465\n",
      "test loss is 0.00029887927128570033\n",
      "Batch: 34600,train loss is: 0.00017631436312166512\n",
      "test loss is 0.00025281623679373143\n",
      "Batch: 34700,train loss is: 0.0002043638131442769\n",
      "test loss is 0.0002938185014133261\n",
      "Batch: 34800,train loss is: 0.00017302421054444101\n",
      "test loss is 0.0002585514467536936\n",
      "Batch: 34900,train loss is: 0.00022390502315325242\n",
      "test loss is 0.00024052397460493658\n",
      "Batch: 35000,train loss is: 0.00031544409885721545\n",
      "test loss is 0.00030607969606227244\n",
      "Batch: 35100,train loss is: 0.0002215387282993911\n",
      "test loss is 0.0002652525716689605\n",
      "Batch: 35200,train loss is: 0.00022871140972708203\n",
      "test loss is 0.00024309184801116942\n",
      "Batch: 35300,train loss is: 0.0002730030265285051\n",
      "test loss is 0.00024782440380384264\n",
      "Batch: 35400,train loss is: 0.00023390956812711515\n",
      "test loss is 0.0003008918585810098\n",
      "Batch: 35500,train loss is: 0.0003720592664254324\n",
      "test loss is 0.0002450059181626676\n",
      "Batch: 35600,train loss is: 0.0002973889168326796\n",
      "test loss is 0.0002828601334950249\n",
      "Batch: 35700,train loss is: 0.0002776647222521709\n",
      "test loss is 0.00023740616834339102\n",
      "Batch: 35800,train loss is: 0.0001925728197771668\n",
      "test loss is 0.00026384495287126437\n",
      "Batch: 35900,train loss is: 0.0001474391677244628\n",
      "test loss is 0.00023993352711110656\n",
      "Batch: 36000,train loss is: 0.00034078109933018846\n",
      "test loss is 0.00023022809469236082\n",
      "Batch: 36100,train loss is: 0.0002669045572185862\n",
      "test loss is 0.0002590457530904704\n",
      "Batch: 36200,train loss is: 0.00020006964081778963\n",
      "test loss is 0.0002882706018793996\n",
      "Batch: 36300,train loss is: 0.00018751990785616544\n",
      "test loss is 0.00026593712468045064\n",
      "Batch: 36400,train loss is: 0.0003083092280922628\n",
      "test loss is 0.00025685942282830933\n",
      "Batch: 36500,train loss is: 0.00017452211211250573\n",
      "test loss is 0.00024338931595992982\n",
      "Batch: 36600,train loss is: 0.00026540112586707226\n",
      "test loss is 0.00026097200464692607\n",
      "Batch: 36700,train loss is: 0.00031190947325110283\n",
      "test loss is 0.0002828973641675689\n",
      "Batch: 36800,train loss is: 0.00023283530472952138\n",
      "test loss is 0.00023517230483904834\n",
      "Batch: 36900,train loss is: 0.00032989273498728314\n",
      "test loss is 0.0002688970691453\n",
      "Batch: 37000,train loss is: 0.00016321868715522033\n",
      "test loss is 0.0002854783774945455\n",
      "Batch: 37100,train loss is: 0.0003912180155005999\n",
      "test loss is 0.0002653574155797548\n",
      "Batch: 37200,train loss is: 0.00031232338593453513\n",
      "test loss is 0.00025604622034653275\n",
      "Batch: 37300,train loss is: 0.00017263850811087878\n",
      "test loss is 0.00023209930228522397\n",
      "Batch: 37400,train loss is: 0.0002342091508200937\n",
      "test loss is 0.000242045093382707\n",
      "Batch: 37500,train loss is: 0.00025486104129642906\n",
      "test loss is 0.00024751439987576217\n",
      "Batch: 37600,train loss is: 0.0003183049425978668\n",
      "test loss is 0.0002957113438799045\n",
      "Batch: 37700,train loss is: 0.00019645041978326385\n",
      "test loss is 0.00024053260897934528\n",
      "Batch: 37800,train loss is: 0.0002366027368751621\n",
      "test loss is 0.00025399561953288053\n",
      "Batch: 37900,train loss is: 0.0003226725773350354\n",
      "test loss is 0.00027635078677915986\n",
      "Batch: 38000,train loss is: 0.0001716253276398088\n",
      "test loss is 0.00025891860521354493\n",
      "Batch: 38100,train loss is: 0.00021018259321615155\n",
      "test loss is 0.00023844753925271806\n",
      "Batch: 38200,train loss is: 0.00026690823046349806\n",
      "test loss is 0.000272105815485291\n",
      "Batch: 38300,train loss is: 0.00018693539375432133\n",
      "test loss is 0.0002565728729923863\n",
      "Batch: 38400,train loss is: 0.0003506648092431469\n",
      "test loss is 0.00025771132308185986\n",
      "Batch: 38500,train loss is: 0.00036198732771477863\n",
      "test loss is 0.00034472238530549275\n",
      "Batch: 38600,train loss is: 0.00031417630117710956\n",
      "test loss is 0.0002645525662489299\n",
      "Batch: 38700,train loss is: 0.00030922738629310636\n",
      "test loss is 0.0003213522254965798\n",
      "Batch: 38800,train loss is: 0.0005321695762662717\n",
      "test loss is 0.00024507384965880095\n",
      "Batch: 38900,train loss is: 0.00023322831008119193\n",
      "test loss is 0.0002882370780085622\n",
      "Batch: 39000,train loss is: 0.00024765092954353256\n",
      "test loss is 0.0002582116126908849\n",
      "Batch: 39100,train loss is: 0.00022311720287960138\n",
      "test loss is 0.00029254023664490695\n",
      "Batch: 39200,train loss is: 0.00018195468594488532\n",
      "test loss is 0.0002517600515108526\n",
      "Batch: 39300,train loss is: 0.00039768022845584504\n",
      "test loss is 0.0002728378492082471\n",
      "Batch: 39400,train loss is: 0.0003865230577488963\n",
      "test loss is 0.0002668595026204644\n",
      "Batch: 39500,train loss is: 0.00042179530662236973\n",
      "test loss is 0.00037491040635193176\n",
      "Batch: 39600,train loss is: 0.0002401633767898039\n",
      "test loss is 0.00023944816731811663\n",
      "Batch: 39700,train loss is: 0.0002612118240165896\n",
      "test loss is 0.00026066397229408493\n",
      "Batch: 39800,train loss is: 0.0002550438408992347\n",
      "test loss is 0.00025011681410941496\n",
      "Batch: 39900,train loss is: 0.0003177039725639846\n",
      "test loss is 0.00033055336267320227\n",
      "Batch: 40000,train loss is: 0.0003108068607586382\n",
      "test loss is 0.00026240730412949815\n",
      "Batch: 40100,train loss is: 0.00014774820553717984\n",
      "test loss is 0.0002612894519192655\n",
      "Batch: 40200,train loss is: 0.0003741149138632205\n",
      "test loss is 0.00025709312088666907\n",
      "Batch: 40300,train loss is: 0.000290329310293237\n",
      "test loss is 0.0003197327662211937\n",
      "Batch: 40400,train loss is: 0.00019706996046947025\n",
      "test loss is 0.00027073028721437046\n",
      "Batch: 40500,train loss is: 0.0001942368410161145\n",
      "test loss is 0.0002460161335770946\n",
      "Batch: 40600,train loss is: 0.0002566857965489431\n",
      "test loss is 0.00023902525241838323\n",
      "Batch: 40700,train loss is: 0.00031761419718193834\n",
      "test loss is 0.00025964683281123213\n",
      "Batch: 40800,train loss is: 0.00020104754762474968\n",
      "test loss is 0.0003081470775762795\n",
      "Batch: 40900,train loss is: 0.0002897017165619127\n",
      "test loss is 0.00030116322909369667\n",
      "Batch: 41000,train loss is: 0.0003706190526393631\n",
      "test loss is 0.00025221203780542896\n",
      "Batch: 41100,train loss is: 0.00026870774753379704\n",
      "test loss is 0.0002573481649764636\n",
      "Batch: 41200,train loss is: 0.0001979726644704789\n",
      "test loss is 0.00024531629518157724\n",
      "Batch: 41300,train loss is: 0.00022131954549377245\n",
      "test loss is 0.00024903553566069114\n",
      "Batch: 41400,train loss is: 0.00020780345347894304\n",
      "test loss is 0.000275237551862735\n",
      "Batch: 41500,train loss is: 0.00030170075494722395\n",
      "test loss is 0.0002972804719922692\n",
      "Batch: 41600,train loss is: 0.00021636330591640838\n",
      "test loss is 0.00030867654421192685\n",
      "Batch: 41700,train loss is: 0.0002942264432072297\n",
      "test loss is 0.00034495002284505816\n",
      "Batch: 41800,train loss is: 0.00019118368580947888\n",
      "test loss is 0.00026989834263182815\n",
      "Batch: 41900,train loss is: 0.0002203480736006716\n",
      "test loss is 0.00027828499198140165\n",
      "Batch: 42000,train loss is: 0.00017951069222975314\n",
      "test loss is 0.00026578752903773806\n",
      "Batch: 42100,train loss is: 0.00020094884225131063\n",
      "test loss is 0.00026508375847350904\n",
      "Batch: 42200,train loss is: 0.0002362597541538892\n",
      "test loss is 0.00027372121284171717\n",
      "Batch: 42300,train loss is: 0.00021606397640817115\n",
      "test loss is 0.00027140692052959165\n",
      "Batch: 42400,train loss is: 0.0002253573537140025\n",
      "test loss is 0.0002431038425108223\n",
      "Batch: 42500,train loss is: 0.0002452855265977057\n",
      "test loss is 0.0002469171651266389\n",
      "Batch: 42600,train loss is: 0.00023047093179805054\n",
      "test loss is 0.0002453489949125932\n",
      "Batch: 42700,train loss is: 0.0002705681311222847\n",
      "test loss is 0.00024566037143115615\n",
      "Batch: 42800,train loss is: 0.00017719582495365053\n",
      "test loss is 0.00027426893985580254\n",
      "Batch: 42900,train loss is: 0.00030260523864589147\n",
      "test loss is 0.00026018418049822455\n",
      "Batch: 43000,train loss is: 0.00015809893736427788\n",
      "test loss is 0.0002582389487001994\n",
      "Batch: 43100,train loss is: 0.00027072205497864267\n",
      "test loss is 0.00028035370909888416\n",
      "Batch: 43200,train loss is: 0.0003847884294728782\n",
      "test loss is 0.0003002645967097161\n",
      "Batch: 43300,train loss is: 0.00016893670265449463\n",
      "test loss is 0.00031065367986415866\n",
      "Batch: 43400,train loss is: 0.00015385295827619478\n",
      "test loss is 0.000260002008633016\n",
      "Batch: 43500,train loss is: 0.0003408883817804564\n",
      "test loss is 0.00028068407028401836\n",
      "Batch: 43600,train loss is: 0.0004058680471426859\n",
      "test loss is 0.0002518889827931555\n",
      "Batch: 43700,train loss is: 0.00024822339410066974\n",
      "test loss is 0.0002447228571166335\n",
      "Batch: 43800,train loss is: 0.00021126100789051948\n",
      "test loss is 0.0002453193540217969\n",
      "Batch: 43900,train loss is: 0.00020880314766327215\n",
      "test loss is 0.00027373775574735784\n",
      "Batch: 44000,train loss is: 0.00022555875977181566\n",
      "test loss is 0.0002640790157874034\n",
      "Batch: 44100,train loss is: 0.00024321389806854465\n",
      "test loss is 0.00026369048024328656\n",
      "Batch: 44200,train loss is: 0.0003709493946909341\n",
      "test loss is 0.000261981369902323\n",
      "Batch: 44300,train loss is: 0.00020094516732223214\n",
      "test loss is 0.0003014427861593682\n",
      "Batch: 44400,train loss is: 0.0004616216133708857\n",
      "test loss is 0.00025630459601519597\n",
      "Batch: 44500,train loss is: 0.00022542852344829714\n",
      "test loss is 0.0002789035394385749\n",
      "Batch: 44600,train loss is: 0.0001871743182389554\n",
      "test loss is 0.00024443940140543276\n",
      "Batch: 44700,train loss is: 0.00013324875359425854\n",
      "test loss is 0.00027639530646144034\n",
      "Batch: 44800,train loss is: 0.00021069138487952652\n",
      "test loss is 0.00024400019445702453\n",
      "Batch: 44900,train loss is: 0.00012143274796333793\n",
      "test loss is 0.0002519346039525691\n",
      "Batch: 45000,train loss is: 0.00014771321525530306\n",
      "test loss is 0.0002501533903861443\n",
      "Batch: 45100,train loss is: 0.0002852330948388118\n",
      "test loss is 0.00024748276928626814\n",
      "Batch: 45200,train loss is: 0.0001846067738147093\n",
      "test loss is 0.00024194886002195843\n",
      "Batch: 45300,train loss is: 0.00020561881249839384\n",
      "test loss is 0.00029360501946666375\n",
      "Batch: 45400,train loss is: 0.0002835512339380192\n",
      "test loss is 0.0002918640237796828\n",
      "Batch: 45500,train loss is: 0.00021337288586671473\n",
      "test loss is 0.000254826495598775\n",
      "Batch: 45600,train loss is: 0.00018535174360121448\n",
      "test loss is 0.00028627204347381603\n",
      "Batch: 45700,train loss is: 0.0005256090450876645\n",
      "test loss is 0.00036043885224767687\n",
      "Batch: 45800,train loss is: 0.00046085984398916457\n",
      "test loss is 0.00032654619279908235\n",
      "Batch: 45900,train loss is: 0.0002972391444842426\n",
      "test loss is 0.00029340275004392487\n",
      "Batch: 46000,train loss is: 0.00012310218850728068\n",
      "test loss is 0.0002971756025149375\n",
      "Batch: 46100,train loss is: 0.0001959172183196365\n",
      "test loss is 0.00025579283316196864\n",
      "Batch: 46200,train loss is: 0.0002944101202251874\n",
      "test loss is 0.0002641426987691852\n",
      "Batch: 46300,train loss is: 0.00029843776555856736\n",
      "test loss is 0.00025499964447068483\n",
      "Batch: 46400,train loss is: 0.00021083384418895724\n",
      "test loss is 0.0002785600175135047\n",
      "Batch: 46500,train loss is: 0.0002535005813759803\n",
      "test loss is 0.0002629442954421762\n",
      "Batch: 46600,train loss is: 0.0002488419945046896\n",
      "test loss is 0.00023389476238718141\n",
      "Batch: 46700,train loss is: 0.00027280684258479046\n",
      "test loss is 0.00027567281245931404\n",
      "-----------------------Epoch: 11----------------------------------\n",
      "Batch: 0,train loss is: 0.0012643594200324871\n",
      "test loss is 0.00026034035906938687\n",
      "Batch: 100,train loss is: 0.00019684640105807204\n",
      "test loss is 0.0002776185460779596\n",
      "Batch: 200,train loss is: 0.00034698542162843406\n",
      "test loss is 0.0003164226557874747\n",
      "Batch: 300,train loss is: 0.00020764792947841788\n",
      "test loss is 0.0002796101005531181\n",
      "Batch: 400,train loss is: 0.00029052594526817567\n",
      "test loss is 0.0002622371026153752\n",
      "Batch: 500,train loss is: 0.00028357268074209686\n",
      "test loss is 0.0002661263926994619\n",
      "Batch: 600,train loss is: 0.00026152120630573266\n",
      "test loss is 0.0002627464645647189\n",
      "Batch: 700,train loss is: 0.0002353979902964649\n",
      "test loss is 0.00029291349703906665\n",
      "Batch: 800,train loss is: 0.0002274746303655032\n",
      "test loss is 0.0002404857224449919\n",
      "Batch: 900,train loss is: 0.000257648397703972\n",
      "test loss is 0.0002479754156649772\n",
      "Batch: 1000,train loss is: 0.00016248502195449627\n",
      "test loss is 0.00023709777172654115\n",
      "Batch: 1100,train loss is: 0.0002949309349551295\n",
      "test loss is 0.00043246470299144975\n",
      "Batch: 1200,train loss is: 0.00020484790848344038\n",
      "test loss is 0.00025484604767805067\n",
      "Batch: 1300,train loss is: 0.0003518317072344244\n",
      "test loss is 0.0003145409902700198\n",
      "Batch: 1400,train loss is: 0.0002725058064143197\n",
      "test loss is 0.0002955499801667735\n",
      "Batch: 1500,train loss is: 0.00026299535896973027\n",
      "test loss is 0.00031039122986653073\n",
      "Batch: 1600,train loss is: 0.00020523749080298356\n",
      "test loss is 0.0002532495723934581\n",
      "Batch: 1700,train loss is: 0.00024966102382397153\n",
      "test loss is 0.00026893459545454426\n",
      "Batch: 1800,train loss is: 0.00022390252561692498\n",
      "test loss is 0.0002511034138163612\n",
      "Batch: 1900,train loss is: 0.00015960534603534816\n",
      "test loss is 0.0002700683178681042\n",
      "Batch: 2000,train loss is: 0.00024466040600030326\n",
      "test loss is 0.00026133856798861503\n",
      "Batch: 2100,train loss is: 0.0001880315676485348\n",
      "test loss is 0.00024499581008054244\n",
      "Batch: 2200,train loss is: 0.00020985969629056677\n",
      "test loss is 0.0003258580752021142\n",
      "Batch: 2300,train loss is: 0.0001571560749497083\n",
      "test loss is 0.0002776068653648843\n",
      "Batch: 2400,train loss is: 0.00017131726667379333\n",
      "test loss is 0.00027593654363263496\n",
      "Batch: 2500,train loss is: 0.0002725640985620402\n",
      "test loss is 0.000291841024977378\n",
      "Batch: 2600,train loss is: 0.00026416696985609363\n",
      "test loss is 0.0002715940260244529\n",
      "Batch: 2700,train loss is: 0.00017069477385112027\n",
      "test loss is 0.00028126543846339347\n",
      "Batch: 2800,train loss is: 0.00034151024441837546\n",
      "test loss is 0.00024103310131994636\n",
      "Batch: 2900,train loss is: 0.00021875971330124382\n",
      "test loss is 0.00023935681052781078\n",
      "Batch: 3000,train loss is: 0.00031306241106955466\n",
      "test loss is 0.00024808463094523283\n",
      "Batch: 3100,train loss is: 0.0002938864373778156\n",
      "test loss is 0.0002556308665559378\n",
      "Batch: 3200,train loss is: 0.0002480589670006219\n",
      "test loss is 0.0003034393400533997\n",
      "Batch: 3300,train loss is: 0.00023447732962359488\n",
      "test loss is 0.00026817343426491593\n",
      "Batch: 3400,train loss is: 0.00018508454429910524\n",
      "test loss is 0.00025285753791119177\n",
      "Batch: 3500,train loss is: 0.00020777989434673825\n",
      "test loss is 0.0003342114005424206\n",
      "Batch: 3600,train loss is: 0.00024133748961395886\n",
      "test loss is 0.000287952784996371\n",
      "Batch: 3700,train loss is: 0.0002540373470968822\n",
      "test loss is 0.00027582666357896636\n",
      "Batch: 3800,train loss is: 0.00023639564910198606\n",
      "test loss is 0.0005289237365907558\n",
      "Batch: 3900,train loss is: 0.00021654541554146198\n",
      "test loss is 0.00031909225667844417\n",
      "Batch: 4000,train loss is: 0.00017698234905190143\n",
      "test loss is 0.00025276196082767323\n",
      "Batch: 4100,train loss is: 0.00023576649599253406\n",
      "test loss is 0.0002549476573225117\n",
      "Batch: 4200,train loss is: 0.00024352772724476108\n",
      "test loss is 0.00023685735321127174\n",
      "Batch: 4300,train loss is: 0.000167208775486382\n",
      "test loss is 0.00024224547855606157\n",
      "Batch: 4400,train loss is: 0.00028266017551953267\n",
      "test loss is 0.0002528676945771655\n",
      "Batch: 4500,train loss is: 0.0001707625401075556\n",
      "test loss is 0.00029396331488866127\n",
      "Batch: 4600,train loss is: 0.0001875557894951181\n",
      "test loss is 0.0002294266035004149\n",
      "Batch: 4700,train loss is: 0.0002299306100731805\n",
      "test loss is 0.00023622639219464503\n",
      "Batch: 4800,train loss is: 0.00019044622812816936\n",
      "test loss is 0.00024659993838307726\n",
      "Batch: 4900,train loss is: 0.0003425655884777819\n",
      "test loss is 0.000279452621769183\n",
      "Batch: 5000,train loss is: 0.0003074184629099362\n",
      "test loss is 0.00025299104167848704\n",
      "Batch: 5100,train loss is: 0.0002854035828474412\n",
      "test loss is 0.00023370368299859316\n",
      "Batch: 5200,train loss is: 0.000362131193669959\n",
      "test loss is 0.0002613832150209013\n",
      "Batch: 5300,train loss is: 0.0002069243476935656\n",
      "test loss is 0.0002967420726662404\n",
      "Batch: 5400,train loss is: 0.00013849574483667984\n",
      "test loss is 0.0002409219052176023\n",
      "Batch: 5500,train loss is: 0.0002153392499753918\n",
      "test loss is 0.00024807602214616746\n",
      "Batch: 5600,train loss is: 0.0003809067200033449\n",
      "test loss is 0.00024123220766732696\n",
      "Batch: 5700,train loss is: 0.00025988429177384325\n",
      "test loss is 0.00024385501692750068\n",
      "Batch: 5800,train loss is: 0.00023544200722364468\n",
      "test loss is 0.00023532754620420726\n",
      "Batch: 5900,train loss is: 0.00022755513595229754\n",
      "test loss is 0.000270104888574667\n",
      "Batch: 6000,train loss is: 0.00050385173768802\n",
      "test loss is 0.0002605155230481066\n",
      "Batch: 6100,train loss is: 0.0005946137987065959\n",
      "test loss is 0.00027792573772853423\n",
      "Batch: 6200,train loss is: 0.0002679812242321212\n",
      "test loss is 0.000240885275190559\n",
      "Batch: 6300,train loss is: 0.00021659496766121347\n",
      "test loss is 0.00025432256084224496\n",
      "Batch: 6400,train loss is: 0.0003851535508898486\n",
      "test loss is 0.00024007407404980473\n",
      "Batch: 6500,train loss is: 0.00027531028538290683\n",
      "test loss is 0.0003023792301753581\n",
      "Batch: 6600,train loss is: 0.000248326656379849\n",
      "test loss is 0.0002707684571708554\n",
      "Batch: 6700,train loss is: 0.00020256617688987997\n",
      "test loss is 0.00026245727290822777\n",
      "Batch: 6800,train loss is: 0.00015294135149450437\n",
      "test loss is 0.00026056425406536865\n",
      "Batch: 6900,train loss is: 0.00017136038990724713\n",
      "test loss is 0.0003225950067620135\n",
      "Batch: 7000,train loss is: 0.00019631421928201172\n",
      "test loss is 0.00025649982148850405\n",
      "Batch: 7100,train loss is: 0.00037652920668480486\n",
      "test loss is 0.00035965939166177844\n",
      "Batch: 7200,train loss is: 0.000223793170895264\n",
      "test loss is 0.0002889434717544427\n",
      "Batch: 7300,train loss is: 0.0003143875171484789\n",
      "test loss is 0.0002913373808159391\n",
      "Batch: 7400,train loss is: 0.0003276742067124995\n",
      "test loss is 0.0003117038053059308\n",
      "Batch: 7500,train loss is: 0.0003020554005759077\n",
      "test loss is 0.00025448717792061164\n",
      "Batch: 7600,train loss is: 0.00017284608924484263\n",
      "test loss is 0.0003436911739188189\n",
      "Batch: 7700,train loss is: 0.0001878375410104456\n",
      "test loss is 0.000264827352937952\n",
      "Batch: 7800,train loss is: 0.00017970717485781587\n",
      "test loss is 0.00024030421206440832\n",
      "Batch: 7900,train loss is: 0.0002538732926864875\n",
      "test loss is 0.0002732865914746273\n",
      "Batch: 8000,train loss is: 0.0002498111517031843\n",
      "test loss is 0.00027687208086658166\n",
      "Batch: 8100,train loss is: 0.00024155108074265535\n",
      "test loss is 0.0002478463759850942\n",
      "Batch: 8200,train loss is: 0.00019291445128878908\n",
      "test loss is 0.00030776569692314955\n",
      "Batch: 8300,train loss is: 0.0001845781111197464\n",
      "test loss is 0.0003005922002702565\n",
      "Batch: 8400,train loss is: 0.00017892893484892334\n",
      "test loss is 0.00034692911199582125\n",
      "Batch: 8500,train loss is: 0.00017208547353739055\n",
      "test loss is 0.00023815216191939068\n",
      "Batch: 8600,train loss is: 0.00018070549603969643\n",
      "test loss is 0.00025118914048978\n",
      "Batch: 8700,train loss is: 0.00016512688800173207\n",
      "test loss is 0.0002511909197055378\n",
      "Batch: 8800,train loss is: 0.00019033429283288618\n",
      "test loss is 0.000275779040263378\n",
      "Batch: 8900,train loss is: 0.00015209378821681537\n",
      "test loss is 0.000267435734284552\n",
      "Batch: 9000,train loss is: 0.00023601943035054835\n",
      "test loss is 0.00027573461294176204\n",
      "Batch: 9100,train loss is: 0.0003101802167186148\n",
      "test loss is 0.0002919864150788519\n",
      "Batch: 9200,train loss is: 0.0002397188222546871\n",
      "test loss is 0.00024660704283608405\n",
      "Batch: 9300,train loss is: 0.0003084034202664322\n",
      "test loss is 0.00026954327390960004\n",
      "Batch: 9400,train loss is: 0.00040599748412680295\n",
      "test loss is 0.00024275644294741744\n",
      "Batch: 9500,train loss is: 0.00022486901232795798\n",
      "test loss is 0.00024522001395926646\n",
      "Batch: 9600,train loss is: 0.00020907178333239406\n",
      "test loss is 0.00022943000680620654\n",
      "Batch: 9700,train loss is: 0.00026819958061441505\n",
      "test loss is 0.0002309588504987409\n",
      "Batch: 9800,train loss is: 0.0002346542395223328\n",
      "test loss is 0.0002713717514273979\n",
      "Batch: 9900,train loss is: 0.00039022085873408714\n",
      "test loss is 0.000313951053327205\n",
      "Batch: 10000,train loss is: 0.00022803495101969497\n",
      "test loss is 0.00027569463775994294\n",
      "Batch: 10100,train loss is: 0.00022265502045445572\n",
      "test loss is 0.0002366649443954389\n",
      "Batch: 10200,train loss is: 0.0001872225153080513\n",
      "test loss is 0.00025764750057996887\n",
      "Batch: 10300,train loss is: 0.00019474444438220904\n",
      "test loss is 0.00023180034092455893\n",
      "Batch: 10400,train loss is: 0.00029219264318313526\n",
      "test loss is 0.00025796211527875706\n",
      "Batch: 10500,train loss is: 0.0002165541452001532\n",
      "test loss is 0.0002984353920566776\n",
      "Batch: 10600,train loss is: 0.0003733224021569342\n",
      "test loss is 0.0002414756694356542\n",
      "Batch: 10700,train loss is: 0.00022432907007844598\n",
      "test loss is 0.0002708127636419244\n",
      "Batch: 10800,train loss is: 0.0002472965132189368\n",
      "test loss is 0.00023392980700120261\n",
      "Batch: 10900,train loss is: 0.00023268734577295752\n",
      "test loss is 0.0002830364066705407\n",
      "Batch: 11000,train loss is: 0.00024889600802559123\n",
      "test loss is 0.000253130487894517\n",
      "Batch: 11100,train loss is: 0.0002792209760107214\n",
      "test loss is 0.00024497647843449124\n",
      "Batch: 11200,train loss is: 0.0003031484852844922\n",
      "test loss is 0.0002909417337735282\n",
      "Batch: 11300,train loss is: 0.00018641760603069956\n",
      "test loss is 0.0002482586023960856\n",
      "Batch: 11400,train loss is: 0.00023232240008441018\n",
      "test loss is 0.00029923365256287\n",
      "Batch: 11500,train loss is: 0.0002888362624778327\n",
      "test loss is 0.00027922350979194536\n",
      "Batch: 11600,train loss is: 0.00018872847018168808\n",
      "test loss is 0.00023437687498443013\n",
      "Batch: 11700,train loss is: 0.0002227053226517153\n",
      "test loss is 0.0002477477243870469\n",
      "Batch: 11800,train loss is: 0.00015985165776552052\n",
      "test loss is 0.0002630713404512776\n",
      "Batch: 11900,train loss is: 0.00013533915766751186\n",
      "test loss is 0.0002543821484376607\n",
      "Batch: 12000,train loss is: 0.00018214244402692996\n",
      "test loss is 0.0002703037918748084\n",
      "Batch: 12100,train loss is: 0.00018542665287740354\n",
      "test loss is 0.00033900289987485674\n",
      "Batch: 12200,train loss is: 0.0001809755401057521\n",
      "test loss is 0.00026102381105186096\n",
      "Batch: 12300,train loss is: 0.0001570108695622031\n",
      "test loss is 0.0003822915875889855\n",
      "Batch: 12400,train loss is: 0.00025195790367753886\n",
      "test loss is 0.00027271225671270154\n",
      "Batch: 12500,train loss is: 0.00020823129538471058\n",
      "test loss is 0.0002705742254083671\n",
      "Batch: 12600,train loss is: 0.0003464256031564918\n",
      "test loss is 0.00024396559223023373\n",
      "Batch: 12700,train loss is: 0.00020851545509167053\n",
      "test loss is 0.00024385711404679614\n",
      "Batch: 12800,train loss is: 0.00016382402221082104\n",
      "test loss is 0.0003096128820517339\n",
      "Batch: 12900,train loss is: 0.0005261474417361827\n",
      "test loss is 0.00031042558361344996\n",
      "Batch: 13000,train loss is: 0.0002751392193389307\n",
      "test loss is 0.00025518087261660547\n",
      "Batch: 13100,train loss is: 0.0002752864464723824\n",
      "test loss is 0.00026882069874745174\n",
      "Batch: 13200,train loss is: 0.00016163662795748164\n",
      "test loss is 0.0002571574078133218\n",
      "Batch: 13300,train loss is: 0.00015726399431400905\n",
      "test loss is 0.00024243719439448353\n",
      "Batch: 13400,train loss is: 0.00016118134410897977\n",
      "test loss is 0.000273334341994391\n",
      "Batch: 13500,train loss is: 0.00019948337648665666\n",
      "test loss is 0.00026793352402551405\n",
      "Batch: 13600,train loss is: 0.00023231794240372844\n",
      "test loss is 0.00025664977132902096\n",
      "Batch: 13700,train loss is: 0.0002541832362937358\n",
      "test loss is 0.0002550405421053947\n",
      "Batch: 13800,train loss is: 0.0001812731085927083\n",
      "test loss is 0.0002377531977624092\n",
      "Batch: 13900,train loss is: 0.00012886811936745006\n",
      "test loss is 0.0002472295748872393\n",
      "Batch: 14000,train loss is: 0.00030168526094261934\n",
      "test loss is 0.00023337654127904343\n",
      "Batch: 14100,train loss is: 0.000139983586834716\n",
      "test loss is 0.0002540542758787378\n",
      "Batch: 14200,train loss is: 0.00025129422106247377\n",
      "test loss is 0.0003258465734868009\n",
      "Batch: 14300,train loss is: 0.00041204561234464915\n",
      "test loss is 0.00025381157082931746\n",
      "Batch: 14400,train loss is: 0.00026374486907859495\n",
      "test loss is 0.00025989244764981594\n",
      "Batch: 14500,train loss is: 0.00028015747973173693\n",
      "test loss is 0.00026470204359813667\n",
      "Batch: 14600,train loss is: 0.00024019698868352462\n",
      "test loss is 0.00028318882937194094\n",
      "Batch: 14700,train loss is: 0.0002482044651333648\n",
      "test loss is 0.00028710488892792324\n",
      "Batch: 14800,train loss is: 0.000294614689871387\n",
      "test loss is 0.00029481044227589386\n",
      "Batch: 14900,train loss is: 0.00028690529183334824\n",
      "test loss is 0.00029384598429080114\n",
      "Batch: 15000,train loss is: 0.00021864080621444731\n",
      "test loss is 0.00025295292332602976\n",
      "Batch: 15100,train loss is: 0.0002269707583557019\n",
      "test loss is 0.00024438932074103054\n",
      "Batch: 15200,train loss is: 0.000191241258467734\n",
      "test loss is 0.0003044501997296759\n",
      "Batch: 15300,train loss is: 0.00020379998474070134\n",
      "test loss is 0.0002705314886255709\n",
      "Batch: 15400,train loss is: 0.00021843595609623357\n",
      "test loss is 0.00028416094757099426\n",
      "Batch: 15500,train loss is: 0.0001794824302047709\n",
      "test loss is 0.0002827741599677606\n",
      "Batch: 15600,train loss is: 0.00025937089008977503\n",
      "test loss is 0.00033554474930085927\n",
      "Batch: 15700,train loss is: 0.00031004044520945473\n",
      "test loss is 0.00028155416950879984\n",
      "Batch: 15800,train loss is: 0.0001565892711367913\n",
      "test loss is 0.00023246595562938485\n",
      "Batch: 15900,train loss is: 0.0002712436145430173\n",
      "test loss is 0.00029641614197174743\n",
      "Batch: 16000,train loss is: 0.00023275585209895635\n",
      "test loss is 0.0002627111564899938\n",
      "Batch: 16100,train loss is: 0.00019635657306573036\n",
      "test loss is 0.00023729517793897063\n",
      "Batch: 16200,train loss is: 0.000206550459324702\n",
      "test loss is 0.00024127943508841398\n",
      "Batch: 16300,train loss is: 0.00017107069847586227\n",
      "test loss is 0.0002419959701843649\n",
      "Batch: 16400,train loss is: 0.00020750602130048244\n",
      "test loss is 0.00027769813026532045\n",
      "Batch: 16500,train loss is: 0.0003125252179461722\n",
      "test loss is 0.0003599511403744258\n",
      "Batch: 16600,train loss is: 0.0002233190684633398\n",
      "test loss is 0.0002390151015652698\n",
      "Batch: 16700,train loss is: 0.00027714821878866406\n",
      "test loss is 0.00024214334328357892\n",
      "Batch: 16800,train loss is: 0.00022633908919142675\n",
      "test loss is 0.0002870070110430278\n",
      "Batch: 16900,train loss is: 0.00020590654091806436\n",
      "test loss is 0.0003017693416960869\n",
      "Batch: 17000,train loss is: 0.00018945540906619286\n",
      "test loss is 0.0002404361548904523\n",
      "Batch: 17100,train loss is: 0.00032783808167633974\n",
      "test loss is 0.0002310727054297695\n",
      "Batch: 17200,train loss is: 0.00029668405160826635\n",
      "test loss is 0.0003042795833726118\n",
      "Batch: 17300,train loss is: 0.0002546916682422765\n",
      "test loss is 0.0002499072179806189\n",
      "Batch: 17400,train loss is: 0.0003001334125088927\n",
      "test loss is 0.00036711083830007397\n",
      "Batch: 17500,train loss is: 0.0003540444847819856\n",
      "test loss is 0.00027639701804220293\n",
      "Batch: 17600,train loss is: 0.000185435462978333\n",
      "test loss is 0.00023952867714648466\n",
      "Batch: 17700,train loss is: 0.0001800242607527962\n",
      "test loss is 0.00025242955906808987\n",
      "Batch: 17800,train loss is: 0.00024619915754928385\n",
      "test loss is 0.00028440424588357206\n",
      "Batch: 17900,train loss is: 0.00019293069510676735\n",
      "test loss is 0.0002323532438371914\n",
      "Batch: 18000,train loss is: 0.00020410515914976685\n",
      "test loss is 0.00027939366832168154\n",
      "Batch: 18100,train loss is: 0.00023256978004148308\n",
      "test loss is 0.00023358332875604134\n",
      "Batch: 18200,train loss is: 0.00028717686406942917\n",
      "test loss is 0.0003443949801125965\n",
      "Batch: 18300,train loss is: 0.00020103770474154387\n",
      "test loss is 0.00025273586081192757\n",
      "Batch: 18400,train loss is: 0.00027034251221514413\n",
      "test loss is 0.00024816603081887384\n",
      "Batch: 18500,train loss is: 0.00028850047096430024\n",
      "test loss is 0.00025666215551399914\n",
      "Batch: 18600,train loss is: 0.00027421112441545863\n",
      "test loss is 0.00024107387915678566\n",
      "Batch: 18700,train loss is: 0.0003107005006448014\n",
      "test loss is 0.0002748204779996061\n",
      "Batch: 18800,train loss is: 0.00021034002408863118\n",
      "test loss is 0.00023193439244209398\n",
      "Batch: 18900,train loss is: 0.0003191727197578769\n",
      "test loss is 0.0002691711029318909\n",
      "Batch: 19000,train loss is: 0.0002267451395818027\n",
      "test loss is 0.00022839909310103427\n",
      "Batch: 19100,train loss is: 0.0003115475386155268\n",
      "test loss is 0.0002592862737191219\n",
      "Batch: 19200,train loss is: 0.0002222675765764854\n",
      "test loss is 0.0002760247676663822\n",
      "Batch: 19300,train loss is: 0.00020005988304663282\n",
      "test loss is 0.00025575977785211214\n",
      "Batch: 19400,train loss is: 0.00029999280856542464\n",
      "test loss is 0.0002659756968089972\n",
      "Batch: 19500,train loss is: 0.00029303937283200034\n",
      "test loss is 0.00024846219653361086\n",
      "Batch: 19600,train loss is: 0.0001734079919083018\n",
      "test loss is 0.000251822086660076\n",
      "Batch: 19700,train loss is: 0.0005053652382323706\n",
      "test loss is 0.00024963811354493\n",
      "Batch: 19800,train loss is: 0.00019317686504485222\n",
      "test loss is 0.00025377206132814854\n",
      "Batch: 19900,train loss is: 0.00029771368565158565\n",
      "test loss is 0.00027640097925931973\n",
      "Batch: 20000,train loss is: 0.00031635727052195815\n",
      "test loss is 0.0005033964409255855\n",
      "Batch: 20100,train loss is: 0.00023750514692024664\n",
      "test loss is 0.00026969572375101807\n",
      "Batch: 20200,train loss is: 0.00024720267519466415\n",
      "test loss is 0.00027323112810510576\n",
      "Batch: 20300,train loss is: 0.0002613506442625126\n",
      "test loss is 0.0002814733553275793\n",
      "Batch: 20400,train loss is: 0.0002323699659667114\n",
      "test loss is 0.00027030723155306504\n",
      "Batch: 20500,train loss is: 0.00016467966262251771\n",
      "test loss is 0.0002358823529968858\n",
      "Batch: 20600,train loss is: 0.00017511078430087245\n",
      "test loss is 0.00025124499658109713\n",
      "Batch: 20700,train loss is: 0.0003546061422514541\n",
      "test loss is 0.0002990738766954063\n",
      "Batch: 20800,train loss is: 0.00022517505733928494\n",
      "test loss is 0.0002867829212752169\n",
      "Batch: 20900,train loss is: 0.00013947393839140067\n",
      "test loss is 0.00023359041127090426\n",
      "Batch: 21000,train loss is: 0.0002768004516135749\n",
      "test loss is 0.00023920568610201107\n",
      "Batch: 21100,train loss is: 0.00038084352889938703\n",
      "test loss is 0.00026041598632799693\n",
      "Batch: 21200,train loss is: 0.0003266841083131178\n",
      "test loss is 0.0002778358773586215\n",
      "Batch: 21300,train loss is: 0.0003685835125777192\n",
      "test loss is 0.0003324110428592051\n",
      "Batch: 21400,train loss is: 0.00021718037865806368\n",
      "test loss is 0.0002525897377543813\n",
      "Batch: 21500,train loss is: 0.00026405593288623936\n",
      "test loss is 0.00023642443631271412\n",
      "Batch: 21600,train loss is: 0.0003768043231159686\n",
      "test loss is 0.0003541590311476722\n",
      "Batch: 21700,train loss is: 0.0003145154806321068\n",
      "test loss is 0.0002524946576213034\n",
      "Batch: 21800,train loss is: 0.00016763312473341217\n",
      "test loss is 0.0002361141367903046\n",
      "Batch: 21900,train loss is: 0.00020561471441041279\n",
      "test loss is 0.00023958096082505692\n",
      "Batch: 22000,train loss is: 0.00020922217415907088\n",
      "test loss is 0.0002350353304390836\n",
      "Batch: 22100,train loss is: 0.0002148944059491387\n",
      "test loss is 0.0002316333720128844\n",
      "Batch: 22200,train loss is: 0.00016704227382206222\n",
      "test loss is 0.000249768760831713\n",
      "Batch: 22300,train loss is: 0.00019379248214022177\n",
      "test loss is 0.0002357694715425865\n",
      "Batch: 22400,train loss is: 0.0002080828379560529\n",
      "test loss is 0.00039420035100156753\n",
      "Batch: 22500,train loss is: 0.00020207853606312923\n",
      "test loss is 0.000248004478672829\n",
      "Batch: 22600,train loss is: 0.00019525689959076255\n",
      "test loss is 0.0002871616293588593\n",
      "Batch: 22700,train loss is: 0.0002277412749675927\n",
      "test loss is 0.0002576675775929961\n",
      "Batch: 22800,train loss is: 0.00017704593819101755\n",
      "test loss is 0.00025813747955032656\n",
      "Batch: 22900,train loss is: 0.000303965591169421\n",
      "test loss is 0.00028258091959606276\n",
      "Batch: 23000,train loss is: 0.00025127001623994884\n",
      "test loss is 0.00026783008741380296\n",
      "Batch: 23100,train loss is: 0.00022508774401052935\n",
      "test loss is 0.0002820054857338485\n",
      "Batch: 23200,train loss is: 0.00028757167094262825\n",
      "test loss is 0.00025252331133666245\n",
      "Batch: 23300,train loss is: 0.0001867629456230562\n",
      "test loss is 0.000241100577658749\n",
      "Batch: 23400,train loss is: 0.00038724422443443677\n",
      "test loss is 0.0002449568140686439\n",
      "Batch: 23500,train loss is: 0.0002882042247450691\n",
      "test loss is 0.00027745674951755515\n",
      "Batch: 23600,train loss is: 0.00013887278504805242\n",
      "test loss is 0.00022959272512397\n",
      "Batch: 23700,train loss is: 0.000326215148825624\n",
      "test loss is 0.00031144472299937746\n",
      "Batch: 23800,train loss is: 0.0002437902858647656\n",
      "test loss is 0.00028423082926905794\n",
      "Batch: 23900,train loss is: 0.0002252887016891187\n",
      "test loss is 0.0002696592402595965\n",
      "Batch: 24000,train loss is: 0.00024586956473833705\n",
      "test loss is 0.00026396749461293035\n",
      "Batch: 24100,train loss is: 0.0002548123334976839\n",
      "test loss is 0.0002564261955149991\n",
      "Batch: 24200,train loss is: 0.0005296728786704558\n",
      "test loss is 0.0002840266894244457\n",
      "Batch: 24300,train loss is: 0.00015219583020571455\n",
      "test loss is 0.0002944691144138555\n",
      "Batch: 24400,train loss is: 0.00018948741536043573\n",
      "test loss is 0.0002458420968418498\n",
      "Batch: 24500,train loss is: 0.00029078666395935106\n",
      "test loss is 0.0003188561258892206\n",
      "Batch: 24600,train loss is: 0.0007282993968700448\n",
      "test loss is 0.0002501487883183847\n",
      "Batch: 24700,train loss is: 0.00025040839645965344\n",
      "test loss is 0.0003723496809576083\n",
      "Batch: 24800,train loss is: 0.00024560435100785946\n",
      "test loss is 0.0002526824521558794\n",
      "Batch: 24900,train loss is: 0.00020826234740005053\n",
      "test loss is 0.00024766597027728554\n",
      "Batch: 25000,train loss is: 0.00022929213890041073\n",
      "test loss is 0.0002346917247407691\n",
      "Batch: 25100,train loss is: 0.0003084823636672171\n",
      "test loss is 0.00023609079855987103\n",
      "Batch: 25200,train loss is: 0.00022276250354128814\n",
      "test loss is 0.0002356058013580094\n",
      "Batch: 25300,train loss is: 0.00016452020930280136\n",
      "test loss is 0.00022615238746399866\n",
      "Batch: 25400,train loss is: 0.00013856957050375037\n",
      "test loss is 0.0002386900741330374\n",
      "Batch: 25500,train loss is: 0.00021122998588719567\n",
      "test loss is 0.0002450274976913596\n",
      "Batch: 25600,train loss is: 0.00022242429593419404\n",
      "test loss is 0.0002499309907237362\n",
      "Batch: 25700,train loss is: 0.0002354837483482552\n",
      "test loss is 0.0002937034973369633\n",
      "Batch: 25800,train loss is: 0.000281033249463136\n",
      "test loss is 0.0002434912857339947\n",
      "Batch: 25900,train loss is: 0.00023050947312404398\n",
      "test loss is 0.0002374545064461523\n",
      "Batch: 26000,train loss is: 0.00022802403885152482\n",
      "test loss is 0.0002732452724769848\n",
      "Batch: 26100,train loss is: 0.00035339998088390194\n",
      "test loss is 0.00034511700257595777\n",
      "Batch: 26200,train loss is: 0.00024299804150386764\n",
      "test loss is 0.0002482489103979325\n",
      "Batch: 26300,train loss is: 0.00020246332627153703\n",
      "test loss is 0.0002395862167169119\n",
      "Batch: 26400,train loss is: 0.00030886173890196426\n",
      "test loss is 0.00029114189448211367\n",
      "Batch: 26500,train loss is: 0.00021041284933833053\n",
      "test loss is 0.00023096731156775635\n",
      "Batch: 26600,train loss is: 0.00028232497386963373\n",
      "test loss is 0.00024309596486792864\n",
      "Batch: 26700,train loss is: 0.0002597801488170734\n",
      "test loss is 0.0002507539470045194\n",
      "Batch: 26800,train loss is: 0.00013525519452621928\n",
      "test loss is 0.0002584748583844236\n",
      "Batch: 26900,train loss is: 0.0003590982132963851\n",
      "test loss is 0.00025093809947137236\n",
      "Batch: 27000,train loss is: 0.00020132813162801762\n",
      "test loss is 0.00025087690462436454\n",
      "Batch: 27100,train loss is: 0.00012111562697194723\n",
      "test loss is 0.0002464250746962182\n",
      "Batch: 27200,train loss is: 0.0006699910742022021\n",
      "test loss is 0.00023834122265632013\n",
      "Batch: 27300,train loss is: 0.0002620374669697332\n",
      "test loss is 0.00024501867520433396\n",
      "Batch: 27400,train loss is: 0.00032460029186857486\n",
      "test loss is 0.00036373242446359846\n",
      "Batch: 27500,train loss is: 0.0002120424748396508\n",
      "test loss is 0.00024390716146044134\n",
      "Batch: 27600,train loss is: 0.00023736974129225753\n",
      "test loss is 0.0002510930576600979\n",
      "Batch: 27700,train loss is: 0.00021643440637187874\n",
      "test loss is 0.0002756457504127322\n",
      "Batch: 27800,train loss is: 0.00022700140842858658\n",
      "test loss is 0.00026611088113298197\n",
      "Batch: 27900,train loss is: 0.0005719072316596571\n",
      "test loss is 0.0002660997313144105\n",
      "Batch: 28000,train loss is: 0.00017143377209135013\n",
      "test loss is 0.000268715498925288\n",
      "Batch: 28100,train loss is: 0.0002654935322018871\n",
      "test loss is 0.0002799663413235559\n",
      "Batch: 28200,train loss is: 0.00020479227684109071\n",
      "test loss is 0.00027025527329257835\n",
      "Batch: 28300,train loss is: 0.0002356810130640132\n",
      "test loss is 0.0003352042803726665\n",
      "Batch: 28400,train loss is: 0.00026360590935703334\n",
      "test loss is 0.00023811331064271308\n",
      "Batch: 28500,train loss is: 0.00036540031422977126\n",
      "test loss is 0.00024544121005704483\n",
      "Batch: 28600,train loss is: 0.00022227948820816474\n",
      "test loss is 0.00024244196630080514\n",
      "Batch: 28700,train loss is: 0.00018330188809885758\n",
      "test loss is 0.00024016733750359802\n",
      "Batch: 28800,train loss is: 0.00029985550011746244\n",
      "test loss is 0.0002603136153938929\n",
      "Batch: 28900,train loss is: 0.00019622257449404193\n",
      "test loss is 0.0002416183731685976\n",
      "Batch: 29000,train loss is: 0.00020428658285682132\n",
      "test loss is 0.0002768137478353451\n",
      "Batch: 29100,train loss is: 0.00025834273379973707\n",
      "test loss is 0.00025226056967091057\n",
      "Batch: 29200,train loss is: 8.470146480383495e-05\n",
      "test loss is 0.00023426094272058123\n",
      "Batch: 29300,train loss is: 0.00028367520904011144\n",
      "test loss is 0.00026526775448629545\n",
      "Batch: 29400,train loss is: 0.00019271943599585804\n",
      "test loss is 0.00028320773978414085\n",
      "Batch: 29500,train loss is: 0.0002697156533530326\n",
      "test loss is 0.00025145258964689684\n",
      "Batch: 29600,train loss is: 0.0003044159411232408\n",
      "test loss is 0.0002385461541727917\n",
      "Batch: 29700,train loss is: 0.00017812061903633145\n",
      "test loss is 0.000269171472188411\n",
      "Batch: 29800,train loss is: 0.00019107007016753882\n",
      "test loss is 0.00023432985215921022\n",
      "Batch: 29900,train loss is: 0.0001675311785995052\n",
      "test loss is 0.00024045148627484266\n",
      "Batch: 30000,train loss is: 0.00020505960176472472\n",
      "test loss is 0.0002913670723997691\n",
      "Batch: 30100,train loss is: 0.00030620604719209404\n",
      "test loss is 0.00029654513502471976\n",
      "Batch: 30200,train loss is: 0.00020761054010985425\n",
      "test loss is 0.00026125610983325764\n",
      "Batch: 30300,train loss is: 0.0004268751105830157\n",
      "test loss is 0.000337113259928844\n",
      "Batch: 30400,train loss is: 0.00022411182587787584\n",
      "test loss is 0.00025198189550727264\n",
      "Batch: 30500,train loss is: 0.0002792110518804041\n",
      "test loss is 0.0002694321482779921\n",
      "Batch: 30600,train loss is: 0.00021721911344898403\n",
      "test loss is 0.00023491296044166575\n",
      "Batch: 30700,train loss is: 0.0001403343994909472\n",
      "test loss is 0.0002507753055941699\n",
      "Batch: 30800,train loss is: 0.00016394572025761985\n",
      "test loss is 0.00027640754476999983\n",
      "Batch: 30900,train loss is: 0.0001891417361254428\n",
      "test loss is 0.00025261380921916087\n",
      "Batch: 31000,train loss is: 0.00028971394953323045\n",
      "test loss is 0.0002617408434053963\n",
      "Batch: 31100,train loss is: 0.0002181863428942748\n",
      "test loss is 0.00027293396349349717\n",
      "Batch: 31200,train loss is: 0.00044118657847147\n",
      "test loss is 0.00027936891245070066\n",
      "Batch: 31300,train loss is: 0.0002619850585541623\n",
      "test loss is 0.0002399638345800205\n",
      "Batch: 31400,train loss is: 0.00020207241680227745\n",
      "test loss is 0.0003484735577734519\n",
      "Batch: 31500,train loss is: 0.0001750676856850106\n",
      "test loss is 0.00026035816434063504\n",
      "Batch: 31600,train loss is: 0.00020453001792304268\n",
      "test loss is 0.00026491195236879017\n",
      "Batch: 31700,train loss is: 0.00025450331748507696\n",
      "test loss is 0.00029426302834655396\n",
      "Batch: 31800,train loss is: 0.00021039490787386417\n",
      "test loss is 0.00029238163535433856\n",
      "Batch: 31900,train loss is: 0.0003017438933393922\n",
      "test loss is 0.00026913031511223895\n",
      "Batch: 32000,train loss is: 0.00014157898189636316\n",
      "test loss is 0.00023940418913751555\n",
      "Batch: 32100,train loss is: 0.00018288896408436356\n",
      "test loss is 0.00023293902903954883\n",
      "Batch: 32200,train loss is: 0.00029481515998183103\n",
      "test loss is 0.0002910622766458231\n",
      "Batch: 32300,train loss is: 0.00015997910106192086\n",
      "test loss is 0.00023110054574932286\n",
      "Batch: 32400,train loss is: 0.0003158921479630598\n",
      "test loss is 0.0003716222192872784\n",
      "Batch: 32500,train loss is: 0.0002612593276227952\n",
      "test loss is 0.0002270557623383487\n",
      "Batch: 32600,train loss is: 0.00023118442161592588\n",
      "test loss is 0.0002440707501952498\n",
      "Batch: 32700,train loss is: 0.00024046755025591718\n",
      "test loss is 0.00026405784910472994\n",
      "Batch: 32800,train loss is: 0.000355093577023999\n",
      "test loss is 0.00023047683804121866\n",
      "Batch: 32900,train loss is: 0.0010236797584197745\n",
      "test loss is 0.00026745388162526467\n",
      "Batch: 33000,train loss is: 0.00018302231703825773\n",
      "test loss is 0.00026428880968100497\n",
      "Batch: 33100,train loss is: 0.00018373336340705727\n",
      "test loss is 0.00023103593128135212\n",
      "Batch: 33200,train loss is: 0.0003330791474867424\n",
      "test loss is 0.0003476154155457083\n",
      "Batch: 33300,train loss is: 0.0003279972399781429\n",
      "test loss is 0.0004009907215461632\n",
      "Batch: 33400,train loss is: 0.00032792857172923807\n",
      "test loss is 0.0002524864005691103\n",
      "Batch: 33500,train loss is: 0.000272876552357103\n",
      "test loss is 0.0002505467377073836\n",
      "Batch: 33600,train loss is: 0.00013932161576809922\n",
      "test loss is 0.00022877309986312603\n",
      "Batch: 33700,train loss is: 0.00016367793897509948\n",
      "test loss is 0.00022923389475391995\n",
      "Batch: 33800,train loss is: 0.00023409545704413333\n",
      "test loss is 0.00025245892588285135\n",
      "Batch: 33900,train loss is: 0.0001477760944244901\n",
      "test loss is 0.00027363676457946225\n",
      "Batch: 34000,train loss is: 0.00019487466152299292\n",
      "test loss is 0.0002994583794530741\n",
      "Batch: 34100,train loss is: 0.00018098268141752041\n",
      "test loss is 0.0002423225396623576\n",
      "Batch: 34200,train loss is: 0.00015209807001355242\n",
      "test loss is 0.0002673711421546387\n",
      "Batch: 34300,train loss is: 0.00016916809656208583\n",
      "test loss is 0.00023996602085397804\n",
      "Batch: 34400,train loss is: 0.0003676352168777892\n",
      "test loss is 0.00024761022799052305\n",
      "Batch: 34500,train loss is: 0.00015594519710763642\n",
      "test loss is 0.0002445543668544475\n",
      "Batch: 34600,train loss is: 0.0002712743183730137\n",
      "test loss is 0.0002431240497398888\n",
      "Batch: 34700,train loss is: 0.0005214483560642723\n",
      "test loss is 0.0002562095949262285\n",
      "Batch: 34800,train loss is: 0.0003165036542049466\n",
      "test loss is 0.0002415923039707546\n",
      "Batch: 34900,train loss is: 0.00018199317548138304\n",
      "test loss is 0.00023230721419766572\n",
      "Batch: 35000,train loss is: 0.0002740361399750852\n",
      "test loss is 0.00023865456640529445\n",
      "Batch: 35100,train loss is: 0.00029898078713611065\n",
      "test loss is 0.0003296538965918579\n",
      "Batch: 35200,train loss is: 0.00023116207044724277\n",
      "test loss is 0.0002400548710564507\n",
      "Batch: 35300,train loss is: 0.0002853635909305196\n",
      "test loss is 0.0002571396338249386\n",
      "Batch: 35400,train loss is: 0.00017189571132864953\n",
      "test loss is 0.0002509804363645695\n",
      "Batch: 35500,train loss is: 0.0002199381618139187\n",
      "test loss is 0.00024310935832472949\n",
      "Batch: 35600,train loss is: 0.00019275561591992435\n",
      "test loss is 0.00028163038131111474\n",
      "Batch: 35700,train loss is: 0.00027495098582661813\n",
      "test loss is 0.00024877338452700365\n",
      "Batch: 35800,train loss is: 0.00022233913379592594\n",
      "test loss is 0.00028365071315497787\n",
      "Batch: 35900,train loss is: 0.00030499326844274875\n",
      "test loss is 0.0002630926776349905\n",
      "Batch: 36000,train loss is: 0.00019902179998019533\n",
      "test loss is 0.0003032316601347745\n",
      "Batch: 36100,train loss is: 0.00017162080586979514\n",
      "test loss is 0.00024049582572786043\n",
      "Batch: 36200,train loss is: 0.00023008799343437463\n",
      "test loss is 0.00024249109099267559\n",
      "Batch: 36300,train loss is: 0.0001625720450060237\n",
      "test loss is 0.00023991483436209513\n",
      "Batch: 36400,train loss is: 0.00027950039847222393\n",
      "test loss is 0.00026855680404729655\n",
      "Batch: 36500,train loss is: 0.0002690681453247548\n",
      "test loss is 0.00023446868321344629\n",
      "Batch: 36600,train loss is: 0.00020776749249670933\n",
      "test loss is 0.0002799769298385318\n",
      "Batch: 36700,train loss is: 0.0002472127258856539\n",
      "test loss is 0.00030378050036849097\n",
      "Batch: 36800,train loss is: 0.0001948935738678944\n",
      "test loss is 0.0002835791023332305\n",
      "Batch: 36900,train loss is: 0.00027891214901296584\n",
      "test loss is 0.0002915404098362868\n",
      "Batch: 37000,train loss is: 0.0003562268988862551\n",
      "test loss is 0.00029552574583939506\n",
      "Batch: 37100,train loss is: 0.0006945282849609716\n",
      "test loss is 0.0002948508635685115\n",
      "Batch: 37200,train loss is: 0.00024812949933516527\n",
      "test loss is 0.0002998949613690804\n",
      "Batch: 37300,train loss is: 0.00030381747923010504\n",
      "test loss is 0.0002418622844888889\n",
      "Batch: 37400,train loss is: 0.00015792073222236621\n",
      "test loss is 0.00026694569107643093\n",
      "Batch: 37500,train loss is: 0.000216368286525725\n",
      "test loss is 0.0002689476237409643\n",
      "Batch: 37600,train loss is: 0.000203884279926248\n",
      "test loss is 0.00023540683138046305\n",
      "Batch: 37700,train loss is: 0.0002675702528556054\n",
      "test loss is 0.00026123910625744406\n",
      "Batch: 37800,train loss is: 0.00034548169622572596\n",
      "test loss is 0.00025268375950455\n",
      "Batch: 37900,train loss is: 0.0005105751855868664\n",
      "test loss is 0.0002743840542155577\n",
      "Batch: 38000,train loss is: 0.0006241291421106999\n",
      "test loss is 0.00034495044107362387\n",
      "Batch: 38100,train loss is: 0.00019445819819478453\n",
      "test loss is 0.00024666203665614733\n",
      "Batch: 38200,train loss is: 0.0002656362063083779\n",
      "test loss is 0.0002567785243221567\n",
      "Batch: 38300,train loss is: 0.00023899799931711318\n",
      "test loss is 0.00029404405474337404\n",
      "Batch: 38400,train loss is: 0.0003504720323265773\n",
      "test loss is 0.0003633743590146031\n",
      "Batch: 38500,train loss is: 0.00024882842687504143\n",
      "test loss is 0.000248561373343955\n",
      "Batch: 38600,train loss is: 0.0003473787797524961\n",
      "test loss is 0.00026376418368400577\n",
      "Batch: 38700,train loss is: 0.00016473710167401099\n",
      "test loss is 0.0002626355603507084\n",
      "Batch: 38800,train loss is: 0.0003455217497682687\n",
      "test loss is 0.00024068004812801146\n",
      "Batch: 38900,train loss is: 0.0001866531965456013\n",
      "test loss is 0.00026826756371353135\n",
      "Batch: 39000,train loss is: 0.00021713101645648962\n",
      "test loss is 0.0002959060073048345\n",
      "Batch: 39100,train loss is: 0.00023404513472943434\n",
      "test loss is 0.000293244877540705\n",
      "Batch: 39200,train loss is: 0.0003180691777329021\n",
      "test loss is 0.0002637995321820074\n",
      "Batch: 39300,train loss is: 0.0002273098129432508\n",
      "test loss is 0.00029870775428196355\n",
      "Batch: 39400,train loss is: 0.00034254689006673125\n",
      "test loss is 0.0002354527526397985\n",
      "Batch: 39500,train loss is: 0.00011936171762641439\n",
      "test loss is 0.0002593770948150014\n",
      "Batch: 39600,train loss is: 0.00019344221996236012\n",
      "test loss is 0.00025476653741237244\n",
      "Batch: 39700,train loss is: 0.0004183284119947927\n",
      "test loss is 0.00030520278986683385\n",
      "Batch: 39800,train loss is: 0.00037459168091864484\n",
      "test loss is 0.0002593179158595566\n",
      "Batch: 39900,train loss is: 0.00020320450723506871\n",
      "test loss is 0.00025837858860294674\n",
      "Batch: 40000,train loss is: 0.00015495992058036712\n",
      "test loss is 0.00024316420491532763\n",
      "Batch: 40100,train loss is: 0.00017959840417799985\n",
      "test loss is 0.00024091199621175768\n",
      "Batch: 40200,train loss is: 0.0002603422861721305\n",
      "test loss is 0.000267701836882966\n",
      "Batch: 40300,train loss is: 0.00021973179444748412\n",
      "test loss is 0.0002707585943641176\n",
      "Batch: 40400,train loss is: 0.00041371600090455373\n",
      "test loss is 0.0002591853734853511\n",
      "Batch: 40500,train loss is: 0.00028888584876010886\n",
      "test loss is 0.0002514724847313551\n",
      "Batch: 40600,train loss is: 0.00023509335994550037\n",
      "test loss is 0.00023618338778334482\n",
      "Batch: 40700,train loss is: 0.0002993423450331701\n",
      "test loss is 0.00023171939104602016\n",
      "Batch: 40800,train loss is: 0.00022978599403674827\n",
      "test loss is 0.000262424013566496\n",
      "Batch: 40900,train loss is: 0.00032055348616723766\n",
      "test loss is 0.00023171988711496438\n",
      "Batch: 41000,train loss is: 0.00021908679945089595\n",
      "test loss is 0.000275222923789726\n",
      "Batch: 41100,train loss is: 0.00017575667106079968\n",
      "test loss is 0.00023786045380754065\n",
      "Batch: 41200,train loss is: 0.0002219232604528172\n",
      "test loss is 0.0002513047163415796\n",
      "Batch: 41300,train loss is: 0.0002322222992004804\n",
      "test loss is 0.0002301516152671019\n",
      "Batch: 41400,train loss is: 0.00024942349650076143\n",
      "test loss is 0.00023768740676239567\n",
      "Batch: 41500,train loss is: 0.0002637699249460932\n",
      "test loss is 0.00024094102539656728\n",
      "Batch: 41600,train loss is: 0.0003124385487601566\n",
      "test loss is 0.00026267745481092775\n",
      "Batch: 41700,train loss is: 0.000209769864076223\n",
      "test loss is 0.0002471146321629762\n",
      "Batch: 41800,train loss is: 0.0002092522443861242\n",
      "test loss is 0.0002631204436912739\n",
      "Batch: 41900,train loss is: 0.00017616631822279163\n",
      "test loss is 0.00023386482945667435\n",
      "Batch: 42000,train loss is: 0.0003463437864142412\n",
      "test loss is 0.00027939419509305876\n",
      "Batch: 42100,train loss is: 0.0002144359656142134\n",
      "test loss is 0.00024252134704811436\n",
      "Batch: 42200,train loss is: 0.00018511156235434935\n",
      "test loss is 0.00028636722383765023\n",
      "Batch: 42300,train loss is: 0.00016251887677668945\n",
      "test loss is 0.00023808927712082892\n",
      "Batch: 42400,train loss is: 0.00026758435459548044\n",
      "test loss is 0.0002617789462324707\n",
      "Batch: 42500,train loss is: 0.0001992793193656554\n",
      "test loss is 0.0002587679700850945\n",
      "Batch: 42600,train loss is: 0.00029551459056519975\n",
      "test loss is 0.000288314784944334\n",
      "Batch: 42700,train loss is: 0.00029157317717483486\n",
      "test loss is 0.0002393753832527076\n",
      "Batch: 42800,train loss is: 0.0002871768085448942\n",
      "test loss is 0.0002629464970976137\n",
      "Batch: 42900,train loss is: 0.00015097700046289657\n",
      "test loss is 0.00032203612160860203\n",
      "Batch: 43000,train loss is: 0.00018822030864859722\n",
      "test loss is 0.000243782732399864\n",
      "Batch: 43100,train loss is: 0.0003367316909856564\n",
      "test loss is 0.00023789177088670525\n",
      "Batch: 43200,train loss is: 0.00026056510826667247\n",
      "test loss is 0.0003204656320709744\n",
      "Batch: 43300,train loss is: 0.00020469021471196687\n",
      "test loss is 0.00028146888657942364\n",
      "Batch: 43400,train loss is: 0.00023445528401523869\n",
      "test loss is 0.0002477721605102364\n",
      "Batch: 43500,train loss is: 0.00016477308408806898\n",
      "test loss is 0.0002941571241220831\n",
      "Batch: 43600,train loss is: 0.00029700250342891074\n",
      "test loss is 0.00029544048237110937\n",
      "Batch: 43700,train loss is: 0.00020714745172631009\n",
      "test loss is 0.0002632372759340385\n",
      "Batch: 43800,train loss is: 0.00030635396023736465\n",
      "test loss is 0.00023505903298429726\n",
      "Batch: 43900,train loss is: 0.0003861629925592983\n",
      "test loss is 0.0003605558683363322\n",
      "Batch: 44000,train loss is: 0.00021687684396147234\n",
      "test loss is 0.00030706458375670466\n",
      "Batch: 44100,train loss is: 0.0003007465731349993\n",
      "test loss is 0.0002886030954664417\n",
      "Batch: 44200,train loss is: 0.0001909088063385975\n",
      "test loss is 0.0002892564401075422\n",
      "Batch: 44300,train loss is: 0.00027802447420840733\n",
      "test loss is 0.00025782313630151924\n",
      "Batch: 44400,train loss is: 0.00025771301669036157\n",
      "test loss is 0.00024126258611713585\n",
      "Batch: 44500,train loss is: 0.00015043425635623232\n",
      "test loss is 0.00025094475901255345\n",
      "Batch: 44600,train loss is: 0.0008412814413675906\n",
      "test loss is 0.0003719148516521222\n",
      "Batch: 44700,train loss is: 0.00021855134884321\n",
      "test loss is 0.00026633066967019067\n",
      "Batch: 44800,train loss is: 0.0001791148222826341\n",
      "test loss is 0.00028759316091794346\n",
      "Batch: 44900,train loss is: 0.0001481556759284614\n",
      "test loss is 0.0002562199687763177\n",
      "Batch: 45000,train loss is: 0.00034975903371805173\n",
      "test loss is 0.00034378538828234236\n",
      "Batch: 45100,train loss is: 0.00018586302725529132\n",
      "test loss is 0.00023594467080874016\n",
      "Batch: 45200,train loss is: 0.00017204997296522718\n",
      "test loss is 0.00029966194956698884\n",
      "Batch: 45300,train loss is: 0.0003731386705346848\n",
      "test loss is 0.0002905834563248043\n",
      "Batch: 45400,train loss is: 0.00020741590200933413\n",
      "test loss is 0.0002362018384741936\n",
      "Batch: 45500,train loss is: 0.00018264243363482028\n",
      "test loss is 0.0002590197774896297\n",
      "Batch: 45600,train loss is: 0.00018883648942153304\n",
      "test loss is 0.0002465388788555279\n",
      "Batch: 45700,train loss is: 0.0002767982152054465\n",
      "test loss is 0.0002704523995158405\n",
      "Batch: 45800,train loss is: 0.00033845323873898134\n",
      "test loss is 0.00023786665296021803\n",
      "Batch: 45900,train loss is: 0.00021760874146831396\n",
      "test loss is 0.00025803172383805697\n",
      "Batch: 46000,train loss is: 0.00024568103539637944\n",
      "test loss is 0.00027141420052784486\n",
      "Batch: 46100,train loss is: 0.00015373375813886362\n",
      "test loss is 0.00022560701713123768\n",
      "Batch: 46200,train loss is: 0.00018446080887874042\n",
      "test loss is 0.00023384105088083976\n",
      "Batch: 46300,train loss is: 0.00026492404390476105\n",
      "test loss is 0.0002964596505511168\n",
      "Batch: 46400,train loss is: 0.0002016423030925115\n",
      "test loss is 0.0002440000912609945\n",
      "Batch: 46500,train loss is: 0.0001835542783049197\n",
      "test loss is 0.00023893219079739933\n",
      "Batch: 46600,train loss is: 0.0002889838396399883\n",
      "test loss is 0.0002618217436167132\n",
      "Batch: 46700,train loss is: 0.0003559951531871851\n",
      "test loss is 0.00024382143433499692\n",
      "-----------------------Epoch: 12----------------------------------\n",
      "Batch: 0,train loss is: 0.0003028547645382182\n",
      "test loss is 0.00029493397610251673\n",
      "Batch: 100,train loss is: 0.00022864078144473114\n",
      "test loss is 0.0002665989118308123\n",
      "Batch: 200,train loss is: 0.00030202635043757425\n",
      "test loss is 0.0002682997315469025\n",
      "Batch: 300,train loss is: 0.0002895381262498902\n",
      "test loss is 0.00023334051452087855\n",
      "Batch: 400,train loss is: 0.0001522003720086824\n",
      "test loss is 0.00023282721494963685\n",
      "Batch: 500,train loss is: 0.00042497349728993106\n",
      "test loss is 0.00024611072560018233\n",
      "Batch: 600,train loss is: 0.00018138313375553645\n",
      "test loss is 0.00024873875705292613\n",
      "Batch: 700,train loss is: 0.00020717110286636156\n",
      "test loss is 0.0002668235785728094\n",
      "Batch: 800,train loss is: 0.00043294781728379255\n",
      "test loss is 0.00028081132016268006\n",
      "Batch: 900,train loss is: 0.000171088033234631\n",
      "test loss is 0.00027079004771942273\n",
      "Batch: 1000,train loss is: 0.00023652793136730665\n",
      "test loss is 0.00024708268002696375\n",
      "Batch: 1100,train loss is: 0.0002086163706464573\n",
      "test loss is 0.00028440167666049797\n",
      "Batch: 1200,train loss is: 0.00018366954720044313\n",
      "test loss is 0.00024083221478241607\n",
      "Batch: 1300,train loss is: 0.00011878270557904693\n",
      "test loss is 0.00036120853413453837\n",
      "Batch: 1400,train loss is: 0.00030673914792528654\n",
      "test loss is 0.0002929315197831707\n",
      "Batch: 1500,train loss is: 0.0001991575807738911\n",
      "test loss is 0.00023314868096025972\n",
      "Batch: 1600,train loss is: 0.0001830099602298054\n",
      "test loss is 0.0002567997815409672\n",
      "Batch: 1700,train loss is: 0.00034863980809019175\n",
      "test loss is 0.00026275561819647604\n",
      "Batch: 1800,train loss is: 0.00033183232177363704\n",
      "test loss is 0.00030139362508328787\n",
      "Batch: 1900,train loss is: 0.00019377223649416078\n",
      "test loss is 0.00022829160597199634\n",
      "Batch: 2000,train loss is: 0.00018589321028644675\n",
      "test loss is 0.0002573853447418562\n",
      "Batch: 2100,train loss is: 0.0002870046732131433\n",
      "test loss is 0.00027589006214515284\n",
      "Batch: 2200,train loss is: 0.000980189629794352\n",
      "test loss is 0.0004310556231353917\n",
      "Batch: 2300,train loss is: 0.00016463718693024093\n",
      "test loss is 0.00026241922170821386\n",
      "Batch: 2400,train loss is: 0.0003183619572741012\n",
      "test loss is 0.0002663116661393729\n",
      "Batch: 2500,train loss is: 0.0003023334606276724\n",
      "test loss is 0.0002587859696828372\n",
      "Batch: 2600,train loss is: 0.00013978913581514334\n",
      "test loss is 0.00024005660493171076\n",
      "Batch: 2700,train loss is: 0.000294393736744628\n",
      "test loss is 0.0003440018601649706\n",
      "Batch: 2800,train loss is: 0.00030257517792239396\n",
      "test loss is 0.00023942188239652358\n",
      "Batch: 2900,train loss is: 0.00026194486809982657\n",
      "test loss is 0.00025391641150535846\n",
      "Batch: 3000,train loss is: 0.00028861908170909516\n",
      "test loss is 0.00023525341671987794\n",
      "Batch: 3100,train loss is: 0.00025615196769807774\n",
      "test loss is 0.0002914610836589558\n",
      "Batch: 3200,train loss is: 0.00020373116307654957\n",
      "test loss is 0.0002675373962629331\n",
      "Batch: 3300,train loss is: 0.0002497758244769916\n",
      "test loss is 0.00026571003404084567\n",
      "Batch: 3400,train loss is: 0.0002749098146763312\n",
      "test loss is 0.0002525436627424846\n",
      "Batch: 3500,train loss is: 0.0003406858008436448\n",
      "test loss is 0.00026245297630130937\n",
      "Batch: 3600,train loss is: 0.00023660885870588387\n",
      "test loss is 0.00024766261067477957\n",
      "Batch: 3700,train loss is: 0.00034242583653634615\n",
      "test loss is 0.0002456745649425477\n",
      "Batch: 3800,train loss is: 0.00039757836717206895\n",
      "test loss is 0.00027723228718435787\n",
      "Batch: 3900,train loss is: 0.00030441398220512703\n",
      "test loss is 0.00023817975129905263\n",
      "Batch: 4000,train loss is: 0.00020680578167914438\n",
      "test loss is 0.00026206516276676455\n",
      "Batch: 4100,train loss is: 0.00019887059498073653\n",
      "test loss is 0.0002654891901099976\n",
      "Batch: 4200,train loss is: 0.0004210101099860121\n",
      "test loss is 0.00027881564968455113\n",
      "Batch: 4300,train loss is: 0.0002231953653146591\n",
      "test loss is 0.0003468576514273056\n",
      "Batch: 4400,train loss is: 0.00022079230948488443\n",
      "test loss is 0.00025407881129639007\n",
      "Batch: 4500,train loss is: 0.00017861634305213204\n",
      "test loss is 0.0002654904986730696\n",
      "Batch: 4600,train loss is: 0.00019280966178913418\n",
      "test loss is 0.0002318518894109064\n",
      "Batch: 4700,train loss is: 0.000331342831876333\n",
      "test loss is 0.0002921338616176082\n",
      "Batch: 4800,train loss is: 0.00014213245887419195\n",
      "test loss is 0.00027547932143604\n",
      "Batch: 4900,train loss is: 0.000249695004394851\n",
      "test loss is 0.00032551945087633944\n",
      "Batch: 5000,train loss is: 0.00024175065100014452\n",
      "test loss is 0.0002367849169562154\n",
      "Batch: 5100,train loss is: 0.00021007388454983954\n",
      "test loss is 0.00025523132187445387\n",
      "Batch: 5200,train loss is: 0.00020633478376304515\n",
      "test loss is 0.0003166500528432702\n",
      "Batch: 5300,train loss is: 0.000206841737497276\n",
      "test loss is 0.0002837365488469432\n",
      "Batch: 5400,train loss is: 0.0002653103936802417\n",
      "test loss is 0.00029973906390604884\n",
      "Batch: 5500,train loss is: 0.0003965900546316339\n",
      "test loss is 0.00030419571172327585\n",
      "Batch: 5600,train loss is: 0.00018837042308916026\n",
      "test loss is 0.0002392779787517915\n",
      "Batch: 5700,train loss is: 0.0002894498758215636\n",
      "test loss is 0.0002927122439680043\n",
      "Batch: 5800,train loss is: 0.00020610736735599916\n",
      "test loss is 0.0002520124506244812\n",
      "Batch: 5900,train loss is: 0.00024243849953603438\n",
      "test loss is 0.00025027244137036913\n",
      "Batch: 6000,train loss is: 0.00028234100138896444\n",
      "test loss is 0.00025211311649950654\n",
      "Batch: 6100,train loss is: 0.000332533180424627\n",
      "test loss is 0.00030264133586758833\n",
      "Batch: 6200,train loss is: 0.00016689407112972858\n",
      "test loss is 0.0002247259841251572\n",
      "Batch: 6300,train loss is: 0.0002055893068549001\n",
      "test loss is 0.00024080323700158973\n",
      "Batch: 6400,train loss is: 0.00027221232583547755\n",
      "test loss is 0.0002679067230452883\n",
      "Batch: 6500,train loss is: 0.00028769796356177004\n",
      "test loss is 0.0002546952876549398\n",
      "Batch: 6600,train loss is: 0.00026232563889866856\n",
      "test loss is 0.0002934082417202225\n",
      "Batch: 6700,train loss is: 0.000308088847120121\n",
      "test loss is 0.00022279079452351444\n",
      "Batch: 6800,train loss is: 0.0003326914701664666\n",
      "test loss is 0.0002789175342300616\n",
      "Batch: 6900,train loss is: 0.0003695613533450741\n",
      "test loss is 0.00027122141245220177\n",
      "Batch: 7000,train loss is: 0.0006202439389381255\n",
      "test loss is 0.00025558902448327963\n",
      "Batch: 7100,train loss is: 0.0003050306902925195\n",
      "test loss is 0.0002987804446445502\n",
      "Batch: 7200,train loss is: 0.00020627819209605137\n",
      "test loss is 0.00023523970263539915\n",
      "Batch: 7300,train loss is: 0.0009654729151080415\n",
      "test loss is 0.0002826822482162008\n",
      "Batch: 7400,train loss is: 0.00015302920621103393\n",
      "test loss is 0.0002413118974692462\n",
      "Batch: 7500,train loss is: 0.0002683266196586243\n",
      "test loss is 0.00024545740531358335\n",
      "Batch: 7600,train loss is: 0.0010821902061125994\n",
      "test loss is 0.00027826214754737117\n",
      "Batch: 7700,train loss is: 0.0002717351123758865\n",
      "test loss is 0.00027686300416737773\n",
      "Batch: 7800,train loss is: 0.00017917345867044112\n",
      "test loss is 0.0002526092679126343\n",
      "Batch: 7900,train loss is: 0.00015085453953256388\n",
      "test loss is 0.00024321060537153274\n",
      "Batch: 8000,train loss is: 0.0002249421166516361\n",
      "test loss is 0.00026852684035073306\n",
      "Batch: 8100,train loss is: 0.00015211248448131825\n",
      "test loss is 0.00024744508884640327\n",
      "Batch: 8200,train loss is: 0.0002855603282870914\n",
      "test loss is 0.0002753195265364836\n",
      "Batch: 8300,train loss is: 0.00025299231139341453\n",
      "test loss is 0.00025877176815067017\n",
      "Batch: 8400,train loss is: 0.0002213376877169383\n",
      "test loss is 0.00029760522585105015\n",
      "Batch: 8500,train loss is: 0.00017853025150310815\n",
      "test loss is 0.0002509143190827001\n",
      "Batch: 8600,train loss is: 0.0003522104699280941\n",
      "test loss is 0.00025109317935248195\n",
      "Batch: 8700,train loss is: 0.0002718240063371487\n",
      "test loss is 0.00029226336810849845\n",
      "Batch: 8800,train loss is: 0.00023443450816965585\n",
      "test loss is 0.00027185345745045687\n",
      "Batch: 8900,train loss is: 0.00021240375684065298\n",
      "test loss is 0.0002806690391542595\n",
      "Batch: 9000,train loss is: 0.0002148051441776958\n",
      "test loss is 0.00030645045773512877\n",
      "Batch: 9100,train loss is: 0.00018368969908490028\n",
      "test loss is 0.000261278443443373\n",
      "Batch: 9200,train loss is: 0.00018989546324344397\n",
      "test loss is 0.0002474417747519981\n",
      "Batch: 9300,train loss is: 0.00030794899378241546\n",
      "test loss is 0.00024475914635739\n",
      "Batch: 9400,train loss is: 0.0001521418648865204\n",
      "test loss is 0.00024132349069974534\n",
      "Batch: 9500,train loss is: 0.0005022958654250114\n",
      "test loss is 0.00029938174148592944\n",
      "Batch: 9600,train loss is: 0.0002731467500496419\n",
      "test loss is 0.00027158695466179735\n",
      "Batch: 9700,train loss is: 0.0004644838521291871\n",
      "test loss is 0.00026517521120522506\n",
      "Batch: 9800,train loss is: 0.00023075513786715145\n",
      "test loss is 0.00023995256263255929\n",
      "Batch: 9900,train loss is: 0.000435409823456258\n",
      "test loss is 0.0004485229099384725\n",
      "Batch: 10000,train loss is: 0.0001669749552882025\n",
      "test loss is 0.000251807777834817\n",
      "Batch: 10100,train loss is: 0.00029307515093581784\n",
      "test loss is 0.00026142226750294707\n",
      "Batch: 10200,train loss is: 0.00025172264021362865\n",
      "test loss is 0.000248898671019085\n",
      "Batch: 10300,train loss is: 0.00043225036679019025\n",
      "test loss is 0.0002490795517048262\n",
      "Batch: 10400,train loss is: 0.00027989611445078584\n",
      "test loss is 0.0002542689230031986\n",
      "Batch: 10500,train loss is: 0.00021185009687528744\n",
      "test loss is 0.0003047856015477192\n",
      "Batch: 10600,train loss is: 0.00019453309993740144\n",
      "test loss is 0.000238757517484833\n",
      "Batch: 10700,train loss is: 0.00017332617822325952\n",
      "test loss is 0.00023231747355251223\n",
      "Batch: 10800,train loss is: 0.00020735644440802336\n",
      "test loss is 0.00026520807590213724\n",
      "Batch: 10900,train loss is: 0.0002077986099495353\n",
      "test loss is 0.0002418577889682828\n",
      "Batch: 11000,train loss is: 0.00027130251448582673\n",
      "test loss is 0.0002803405112741346\n",
      "Batch: 11100,train loss is: 0.0001697165950536188\n",
      "test loss is 0.00026726144082371615\n",
      "Batch: 11200,train loss is: 0.00019079501944040703\n",
      "test loss is 0.00030196148277649836\n",
      "Batch: 11300,train loss is: 0.0003740105414062211\n",
      "test loss is 0.00022580529487204644\n",
      "Batch: 11400,train loss is: 0.00023503622643562586\n",
      "test loss is 0.00024081878020054708\n",
      "Batch: 11500,train loss is: 0.00038182649446813727\n",
      "test loss is 0.00026491480766103895\n",
      "Batch: 11600,train loss is: 0.0002595055160840424\n",
      "test loss is 0.00023358717377609583\n",
      "Batch: 11700,train loss is: 0.00026147922860475926\n",
      "test loss is 0.0002939701760456129\n",
      "Batch: 11800,train loss is: 0.00048284101228428407\n",
      "test loss is 0.00027953919922561613\n",
      "Batch: 11900,train loss is: 0.00023793912637813002\n",
      "test loss is 0.00025711679625340034\n",
      "Batch: 12000,train loss is: 0.00010326238335658285\n",
      "test loss is 0.00022753786338973646\n",
      "Batch: 12100,train loss is: 0.00020326048136937858\n",
      "test loss is 0.0002451480232577956\n",
      "Batch: 12200,train loss is: 0.00021680310385263944\n",
      "test loss is 0.00023899131053051556\n",
      "Batch: 12300,train loss is: 0.00033940800396787553\n",
      "test loss is 0.00025405902702026873\n",
      "Batch: 12400,train loss is: 0.0002397672601404019\n",
      "test loss is 0.0002458397162380538\n",
      "Batch: 12500,train loss is: 0.000272905000510096\n",
      "test loss is 0.0003248351101298752\n",
      "Batch: 12600,train loss is: 0.0002121229629225531\n",
      "test loss is 0.0002371340329948957\n",
      "Batch: 12700,train loss is: 0.0004106471757791511\n",
      "test loss is 0.00023789123569772623\n",
      "Batch: 12800,train loss is: 0.0003629676687026811\n",
      "test loss is 0.0002643229943743325\n",
      "Batch: 12900,train loss is: 0.0002227225398043062\n",
      "test loss is 0.00027485129749102986\n",
      "Batch: 13000,train loss is: 0.00022859541008763424\n",
      "test loss is 0.0002215705424049272\n",
      "Batch: 13100,train loss is: 0.0001985912586479577\n",
      "test loss is 0.0002681449268837717\n",
      "Batch: 13200,train loss is: 0.00027500932977230787\n",
      "test loss is 0.00028470248119861206\n",
      "Batch: 13300,train loss is: 0.0003133492706106544\n",
      "test loss is 0.000252432137918973\n",
      "Batch: 13400,train loss is: 0.00024196599979960952\n",
      "test loss is 0.00034877366395187875\n",
      "Batch: 13500,train loss is: 0.00022186691838071206\n",
      "test loss is 0.00027795681167417795\n",
      "Batch: 13600,train loss is: 0.00024062814000353454\n",
      "test loss is 0.00024643651804107266\n",
      "Batch: 13700,train loss is: 0.000210308620917011\n",
      "test loss is 0.00025119924768171484\n",
      "Batch: 13800,train loss is: 0.00019113869378545138\n",
      "test loss is 0.0002422102376461956\n",
      "Batch: 13900,train loss is: 0.00016667493819234053\n",
      "test loss is 0.00025539904005684034\n",
      "Batch: 14000,train loss is: 0.00016375719259501497\n",
      "test loss is 0.0002389464444396918\n",
      "Batch: 14100,train loss is: 0.00025439458141275265\n",
      "test loss is 0.0002635662529502017\n",
      "Batch: 14200,train loss is: 0.0002605083447360791\n",
      "test loss is 0.00029788947947535365\n",
      "Batch: 14300,train loss is: 0.00017336331079145195\n",
      "test loss is 0.00026800425957896636\n",
      "Batch: 14400,train loss is: 0.00021710855558900051\n",
      "test loss is 0.00022539354977535307\n",
      "Batch: 14500,train loss is: 0.00019270275113098282\n",
      "test loss is 0.00026476634712542475\n",
      "Batch: 14600,train loss is: 0.0005739630344019262\n",
      "test loss is 0.0002463894152948214\n",
      "Batch: 14700,train loss is: 0.00016166184093114458\n",
      "test loss is 0.00023065721622549012\n",
      "Batch: 14800,train loss is: 0.00031607719923249487\n",
      "test loss is 0.00024629600676609096\n",
      "Batch: 14900,train loss is: 0.00019948521715141377\n",
      "test loss is 0.0002618524474387023\n",
      "Batch: 15000,train loss is: 0.00021773729385442568\n",
      "test loss is 0.00025001530443471054\n",
      "Batch: 15100,train loss is: 0.00035202182319139384\n",
      "test loss is 0.0002499164285997142\n",
      "Batch: 15200,train loss is: 0.00031371563192640156\n",
      "test loss is 0.00022946835882279768\n",
      "Batch: 15300,train loss is: 0.00024411559328802067\n",
      "test loss is 0.0002719861646298307\n",
      "Batch: 15400,train loss is: 0.00028915552089378354\n",
      "test loss is 0.0002475869005075079\n",
      "Batch: 15500,train loss is: 0.00019634607091778534\n",
      "test loss is 0.000240998153448345\n",
      "Batch: 15600,train loss is: 0.0002663412388026764\n",
      "test loss is 0.0002833357199202054\n",
      "Batch: 15700,train loss is: 0.0003056367829527308\n",
      "test loss is 0.00023342269572951643\n",
      "Batch: 15800,train loss is: 0.00019407966863412495\n",
      "test loss is 0.000261601031271311\n",
      "Batch: 15900,train loss is: 0.00014619465908997432\n",
      "test loss is 0.00023829464555478742\n",
      "Batch: 16000,train loss is: 0.0004571686627267544\n",
      "test loss is 0.0005522521334145176\n",
      "Batch: 16100,train loss is: 0.00011485092722162487\n",
      "test loss is 0.0002575475076955231\n",
      "Batch: 16200,train loss is: 0.00016383190650140549\n",
      "test loss is 0.0002680302343064894\n",
      "Batch: 16300,train loss is: 0.0004158684656156227\n",
      "test loss is 0.0002620592048631173\n",
      "Batch: 16400,train loss is: 0.0002131040191990475\n",
      "test loss is 0.00026711537226093796\n",
      "Batch: 16500,train loss is: 0.000262831606577696\n",
      "test loss is 0.00024616774727363895\n",
      "Batch: 16600,train loss is: 0.00015381460053933273\n",
      "test loss is 0.00023910922540461616\n",
      "Batch: 16700,train loss is: 0.00019401934128732097\n",
      "test loss is 0.00028294542843645455\n",
      "Batch: 16800,train loss is: 0.00018236175224284603\n",
      "test loss is 0.00024486026587866886\n",
      "Batch: 16900,train loss is: 0.00019943726468878837\n",
      "test loss is 0.00027131946978936947\n",
      "Batch: 17000,train loss is: 0.00023846577553424293\n",
      "test loss is 0.0002499038715396161\n",
      "Batch: 17100,train loss is: 0.00020219237284569322\n",
      "test loss is 0.0002346126529406879\n",
      "Batch: 17200,train loss is: 0.00023814995193235513\n",
      "test loss is 0.0002727391051174999\n",
      "Batch: 17300,train loss is: 0.00019287889765988998\n",
      "test loss is 0.0002502787365255881\n",
      "Batch: 17400,train loss is: 0.0003010407688190756\n",
      "test loss is 0.00023931169196891216\n",
      "Batch: 17500,train loss is: 0.0004039849418758582\n",
      "test loss is 0.00022895324518178156\n",
      "Batch: 17600,train loss is: 0.0002028137026514102\n",
      "test loss is 0.00029682275659415335\n",
      "Batch: 17700,train loss is: 0.00015222208026911853\n",
      "test loss is 0.00025652920373889194\n",
      "Batch: 17800,train loss is: 0.00021729570967368754\n",
      "test loss is 0.00028846471020962597\n",
      "Batch: 17900,train loss is: 0.000195423305543304\n",
      "test loss is 0.00026930452155424326\n",
      "Batch: 18000,train loss is: 0.00030316913584319346\n",
      "test loss is 0.00024254997693769122\n",
      "Batch: 18100,train loss is: 0.00021867126372525906\n",
      "test loss is 0.0002457250643893567\n",
      "Batch: 18200,train loss is: 0.00022807032279555518\n",
      "test loss is 0.00023664680835085804\n",
      "Batch: 18300,train loss is: 0.00020373073552791622\n",
      "test loss is 0.0002662921183541739\n",
      "Batch: 18400,train loss is: 0.0002497188322998334\n",
      "test loss is 0.00028698755357665576\n",
      "Batch: 18500,train loss is: 0.0002174265519497031\n",
      "test loss is 0.0002483484594568254\n",
      "Batch: 18600,train loss is: 0.00022055253498396194\n",
      "test loss is 0.000256848319623873\n",
      "Batch: 18700,train loss is: 0.0003632115074235563\n",
      "test loss is 0.0003064342694138761\n",
      "Batch: 18800,train loss is: 0.0002509689864419396\n",
      "test loss is 0.00022480065128326356\n",
      "Batch: 18900,train loss is: 0.00017085815067708222\n",
      "test loss is 0.0002646876595160615\n",
      "Batch: 19000,train loss is: 0.00029950549593269615\n",
      "test loss is 0.00025356013103885826\n",
      "Batch: 19100,train loss is: 0.0002503600044571868\n",
      "test loss is 0.00024017934798099407\n",
      "Batch: 19200,train loss is: 0.0002625980118420938\n",
      "test loss is 0.00024145732959114\n",
      "Batch: 19300,train loss is: 0.00042065489641781423\n",
      "test loss is 0.0002565746242327943\n",
      "Batch: 19400,train loss is: 0.00030296615851758794\n",
      "test loss is 0.0002729896500967174\n",
      "Batch: 19500,train loss is: 0.00018553107151013566\n",
      "test loss is 0.0002667014330349325\n",
      "Batch: 19600,train loss is: 0.0001975338859850895\n",
      "test loss is 0.00028807220688218006\n",
      "Batch: 19700,train loss is: 0.00022965744097785772\n",
      "test loss is 0.0002401415799384136\n",
      "Batch: 19800,train loss is: 0.0001316289544356464\n",
      "test loss is 0.0002665188050638081\n",
      "Batch: 19900,train loss is: 0.00019321544625437126\n",
      "test loss is 0.00022564037577312032\n",
      "Batch: 20000,train loss is: 0.00024900558864901056\n",
      "test loss is 0.00024053731862641494\n",
      "Batch: 20100,train loss is: 0.00012920527896036595\n",
      "test loss is 0.00027731446001916173\n",
      "Batch: 20200,train loss is: 0.00023059634603718058\n",
      "test loss is 0.00024814808854810235\n",
      "Batch: 20300,train loss is: 0.0002462360383899597\n",
      "test loss is 0.0003096565017829688\n",
      "Batch: 20400,train loss is: 0.0002265894995961823\n",
      "test loss is 0.00022844104758148654\n",
      "Batch: 20500,train loss is: 0.000213693303325503\n",
      "test loss is 0.0002536308945677717\n",
      "Batch: 20600,train loss is: 0.00027731328777569884\n",
      "test loss is 0.00025118911739013475\n",
      "Batch: 20700,train loss is: 0.0002618987132006051\n",
      "test loss is 0.00023177904814700086\n",
      "Batch: 20800,train loss is: 0.0002416078900745963\n",
      "test loss is 0.00027503228413861724\n",
      "Batch: 20900,train loss is: 0.0002469815761373935\n",
      "test loss is 0.000244005922098347\n",
      "Batch: 21000,train loss is: 0.00030700666058670596\n",
      "test loss is 0.0002621350450150062\n",
      "Batch: 21100,train loss is: 0.0002076215564873135\n",
      "test loss is 0.00026904942846418773\n",
      "Batch: 21200,train loss is: 0.00023129609959114504\n",
      "test loss is 0.0002627203498206994\n",
      "Batch: 21300,train loss is: 0.0003608213352508947\n",
      "test loss is 0.0003258486383449683\n",
      "Batch: 21400,train loss is: 0.00031395476413335975\n",
      "test loss is 0.0002928846960281007\n",
      "Batch: 21500,train loss is: 0.00028113778886873763\n",
      "test loss is 0.0002370204611161426\n",
      "Batch: 21600,train loss is: 0.0003453780572165015\n",
      "test loss is 0.0002553388867836481\n",
      "Batch: 21700,train loss is: 0.00035029998041700213\n",
      "test loss is 0.00023776486709396635\n",
      "Batch: 21800,train loss is: 0.0002203172923785606\n",
      "test loss is 0.00022374808177510306\n",
      "Batch: 21900,train loss is: 0.00021803704532396512\n",
      "test loss is 0.0002603615903388249\n",
      "Batch: 22000,train loss is: 0.0002456266266835932\n",
      "test loss is 0.00028187654894260163\n",
      "Batch: 22100,train loss is: 0.0002758628456768567\n",
      "test loss is 0.000233974175445249\n",
      "Batch: 22200,train loss is: 0.00023095046400770284\n",
      "test loss is 0.0002552404420943387\n",
      "Batch: 22300,train loss is: 0.0001548419922997991\n",
      "test loss is 0.00023480811456238197\n",
      "Batch: 22400,train loss is: 0.00013553752217363422\n",
      "test loss is 0.0002650917799600683\n",
      "Batch: 22500,train loss is: 0.00021354124313065236\n",
      "test loss is 0.00023277164497294866\n",
      "Batch: 22600,train loss is: 0.00015475710022743984\n",
      "test loss is 0.00023451787794667974\n",
      "Batch: 22700,train loss is: 0.00021124006361858126\n",
      "test loss is 0.0002622221624897539\n",
      "Batch: 22800,train loss is: 0.0001724105582449091\n",
      "test loss is 0.00024445565631612165\n",
      "Batch: 22900,train loss is: 0.0002250359154947178\n",
      "test loss is 0.0002933905457632916\n",
      "Batch: 23000,train loss is: 0.0002693092070645337\n",
      "test loss is 0.00024441288829205374\n",
      "Batch: 23100,train loss is: 0.000267811396573904\n",
      "test loss is 0.00027359057686322905\n",
      "Batch: 23200,train loss is: 0.00021446132819402395\n",
      "test loss is 0.00027407651182627694\n",
      "Batch: 23300,train loss is: 0.00036611221830979606\n",
      "test loss is 0.0002862444831505391\n",
      "Batch: 23400,train loss is: 0.00037143899119190425\n",
      "test loss is 0.0002457648776857164\n",
      "Batch: 23500,train loss is: 0.00018691969688456152\n",
      "test loss is 0.0002315358758139357\n",
      "Batch: 23600,train loss is: 0.0003425813192831997\n",
      "test loss is 0.000248629937001231\n",
      "Batch: 23700,train loss is: 0.00021204753538823652\n",
      "test loss is 0.0003220773041927443\n",
      "Batch: 23800,train loss is: 0.0002455844896885028\n",
      "test loss is 0.0002718958627645973\n",
      "Batch: 23900,train loss is: 0.00022944719214721963\n",
      "test loss is 0.00026396526722212893\n",
      "Batch: 24000,train loss is: 0.0002496502569252641\n",
      "test loss is 0.00028071160861112656\n",
      "Batch: 24100,train loss is: 0.00022917605598405175\n",
      "test loss is 0.0002496429265653903\n",
      "Batch: 24200,train loss is: 0.00034499156279187187\n",
      "test loss is 0.0002574536017949078\n",
      "Batch: 24300,train loss is: 0.0007468016582760573\n",
      "test loss is 0.0002766081347938113\n",
      "Batch: 24400,train loss is: 0.000305947355881789\n",
      "test loss is 0.0002556372157878844\n",
      "Batch: 24500,train loss is: 0.0002305874730361023\n",
      "test loss is 0.00029557004753209744\n",
      "Batch: 24600,train loss is: 0.0001835675967887182\n",
      "test loss is 0.0002455382954434635\n",
      "Batch: 24700,train loss is: 0.00021799848863869223\n",
      "test loss is 0.0002492668680182112\n",
      "Batch: 24800,train loss is: 0.00034726173951757117\n",
      "test loss is 0.00028590842909268144\n",
      "Batch: 24900,train loss is: 0.0003076209531404824\n",
      "test loss is 0.0002876186306160808\n",
      "Batch: 25000,train loss is: 0.000305671501240837\n",
      "test loss is 0.00029263431958996805\n",
      "Batch: 25100,train loss is: 0.00022145253936548388\n",
      "test loss is 0.0003235344037967477\n",
      "Batch: 25200,train loss is: 0.0002496125806188647\n",
      "test loss is 0.0002738414868725807\n",
      "Batch: 25300,train loss is: 0.00022035541898779955\n",
      "test loss is 0.00024638413462409024\n",
      "Batch: 25400,train loss is: 0.00018835736215298845\n",
      "test loss is 0.0002760583792677237\n",
      "Batch: 25500,train loss is: 0.000246140708450268\n",
      "test loss is 0.0003414119383807301\n",
      "Batch: 25600,train loss is: 0.0001420138262955797\n",
      "test loss is 0.00028498911740552967\n",
      "Batch: 25700,train loss is: 0.0003208390864409194\n",
      "test loss is 0.00032888429637713593\n",
      "Batch: 25800,train loss is: 0.00019889296305213732\n",
      "test loss is 0.0002647191984998151\n",
      "Batch: 25900,train loss is: 0.00020938514867419378\n",
      "test loss is 0.00024267489685953584\n",
      "Batch: 26000,train loss is: 0.00017494239769517083\n",
      "test loss is 0.00023515420688101653\n",
      "Batch: 26100,train loss is: 0.00021457407959423228\n",
      "test loss is 0.00024170194296894022\n",
      "Batch: 26200,train loss is: 0.00021166979569515874\n",
      "test loss is 0.0002700250833295803\n",
      "Batch: 26300,train loss is: 0.0001833010929622605\n",
      "test loss is 0.0002580047154098609\n",
      "Batch: 26400,train loss is: 0.0002638608365614942\n",
      "test loss is 0.000246568970688998\n",
      "Batch: 26500,train loss is: 0.0002630794533784573\n",
      "test loss is 0.00029105424348366835\n",
      "Batch: 26600,train loss is: 0.00028306541134139333\n",
      "test loss is 0.00024511083346939\n",
      "Batch: 26700,train loss is: 0.0002198338671117259\n",
      "test loss is 0.00022546088441787037\n",
      "Batch: 26800,train loss is: 0.0001444196927297576\n",
      "test loss is 0.00023225982563117455\n",
      "Batch: 26900,train loss is: 0.00011955824866100419\n",
      "test loss is 0.0002317426812046049\n",
      "Batch: 27000,train loss is: 0.0003060553838810739\n",
      "test loss is 0.0002670376374658387\n",
      "Batch: 27100,train loss is: 0.00030330536908899843\n",
      "test loss is 0.0002527742833659207\n",
      "Batch: 27200,train loss is: 0.0002222807493510136\n",
      "test loss is 0.00023023523918804092\n",
      "Batch: 27300,train loss is: 0.0002926377428045442\n",
      "test loss is 0.0002603094398010514\n",
      "Batch: 27400,train loss is: 0.0004873385246699281\n",
      "test loss is 0.0003248294784077778\n",
      "Batch: 27500,train loss is: 0.0002631798258106594\n",
      "test loss is 0.00023176017148528744\n",
      "Batch: 27600,train loss is: 0.00025126422367382313\n",
      "test loss is 0.00024445874569030385\n",
      "Batch: 27700,train loss is: 0.00022997435111961537\n",
      "test loss is 0.0002188411891061235\n",
      "Batch: 27800,train loss is: 0.00023896318998926095\n",
      "test loss is 0.00030083704309508515\n",
      "Batch: 27900,train loss is: 0.00028681221943389326\n",
      "test loss is 0.0002621072588878507\n",
      "Batch: 28000,train loss is: 0.0003211454116767001\n",
      "test loss is 0.0003048820152385318\n",
      "Batch: 28100,train loss is: 0.0002860707759304371\n",
      "test loss is 0.00023249433781974776\n",
      "Batch: 28200,train loss is: 0.00029610083437751043\n",
      "test loss is 0.00024271814567119592\n",
      "Batch: 28300,train loss is: 0.00020483233998984472\n",
      "test loss is 0.0002464383265070048\n",
      "Batch: 28400,train loss is: 0.0001413877810191855\n",
      "test loss is 0.00023135156988230145\n",
      "Batch: 28500,train loss is: 0.00028030912653315414\n",
      "test loss is 0.00024305613098297985\n",
      "Batch: 28600,train loss is: 0.00022997407385487475\n",
      "test loss is 0.00027822503444387695\n",
      "Batch: 28700,train loss is: 0.0002840556923138271\n",
      "test loss is 0.0002469361706212634\n",
      "Batch: 28800,train loss is: 0.0002434841496109244\n",
      "test loss is 0.0002835994897818882\n",
      "Batch: 28900,train loss is: 0.000244865815066626\n",
      "test loss is 0.00027061319241367327\n",
      "Batch: 29000,train loss is: 0.00021889437779965958\n",
      "test loss is 0.0002941779567894173\n",
      "Batch: 29100,train loss is: 0.0002035105249293457\n",
      "test loss is 0.0002637367278353664\n",
      "Batch: 29200,train loss is: 0.00025846454395266614\n",
      "test loss is 0.0002869063197597525\n",
      "Batch: 29300,train loss is: 0.00024043906961440366\n",
      "test loss is 0.00022559113842146743\n",
      "Batch: 29400,train loss is: 0.0002880004465899476\n",
      "test loss is 0.00023417845101854487\n",
      "Batch: 29500,train loss is: 0.00022839227046585236\n",
      "test loss is 0.000285723605810242\n",
      "Batch: 29600,train loss is: 0.00015486760461768485\n",
      "test loss is 0.00024381289321882355\n",
      "Batch: 29700,train loss is: 0.00022964332470407306\n",
      "test loss is 0.00025586215227394084\n",
      "Batch: 29800,train loss is: 0.00016311740445292176\n",
      "test loss is 0.00026192327849305566\n",
      "Batch: 29900,train loss is: 0.00027286659836894757\n",
      "test loss is 0.0002525842082841237\n",
      "Batch: 30000,train loss is: 0.00016816063185757206\n",
      "test loss is 0.00024488996676805643\n",
      "Batch: 30100,train loss is: 0.00024318391798023087\n",
      "test loss is 0.00022614509216165355\n",
      "Batch: 30200,train loss is: 0.0002525694837751698\n",
      "test loss is 0.0002594407349404254\n",
      "Batch: 30300,train loss is: 0.0001444394392538481\n",
      "test loss is 0.0002467447461390647\n",
      "Batch: 30400,train loss is: 0.0001913424789110118\n",
      "test loss is 0.00023893268950160988\n",
      "Batch: 30500,train loss is: 0.00028852603198230146\n",
      "test loss is 0.0003287723653308919\n",
      "Batch: 30600,train loss is: 0.0003883031037533716\n",
      "test loss is 0.0002452547356412666\n",
      "Batch: 30700,train loss is: 0.00021412004631602604\n",
      "test loss is 0.00022462178361985386\n",
      "Batch: 30800,train loss is: 0.0003071557383585127\n",
      "test loss is 0.00025245388837390474\n",
      "Batch: 30900,train loss is: 0.000216479530906067\n",
      "test loss is 0.0002499273455359707\n",
      "Batch: 31000,train loss is: 0.00023673102420601158\n",
      "test loss is 0.0002912808628785602\n",
      "Batch: 31100,train loss is: 0.0003592375927090025\n",
      "test loss is 0.00038339600776624504\n",
      "Batch: 31200,train loss is: 0.00013656555926067497\n",
      "test loss is 0.00023066747707790398\n",
      "Batch: 31300,train loss is: 0.0002697408641847472\n",
      "test loss is 0.0002245794745761078\n",
      "Batch: 31400,train loss is: 0.00019424617158745702\n",
      "test loss is 0.00024198723562466075\n",
      "Batch: 31500,train loss is: 0.0002785872129523011\n",
      "test loss is 0.00027270399331215796\n",
      "Batch: 31600,train loss is: 0.0003174281070001182\n",
      "test loss is 0.0002486131376438353\n",
      "Batch: 31700,train loss is: 0.0002343469289846904\n",
      "test loss is 0.00025293294176510344\n",
      "Batch: 31800,train loss is: 0.0002197861241101348\n",
      "test loss is 0.00024048704263700225\n",
      "Batch: 31900,train loss is: 0.00018267194968692375\n",
      "test loss is 0.00026059248919095486\n",
      "Batch: 32000,train loss is: 0.00019357213910738027\n",
      "test loss is 0.00024231438712036217\n",
      "Batch: 32100,train loss is: 0.000353898878432722\n",
      "test loss is 0.0002774687459986083\n",
      "Batch: 32200,train loss is: 0.00023623265895067146\n",
      "test loss is 0.00022689201190308\n",
      "Batch: 32300,train loss is: 0.00020620712112662633\n",
      "test loss is 0.0002650218203612337\n",
      "Batch: 32400,train loss is: 0.0004028477058960206\n",
      "test loss is 0.00023520991626389663\n",
      "Batch: 32500,train loss is: 0.00019915132115759355\n",
      "test loss is 0.0002459935109501278\n",
      "Batch: 32600,train loss is: 0.00019807993533292983\n",
      "test loss is 0.00032026215414945105\n",
      "Batch: 32700,train loss is: 0.00032562672237764655\n",
      "test loss is 0.00027610270286300317\n",
      "Batch: 32800,train loss is: 0.00044493100643561975\n",
      "test loss is 0.00028822039270631907\n",
      "Batch: 32900,train loss is: 0.0003134210244273478\n",
      "test loss is 0.00026221203374183566\n",
      "Batch: 33000,train loss is: 0.0001914810827260874\n",
      "test loss is 0.00023473565637224727\n",
      "Batch: 33100,train loss is: 0.0002082782067620972\n",
      "test loss is 0.00026420651745111197\n",
      "Batch: 33200,train loss is: 0.0003324418236620777\n",
      "test loss is 0.0002437291192966509\n",
      "Batch: 33300,train loss is: 0.00036595278116544874\n",
      "test loss is 0.0002515832855235978\n",
      "Batch: 33400,train loss is: 0.0004141242532349289\n",
      "test loss is 0.0002782414114086651\n",
      "Batch: 33500,train loss is: 0.00028619423295661784\n",
      "test loss is 0.0003040065272091274\n",
      "Batch: 33600,train loss is: 0.00016966263302942543\n",
      "test loss is 0.0002442165635060007\n",
      "Batch: 33700,train loss is: 0.00018688143962827965\n",
      "test loss is 0.00022854437000630604\n",
      "Batch: 33800,train loss is: 0.00022473438684339858\n",
      "test loss is 0.0002493137829266851\n",
      "Batch: 33900,train loss is: 0.0002607440572119281\n",
      "test loss is 0.00023790571768887088\n",
      "Batch: 34000,train loss is: 0.0002665017503230008\n",
      "test loss is 0.0002745806174886198\n",
      "Batch: 34100,train loss is: 0.0001749434396687624\n",
      "test loss is 0.00024234002662467413\n",
      "Batch: 34200,train loss is: 0.0003434419847656758\n",
      "test loss is 0.00029930861465381464\n",
      "Batch: 34300,train loss is: 0.00018511459168336913\n",
      "test loss is 0.0002760055965881569\n",
      "Batch: 34400,train loss is: 0.00027558749801756365\n",
      "test loss is 0.00026165710980296917\n",
      "Batch: 34500,train loss is: 0.00024259569954270096\n",
      "test loss is 0.0002618034140159263\n",
      "Batch: 34600,train loss is: 0.00022400480139178803\n",
      "test loss is 0.0002690996136740074\n",
      "Batch: 34700,train loss is: 0.00029233588283331623\n",
      "test loss is 0.0002979068732678122\n",
      "Batch: 34800,train loss is: 0.00015847569407021167\n",
      "test loss is 0.00023866190755174029\n",
      "Batch: 34900,train loss is: 0.00026649417496332707\n",
      "test loss is 0.00033901100810305415\n",
      "Batch: 35000,train loss is: 0.0001262808020468699\n",
      "test loss is 0.0002379256393898231\n",
      "Batch: 35100,train loss is: 0.00022140572601560613\n",
      "test loss is 0.0003075002862197146\n",
      "Batch: 35200,train loss is: 0.00019765005831040528\n",
      "test loss is 0.00028347873645114105\n",
      "Batch: 35300,train loss is: 0.00027438593237302024\n",
      "test loss is 0.0002887286957887374\n",
      "Batch: 35400,train loss is: 0.00040296769147165885\n",
      "test loss is 0.0002808276244751654\n",
      "Batch: 35500,train loss is: 0.00022287914965624372\n",
      "test loss is 0.00036139017626378895\n",
      "Batch: 35600,train loss is: 0.0001811394637678137\n",
      "test loss is 0.0002558203021194482\n",
      "Batch: 35700,train loss is: 0.00019380094355551122\n",
      "test loss is 0.00026768460859265643\n",
      "Batch: 35800,train loss is: 0.0003526829966510353\n",
      "test loss is 0.00023325705318750776\n",
      "Batch: 35900,train loss is: 0.00018370996950957744\n",
      "test loss is 0.00030590342379405405\n",
      "Batch: 36000,train loss is: 0.00037584694249922124\n",
      "test loss is 0.00023791056886051358\n",
      "Batch: 36100,train loss is: 0.00025060797431215615\n",
      "test loss is 0.0002993965421775754\n",
      "Batch: 36200,train loss is: 0.00020776426390232527\n",
      "test loss is 0.00023347116705501244\n",
      "Batch: 36300,train loss is: 0.0002451515593481932\n",
      "test loss is 0.00030333635889831387\n",
      "Batch: 36400,train loss is: 0.00021412982550052072\n",
      "test loss is 0.0004027518763711856\n",
      "Batch: 36500,train loss is: 0.00023567015066357807\n",
      "test loss is 0.0002508991803457709\n",
      "Batch: 36600,train loss is: 0.00036808913208346335\n",
      "test loss is 0.00026007733069406013\n",
      "Batch: 36700,train loss is: 0.00019629753155540257\n",
      "test loss is 0.00024827586493408497\n",
      "Batch: 36800,train loss is: 0.00023747423916640052\n",
      "test loss is 0.0002553865459615818\n",
      "Batch: 36900,train loss is: 0.00030341589294009824\n",
      "test loss is 0.00024212359725886263\n",
      "Batch: 37000,train loss is: 0.000284185962939658\n",
      "test loss is 0.00035061174713237514\n",
      "Batch: 37100,train loss is: 0.0002578029405605317\n",
      "test loss is 0.0002357895292300154\n",
      "Batch: 37200,train loss is: 0.0002415879263288636\n",
      "test loss is 0.00027831556588854474\n",
      "Batch: 37300,train loss is: 0.00019115351909378201\n",
      "test loss is 0.0002428037532940282\n",
      "Batch: 37400,train loss is: 0.0002360738166225078\n",
      "test loss is 0.0002588449759853691\n",
      "Batch: 37500,train loss is: 0.0006464079445895109\n",
      "test loss is 0.00026671349905623234\n",
      "Batch: 37600,train loss is: 0.0001818380063800134\n",
      "test loss is 0.00026398401147572606\n",
      "Batch: 37700,train loss is: 0.00028123209062964384\n",
      "test loss is 0.00023150766486027715\n",
      "Batch: 37800,train loss is: 0.0002220731955488004\n",
      "test loss is 0.000267246040357647\n",
      "Batch: 37900,train loss is: 0.0002062336261233216\n",
      "test loss is 0.00024003355592909013\n",
      "Batch: 38000,train loss is: 0.00020843383061841937\n",
      "test loss is 0.0002442905607369638\n",
      "Batch: 38100,train loss is: 0.00022221287932949903\n",
      "test loss is 0.00024785310100825994\n",
      "Batch: 38200,train loss is: 0.00015789639863457338\n",
      "test loss is 0.000230836318412928\n",
      "Batch: 38300,train loss is: 0.00015818070509891887\n",
      "test loss is 0.0002431633759636743\n",
      "Batch: 38400,train loss is: 0.0002273269735405443\n",
      "test loss is 0.0002373930730713645\n",
      "Batch: 38500,train loss is: 0.0001852280439054194\n",
      "test loss is 0.0002458990900936574\n",
      "Batch: 38600,train loss is: 0.00036077012968468666\n",
      "test loss is 0.00024062413659543084\n",
      "Batch: 38700,train loss is: 0.00017923105843410003\n",
      "test loss is 0.00024054969360523843\n",
      "Batch: 38800,train loss is: 0.0003151514269928754\n",
      "test loss is 0.0003154819053138849\n",
      "Batch: 38900,train loss is: 0.0002845404259958933\n",
      "test loss is 0.00025999539958578976\n",
      "Batch: 39000,train loss is: 0.00018406160468806016\n",
      "test loss is 0.00023271209453080903\n",
      "Batch: 39100,train loss is: 0.0002446268431408382\n",
      "test loss is 0.0002599349236211714\n",
      "Batch: 39200,train loss is: 0.00015536456307512013\n",
      "test loss is 0.00024872208134548893\n",
      "Batch: 39300,train loss is: 0.00020675599746961172\n",
      "test loss is 0.00032131428704322595\n",
      "Batch: 39400,train loss is: 0.0002806176874783959\n",
      "test loss is 0.00023726676866349362\n",
      "Batch: 39500,train loss is: 0.0001366298345797542\n",
      "test loss is 0.00032770812913133384\n",
      "Batch: 39600,train loss is: 0.0002238778876799466\n",
      "test loss is 0.0002577978244591706\n",
      "Batch: 39700,train loss is: 0.0002381423788399426\n",
      "test loss is 0.00025954733243073566\n",
      "Batch: 39800,train loss is: 0.00016341608140386856\n",
      "test loss is 0.00023064175571108444\n",
      "Batch: 39900,train loss is: 0.0002017040974510772\n",
      "test loss is 0.0002443141402579114\n",
      "Batch: 40000,train loss is: 0.0003949703387519329\n",
      "test loss is 0.0002654502890481674\n",
      "Batch: 40100,train loss is: 0.00011039330315451103\n",
      "test loss is 0.00024418369451924694\n",
      "Batch: 40200,train loss is: 0.00017526505743917359\n",
      "test loss is 0.00034989744260613637\n",
      "Batch: 40300,train loss is: 0.00015598504604727784\n",
      "test loss is 0.00027892467750536594\n",
      "Batch: 40400,train loss is: 0.00031901569151113017\n",
      "test loss is 0.0002530506894337222\n",
      "Batch: 40500,train loss is: 0.0005626528999402881\n",
      "test loss is 0.00025078479410521685\n",
      "Batch: 40600,train loss is: 0.00015380061136098118\n",
      "test loss is 0.0002546771908808995\n",
      "Batch: 40700,train loss is: 0.00028691690955809\n",
      "test loss is 0.0003085250589066874\n",
      "Batch: 40800,train loss is: 0.00024950122975554137\n",
      "test loss is 0.0002650926943819292\n",
      "Batch: 40900,train loss is: 0.0002575388284249566\n",
      "test loss is 0.00025540895975587405\n",
      "Batch: 41000,train loss is: 0.00021836083069997231\n",
      "test loss is 0.000329884850006567\n",
      "Batch: 41100,train loss is: 0.0002740086218894819\n",
      "test loss is 0.00023437895072794847\n",
      "Batch: 41200,train loss is: 0.0002278468100742821\n",
      "test loss is 0.0002390874007278688\n",
      "Batch: 41300,train loss is: 0.00010744108653954719\n",
      "test loss is 0.00022674851873124525\n",
      "Batch: 41400,train loss is: 0.00031390102026908806\n",
      "test loss is 0.0002832091545779938\n",
      "Batch: 41500,train loss is: 0.00015578994006808155\n",
      "test loss is 0.00025470173426521235\n",
      "Batch: 41600,train loss is: 0.0005949062590677249\n",
      "test loss is 0.00024287189973480188\n",
      "Batch: 41700,train loss is: 0.00023602770560340777\n",
      "test loss is 0.0002534628327182266\n",
      "Batch: 41800,train loss is: 0.00017550095394373898\n",
      "test loss is 0.000257961829384453\n",
      "Batch: 41900,train loss is: 0.00014432541043384487\n",
      "test loss is 0.00023834773719637426\n",
      "Batch: 42000,train loss is: 0.0002969524407437953\n",
      "test loss is 0.0002580319021191048\n",
      "Batch: 42100,train loss is: 0.00022815398873552497\n",
      "test loss is 0.00024126201651177942\n",
      "Batch: 42200,train loss is: 0.0002115509385267195\n",
      "test loss is 0.0002504934021280125\n",
      "Batch: 42300,train loss is: 0.00019327136199634925\n",
      "test loss is 0.00023938329396235334\n",
      "Batch: 42400,train loss is: 0.00020844050994176655\n",
      "test loss is 0.00023597026589434363\n",
      "Batch: 42500,train loss is: 0.0001716976006468194\n",
      "test loss is 0.00024127037717584627\n",
      "Batch: 42600,train loss is: 0.0002472607366042634\n",
      "test loss is 0.00027448063804988974\n",
      "Batch: 42700,train loss is: 0.00027510016210322366\n",
      "test loss is 0.0002294489624193835\n",
      "Batch: 42800,train loss is: 0.00019229056074071392\n",
      "test loss is 0.00027722572306286027\n",
      "Batch: 42900,train loss is: 0.00017770659554819596\n",
      "test loss is 0.00029037419253843987\n",
      "Batch: 43000,train loss is: 0.00029864660918950175\n",
      "test loss is 0.00023067500560312163\n",
      "Batch: 43100,train loss is: 0.00026692436504044664\n",
      "test loss is 0.0002676726902827431\n",
      "Batch: 43200,train loss is: 0.0003493954674258883\n",
      "test loss is 0.0002503863730391942\n",
      "Batch: 43300,train loss is: 0.0003154006275587883\n",
      "test loss is 0.00022849920015468667\n",
      "Batch: 43400,train loss is: 0.00022206988704455237\n",
      "test loss is 0.0002838254782329958\n",
      "Batch: 43500,train loss is: 0.00047096163467336816\n",
      "test loss is 0.00024682366521249855\n",
      "Batch: 43600,train loss is: 0.00019560147661678597\n",
      "test loss is 0.00024966900259482794\n",
      "Batch: 43700,train loss is: 0.0001771652542442091\n",
      "test loss is 0.00030281604156101615\n",
      "Batch: 43800,train loss is: 0.0002585174382708034\n",
      "test loss is 0.00025412642047728344\n",
      "Batch: 43900,train loss is: 0.00025154408366408513\n",
      "test loss is 0.00024815922183209395\n",
      "Batch: 44000,train loss is: 0.00022980777851222946\n",
      "test loss is 0.000240182320182362\n",
      "Batch: 44100,train loss is: 0.0002736157325696026\n",
      "test loss is 0.00022691868068387504\n",
      "Batch: 44200,train loss is: 0.00023806726667453143\n",
      "test loss is 0.00024643424636049577\n",
      "Batch: 44300,train loss is: 0.0003045904918876053\n",
      "test loss is 0.00024895748420644047\n",
      "Batch: 44400,train loss is: 0.00016321200672494456\n",
      "test loss is 0.00025367917330389346\n",
      "Batch: 44500,train loss is: 0.0004489601959072821\n",
      "test loss is 0.0002530531675875654\n",
      "Batch: 44600,train loss is: 0.00034801257687602766\n",
      "test loss is 0.0003002576669866669\n",
      "Batch: 44700,train loss is: 0.0001786379718997265\n",
      "test loss is 0.000224241907287976\n",
      "Batch: 44800,train loss is: 0.00016320551491178997\n",
      "test loss is 0.00025182651706407447\n",
      "Batch: 44900,train loss is: 0.0002232632052483316\n",
      "test loss is 0.00030886253854829794\n",
      "Batch: 45000,train loss is: 0.00023531416845174935\n",
      "test loss is 0.0002907798353662621\n",
      "Batch: 45100,train loss is: 0.000205695156554126\n",
      "test loss is 0.00023032242346660218\n",
      "Batch: 45200,train loss is: 0.00036055688785862543\n",
      "test loss is 0.00026896907635428\n",
      "Batch: 45300,train loss is: 0.0003676325741363142\n",
      "test loss is 0.00025826554949930547\n",
      "Batch: 45400,train loss is: 0.00024086097011603738\n",
      "test loss is 0.00027536260714735596\n",
      "Batch: 45500,train loss is: 0.00015530902107177907\n",
      "test loss is 0.00025553469200979904\n",
      "Batch: 45600,train loss is: 0.00023562662194300294\n",
      "test loss is 0.0002951419378634039\n",
      "Batch: 45700,train loss is: 0.00035020593482618713\n",
      "test loss is 0.0002679293285515349\n",
      "Batch: 45800,train loss is: 0.00024248502793438093\n",
      "test loss is 0.00024507812209169014\n",
      "Batch: 45900,train loss is: 0.00022735553173281782\n",
      "test loss is 0.00024566075702638584\n",
      "Batch: 46000,train loss is: 0.00014235303678878449\n",
      "test loss is 0.0002422656030864763\n",
      "Batch: 46100,train loss is: 0.00026217799963052706\n",
      "test loss is 0.00027490598518737047\n",
      "Batch: 46200,train loss is: 0.00020691653689659491\n",
      "test loss is 0.0002234181922637724\n",
      "Batch: 46300,train loss is: 0.0002406262292527011\n",
      "test loss is 0.00023907990671479966\n",
      "Batch: 46400,train loss is: 0.0002743549131785959\n",
      "test loss is 0.00023168735923634128\n",
      "Batch: 46500,train loss is: 0.0002605662485585728\n",
      "test loss is 0.00024854612005781253\n",
      "Batch: 46600,train loss is: 0.0003588556137391093\n",
      "test loss is 0.000272675141063008\n",
      "Batch: 46700,train loss is: 0.00022031747656545936\n",
      "test loss is 0.00023587371605331102\n",
      "-----------------------Epoch: 13----------------------------------\n",
      "Batch: 0,train loss is: 0.00021423828603425304\n",
      "test loss is 0.00025664380744824713\n",
      "Batch: 100,train loss is: 0.0001810341420661674\n",
      "test loss is 0.0002623002883357609\n",
      "Batch: 200,train loss is: 0.00020330804432481059\n",
      "test loss is 0.0002680332177484473\n",
      "Batch: 300,train loss is: 0.00021143477107383232\n",
      "test loss is 0.0002533349229341043\n",
      "Batch: 400,train loss is: 0.00017675259499992055\n",
      "test loss is 0.0002553779911869126\n",
      "Batch: 500,train loss is: 0.00023608352323568579\n",
      "test loss is 0.0002550657853897365\n",
      "Batch: 600,train loss is: 0.00015729834992905426\n",
      "test loss is 0.0002345547703233981\n",
      "Batch: 700,train loss is: 0.00021908141712272307\n",
      "test loss is 0.0002577760332172091\n",
      "Batch: 800,train loss is: 0.000290760571449842\n",
      "test loss is 0.0002491002587870774\n",
      "Batch: 900,train loss is: 0.0003237721226144665\n",
      "test loss is 0.0003619350963851498\n",
      "Batch: 1000,train loss is: 0.0002562061595141994\n",
      "test loss is 0.00026803195909905664\n",
      "Batch: 1100,train loss is: 0.0001605947978170324\n",
      "test loss is 0.00023045898945125724\n",
      "Batch: 1200,train loss is: 0.00022270769233652686\n",
      "test loss is 0.0003013639089564763\n",
      "Batch: 1300,train loss is: 0.00024103728465056773\n",
      "test loss is 0.00023422118248875073\n",
      "Batch: 1400,train loss is: 0.00035760769506292423\n",
      "test loss is 0.00031530207811396874\n",
      "Batch: 1500,train loss is: 0.000368971175039947\n",
      "test loss is 0.00027797781754324117\n",
      "Batch: 1600,train loss is: 0.00026747721519317254\n",
      "test loss is 0.00024714940026879543\n",
      "Batch: 1700,train loss is: 0.00020644253805273392\n",
      "test loss is 0.00022966999817185707\n",
      "Batch: 1800,train loss is: 0.0002411322854283638\n",
      "test loss is 0.00029603728929945356\n",
      "Batch: 1900,train loss is: 0.00032353371903866794\n",
      "test loss is 0.00022554929226916473\n",
      "Batch: 2000,train loss is: 0.00020519912471557775\n",
      "test loss is 0.00031895343672934417\n",
      "Batch: 2100,train loss is: 0.0002701340863769055\n",
      "test loss is 0.0002586675379213876\n",
      "Batch: 2200,train loss is: 0.00022739711476477305\n",
      "test loss is 0.00024278575423974697\n",
      "Batch: 2300,train loss is: 0.0001768016434815659\n",
      "test loss is 0.00025093791088928453\n",
      "Batch: 2400,train loss is: 0.00030036573818431947\n",
      "test loss is 0.0002779101651619002\n",
      "Batch: 2500,train loss is: 0.00027204660845405365\n",
      "test loss is 0.0002555173713728283\n",
      "Batch: 2600,train loss is: 0.00032914660876307933\n",
      "test loss is 0.00025048750734169554\n",
      "Batch: 2700,train loss is: 0.0002110535426940106\n",
      "test loss is 0.00024950287657278644\n",
      "Batch: 2800,train loss is: 0.00017034258171507225\n",
      "test loss is 0.0002533957544971556\n",
      "Batch: 2900,train loss is: 0.00030957854820736733\n",
      "test loss is 0.0002645154987512792\n",
      "Batch: 3000,train loss is: 0.00029345801107964027\n",
      "test loss is 0.0002805899027888527\n",
      "Batch: 3100,train loss is: 0.0002885849804314447\n",
      "test loss is 0.0002706876916922163\n",
      "Batch: 3200,train loss is: 0.0001916550416809381\n",
      "test loss is 0.00025919992113097835\n",
      "Batch: 3300,train loss is: 0.00017837538588595788\n",
      "test loss is 0.0002524050218129002\n",
      "Batch: 3400,train loss is: 0.0003618624181356764\n",
      "test loss is 0.00031824528270723765\n",
      "Batch: 3500,train loss is: 0.00028519674578896735\n",
      "test loss is 0.0002350644187903615\n",
      "Batch: 3600,train loss is: 0.0002591722015881016\n",
      "test loss is 0.00026774304749109317\n",
      "Batch: 3700,train loss is: 0.00026039357307956095\n",
      "test loss is 0.0002457390175409991\n",
      "Batch: 3800,train loss is: 0.0010333421090335697\n",
      "test loss is 0.0003020706579683292\n",
      "Batch: 3900,train loss is: 0.00016389784016175856\n",
      "test loss is 0.0002605743187205951\n",
      "Batch: 4000,train loss is: 0.0002739041446230281\n",
      "test loss is 0.0002592250300463367\n",
      "Batch: 4100,train loss is: 0.0003206727911645264\n",
      "test loss is 0.0002813904810496265\n",
      "Batch: 4200,train loss is: 0.00040251766271289\n",
      "test loss is 0.000338139050659283\n",
      "Batch: 4300,train loss is: 0.00021023273395733667\n",
      "test loss is 0.00024973757949381994\n",
      "Batch: 4400,train loss is: 0.00021657262220992437\n",
      "test loss is 0.00025137546474717236\n",
      "Batch: 4500,train loss is: 0.00022344833180179985\n",
      "test loss is 0.00028300936847144533\n",
      "Batch: 4600,train loss is: 0.0001981550000121881\n",
      "test loss is 0.00024966533836851354\n",
      "Batch: 4700,train loss is: 0.00018096872220059614\n",
      "test loss is 0.0002832438099723328\n",
      "Batch: 4800,train loss is: 0.0002556553865942192\n",
      "test loss is 0.0003091256405683648\n",
      "Batch: 4900,train loss is: 0.0002060215808188258\n",
      "test loss is 0.00023470539867103294\n",
      "Batch: 5000,train loss is: 0.0001724288278023663\n",
      "test loss is 0.0002701368526152193\n",
      "Batch: 5100,train loss is: 0.00026983604508079253\n",
      "test loss is 0.0002673219060121009\n",
      "Batch: 5200,train loss is: 0.00023582298665566513\n",
      "test loss is 0.00023372466879443564\n",
      "Batch: 5300,train loss is: 0.0002148959425181396\n",
      "test loss is 0.0002444568403124802\n",
      "Batch: 5400,train loss is: 0.00034532793247732277\n",
      "test loss is 0.0002513914246117024\n",
      "Batch: 5500,train loss is: 0.00034907999506513855\n",
      "test loss is 0.00023518464790352956\n",
      "Batch: 5600,train loss is: 0.00016180806478647067\n",
      "test loss is 0.00024997670620972493\n",
      "Batch: 5700,train loss is: 0.00016022692123788418\n",
      "test loss is 0.00023530384193576678\n",
      "Batch: 5800,train loss is: 0.0002205355817823314\n",
      "test loss is 0.00030917038908776786\n",
      "Batch: 5900,train loss is: 0.0001879441387313027\n",
      "test loss is 0.0002816090211986087\n",
      "Batch: 6000,train loss is: 0.00014963696625440256\n",
      "test loss is 0.0002509966496291109\n",
      "Batch: 6100,train loss is: 0.00024040999127367737\n",
      "test loss is 0.00029122583871584964\n",
      "Batch: 6200,train loss is: 0.0002863540148045219\n",
      "test loss is 0.00024386013260735987\n",
      "Batch: 6300,train loss is: 0.00013034919112869418\n",
      "test loss is 0.0002386519947178578\n",
      "Batch: 6400,train loss is: 0.00025542511330864045\n",
      "test loss is 0.0002465205950596887\n",
      "Batch: 6500,train loss is: 0.0002879605527288955\n",
      "test loss is 0.000254660734334091\n",
      "Batch: 6600,train loss is: 0.0002171625063879284\n",
      "test loss is 0.0002239064434693949\n",
      "Batch: 6700,train loss is: 0.0001488581421916212\n",
      "test loss is 0.0002573534028417948\n",
      "Batch: 6800,train loss is: 0.0002363450464780541\n",
      "test loss is 0.00030000417893269677\n",
      "Batch: 6900,train loss is: 0.00017729661324912083\n",
      "test loss is 0.00026543399432312625\n",
      "Batch: 7000,train loss is: 0.0005158720468520753\n",
      "test loss is 0.00024870088642416397\n",
      "Batch: 7100,train loss is: 0.00014427228751433008\n",
      "test loss is 0.00024350368556656812\n",
      "Batch: 7200,train loss is: 0.0002123204123116596\n",
      "test loss is 0.0002688509336377062\n",
      "Batch: 7300,train loss is: 0.0001853465723717482\n",
      "test loss is 0.0002641740670728858\n",
      "Batch: 7400,train loss is: 0.00022210165460134583\n",
      "test loss is 0.00025157503895141656\n",
      "Batch: 7500,train loss is: 0.00017404809663228724\n",
      "test loss is 0.0002378981168137456\n",
      "Batch: 7600,train loss is: 0.0003098678278913295\n",
      "test loss is 0.0002596699849528036\n",
      "Batch: 7700,train loss is: 0.0001790005398291896\n",
      "test loss is 0.00022643182638901183\n",
      "Batch: 7800,train loss is: 0.0003375330291898193\n",
      "test loss is 0.00026513200503967274\n",
      "Batch: 7900,train loss is: 0.00019233580730338757\n",
      "test loss is 0.00026477776527290115\n",
      "Batch: 8000,train loss is: 0.00013402543145835447\n",
      "test loss is 0.0002682062202639611\n",
      "Batch: 8100,train loss is: 0.00014228138612737044\n",
      "test loss is 0.00022823412396862832\n",
      "Batch: 8200,train loss is: 0.00044536191188515237\n",
      "test loss is 0.00023444232468493523\n",
      "Batch: 8300,train loss is: 0.00016207641017177885\n",
      "test loss is 0.0002492437357971078\n",
      "Batch: 8400,train loss is: 0.00014807363211512053\n",
      "test loss is 0.00024263993855766773\n",
      "Batch: 8500,train loss is: 0.00011193584017880633\n",
      "test loss is 0.00023457818012361572\n",
      "Batch: 8600,train loss is: 0.00040477503056460387\n",
      "test loss is 0.0002839196326140077\n",
      "Batch: 8700,train loss is: 0.00026243375647370243\n",
      "test loss is 0.0002526433313044152\n",
      "Batch: 8800,train loss is: 0.00023504189419522737\n",
      "test loss is 0.00024474533219159735\n",
      "Batch: 8900,train loss is: 0.000198004630517165\n",
      "test loss is 0.0002690195025419865\n",
      "Batch: 9000,train loss is: 0.0002717670549382089\n",
      "test loss is 0.00025421248490669653\n",
      "Batch: 9100,train loss is: 0.00023245781721637713\n",
      "test loss is 0.0002996959083466696\n",
      "Batch: 9200,train loss is: 0.00023580636054831497\n",
      "test loss is 0.0002644268882437\n",
      "Batch: 9300,train loss is: 0.00031723325121197797\n",
      "test loss is 0.00026133040257797095\n",
      "Batch: 9400,train loss is: 0.0006837586549028364\n",
      "test loss is 0.0002447295149684005\n",
      "Batch: 9500,train loss is: 0.00022639495698364854\n",
      "test loss is 0.00022440424499123185\n",
      "Batch: 9600,train loss is: 0.00028422121030527133\n",
      "test loss is 0.0002488209757382249\n",
      "Batch: 9700,train loss is: 0.00019993177552140803\n",
      "test loss is 0.00024607891053837907\n",
      "Batch: 9800,train loss is: 0.00027865827934354985\n",
      "test loss is 0.00025768647307664884\n",
      "Batch: 9900,train loss is: 0.0002512326548939615\n",
      "test loss is 0.0002584656410375239\n",
      "Batch: 10000,train loss is: 0.00037316637041313525\n",
      "test loss is 0.00026303957811425163\n",
      "Batch: 10100,train loss is: 0.0002144910993559028\n",
      "test loss is 0.0002546990592781117\n",
      "Batch: 10200,train loss is: 0.00016612460089561958\n",
      "test loss is 0.00027817529520574995\n",
      "Batch: 10300,train loss is: 0.0003230677742273285\n",
      "test loss is 0.0004853192548099725\n",
      "Batch: 10400,train loss is: 0.00020958255425469074\n",
      "test loss is 0.00027936428622329077\n",
      "Batch: 10500,train loss is: 0.00030384366716462957\n",
      "test loss is 0.0002779436648220019\n",
      "Batch: 10600,train loss is: 0.00023375722047828395\n",
      "test loss is 0.00021849148742447642\n",
      "Batch: 10700,train loss is: 0.00021987219697902212\n",
      "test loss is 0.00022769691369143485\n",
      "Batch: 10800,train loss is: 0.00022012629903888034\n",
      "test loss is 0.0002430714290178882\n",
      "Batch: 10900,train loss is: 0.0002192180227652115\n",
      "test loss is 0.0002778118450386601\n",
      "Batch: 11000,train loss is: 0.00018514710564163415\n",
      "test loss is 0.0002339278570689071\n",
      "Batch: 11100,train loss is: 0.0001761741471703212\n",
      "test loss is 0.0003279359327281603\n",
      "Batch: 11200,train loss is: 0.00022573275849113822\n",
      "test loss is 0.00024317578191768345\n",
      "Batch: 11300,train loss is: 0.00027473201026720215\n",
      "test loss is 0.0003266006458861614\n",
      "Batch: 11400,train loss is: 0.00013272941008387172\n",
      "test loss is 0.0002993455742495544\n",
      "Batch: 11500,train loss is: 0.00018755056454119675\n",
      "test loss is 0.00023149926610850123\n",
      "Batch: 11600,train loss is: 0.0004962337796321401\n",
      "test loss is 0.00023137204096900033\n",
      "Batch: 11700,train loss is: 0.00025978107710897735\n",
      "test loss is 0.000283022810261159\n",
      "Batch: 11800,train loss is: 0.00018038757874592635\n",
      "test loss is 0.00025213906956151824\n",
      "Batch: 11900,train loss is: 0.0002494862231020352\n",
      "test loss is 0.0002661244771914563\n",
      "Batch: 12000,train loss is: 0.0001490502009916959\n",
      "test loss is 0.0002358259053610979\n",
      "Batch: 12100,train loss is: 0.00018656176162841427\n",
      "test loss is 0.0002911423668626384\n",
      "Batch: 12200,train loss is: 0.00023694612540292527\n",
      "test loss is 0.00032629130328899834\n",
      "Batch: 12300,train loss is: 0.00019459361927218324\n",
      "test loss is 0.0002636380740858402\n",
      "Batch: 12400,train loss is: 0.0001932302101958882\n",
      "test loss is 0.00026644093590001976\n",
      "Batch: 12500,train loss is: 0.00014813727782418527\n",
      "test loss is 0.00026647684321184727\n",
      "Batch: 12600,train loss is: 0.00026808436797208903\n",
      "test loss is 0.00024700938779245686\n",
      "Batch: 12700,train loss is: 0.0002313378953866874\n",
      "test loss is 0.000229668919279179\n",
      "Batch: 12800,train loss is: 0.0002252479577036143\n",
      "test loss is 0.0002386743223487237\n",
      "Batch: 12900,train loss is: 0.0005519245014880074\n",
      "test loss is 0.0004065524638924793\n",
      "Batch: 13000,train loss is: 0.0001701788457368756\n",
      "test loss is 0.00029717864504255436\n",
      "Batch: 13100,train loss is: 0.0002216088213499137\n",
      "test loss is 0.0002734436066557846\n",
      "Batch: 13200,train loss is: 0.00021789162832723465\n",
      "test loss is 0.00022431321863896082\n",
      "Batch: 13300,train loss is: 0.00017288081668713565\n",
      "test loss is 0.00024328979316375263\n",
      "Batch: 13400,train loss is: 0.00015999376701594326\n",
      "test loss is 0.00023708136538865183\n",
      "Batch: 13500,train loss is: 0.0001725025321136183\n",
      "test loss is 0.0002450724668790567\n",
      "Batch: 13600,train loss is: 0.0002546404705734099\n",
      "test loss is 0.0002704641533156428\n",
      "Batch: 13700,train loss is: 0.00022150956568290096\n",
      "test loss is 0.00027489994459795205\n",
      "Batch: 13800,train loss is: 0.00015881580234578918\n",
      "test loss is 0.00022638418363183299\n",
      "Batch: 13900,train loss is: 0.00019742009769770067\n",
      "test loss is 0.00023882792531313603\n",
      "Batch: 14000,train loss is: 0.0002477149330962105\n",
      "test loss is 0.0002711492987385516\n",
      "Batch: 14100,train loss is: 0.00021192352252080468\n",
      "test loss is 0.00023576254771429732\n",
      "Batch: 14200,train loss is: 0.00029382054930298616\n",
      "test loss is 0.0002392592459763582\n",
      "Batch: 14300,train loss is: 0.0002171376185191316\n",
      "test loss is 0.0002531276028912649\n",
      "Batch: 14400,train loss is: 0.00024486107757554687\n",
      "test loss is 0.00025492366334506486\n",
      "Batch: 14500,train loss is: 0.00019491910635216264\n",
      "test loss is 0.00023724090306610855\n",
      "Batch: 14600,train loss is: 0.00022809088262668508\n",
      "test loss is 0.0003432697106905686\n",
      "Batch: 14700,train loss is: 0.00022723797821435363\n",
      "test loss is 0.00024233213731195922\n",
      "Batch: 14800,train loss is: 0.00019388259241077277\n",
      "test loss is 0.00029119829279566505\n",
      "Batch: 14900,train loss is: 0.000260035721475985\n",
      "test loss is 0.00026862324996192204\n",
      "Batch: 15000,train loss is: 0.00021658693157715372\n",
      "test loss is 0.00029116305222002663\n",
      "Batch: 15100,train loss is: 0.00022252354963136199\n",
      "test loss is 0.0002780614142972083\n",
      "Batch: 15200,train loss is: 0.00024426156064716906\n",
      "test loss is 0.0002582713625076084\n",
      "Batch: 15300,train loss is: 0.00020363163495139757\n",
      "test loss is 0.0002391922871760636\n",
      "Batch: 15400,train loss is: 0.00025821272750077437\n",
      "test loss is 0.00024864917165841273\n",
      "Batch: 15500,train loss is: 0.00015555617173977697\n",
      "test loss is 0.00024013709872416458\n",
      "Batch: 15600,train loss is: 0.00045877758950131696\n",
      "test loss is 0.00027189941722753484\n",
      "Batch: 15700,train loss is: 0.00016897786588191516\n",
      "test loss is 0.0002429621758811152\n",
      "Batch: 15800,train loss is: 0.000221792949330924\n",
      "test loss is 0.00024087829803527726\n",
      "Batch: 15900,train loss is: 0.0003675783847784331\n",
      "test loss is 0.00030075431367908875\n",
      "Batch: 16000,train loss is: 0.0001988510308214708\n",
      "test loss is 0.0002641561839615207\n",
      "Batch: 16100,train loss is: 0.00023636030895334454\n",
      "test loss is 0.00023159543557140888\n",
      "Batch: 16200,train loss is: 0.0002552566592644767\n",
      "test loss is 0.0002889577897635953\n",
      "Batch: 16300,train loss is: 0.00022476245588834215\n",
      "test loss is 0.0002600607063267088\n",
      "Batch: 16400,train loss is: 0.0015093718067966922\n",
      "test loss is 0.00022374789967308334\n",
      "Batch: 16500,train loss is: 0.0001912933039386415\n",
      "test loss is 0.00029537688371195504\n",
      "Batch: 16600,train loss is: 0.00021661728448446805\n",
      "test loss is 0.00023251981956723552\n",
      "Batch: 16700,train loss is: 0.00016542217506487132\n",
      "test loss is 0.0002593321325264498\n",
      "Batch: 16800,train loss is: 0.00021963285780291633\n",
      "test loss is 0.00025721686789105676\n",
      "Batch: 16900,train loss is: 0.0001691056819093115\n",
      "test loss is 0.0002884121149853838\n",
      "Batch: 17000,train loss is: 0.0002034557678249556\n",
      "test loss is 0.0002612250036349223\n",
      "Batch: 17100,train loss is: 0.0004125469084094562\n",
      "test loss is 0.00027427400918646146\n",
      "Batch: 17200,train loss is: 0.0001534772289567262\n",
      "test loss is 0.0002478529969908523\n",
      "Batch: 17300,train loss is: 0.0003604234909367604\n",
      "test loss is 0.0002530246200251512\n",
      "Batch: 17400,train loss is: 0.00017817695196852785\n",
      "test loss is 0.0002451312520619394\n",
      "Batch: 17500,train loss is: 0.000234318745212713\n",
      "test loss is 0.00027183419328971934\n",
      "Batch: 17600,train loss is: 0.00014454107083564473\n",
      "test loss is 0.00022833737724543281\n",
      "Batch: 17700,train loss is: 0.000495655964276835\n",
      "test loss is 0.00029121686194172303\n",
      "Batch: 17800,train loss is: 0.0001572980148008441\n",
      "test loss is 0.00024218014364535596\n",
      "Batch: 17900,train loss is: 0.0003740267719577736\n",
      "test loss is 0.00027137085585658867\n",
      "Batch: 18000,train loss is: 0.00022541929449820724\n",
      "test loss is 0.00026438201142066174\n",
      "Batch: 18100,train loss is: 0.00021790282719300383\n",
      "test loss is 0.00023171101669050367\n",
      "Batch: 18200,train loss is: 0.00018413708509286054\n",
      "test loss is 0.000254304360257697\n",
      "Batch: 18300,train loss is: 0.00020797460638906145\n",
      "test loss is 0.0002585891391022826\n",
      "Batch: 18400,train loss is: 0.0002721054854774176\n",
      "test loss is 0.00025303480686371014\n",
      "Batch: 18500,train loss is: 0.0002278863804204844\n",
      "test loss is 0.00022476012551736745\n",
      "Batch: 18600,train loss is: 0.00024078207710218552\n",
      "test loss is 0.0002275652974719662\n",
      "Batch: 18700,train loss is: 0.0002246104156176968\n",
      "test loss is 0.0002649105306190911\n",
      "Batch: 18800,train loss is: 0.00013913622661297062\n",
      "test loss is 0.0002507086317866831\n",
      "Batch: 18900,train loss is: 0.00020994575588924254\n",
      "test loss is 0.00024715303532507724\n",
      "Batch: 19000,train loss is: 0.00020996059095981242\n",
      "test loss is 0.00036802724257149847\n",
      "Batch: 19100,train loss is: 0.00028199532231598505\n",
      "test loss is 0.00032379331826974406\n",
      "Batch: 19200,train loss is: 0.00028981948437172944\n",
      "test loss is 0.00024642903108775167\n",
      "Batch: 19300,train loss is: 0.0003195787794484091\n",
      "test loss is 0.00027087279485502914\n",
      "Batch: 19400,train loss is: 0.0001745364540016434\n",
      "test loss is 0.00025737078977660586\n",
      "Batch: 19500,train loss is: 0.00031252095453104074\n",
      "test loss is 0.000244260446784985\n",
      "Batch: 19600,train loss is: 0.00023733479810267833\n",
      "test loss is 0.00023545952497715423\n",
      "Batch: 19700,train loss is: 0.00022068276230096274\n",
      "test loss is 0.00029112549208361234\n",
      "Batch: 19800,train loss is: 0.00017995634263932105\n",
      "test loss is 0.0003039785565659881\n",
      "Batch: 19900,train loss is: 0.0002970828401730787\n",
      "test loss is 0.00031794195926247026\n",
      "Batch: 20000,train loss is: 0.0003085533955775391\n",
      "test loss is 0.0002522545962077694\n",
      "Batch: 20100,train loss is: 0.0002666757973132913\n",
      "test loss is 0.0002546372444759353\n",
      "Batch: 20200,train loss is: 0.00044282523626328974\n",
      "test loss is 0.0003222280068666701\n",
      "Batch: 20300,train loss is: 0.0002264078656407139\n",
      "test loss is 0.00026634979079568255\n",
      "Batch: 20400,train loss is: 0.00019598754690115675\n",
      "test loss is 0.00023817556421097831\n",
      "Batch: 20500,train loss is: 0.0003159819426205129\n",
      "test loss is 0.0002629838450812842\n",
      "Batch: 20600,train loss is: 0.0002609848221059198\n",
      "test loss is 0.0003110460784336297\n",
      "Batch: 20700,train loss is: 0.00030303279339030306\n",
      "test loss is 0.00022580880068848814\n",
      "Batch: 20800,train loss is: 0.00022234590344252713\n",
      "test loss is 0.00023986372360474323\n",
      "Batch: 20900,train loss is: 0.00019335902624258766\n",
      "test loss is 0.00023708875030314564\n",
      "Batch: 21000,train loss is: 0.0002056726367304042\n",
      "test loss is 0.00023859357221157703\n",
      "Batch: 21100,train loss is: 0.00019644178197224455\n",
      "test loss is 0.00024352583391925282\n",
      "Batch: 21200,train loss is: 0.0002261041854297721\n",
      "test loss is 0.00023345917137621946\n",
      "Batch: 21300,train loss is: 0.00019479968029384768\n",
      "test loss is 0.0002499279689277452\n",
      "Batch: 21400,train loss is: 0.0003607231222757326\n",
      "test loss is 0.00024199643221884176\n",
      "Batch: 21500,train loss is: 0.0002787969150412319\n",
      "test loss is 0.00024863873548651677\n",
      "Batch: 21600,train loss is: 0.00038594048245216\n",
      "test loss is 0.00026482667720075106\n",
      "Batch: 21700,train loss is: 0.0002959267368912827\n",
      "test loss is 0.00025876888546060137\n",
      "Batch: 21800,train loss is: 0.0009646809944355636\n",
      "test loss is 0.000230575406708989\n",
      "Batch: 21900,train loss is: 0.00020353354270840897\n",
      "test loss is 0.00023164033476110658\n",
      "Batch: 22000,train loss is: 0.00021228595479475071\n",
      "test loss is 0.0002374061608046973\n",
      "Batch: 22100,train loss is: 0.00022674000395417942\n",
      "test loss is 0.0002743122563318576\n",
      "Batch: 22200,train loss is: 0.0001931514247956402\n",
      "test loss is 0.00029553930047993086\n",
      "Batch: 22300,train loss is: 0.00023329359937935401\n",
      "test loss is 0.0002895847874032025\n",
      "Batch: 22400,train loss is: 0.00015912188981811407\n",
      "test loss is 0.00033325790618254605\n",
      "Batch: 22500,train loss is: 0.0001704867419795218\n",
      "test loss is 0.00023265470805923697\n",
      "Batch: 22600,train loss is: 0.0002690797447344568\n",
      "test loss is 0.00031901931011895355\n",
      "Batch: 22700,train loss is: 0.00022693109299961993\n",
      "test loss is 0.00027954230630998527\n",
      "Batch: 22800,train loss is: 0.00016749237792522322\n",
      "test loss is 0.00023715692767665346\n",
      "Batch: 22900,train loss is: 0.00029634924945556005\n",
      "test loss is 0.00023170053931419676\n",
      "Batch: 23000,train loss is: 0.0002482162012956764\n",
      "test loss is 0.0002534014488432045\n",
      "Batch: 23100,train loss is: 0.00020247169348936827\n",
      "test loss is 0.0002725593604968556\n",
      "Batch: 23200,train loss is: 0.0005330567056024643\n",
      "test loss is 0.00024276642325956173\n",
      "Batch: 23300,train loss is: 0.00018760380987072755\n",
      "test loss is 0.00024640257790883147\n",
      "Batch: 23400,train loss is: 0.00017154047603875244\n",
      "test loss is 0.00029917412579923695\n",
      "Batch: 23500,train loss is: 0.0004409585974281705\n",
      "test loss is 0.0002663143912983958\n",
      "Batch: 23600,train loss is: 0.00030596082999565806\n",
      "test loss is 0.00024873728213041025\n",
      "Batch: 23700,train loss is: 0.00028345101280744997\n",
      "test loss is 0.000269362080264737\n",
      "Batch: 23800,train loss is: 0.00015016634780664394\n",
      "test loss is 0.0002746490230594996\n",
      "Batch: 23900,train loss is: 0.00028485192051189845\n",
      "test loss is 0.00023385538604790964\n",
      "Batch: 24000,train loss is: 0.00016743122756476993\n",
      "test loss is 0.0002462559045328904\n",
      "Batch: 24100,train loss is: 0.0003403469433066426\n",
      "test loss is 0.00023916656296721467\n",
      "Batch: 24200,train loss is: 0.0003051165806403313\n",
      "test loss is 0.00024082016122839054\n",
      "Batch: 24300,train loss is: 0.000260516123160235\n",
      "test loss is 0.0003149594809479278\n",
      "Batch: 24400,train loss is: 0.00024198822552613157\n",
      "test loss is 0.0002761064799666265\n",
      "Batch: 24500,train loss is: 0.00020904843971807937\n",
      "test loss is 0.0002689719653379573\n",
      "Batch: 24600,train loss is: 0.00021894393170848817\n",
      "test loss is 0.00025017265127432554\n",
      "Batch: 24700,train loss is: 0.00023970583063364915\n",
      "test loss is 0.00022342967362317035\n",
      "Batch: 24800,train loss is: 0.0003010772860563859\n",
      "test loss is 0.0002480556300060493\n",
      "Batch: 24900,train loss is: 0.00033286159705468173\n",
      "test loss is 0.00028241068603402394\n",
      "Batch: 25000,train loss is: 0.00015498359171880896\n",
      "test loss is 0.0002668507355534325\n",
      "Batch: 25100,train loss is: 0.000176759424454533\n",
      "test loss is 0.00024378567237763767\n",
      "Batch: 25200,train loss is: 0.0002072994840194083\n",
      "test loss is 0.0002575132325487219\n",
      "Batch: 25300,train loss is: 0.0003064469688537783\n",
      "test loss is 0.0002457448968490972\n",
      "Batch: 25400,train loss is: 0.00025368020483869814\n",
      "test loss is 0.00026966633357802854\n",
      "Batch: 25500,train loss is: 0.0003682708868749736\n",
      "test loss is 0.00026714490744933017\n",
      "Batch: 25600,train loss is: 0.00019712078639009905\n",
      "test loss is 0.00023292863683921705\n",
      "Batch: 25700,train loss is: 0.00023168821992366676\n",
      "test loss is 0.00026189118291008746\n",
      "Batch: 25800,train loss is: 0.00018780085958506915\n",
      "test loss is 0.0002706194099751568\n",
      "Batch: 25900,train loss is: 0.0002938365232371407\n",
      "test loss is 0.00022520931570251904\n",
      "Batch: 26000,train loss is: 0.00023672353216342394\n",
      "test loss is 0.00024242064533420288\n",
      "Batch: 26100,train loss is: 0.00019941336297190637\n",
      "test loss is 0.0002367602762855164\n",
      "Batch: 26200,train loss is: 0.00015458527666111958\n",
      "test loss is 0.00022859064185602772\n",
      "Batch: 26300,train loss is: 0.00022904203310733273\n",
      "test loss is 0.00031352004728190763\n",
      "Batch: 26400,train loss is: 0.00025584809463344326\n",
      "test loss is 0.00026165929699289585\n",
      "Batch: 26500,train loss is: 0.00020517983841199136\n",
      "test loss is 0.0002447555742836406\n",
      "Batch: 26600,train loss is: 0.00018115954158172474\n",
      "test loss is 0.0002585768586071944\n",
      "Batch: 26700,train loss is: 0.0002316581258393702\n",
      "test loss is 0.00023931054872012284\n",
      "Batch: 26800,train loss is: 0.00030570990107491817\n",
      "test loss is 0.00022351469915469715\n",
      "Batch: 26900,train loss is: 0.00021480585275798032\n",
      "test loss is 0.0002542111411043679\n",
      "Batch: 27000,train loss is: 0.0003218584041529272\n",
      "test loss is 0.0002277121735206416\n",
      "Batch: 27100,train loss is: 0.00025507684048226\n",
      "test loss is 0.00023388950284110627\n",
      "Batch: 27200,train loss is: 0.0002496396009452314\n",
      "test loss is 0.00029686506938614033\n",
      "Batch: 27300,train loss is: 0.00017620388954547986\n",
      "test loss is 0.0002593307094257459\n",
      "Batch: 27400,train loss is: 0.00017787354460441993\n",
      "test loss is 0.00022402562037148138\n",
      "Batch: 27500,train loss is: 0.0001776392293184148\n",
      "test loss is 0.0002328853718811039\n",
      "Batch: 27600,train loss is: 0.0003786629951166876\n",
      "test loss is 0.0002590982659387811\n",
      "Batch: 27700,train loss is: 0.00025410907564471036\n",
      "test loss is 0.00023398458512869383\n",
      "Batch: 27800,train loss is: 0.00020237731793012483\n",
      "test loss is 0.0002881367337881717\n",
      "Batch: 27900,train loss is: 0.00023225387755153705\n",
      "test loss is 0.0002694896794550465\n",
      "Batch: 28000,train loss is: 0.00019722698498195955\n",
      "test loss is 0.00022456245146213648\n",
      "Batch: 28100,train loss is: 0.00041772385013430914\n",
      "test loss is 0.00032471181865751124\n",
      "Batch: 28200,train loss is: 0.00025297017360216307\n",
      "test loss is 0.0002673251116863192\n",
      "Batch: 28300,train loss is: 0.0001447788680009186\n",
      "test loss is 0.0002221037855121215\n",
      "Batch: 28400,train loss is: 0.0001765044326092592\n",
      "test loss is 0.00025501924636806526\n",
      "Batch: 28500,train loss is: 0.0006016153472465909\n",
      "test loss is 0.0002864512928171535\n",
      "Batch: 28600,train loss is: 0.00027192143601556236\n",
      "test loss is 0.0002470919019466461\n",
      "Batch: 28700,train loss is: 0.00019083724759782182\n",
      "test loss is 0.00025132156133694935\n",
      "Batch: 28800,train loss is: 0.00019686644121339088\n",
      "test loss is 0.0002997795754013081\n",
      "Batch: 28900,train loss is: 0.00032640501075116275\n",
      "test loss is 0.0002407038262722581\n",
      "Batch: 29000,train loss is: 0.00019805255361797036\n",
      "test loss is 0.0002644995606034733\n",
      "Batch: 29100,train loss is: 0.0003288567227781765\n",
      "test loss is 0.00023651237338681553\n",
      "Batch: 29200,train loss is: 0.00021941515767233443\n",
      "test loss is 0.0002569383522158067\n",
      "Batch: 29300,train loss is: 0.00019460516366651644\n",
      "test loss is 0.0002351141598860224\n",
      "Batch: 29400,train loss is: 0.0004130943949910736\n",
      "test loss is 0.00022068060045832914\n",
      "Batch: 29500,train loss is: 0.0001816748384041832\n",
      "test loss is 0.00023809055959591504\n",
      "Batch: 29600,train loss is: 0.00023863247614825848\n",
      "test loss is 0.0002463127176931403\n",
      "Batch: 29700,train loss is: 0.0002594127029972151\n",
      "test loss is 0.0002343410319848852\n",
      "Batch: 29800,train loss is: 0.00022704777185000635\n",
      "test loss is 0.00024853206624294807\n",
      "Batch: 29900,train loss is: 0.0002280759528524849\n",
      "test loss is 0.0002511925562888256\n",
      "Batch: 30000,train loss is: 0.00021111916384407574\n",
      "test loss is 0.0002904308641999091\n",
      "Batch: 30100,train loss is: 0.00022870962377118947\n",
      "test loss is 0.0002253775157967627\n",
      "Batch: 30200,train loss is: 0.0002823969582629765\n",
      "test loss is 0.00030696259378901826\n",
      "Batch: 30300,train loss is: 0.00021779567499088804\n",
      "test loss is 0.0002480638004388009\n",
      "Batch: 30400,train loss is: 0.0002563654896827613\n",
      "test loss is 0.00029001202903548466\n",
      "Batch: 30500,train loss is: 0.00018323189345496917\n",
      "test loss is 0.00023991509307848591\n",
      "Batch: 30600,train loss is: 0.0003841707268222828\n",
      "test loss is 0.0002831095430430916\n",
      "Batch: 30700,train loss is: 0.0002541892328743968\n",
      "test loss is 0.00026532254139759306\n",
      "Batch: 30800,train loss is: 0.00019921551417832452\n",
      "test loss is 0.00023036291342205483\n",
      "Batch: 30900,train loss is: 0.0002501869757945405\n",
      "test loss is 0.00022750065279641726\n",
      "Batch: 31000,train loss is: 0.00020474483785098619\n",
      "test loss is 0.00022670675180807963\n",
      "Batch: 31100,train loss is: 0.00021218139444224846\n",
      "test loss is 0.00024131425013031442\n",
      "Batch: 31200,train loss is: 0.00028129248954611083\n",
      "test loss is 0.00024398102781549093\n",
      "Batch: 31300,train loss is: 0.00028246020104737363\n",
      "test loss is 0.00024249424015147184\n",
      "Batch: 31400,train loss is: 0.00029454779667678144\n",
      "test loss is 0.0003704718891638815\n",
      "Batch: 31500,train loss is: 0.00019633569596632242\n",
      "test loss is 0.00024453782952168465\n",
      "Batch: 31600,train loss is: 0.00030856263263177137\n",
      "test loss is 0.00043719633348056683\n",
      "Batch: 31700,train loss is: 0.00023673254888205566\n",
      "test loss is 0.00026646373929884105\n",
      "Batch: 31800,train loss is: 0.0003820781105486332\n",
      "test loss is 0.0002947727479565792\n",
      "Batch: 31900,train loss is: 0.0002414397779674584\n",
      "test loss is 0.00028964544332305014\n",
      "Batch: 32000,train loss is: 0.00027522905920961447\n",
      "test loss is 0.0002894429857176874\n",
      "Batch: 32100,train loss is: 0.0002291692201463382\n",
      "test loss is 0.00029448119983512564\n",
      "Batch: 32200,train loss is: 0.0002746119979674247\n",
      "test loss is 0.00026034704206820837\n",
      "Batch: 32300,train loss is: 0.0004207854771353461\n",
      "test loss is 0.00028660635373339106\n",
      "Batch: 32400,train loss is: 0.00021064910721532221\n",
      "test loss is 0.00022698904680069538\n",
      "Batch: 32500,train loss is: 0.0001382085279867463\n",
      "test loss is 0.00023370741803548783\n",
      "Batch: 32600,train loss is: 0.0002711588669111572\n",
      "test loss is 0.00025524597340278955\n",
      "Batch: 32700,train loss is: 0.00027042382254563327\n",
      "test loss is 0.00025253840552395525\n",
      "Batch: 32800,train loss is: 0.0003073973472916594\n",
      "test loss is 0.00027302538630185886\n",
      "Batch: 32900,train loss is: 0.0001734060429923987\n",
      "test loss is 0.00026779973473946484\n",
      "Batch: 33000,train loss is: 0.00014884299192935609\n",
      "test loss is 0.00025449258066823825\n",
      "Batch: 33100,train loss is: 0.00028731802144922467\n",
      "test loss is 0.0002647951903839302\n",
      "Batch: 33200,train loss is: 0.00019071008621760548\n",
      "test loss is 0.00031389889410236616\n",
      "Batch: 33300,train loss is: 0.0002145997711157541\n",
      "test loss is 0.00025394432519647693\n",
      "Batch: 33400,train loss is: 0.00021995359159923396\n",
      "test loss is 0.00024619735874215656\n",
      "Batch: 33500,train loss is: 0.00013978697625773723\n",
      "test loss is 0.00022770446346680924\n",
      "Batch: 33600,train loss is: 0.00031931141677882626\n",
      "test loss is 0.0002675772932152824\n",
      "Batch: 33700,train loss is: 0.00014665091509571124\n",
      "test loss is 0.00024112268904745524\n",
      "Batch: 33800,train loss is: 0.00027601672058004997\n",
      "test loss is 0.0002592699737340537\n",
      "Batch: 33900,train loss is: 0.00023202611345880087\n",
      "test loss is 0.00024390762049914975\n",
      "Batch: 34000,train loss is: 0.00021849891421584704\n",
      "test loss is 0.00022707822020735232\n",
      "Batch: 34100,train loss is: 0.00024923242564026734\n",
      "test loss is 0.00026633868190412125\n",
      "Batch: 34200,train loss is: 0.00012575130834561703\n",
      "test loss is 0.000259885260306148\n",
      "Batch: 34300,train loss is: 0.0002997376619129345\n",
      "test loss is 0.00028140852476851177\n",
      "Batch: 34400,train loss is: 0.0003364223757464014\n",
      "test loss is 0.00028072790388943675\n",
      "Batch: 34500,train loss is: 0.0003627554447647026\n",
      "test loss is 0.00026079424664045834\n",
      "Batch: 34600,train loss is: 0.00011669819880798197\n",
      "test loss is 0.00022919338407234634\n",
      "Batch: 34700,train loss is: 0.00029580014285243933\n",
      "test loss is 0.00036382534403219566\n",
      "Batch: 34800,train loss is: 0.00029772925143907794\n",
      "test loss is 0.00022906392858623134\n",
      "Batch: 34900,train loss is: 0.00017430091046770997\n",
      "test loss is 0.00023957143510862112\n",
      "Batch: 35000,train loss is: 0.00023235318029356692\n",
      "test loss is 0.00028995230424950873\n",
      "Batch: 35100,train loss is: 0.00023251384317769932\n",
      "test loss is 0.00024298189058582882\n",
      "Batch: 35200,train loss is: 0.00016142984892399706\n",
      "test loss is 0.00032692389744665227\n",
      "Batch: 35300,train loss is: 0.00020164186142419444\n",
      "test loss is 0.00022854450354076772\n",
      "Batch: 35400,train loss is: 0.00041382925449790474\n",
      "test loss is 0.00027761663671924477\n",
      "Batch: 35500,train loss is: 0.00016136426507831503\n",
      "test loss is 0.00024633876345965354\n",
      "Batch: 35600,train loss is: 0.0001908030750873653\n",
      "test loss is 0.00027204079984481216\n",
      "Batch: 35700,train loss is: 0.00024791732094513966\n",
      "test loss is 0.00031754902160612713\n",
      "Batch: 35800,train loss is: 0.0007008385543361992\n",
      "test loss is 0.00024799853649874087\n",
      "Batch: 35900,train loss is: 0.0004714314459717476\n",
      "test loss is 0.00025244931559296026\n",
      "Batch: 36000,train loss is: 0.0001950247792855378\n",
      "test loss is 0.0002846148049351757\n",
      "Batch: 36100,train loss is: 0.00020730211119549932\n",
      "test loss is 0.0002375407235100065\n",
      "Batch: 36200,train loss is: 0.00037970651854955914\n",
      "test loss is 0.0002924575671602749\n",
      "Batch: 36300,train loss is: 0.0002861255402695697\n",
      "test loss is 0.00023785028259023808\n",
      "Batch: 36400,train loss is: 0.0001693323670163315\n",
      "test loss is 0.000250301149183348\n",
      "Batch: 36500,train loss is: 0.00017285411880681663\n",
      "test loss is 0.00024425881117549864\n",
      "Batch: 36600,train loss is: 0.0003465929801582826\n",
      "test loss is 0.00022840825135392908\n",
      "Batch: 36700,train loss is: 0.0002450687108699028\n",
      "test loss is 0.0002566064816498034\n",
      "Batch: 36800,train loss is: 0.0001383046043628667\n",
      "test loss is 0.00028092914689994866\n",
      "Batch: 36900,train loss is: 0.00020311499657020427\n",
      "test loss is 0.00023860670139764768\n",
      "Batch: 37000,train loss is: 0.00013613283414541804\n",
      "test loss is 0.00023720379937766393\n",
      "Batch: 37100,train loss is: 0.00019328871901358336\n",
      "test loss is 0.0002556588946869616\n",
      "Batch: 37200,train loss is: 0.000324318177948402\n",
      "test loss is 0.0003463219778633439\n",
      "Batch: 37300,train loss is: 0.00024351100578900856\n",
      "test loss is 0.0002593042829537065\n",
      "Batch: 37400,train loss is: 0.0002241972514901975\n",
      "test loss is 0.00022099553498362772\n",
      "Batch: 37500,train loss is: 0.00027674410858230595\n",
      "test loss is 0.00023750611006364612\n",
      "Batch: 37600,train loss is: 0.00021445380126888233\n",
      "test loss is 0.0002765356415467361\n",
      "Batch: 37700,train loss is: 0.00028297358068958306\n",
      "test loss is 0.0002561113352259716\n",
      "Batch: 37800,train loss is: 0.00023551151078421637\n",
      "test loss is 0.0003324242122275069\n",
      "Batch: 37900,train loss is: 0.00021118010802837736\n",
      "test loss is 0.00025598514901629287\n",
      "Batch: 38000,train loss is: 0.0007504844349412284\n",
      "test loss is 0.00031648281945993934\n",
      "Batch: 38100,train loss is: 0.00029458787751402067\n",
      "test loss is 0.00028396094290085357\n",
      "Batch: 38200,train loss is: 0.00021702936111919998\n",
      "test loss is 0.00029476588453903214\n",
      "Batch: 38300,train loss is: 0.00024014335287517568\n",
      "test loss is 0.0002626833659283537\n",
      "Batch: 38400,train loss is: 0.00027340807840437343\n",
      "test loss is 0.0002457458131938026\n",
      "Batch: 38500,train loss is: 0.0003275142498204719\n",
      "test loss is 0.0002329594984370986\n",
      "Batch: 38600,train loss is: 0.00020096593177648447\n",
      "test loss is 0.00024954754526393565\n",
      "Batch: 38700,train loss is: 0.0002673653041075887\n",
      "test loss is 0.000237368089376632\n",
      "Batch: 38800,train loss is: 0.0005365763873895484\n",
      "test loss is 0.00027256166802635336\n",
      "Batch: 38900,train loss is: 0.00023209616045380944\n",
      "test loss is 0.00022818115373318958\n",
      "Batch: 39000,train loss is: 0.00014819358922988846\n",
      "test loss is 0.0002267307741462606\n",
      "Batch: 39100,train loss is: 0.00016786495959015635\n",
      "test loss is 0.00023940971352361148\n",
      "Batch: 39200,train loss is: 0.00022438105293510203\n",
      "test loss is 0.00029654561286134327\n",
      "Batch: 39300,train loss is: 0.0003901985252740951\n",
      "test loss is 0.00027694264442630986\n",
      "Batch: 39400,train loss is: 0.0001678728340291891\n",
      "test loss is 0.00029478373628144473\n",
      "Batch: 39500,train loss is: 0.0002495493395103361\n",
      "test loss is 0.0002671328611211509\n",
      "Batch: 39600,train loss is: 0.00014301999106720018\n",
      "test loss is 0.000281037851155614\n",
      "Batch: 39700,train loss is: 0.00026500178938132897\n",
      "test loss is 0.00025935858103755347\n",
      "Batch: 39800,train loss is: 0.00022060658485069392\n",
      "test loss is 0.0002331932663136315\n",
      "Batch: 39900,train loss is: 0.0002175018764646284\n",
      "test loss is 0.00023512884919167228\n",
      "Batch: 40000,train loss is: 0.0003153388648663658\n",
      "test loss is 0.0002493703399435248\n",
      "Batch: 40100,train loss is: 0.00025875338474110076\n",
      "test loss is 0.0002263901444601724\n",
      "Batch: 40200,train loss is: 0.0002894414471595231\n",
      "test loss is 0.00030839480413789215\n",
      "Batch: 40300,train loss is: 0.0003415594455123821\n",
      "test loss is 0.00027563134377700706\n",
      "Batch: 40400,train loss is: 0.0002789951116475118\n",
      "test loss is 0.00025587314811923546\n",
      "Batch: 40500,train loss is: 0.00016396020245219354\n",
      "test loss is 0.0002289038733559125\n",
      "Batch: 40600,train loss is: 0.0004661532966928392\n",
      "test loss is 0.00023836588264070798\n",
      "Batch: 40700,train loss is: 0.00024811723904037507\n",
      "test loss is 0.00024351353436487293\n",
      "Batch: 40800,train loss is: 0.00023827830352250875\n",
      "test loss is 0.00025517012375003315\n",
      "Batch: 40900,train loss is: 0.0005611376828216264\n",
      "test loss is 0.0003024287000333538\n",
      "Batch: 41000,train loss is: 0.00026771133994685616\n",
      "test loss is 0.0002581886794327721\n",
      "Batch: 41100,train loss is: 0.00027032698300254057\n",
      "test loss is 0.00024634900789438955\n",
      "Batch: 41200,train loss is: 0.00022396125135170568\n",
      "test loss is 0.00023453750519527363\n",
      "Batch: 41300,train loss is: 0.00028306023506020076\n",
      "test loss is 0.00025408772633264275\n",
      "Batch: 41400,train loss is: 0.00035339603544813115\n",
      "test loss is 0.00022915268218545162\n",
      "Batch: 41500,train loss is: 0.0003364315776873989\n",
      "test loss is 0.0002325116574222296\n",
      "Batch: 41600,train loss is: 0.0001611020148864702\n",
      "test loss is 0.00024730579573386256\n",
      "Batch: 41700,train loss is: 0.00018154515354853204\n",
      "test loss is 0.0002575521804186027\n",
      "Batch: 41800,train loss is: 0.00017349267018952204\n",
      "test loss is 0.00027680319547154005\n",
      "Batch: 41900,train loss is: 0.0003125700558404172\n",
      "test loss is 0.00028294686893013776\n",
      "Batch: 42000,train loss is: 0.00024300002034742042\n",
      "test loss is 0.00021719039827491302\n",
      "Batch: 42100,train loss is: 0.0001301607995324108\n",
      "test loss is 0.0002286570407011314\n",
      "Batch: 42200,train loss is: 0.0001968948849259026\n",
      "test loss is 0.00027872753941794894\n",
      "Batch: 42300,train loss is: 0.000270156778032993\n",
      "test loss is 0.0002929108344444216\n",
      "Batch: 42400,train loss is: 0.00041780534568262276\n",
      "test loss is 0.00023698666862974953\n",
      "Batch: 42500,train loss is: 0.00019048278394843894\n",
      "test loss is 0.0002416932348760901\n",
      "Batch: 42600,train loss is: 0.00022731974114396458\n",
      "test loss is 0.00023816697705560875\n",
      "Batch: 42700,train loss is: 0.0002998794395719733\n",
      "test loss is 0.00023523162937889657\n",
      "Batch: 42800,train loss is: 0.00018806724058317706\n",
      "test loss is 0.0002405178869960192\n",
      "Batch: 42900,train loss is: 0.00034524815813574824\n",
      "test loss is 0.00026320895217826865\n",
      "Batch: 43000,train loss is: 0.0002699525582274775\n",
      "test loss is 0.0003216175016901494\n",
      "Batch: 43100,train loss is: 0.00036936786897868453\n",
      "test loss is 0.00022741896652804758\n",
      "Batch: 43200,train loss is: 0.00025960793662018963\n",
      "test loss is 0.00023862316241357964\n",
      "Batch: 43300,train loss is: 0.00022776945045448858\n",
      "test loss is 0.00029250800084252394\n",
      "Batch: 43400,train loss is: 0.00017470819777956468\n",
      "test loss is 0.0002561031273143879\n",
      "Batch: 43500,train loss is: 0.0002540966282095844\n",
      "test loss is 0.000312134692820167\n",
      "Batch: 43600,train loss is: 0.0002977198375322471\n",
      "test loss is 0.00026433047458471523\n",
      "Batch: 43700,train loss is: 0.0003085654151363499\n",
      "test loss is 0.00026093567816154955\n",
      "Batch: 43800,train loss is: 0.0002496647223307503\n",
      "test loss is 0.00022353225645275899\n",
      "Batch: 43900,train loss is: 0.0003099714637149094\n",
      "test loss is 0.0002616807816825575\n",
      "Batch: 44000,train loss is: 0.00023036705060383464\n",
      "test loss is 0.00027548252933370877\n",
      "Batch: 44100,train loss is: 0.0002679066408771068\n",
      "test loss is 0.00023034649715378486\n",
      "Batch: 44200,train loss is: 0.00015826326959488256\n",
      "test loss is 0.00024406464683353525\n",
      "Batch: 44300,train loss is: 0.00016333645682575115\n",
      "test loss is 0.00023844558836249007\n",
      "Batch: 44400,train loss is: 0.0001963234612671406\n",
      "test loss is 0.00025785012415095385\n",
      "Batch: 44500,train loss is: 0.00024849204507960477\n",
      "test loss is 0.0003136699562223134\n",
      "Batch: 44600,train loss is: 0.0003197286503556159\n",
      "test loss is 0.00024726478564458267\n",
      "Batch: 44700,train loss is: 0.00030244871547100533\n",
      "test loss is 0.0002919808217579587\n",
      "Batch: 44800,train loss is: 0.0003361814177955259\n",
      "test loss is 0.00023187700395097257\n",
      "Batch: 44900,train loss is: 0.000243020200946417\n",
      "test loss is 0.00023573548004648728\n",
      "Batch: 45000,train loss is: 0.0002307893617964984\n",
      "test loss is 0.00025817163028456913\n",
      "Batch: 45100,train loss is: 0.00019619555250425565\n",
      "test loss is 0.00024263663399883635\n",
      "Batch: 45200,train loss is: 0.0003004875637220553\n",
      "test loss is 0.00023784888602841194\n",
      "Batch: 45300,train loss is: 0.0003348238183320356\n",
      "test loss is 0.0002992786947093121\n",
      "Batch: 45400,train loss is: 0.00017622143611128183\n",
      "test loss is 0.00024425219191798734\n",
      "Batch: 45500,train loss is: 0.0002778432673996821\n",
      "test loss is 0.00025646112031745975\n",
      "Batch: 45600,train loss is: 0.000171046929548638\n",
      "test loss is 0.0002993132204337615\n",
      "Batch: 45700,train loss is: 0.00020935541227082787\n",
      "test loss is 0.00033392816657588546\n",
      "Batch: 45800,train loss is: 0.00022392717150466633\n",
      "test loss is 0.0002852584406115095\n",
      "Batch: 45900,train loss is: 0.0007870921905142123\n",
      "test loss is 0.00025081995057565644\n",
      "Batch: 46000,train loss is: 0.0002421464390633056\n",
      "test loss is 0.00024665743152222345\n",
      "Batch: 46100,train loss is: 0.0005099073556551182\n",
      "test loss is 0.0002756109759646206\n",
      "Batch: 46200,train loss is: 0.00019409453334440834\n",
      "test loss is 0.0003071379816045394\n",
      "Batch: 46300,train loss is: 0.00023938366921311154\n",
      "test loss is 0.00029300263960542743\n",
      "Batch: 46400,train loss is: 0.00023981805041566024\n",
      "test loss is 0.0002269770258084961\n",
      "Batch: 46500,train loss is: 0.00019624007399535247\n",
      "test loss is 0.0002352352475565586\n",
      "Batch: 46600,train loss is: 0.0002479489135517174\n",
      "test loss is 0.00023955034023416004\n",
      "Batch: 46700,train loss is: 0.0002998797690599219\n",
      "test loss is 0.00022622587577476674\n",
      "-----------------------Epoch: 14----------------------------------\n",
      "Batch: 0,train loss is: 0.0003751401727915731\n",
      "test loss is 0.00023146313930613786\n",
      "Batch: 100,train loss is: 0.0002118259253371689\n",
      "test loss is 0.0002426376000140704\n",
      "Batch: 200,train loss is: 0.00033083844399035454\n",
      "test loss is 0.0002959905463136643\n",
      "Batch: 300,train loss is: 0.00021400453815105606\n",
      "test loss is 0.00023744502763805282\n",
      "Batch: 400,train loss is: 0.000248681394303213\n",
      "test loss is 0.00023850539598196152\n",
      "Batch: 500,train loss is: 0.000750807040339217\n",
      "test loss is 0.00024278650034283185\n",
      "Batch: 600,train loss is: 0.00037441219934167497\n",
      "test loss is 0.00026274669905337596\n",
      "Batch: 700,train loss is: 0.00016197427143064863\n",
      "test loss is 0.00022624349486061022\n",
      "Batch: 800,train loss is: 0.00030129447109722267\n",
      "test loss is 0.0002788496537767631\n",
      "Batch: 900,train loss is: 0.00016901922266858769\n",
      "test loss is 0.000246643680747386\n",
      "Batch: 1000,train loss is: 0.0002994719101423925\n",
      "test loss is 0.0002315367479163145\n",
      "Batch: 1100,train loss is: 0.00013507420481817729\n",
      "test loss is 0.00022907184322823118\n",
      "Batch: 1200,train loss is: 0.0003642088633805948\n",
      "test loss is 0.00034320254717209753\n",
      "Batch: 1300,train loss is: 0.00018995540033400453\n",
      "test loss is 0.0002715908343977037\n",
      "Batch: 1400,train loss is: 0.00025818821139167745\n",
      "test loss is 0.00024518095606874057\n",
      "Batch: 1500,train loss is: 0.00045792725968629515\n",
      "test loss is 0.00039585109812413047\n",
      "Batch: 1600,train loss is: 0.0002604921911107368\n",
      "test loss is 0.00024220970834832868\n",
      "Batch: 1700,train loss is: 0.0002281069244254083\n",
      "test loss is 0.0002541087043933798\n",
      "Batch: 1800,train loss is: 0.00020377650885325578\n",
      "test loss is 0.00022827900021520938\n",
      "Batch: 1900,train loss is: 0.00018432310677432697\n",
      "test loss is 0.00025214030270189614\n",
      "Batch: 2000,train loss is: 0.00020150229638700294\n",
      "test loss is 0.0003790094007863045\n",
      "Batch: 2100,train loss is: 0.0004146105936341299\n",
      "test loss is 0.0002521427916355025\n",
      "Batch: 2200,train loss is: 0.00015844047631818022\n",
      "test loss is 0.0002807302530495663\n",
      "Batch: 2300,train loss is: 0.00022845838266689526\n",
      "test loss is 0.00025165398478600465\n",
      "Batch: 2400,train loss is: 0.0002705980860962241\n",
      "test loss is 0.00026359920782015626\n",
      "Batch: 2500,train loss is: 0.000413054971029872\n",
      "test loss is 0.0003222246810172311\n",
      "Batch: 2600,train loss is: 0.00043693242628193836\n",
      "test loss is 0.0002240406561851271\n",
      "Batch: 2700,train loss is: 0.00016030748493276392\n",
      "test loss is 0.00022489571269696302\n",
      "Batch: 2800,train loss is: 0.00021056136398404977\n",
      "test loss is 0.0002717093268755305\n",
      "Batch: 2900,train loss is: 0.00024322537427834404\n",
      "test loss is 0.00023500281556720933\n",
      "Batch: 3000,train loss is: 0.00030905917939291267\n",
      "test loss is 0.0002427139338669975\n",
      "Batch: 3100,train loss is: 0.00018424258529363934\n",
      "test loss is 0.0002670803685214918\n",
      "Batch: 3200,train loss is: 0.0003463545714959169\n",
      "test loss is 0.0002788727392947681\n",
      "Batch: 3300,train loss is: 0.0005821812549891984\n",
      "test loss is 0.0002553979972443596\n",
      "Batch: 3400,train loss is: 0.00025417521095869814\n",
      "test loss is 0.00022647367345624472\n",
      "Batch: 3500,train loss is: 0.00014064579463090153\n",
      "test loss is 0.0003085149914060944\n",
      "Batch: 3600,train loss is: 0.00015715149076718634\n",
      "test loss is 0.0003004604370950353\n",
      "Batch: 3700,train loss is: 0.0002676945547511715\n",
      "test loss is 0.00022518984683214162\n",
      "Batch: 3800,train loss is: 0.000311627978105851\n",
      "test loss is 0.00027586349307927386\n",
      "Batch: 3900,train loss is: 0.00017541699356455096\n",
      "test loss is 0.0002361545449661847\n",
      "Batch: 4000,train loss is: 0.000232845460319259\n",
      "test loss is 0.00023564604876015452\n",
      "Batch: 4100,train loss is: 0.00017690711917819828\n",
      "test loss is 0.00023774213404368017\n",
      "Batch: 4200,train loss is: 0.00025676048658033963\n",
      "test loss is 0.00026696220738553944\n",
      "Batch: 4300,train loss is: 0.0002695201130952414\n",
      "test loss is 0.00022409947134897877\n",
      "Batch: 4400,train loss is: 0.00015665222719995285\n",
      "test loss is 0.00025415784416865574\n",
      "Batch: 4500,train loss is: 0.00015953456596481169\n",
      "test loss is 0.000229598356756273\n",
      "Batch: 4600,train loss is: 0.00022419256105044515\n",
      "test loss is 0.0002671016063921451\n",
      "Batch: 4700,train loss is: 0.0002632015167891591\n",
      "test loss is 0.0002656871769021321\n",
      "Batch: 4800,train loss is: 0.00026365185708593326\n",
      "test loss is 0.00027793901863674676\n",
      "Batch: 4900,train loss is: 0.00021653221554056466\n",
      "test loss is 0.0002629851445945664\n",
      "Batch: 5000,train loss is: 0.0003656823034468812\n",
      "test loss is 0.00028827292266263776\n",
      "Batch: 5100,train loss is: 0.0004308664222782688\n",
      "test loss is 0.00033334517603204953\n",
      "Batch: 5200,train loss is: 0.00013721198190363542\n",
      "test loss is 0.00022700313411055101\n",
      "Batch: 5300,train loss is: 0.00019153249935731798\n",
      "test loss is 0.0002512573748164929\n",
      "Batch: 5400,train loss is: 0.00029580610271589115\n",
      "test loss is 0.00026879331511157087\n",
      "Batch: 5500,train loss is: 0.00041272195632895625\n",
      "test loss is 0.00028056350805277623\n",
      "Batch: 5600,train loss is: 0.0002573706126899945\n",
      "test loss is 0.00022333562703439952\n",
      "Batch: 5700,train loss is: 0.0002292838172939845\n",
      "test loss is 0.00027398556338676245\n",
      "Batch: 5800,train loss is: 0.00021302076602404966\n",
      "test loss is 0.0002575758935690966\n",
      "Batch: 5900,train loss is: 0.0001893515634060142\n",
      "test loss is 0.0002482772200623696\n",
      "Batch: 6000,train loss is: 0.00026300587927205617\n",
      "test loss is 0.00023826759615319171\n",
      "Batch: 6100,train loss is: 0.000414860348633896\n",
      "test loss is 0.00022729314466881043\n",
      "Batch: 6200,train loss is: 0.00016894577064956016\n",
      "test loss is 0.00026828423191027205\n",
      "Batch: 6300,train loss is: 0.0004148416545735624\n",
      "test loss is 0.00022879831494508187\n",
      "Batch: 6400,train loss is: 0.00018085792138018816\n",
      "test loss is 0.00023224646694018102\n",
      "Batch: 6500,train loss is: 0.00021880666542279623\n",
      "test loss is 0.0002483489677884258\n",
      "Batch: 6600,train loss is: 0.00045468037376481657\n",
      "test loss is 0.00030270589913960284\n",
      "Batch: 6700,train loss is: 0.0002456994127434535\n",
      "test loss is 0.0002816959428151857\n",
      "Batch: 6800,train loss is: 0.0002313688099524717\n",
      "test loss is 0.000247921710686797\n",
      "Batch: 6900,train loss is: 0.00016047838620590399\n",
      "test loss is 0.0002575614560802462\n",
      "Batch: 7000,train loss is: 0.0002848094449593933\n",
      "test loss is 0.00029647612255720093\n",
      "Batch: 7100,train loss is: 0.00022987019398472354\n",
      "test loss is 0.00023137102487211578\n",
      "Batch: 7200,train loss is: 0.00030628137594500114\n",
      "test loss is 0.0002405303532512379\n",
      "Batch: 7300,train loss is: 0.00016974849708600555\n",
      "test loss is 0.00027570200712196004\n",
      "Batch: 7400,train loss is: 0.00048024982799353455\n",
      "test loss is 0.00023846553931144743\n",
      "Batch: 7500,train loss is: 0.00023409954852782118\n",
      "test loss is 0.0002528045849348696\n",
      "Batch: 7600,train loss is: 0.00029785680439073207\n",
      "test loss is 0.0002938072959156874\n",
      "Batch: 7700,train loss is: 0.00020202587948141748\n",
      "test loss is 0.000259134820832214\n",
      "Batch: 7800,train loss is: 0.00021992768797429168\n",
      "test loss is 0.0002237310925067647\n",
      "Batch: 7900,train loss is: 0.00030113204306426826\n",
      "test loss is 0.0002421546283838078\n",
      "Batch: 8000,train loss is: 0.00015094406251961294\n",
      "test loss is 0.00023373588318562757\n",
      "Batch: 8100,train loss is: 0.0001679137367406485\n",
      "test loss is 0.00030973249367593353\n",
      "Batch: 8200,train loss is: 0.0002606244651084346\n",
      "test loss is 0.0002534276712869878\n",
      "Batch: 8300,train loss is: 0.00021292359207603282\n",
      "test loss is 0.00023644669604584603\n",
      "Batch: 8400,train loss is: 0.00026552927486338924\n",
      "test loss is 0.0003309243572204069\n",
      "Batch: 8500,train loss is: 0.00021199080709315805\n",
      "test loss is 0.0002652977780166987\n",
      "Batch: 8600,train loss is: 0.0003315866009589635\n",
      "test loss is 0.00024073016165465164\n",
      "Batch: 8700,train loss is: 0.00020363193967569765\n",
      "test loss is 0.00023203139970886966\n",
      "Batch: 8800,train loss is: 0.0001649768310349144\n",
      "test loss is 0.00024056322299637726\n",
      "Batch: 8900,train loss is: 0.0004263415849356185\n",
      "test loss is 0.0002745882039499605\n",
      "Batch: 9000,train loss is: 0.00024214121462975605\n",
      "test loss is 0.00027885852107578153\n",
      "Batch: 9100,train loss is: 0.0002547131991648276\n",
      "test loss is 0.00024809688593859945\n",
      "Batch: 9200,train loss is: 0.00021074985865633905\n",
      "test loss is 0.00025780882645582386\n",
      "Batch: 9300,train loss is: 0.00028608848295549844\n",
      "test loss is 0.00024023256975104916\n",
      "Batch: 9400,train loss is: 0.0004138375987681838\n",
      "test loss is 0.00037345718597600006\n",
      "Batch: 9500,train loss is: 0.00014951481961260864\n",
      "test loss is 0.0002419869505027468\n",
      "Batch: 9600,train loss is: 0.0002179709645395981\n",
      "test loss is 0.00025502259003985404\n",
      "Batch: 9700,train loss is: 0.0001643563184896641\n",
      "test loss is 0.00023644402754187708\n",
      "Batch: 9800,train loss is: 0.00039409283672035725\n",
      "test loss is 0.0002322749093516495\n",
      "Batch: 9900,train loss is: 0.00019882710120049007\n",
      "test loss is 0.00023312015305210494\n",
      "Batch: 10000,train loss is: 0.0002213711675916226\n",
      "test loss is 0.00025446330268385695\n",
      "Batch: 10100,train loss is: 0.0003686541530181565\n",
      "test loss is 0.00025468284712786465\n",
      "Batch: 10200,train loss is: 0.00020391679016502803\n",
      "test loss is 0.00022397792521331825\n",
      "Batch: 10300,train loss is: 0.00029326836657026087\n",
      "test loss is 0.00023880727153670749\n",
      "Batch: 10400,train loss is: 0.0001937999260106873\n",
      "test loss is 0.000273608159008943\n",
      "Batch: 10500,train loss is: 0.0003588479558825003\n",
      "test loss is 0.00033391117615591765\n",
      "Batch: 10600,train loss is: 0.00021363131204514888\n",
      "test loss is 0.0002340130510067843\n",
      "Batch: 10700,train loss is: 0.00016380225351675945\n",
      "test loss is 0.00023953711111071745\n",
      "Batch: 10800,train loss is: 0.0001678026078309928\n",
      "test loss is 0.0002916168070861908\n",
      "Batch: 10900,train loss is: 0.00015415998942798555\n",
      "test loss is 0.0002522451100852273\n",
      "Batch: 11000,train loss is: 0.00024081378620066158\n",
      "test loss is 0.00025729753639354257\n",
      "Batch: 11100,train loss is: 0.00021519894704036068\n",
      "test loss is 0.0002486319304706058\n",
      "Batch: 11200,train loss is: 0.00042485298640367806\n",
      "test loss is 0.00030206284314224187\n",
      "Batch: 11300,train loss is: 0.0002705611516335865\n",
      "test loss is 0.00023944894677751835\n",
      "Batch: 11400,train loss is: 0.0004928512503709099\n",
      "test loss is 0.0002848762711261869\n",
      "Batch: 11500,train loss is: 0.00018342787208490164\n",
      "test loss is 0.00024077116009597164\n",
      "Batch: 11600,train loss is: 0.00031284862703031765\n",
      "test loss is 0.0002450201900260792\n",
      "Batch: 11700,train loss is: 0.00014048713801141058\n",
      "test loss is 0.0002640531088857239\n",
      "Batch: 11800,train loss is: 0.000251504292007247\n",
      "test loss is 0.00023836609737297652\n",
      "Batch: 11900,train loss is: 0.00026046886315265637\n",
      "test loss is 0.0003054186836120805\n",
      "Batch: 12000,train loss is: 0.00015848098606477856\n",
      "test loss is 0.00023927233940974023\n",
      "Batch: 12100,train loss is: 0.0002542440745237206\n",
      "test loss is 0.00025850923981490565\n",
      "Batch: 12200,train loss is: 0.00026643389031941195\n",
      "test loss is 0.00022653357585994\n",
      "Batch: 12300,train loss is: 0.00029682728481557395\n",
      "test loss is 0.0002808369309301038\n",
      "Batch: 12400,train loss is: 0.0001675376004629846\n",
      "test loss is 0.0002654478615182684\n",
      "Batch: 12500,train loss is: 0.00020494450202569714\n",
      "test loss is 0.0002236600032196034\n",
      "Batch: 12600,train loss is: 0.0001450022468292892\n",
      "test loss is 0.0002700304571766791\n",
      "Batch: 12700,train loss is: 0.00017621166466083883\n",
      "test loss is 0.0002998280890071094\n",
      "Batch: 12800,train loss is: 0.00026134844729245287\n",
      "test loss is 0.00023918194833931418\n",
      "Batch: 12900,train loss is: 0.00021554694596116495\n",
      "test loss is 0.00025020721278674505\n",
      "Batch: 13000,train loss is: 0.00023699679293232302\n",
      "test loss is 0.0002609847529882085\n",
      "Batch: 13100,train loss is: 0.00024381635761630007\n",
      "test loss is 0.00022130219922259146\n",
      "Batch: 13200,train loss is: 0.0002533102695531978\n",
      "test loss is 0.0002383888212194929\n",
      "Batch: 13300,train loss is: 0.00016997502954296166\n",
      "test loss is 0.0002205378300523494\n",
      "Batch: 13400,train loss is: 0.00020681629335052857\n",
      "test loss is 0.0002587131593807099\n",
      "Batch: 13500,train loss is: 0.0004214657708532355\n",
      "test loss is 0.0002290372190164241\n",
      "Batch: 13600,train loss is: 0.0003319712813975274\n",
      "test loss is 0.0002697892635192651\n",
      "Batch: 13700,train loss is: 0.0005724415594835965\n",
      "test loss is 0.00023339882067301127\n",
      "Batch: 13800,train loss is: 0.00034043543752222314\n",
      "test loss is 0.0002704859611471022\n",
      "Batch: 13900,train loss is: 0.0002613826983120816\n",
      "test loss is 0.0002534158117626084\n",
      "Batch: 14000,train loss is: 0.0002322161948558556\n",
      "test loss is 0.0002529911007159827\n",
      "Batch: 14100,train loss is: 0.00028389265178543796\n",
      "test loss is 0.000272588935258745\n",
      "Batch: 14200,train loss is: 0.00030740111455489905\n",
      "test loss is 0.00023374539646433724\n",
      "Batch: 14300,train loss is: 0.0008723861328190101\n",
      "test loss is 0.000320257920639031\n",
      "Batch: 14400,train loss is: 0.00022975426166905934\n",
      "test loss is 0.00027042759599714243\n",
      "Batch: 14500,train loss is: 0.00041978534646345135\n",
      "test loss is 0.00023322740446736509\n",
      "Batch: 14600,train loss is: 0.00019009327229383764\n",
      "test loss is 0.00023417106112059958\n",
      "Batch: 14700,train loss is: 0.0002642805635188008\n",
      "test loss is 0.00024298685892879695\n",
      "Batch: 14800,train loss is: 0.0002342977000740048\n",
      "test loss is 0.0002311582036571033\n",
      "Batch: 14900,train loss is: 0.00025250126053981165\n",
      "test loss is 0.00023229164721264966\n",
      "Batch: 15000,train loss is: 0.00038555078635853256\n",
      "test loss is 0.0003029001035981554\n",
      "Batch: 15100,train loss is: 0.00028507782084894975\n",
      "test loss is 0.0002738577199276024\n",
      "Batch: 15200,train loss is: 0.00022701291420832204\n",
      "test loss is 0.0002704152717350992\n",
      "Batch: 15300,train loss is: 0.00013336249085075223\n",
      "test loss is 0.0002936584213668973\n",
      "Batch: 15400,train loss is: 0.00016262985106258482\n",
      "test loss is 0.0002730122588620238\n",
      "Batch: 15500,train loss is: 0.0002587941912998283\n",
      "test loss is 0.0003229558445303506\n",
      "Batch: 15600,train loss is: 0.00028647998405846803\n",
      "test loss is 0.0002300290689402386\n",
      "Batch: 15700,train loss is: 0.000258889230073566\n",
      "test loss is 0.0002811785885446298\n",
      "Batch: 15800,train loss is: 0.0001766544283914833\n",
      "test loss is 0.00025055827739243683\n",
      "Batch: 15900,train loss is: 0.00028215223107042146\n",
      "test loss is 0.00025837267231622037\n",
      "Batch: 16000,train loss is: 0.0004332135792374736\n",
      "test loss is 0.000345993599083664\n",
      "Batch: 16100,train loss is: 0.00027524629390320764\n",
      "test loss is 0.00028102330042084026\n",
      "Batch: 16200,train loss is: 0.0002341001637361338\n",
      "test loss is 0.0002303959381282513\n",
      "Batch: 16300,train loss is: 0.00022014151762519973\n",
      "test loss is 0.0002506921322902003\n",
      "Batch: 16400,train loss is: 0.00031126878792659215\n",
      "test loss is 0.00026025183229705097\n",
      "Batch: 16500,train loss is: 0.0002671342543993594\n",
      "test loss is 0.00022650782923221326\n",
      "Batch: 16600,train loss is: 0.00019438955963585846\n",
      "test loss is 0.00022683661017605333\n",
      "Batch: 16700,train loss is: 0.0002691348440932061\n",
      "test loss is 0.0002459029230764113\n",
      "Batch: 16800,train loss is: 0.00018757591034193112\n",
      "test loss is 0.0003079957043105899\n",
      "Batch: 16900,train loss is: 0.00021073812742222102\n",
      "test loss is 0.00023157713620522186\n",
      "Batch: 17000,train loss is: 0.0001564178284760362\n",
      "test loss is 0.00034218985980344774\n",
      "Batch: 17100,train loss is: 0.0001597611218210711\n",
      "test loss is 0.00029073674031372844\n",
      "Batch: 17200,train loss is: 0.00025087661680591605\n",
      "test loss is 0.00027291822940358085\n",
      "Batch: 17300,train loss is: 0.000174290713958477\n",
      "test loss is 0.00023340537992687843\n",
      "Batch: 17400,train loss is: 0.00031505420768303423\n",
      "test loss is 0.00022569429516721453\n",
      "Batch: 17500,train loss is: 0.00023168229321511924\n",
      "test loss is 0.0002676117549246219\n",
      "Batch: 17600,train loss is: 0.0002983531603705525\n",
      "test loss is 0.00028956963920672324\n",
      "Batch: 17700,train loss is: 0.0001839440148648226\n",
      "test loss is 0.00022739716533167294\n",
      "Batch: 17800,train loss is: 0.0003058793777786559\n",
      "test loss is 0.0002304434077603231\n",
      "Batch: 17900,train loss is: 0.00022017686042263477\n",
      "test loss is 0.0002483270497106714\n",
      "Batch: 18000,train loss is: 0.0001779826708014382\n",
      "test loss is 0.00029089074498873626\n",
      "Batch: 18100,train loss is: 0.00017142510183911863\n",
      "test loss is 0.00028976177191114474\n",
      "Batch: 18200,train loss is: 0.0005218582473397004\n",
      "test loss is 0.00024744527288608385\n",
      "Batch: 18300,train loss is: 0.00020626498455835016\n",
      "test loss is 0.00026505450787867035\n",
      "Batch: 18400,train loss is: 0.0003171805235788358\n",
      "test loss is 0.00022138290625034154\n",
      "Batch: 18500,train loss is: 0.00017583078919276217\n",
      "test loss is 0.00027915791770429156\n",
      "Batch: 18600,train loss is: 0.00018129290578654874\n",
      "test loss is 0.00023335709026778325\n",
      "Batch: 18700,train loss is: 0.00018579595150752072\n",
      "test loss is 0.00026330489269449673\n",
      "Batch: 18800,train loss is: 0.0004221319326344447\n",
      "test loss is 0.00023676733082907467\n",
      "Batch: 18900,train loss is: 0.00018272912887579146\n",
      "test loss is 0.00025865548027413827\n",
      "Batch: 19000,train loss is: 0.0003042120016018612\n",
      "test loss is 0.00029796907931717265\n",
      "Batch: 19100,train loss is: 0.0001950155530594256\n",
      "test loss is 0.00027718513653588194\n",
      "Batch: 19200,train loss is: 0.00011107599531882853\n",
      "test loss is 0.00022603959880811392\n",
      "Batch: 19300,train loss is: 0.0002604847422127954\n",
      "test loss is 0.0003027943452068302\n",
      "Batch: 19400,train loss is: 0.0002702705691382405\n",
      "test loss is 0.00023676534468782047\n",
      "Batch: 19500,train loss is: 0.00026675155970948115\n",
      "test loss is 0.0002490055198462756\n",
      "Batch: 19600,train loss is: 0.0002984961897508748\n",
      "test loss is 0.0002262170610773468\n",
      "Batch: 19700,train loss is: 0.00032372984850516086\n",
      "test loss is 0.00023038323566524252\n",
      "Batch: 19800,train loss is: 0.00012585989607319842\n",
      "test loss is 0.00025914073438760237\n",
      "Batch: 19900,train loss is: 0.000411953491803071\n",
      "test loss is 0.0002907485086118146\n",
      "Batch: 20000,train loss is: 0.00018751315014628044\n",
      "test loss is 0.0002237993791854827\n",
      "Batch: 20100,train loss is: 0.0002055298687071064\n",
      "test loss is 0.0002280717305136028\n",
      "Batch: 20200,train loss is: 0.00015612518422368715\n",
      "test loss is 0.00022696772953854805\n",
      "Batch: 20300,train loss is: 0.0002470324725213636\n",
      "test loss is 0.0002550052810015275\n",
      "Batch: 20400,train loss is: 0.00032696753471523005\n",
      "test loss is 0.0002298910467622193\n",
      "Batch: 20500,train loss is: 0.0003070471423812991\n",
      "test loss is 0.00025321799094018594\n",
      "Batch: 20600,train loss is: 0.00018769774388741934\n",
      "test loss is 0.0002376950537689224\n",
      "Batch: 20700,train loss is: 0.00019935821179100892\n",
      "test loss is 0.00027440781139963493\n",
      "Batch: 20800,train loss is: 0.00028545800553947127\n",
      "test loss is 0.0002784459117616532\n",
      "Batch: 20900,train loss is: 0.0002164976700552171\n",
      "test loss is 0.00024478031405512473\n",
      "Batch: 21000,train loss is: 0.000249357420248285\n",
      "test loss is 0.0002287506504096949\n",
      "Batch: 21100,train loss is: 0.00018775660857565354\n",
      "test loss is 0.00023966733553198614\n",
      "Batch: 21200,train loss is: 0.0004454453255472667\n",
      "test loss is 0.0002812623729380272\n",
      "Batch: 21300,train loss is: 0.00018925454694997205\n",
      "test loss is 0.0002556742050016755\n",
      "Batch: 21400,train loss is: 0.000316192981783651\n",
      "test loss is 0.00028007128721248747\n",
      "Batch: 21500,train loss is: 0.0001606485947926895\n",
      "test loss is 0.00024279289985010112\n",
      "Batch: 21600,train loss is: 0.0002752631025997103\n",
      "test loss is 0.00023771770214364333\n",
      "Batch: 21700,train loss is: 0.00023886973163165135\n",
      "test loss is 0.00023452789995410383\n",
      "Batch: 21800,train loss is: 0.00026056016926282344\n",
      "test loss is 0.0002468863799274262\n",
      "Batch: 21900,train loss is: 0.00013493935559835725\n",
      "test loss is 0.00023754895454416221\n",
      "Batch: 22000,train loss is: 0.00024267929121136604\n",
      "test loss is 0.00027308096493240615\n",
      "Batch: 22100,train loss is: 0.00021556040222929726\n",
      "test loss is 0.0002145581306148209\n",
      "Batch: 22200,train loss is: 0.0002828161771127113\n",
      "test loss is 0.00025876034976793474\n",
      "Batch: 22300,train loss is: 0.00024078012374787518\n",
      "test loss is 0.0002622444541421414\n",
      "Batch: 22400,train loss is: 0.00026118779929354326\n",
      "test loss is 0.00026809028260459214\n",
      "Batch: 22500,train loss is: 0.0002325209301625164\n",
      "test loss is 0.00024183482112672642\n",
      "Batch: 22600,train loss is: 0.00019787289013106882\n",
      "test loss is 0.0002828509958845049\n",
      "Batch: 22700,train loss is: 0.00014262521645514847\n",
      "test loss is 0.00025306917468849156\n",
      "Batch: 22800,train loss is: 0.0003071302838727441\n",
      "test loss is 0.0004016459340280723\n",
      "Batch: 22900,train loss is: 0.00019509103886097303\n",
      "test loss is 0.0002521566293209601\n",
      "Batch: 23000,train loss is: 0.00016325873203462172\n",
      "test loss is 0.0002494708592924934\n",
      "Batch: 23100,train loss is: 0.0004173217633695245\n",
      "test loss is 0.0002872086738827276\n",
      "Batch: 23200,train loss is: 0.000190808154122978\n",
      "test loss is 0.0002570737065654943\n",
      "Batch: 23300,train loss is: 0.0006408140788127175\n",
      "test loss is 0.00024781892463309233\n",
      "Batch: 23400,train loss is: 0.000342977123338431\n",
      "test loss is 0.00027046414864060513\n",
      "Batch: 23500,train loss is: 0.0001961511295195091\n",
      "test loss is 0.0002635512027049442\n",
      "Batch: 23600,train loss is: 0.00034001764622338445\n",
      "test loss is 0.00023078936135876007\n",
      "Batch: 23700,train loss is: 0.0002808461582943262\n",
      "test loss is 0.00023516871844955347\n",
      "Batch: 23800,train loss is: 0.0001651017371838008\n",
      "test loss is 0.00024326195274456452\n",
      "Batch: 23900,train loss is: 0.00015145023430122352\n",
      "test loss is 0.0002713684737370158\n",
      "Batch: 24000,train loss is: 0.00014505832072521374\n",
      "test loss is 0.00021938605910055536\n",
      "Batch: 24100,train loss is: 0.0001547942036843132\n",
      "test loss is 0.00022640527967786984\n",
      "Batch: 24200,train loss is: 0.00024956382855438565\n",
      "test loss is 0.0002634155273220352\n",
      "Batch: 24300,train loss is: 0.00033802220558071884\n",
      "test loss is 0.0002861696790965177\n",
      "Batch: 24400,train loss is: 0.00030437455626739125\n",
      "test loss is 0.00026519717412615246\n",
      "Batch: 24500,train loss is: 0.0002816307329266775\n",
      "test loss is 0.0002867453040525125\n",
      "Batch: 24600,train loss is: 0.000193076445398404\n",
      "test loss is 0.00027381609110390596\n",
      "Batch: 24700,train loss is: 0.00018723569441912404\n",
      "test loss is 0.00023214333377268034\n",
      "Batch: 24800,train loss is: 0.00021874849598663685\n",
      "test loss is 0.0002799952909379169\n",
      "Batch: 24900,train loss is: 0.00019048954413355507\n",
      "test loss is 0.0002760045738712149\n",
      "Batch: 25000,train loss is: 0.00033455553990319496\n",
      "test loss is 0.0002733830648742199\n",
      "Batch: 25100,train loss is: 0.00023317311044634896\n",
      "test loss is 0.0002454625096836596\n",
      "Batch: 25200,train loss is: 0.00017284118704484968\n",
      "test loss is 0.00022386301386732955\n",
      "Batch: 25300,train loss is: 0.0002493385810421413\n",
      "test loss is 0.00030174873042666486\n",
      "Batch: 25400,train loss is: 0.0002512005513489275\n",
      "test loss is 0.00023751525392610203\n",
      "Batch: 25500,train loss is: 0.0002948808939748712\n",
      "test loss is 0.00022711966332238247\n",
      "Batch: 25600,train loss is: 0.00043618820267936565\n",
      "test loss is 0.0002768283499077023\n",
      "Batch: 25700,train loss is: 0.00021225257986099804\n",
      "test loss is 0.00024786070641421003\n",
      "Batch: 25800,train loss is: 0.00017655510929443717\n",
      "test loss is 0.00025380393912301203\n",
      "Batch: 25900,train loss is: 0.00030048919673765126\n",
      "test loss is 0.00029212053330819886\n",
      "Batch: 26000,train loss is: 0.00019887560660498477\n",
      "test loss is 0.0002518273277910829\n",
      "Batch: 26100,train loss is: 0.00023179169474665958\n",
      "test loss is 0.0002839759243382993\n",
      "Batch: 26200,train loss is: 0.00021732877149373275\n",
      "test loss is 0.0002512369373266778\n",
      "Batch: 26300,train loss is: 0.0002764973929673313\n",
      "test loss is 0.00027864553252971244\n",
      "Batch: 26400,train loss is: 0.0001723541319388591\n",
      "test loss is 0.00023904795458096288\n",
      "Batch: 26500,train loss is: 0.00024029119666707207\n",
      "test loss is 0.00025073211082763395\n",
      "Batch: 26600,train loss is: 0.00024205842369919944\n",
      "test loss is 0.0002357028293845953\n",
      "Batch: 26700,train loss is: 0.00022392014422006282\n",
      "test loss is 0.00024510829536362284\n",
      "Batch: 26800,train loss is: 0.00027371537172905705\n",
      "test loss is 0.00025417994253606547\n",
      "Batch: 26900,train loss is: 0.00023366843419288548\n",
      "test loss is 0.00024524365027789953\n",
      "Batch: 27000,train loss is: 0.00023593378026810454\n",
      "test loss is 0.000254353913067555\n",
      "Batch: 27100,train loss is: 0.00017401630201130654\n",
      "test loss is 0.00023212622318569842\n",
      "Batch: 27200,train loss is: 0.00018168101997840407\n",
      "test loss is 0.00023658645981157044\n",
      "Batch: 27300,train loss is: 0.00019908708051750914\n",
      "test loss is 0.00024523565163677997\n",
      "Batch: 27400,train loss is: 0.00015955539999216144\n",
      "test loss is 0.00022741913272197927\n",
      "Batch: 27500,train loss is: 0.00015336426585942607\n",
      "test loss is 0.00024850241068712515\n",
      "Batch: 27600,train loss is: 0.0001152484562004123\n",
      "test loss is 0.00026567344009142507\n",
      "Batch: 27700,train loss is: 0.00024156369667486675\n",
      "test loss is 0.0002779371776702055\n",
      "Batch: 27800,train loss is: 0.00014687408879291526\n",
      "test loss is 0.00024690111576765187\n",
      "Batch: 27900,train loss is: 0.0001619012064258346\n",
      "test loss is 0.00023219431254681698\n",
      "Batch: 28000,train loss is: 0.00022984823540565612\n",
      "test loss is 0.00025876247518828125\n",
      "Batch: 28100,train loss is: 0.00027125829862065476\n",
      "test loss is 0.00022019408313591016\n",
      "Batch: 28200,train loss is: 0.00019985879673017307\n",
      "test loss is 0.0002408699086605752\n",
      "Batch: 28300,train loss is: 0.00024416403658444\n",
      "test loss is 0.0002617331112034057\n",
      "Batch: 28400,train loss is: 0.0003026900433477009\n",
      "test loss is 0.0002777846083970119\n",
      "Batch: 28500,train loss is: 0.000262080124528578\n",
      "test loss is 0.0002538313128024702\n",
      "Batch: 28600,train loss is: 0.00014131345229163907\n",
      "test loss is 0.00024454284702222763\n",
      "Batch: 28700,train loss is: 0.00019338686095575808\n",
      "test loss is 0.0002532994974946393\n",
      "Batch: 28800,train loss is: 0.00018127505198206\n",
      "test loss is 0.00030082806790110466\n",
      "Batch: 28900,train loss is: 0.00024132135363895188\n",
      "test loss is 0.0002934023951321976\n",
      "Batch: 29000,train loss is: 0.00018384833081612665\n",
      "test loss is 0.00023685644846184678\n",
      "Batch: 29100,train loss is: 0.00014102819661532086\n",
      "test loss is 0.0002652374876203302\n",
      "Batch: 29200,train loss is: 0.00011434987163840222\n",
      "test loss is 0.00024355595352058886\n",
      "Batch: 29300,train loss is: 0.00018021239220010552\n",
      "test loss is 0.000254301103802562\n",
      "Batch: 29400,train loss is: 0.00025966734301479907\n",
      "test loss is 0.0002592353752962819\n",
      "Batch: 29500,train loss is: 0.00028233346343699186\n",
      "test loss is 0.0002821786920011672\n",
      "Batch: 29600,train loss is: 0.0001931823404459585\n",
      "test loss is 0.00023303929531519086\n",
      "Batch: 29700,train loss is: 0.00020658279929836976\n",
      "test loss is 0.0002418230981160483\n",
      "Batch: 29800,train loss is: 0.00046353254904882193\n",
      "test loss is 0.0002479724774703253\n",
      "Batch: 29900,train loss is: 0.00026520493825975895\n",
      "test loss is 0.0002493639308993202\n",
      "Batch: 30000,train loss is: 0.0003660802108557894\n",
      "test loss is 0.0002754591336489594\n",
      "Batch: 30100,train loss is: 0.00019055588854222262\n",
      "test loss is 0.00022119955088547615\n",
      "Batch: 30200,train loss is: 0.0001966792575991997\n",
      "test loss is 0.00022442319694491284\n",
      "Batch: 30300,train loss is: 0.0002742578054791054\n",
      "test loss is 0.0002512430782294949\n",
      "Batch: 30400,train loss is: 0.00026574731522415254\n",
      "test loss is 0.0002736196882692101\n",
      "Batch: 30500,train loss is: 0.00024275242500843122\n",
      "test loss is 0.00023210751090039686\n",
      "Batch: 30600,train loss is: 0.000187597719736274\n",
      "test loss is 0.0002924583009090561\n",
      "Batch: 30700,train loss is: 0.00019751822515386781\n",
      "test loss is 0.0002897521654914589\n",
      "Batch: 30800,train loss is: 0.00012394467006796654\n",
      "test loss is 0.00023743173015619584\n",
      "Batch: 30900,train loss is: 0.00021171901716349412\n",
      "test loss is 0.00025918481107257095\n",
      "Batch: 31000,train loss is: 0.00033057432687190084\n",
      "test loss is 0.00023037925608807377\n",
      "Batch: 31100,train loss is: 0.0002175091403871179\n",
      "test loss is 0.0002726653683547163\n",
      "Batch: 31200,train loss is: 0.00023901326275536577\n",
      "test loss is 0.0002699790428960894\n",
      "Batch: 31300,train loss is: 0.00021777909456414544\n",
      "test loss is 0.00026822572700364546\n",
      "Batch: 31400,train loss is: 0.0002703273692554067\n",
      "test loss is 0.000284246218115009\n",
      "Batch: 31500,train loss is: 0.0005017367481299592\n",
      "test loss is 0.0002438069611546858\n",
      "Batch: 31600,train loss is: 0.0002112802271058767\n",
      "test loss is 0.0003517702602153808\n",
      "Batch: 31700,train loss is: 0.0002491533269736942\n",
      "test loss is 0.0003177239181597947\n",
      "Batch: 31800,train loss is: 0.00020210075326251327\n",
      "test loss is 0.00028071400110805727\n",
      "Batch: 31900,train loss is: 0.00014402639626246756\n",
      "test loss is 0.0002438070460425314\n",
      "Batch: 32000,train loss is: 0.00016694101794913424\n",
      "test loss is 0.00025596206790128343\n",
      "Batch: 32100,train loss is: 0.00020024882229887034\n",
      "test loss is 0.00023362627613353378\n",
      "Batch: 32200,train loss is: 0.0003007385468874416\n",
      "test loss is 0.00029111228758892625\n",
      "Batch: 32300,train loss is: 0.000176620891724884\n",
      "test loss is 0.0003275055022397271\n",
      "Batch: 32400,train loss is: 0.0003074584018891365\n",
      "test loss is 0.00021877618442046342\n",
      "Batch: 32500,train loss is: 0.0002264045769748653\n",
      "test loss is 0.0002804437390651915\n",
      "Batch: 32600,train loss is: 0.00016310557234927635\n",
      "test loss is 0.00023362548291619895\n",
      "Batch: 32700,train loss is: 0.00023719887313998811\n",
      "test loss is 0.0002571454035434736\n",
      "Batch: 32800,train loss is: 0.00025794933013937986\n",
      "test loss is 0.00030682501004651477\n",
      "Batch: 32900,train loss is: 0.00018933000195863581\n",
      "test loss is 0.0002777206198006797\n",
      "Batch: 33000,train loss is: 0.0004438243530282207\n",
      "test loss is 0.0002479163355899219\n",
      "Batch: 33100,train loss is: 0.00019085672706107334\n",
      "test loss is 0.0002413620956612848\n",
      "Batch: 33200,train loss is: 0.0002181509831447039\n",
      "test loss is 0.00022759834871572882\n",
      "Batch: 33300,train loss is: 0.00021942250986895466\n",
      "test loss is 0.00024677785689900726\n",
      "Batch: 33400,train loss is: 0.00021435764530041817\n",
      "test loss is 0.0002422019479479561\n",
      "Batch: 33500,train loss is: 0.00029557261417019655\n",
      "test loss is 0.0002428015889198116\n",
      "Batch: 33600,train loss is: 0.00026434191623076733\n",
      "test loss is 0.0003351379435955496\n",
      "Batch: 33700,train loss is: 0.0007159272737229488\n",
      "test loss is 0.00023150801704377785\n",
      "Batch: 33800,train loss is: 0.0004492452109189398\n",
      "test loss is 0.00024197084623099586\n",
      "Batch: 33900,train loss is: 0.00031295246370634683\n",
      "test loss is 0.00036516549387946996\n",
      "Batch: 34000,train loss is: 0.00017720743009201327\n",
      "test loss is 0.0002224105403370061\n",
      "Batch: 34100,train loss is: 0.0001115218727749493\n",
      "test loss is 0.00022203130614306752\n",
      "Batch: 34200,train loss is: 0.00023334542684358905\n",
      "test loss is 0.000252384661857355\n",
      "Batch: 34300,train loss is: 0.00027110048225790107\n",
      "test loss is 0.00029152395584201455\n",
      "Batch: 34400,train loss is: 0.000386070884313357\n",
      "test loss is 0.00030877689776993665\n",
      "Batch: 34500,train loss is: 0.00015351487598171782\n",
      "test loss is 0.00025472304839165846\n",
      "Batch: 34600,train loss is: 0.00022394156273925105\n",
      "test loss is 0.00024045172834249483\n",
      "Batch: 34700,train loss is: 0.00023605073236622362\n",
      "test loss is 0.0002669677185025876\n",
      "Batch: 34800,train loss is: 0.00020355733774404474\n",
      "test loss is 0.00023507289973880298\n",
      "Batch: 34900,train loss is: 0.00019878079199214067\n",
      "test loss is 0.0002618412628958374\n",
      "Batch: 35000,train loss is: 0.0001607650545285844\n",
      "test loss is 0.0002694347720803493\n",
      "Batch: 35100,train loss is: 0.0002450929106800816\n",
      "test loss is 0.0002529515595070285\n",
      "Batch: 35200,train loss is: 0.00016548222746823964\n",
      "test loss is 0.00022896586531107174\n",
      "Batch: 35300,train loss is: 0.000160440878409753\n",
      "test loss is 0.00026359466063609555\n",
      "Batch: 35400,train loss is: 0.00015581684415574674\n",
      "test loss is 0.00025070121951182385\n",
      "Batch: 35500,train loss is: 0.00024749640133091093\n",
      "test loss is 0.00023430106561359858\n",
      "Batch: 35600,train loss is: 0.000165243128491714\n",
      "test loss is 0.00023036449156444867\n",
      "Batch: 35700,train loss is: 0.00022892708642087976\n",
      "test loss is 0.00025594361922531004\n",
      "Batch: 35800,train loss is: 0.00019812772690448234\n",
      "test loss is 0.0002705985037995871\n",
      "Batch: 35900,train loss is: 0.00015069217788225148\n",
      "test loss is 0.00027574912146650563\n",
      "Batch: 36000,train loss is: 0.00018407704799071524\n",
      "test loss is 0.00021631119876116154\n",
      "Batch: 36100,train loss is: 0.00012816646919662137\n",
      "test loss is 0.00028754351031321835\n",
      "Batch: 36200,train loss is: 0.00018479125292540118\n",
      "test loss is 0.0002818162675701745\n",
      "Batch: 36300,train loss is: 0.00044449750240205336\n",
      "test loss is 0.00022476449657877866\n",
      "Batch: 36400,train loss is: 0.0002469182467476249\n",
      "test loss is 0.00023274078419622952\n",
      "Batch: 36500,train loss is: 0.00030934792839236886\n",
      "test loss is 0.0002384583087453319\n",
      "Batch: 36600,train loss is: 0.0001668711889764332\n",
      "test loss is 0.00024686876848107735\n",
      "Batch: 36700,train loss is: 0.00025020485985303336\n",
      "test loss is 0.00022940706652682072\n",
      "Batch: 36800,train loss is: 0.00018460428312392957\n",
      "test loss is 0.00024711718985248376\n",
      "Batch: 36900,train loss is: 0.00020000241055758425\n",
      "test loss is 0.0002757985637858999\n",
      "Batch: 37000,train loss is: 0.000252565002083099\n",
      "test loss is 0.00023906420328057766\n",
      "Batch: 37100,train loss is: 0.00037290973793000274\n",
      "test loss is 0.00022487825315711547\n",
      "Batch: 37200,train loss is: 0.00018064836130674287\n",
      "test loss is 0.00024809039733164514\n",
      "Batch: 37300,train loss is: 0.00028610284727633123\n",
      "test loss is 0.00024713788104057364\n",
      "Batch: 37400,train loss is: 0.00021675894320607954\n",
      "test loss is 0.0002460566881927086\n",
      "Batch: 37500,train loss is: 0.000306599164698879\n",
      "test loss is 0.0002837939034530143\n",
      "Batch: 37600,train loss is: 0.000416780096962631\n",
      "test loss is 0.0002579791280019737\n",
      "Batch: 37700,train loss is: 0.00016377173389063164\n",
      "test loss is 0.00021417849065500117\n",
      "Batch: 37800,train loss is: 0.0003282559778501954\n",
      "test loss is 0.00023103449359966973\n",
      "Batch: 37900,train loss is: 0.0001863361986678245\n",
      "test loss is 0.00022607333383238456\n",
      "Batch: 38000,train loss is: 0.00017295225358329247\n",
      "test loss is 0.0002354955093104289\n",
      "Batch: 38100,train loss is: 0.00041645654818001337\n",
      "test loss is 0.00023616173756787164\n",
      "Batch: 38200,train loss is: 0.00027027417372468013\n",
      "test loss is 0.00021975633457537247\n",
      "Batch: 38300,train loss is: 0.00026715676740400845\n",
      "test loss is 0.0002356580379462441\n",
      "Batch: 38400,train loss is: 0.00019859864346682428\n",
      "test loss is 0.00024233284722752975\n",
      "Batch: 38500,train loss is: 0.0002822832741394819\n",
      "test loss is 0.00023719958224763376\n",
      "Batch: 38600,train loss is: 0.0002405831747837391\n",
      "test loss is 0.0003360944559678986\n",
      "Batch: 38700,train loss is: 0.00018661687502450594\n",
      "test loss is 0.00023823080980263624\n",
      "Batch: 38800,train loss is: 0.00022885188640307138\n",
      "test loss is 0.00023793160339251372\n",
      "Batch: 38900,train loss is: 0.0002508859432175704\n",
      "test loss is 0.0002459162868348262\n",
      "Batch: 39000,train loss is: 0.0002195708306787088\n",
      "test loss is 0.00024253678416711588\n",
      "Batch: 39100,train loss is: 0.00025096949687998375\n",
      "test loss is 0.00023037349885210242\n",
      "Batch: 39200,train loss is: 0.00044654300367013265\n",
      "test loss is 0.0002733846243959423\n",
      "Batch: 39300,train loss is: 0.00020095770570927258\n",
      "test loss is 0.0002303711980354663\n",
      "Batch: 39400,train loss is: 0.00019844009619908128\n",
      "test loss is 0.0002350989637544656\n",
      "Batch: 39500,train loss is: 0.0002508626895576884\n",
      "test loss is 0.0002628334442561678\n",
      "Batch: 39600,train loss is: 0.0002992334799130072\n",
      "test loss is 0.0002695762529883872\n",
      "Batch: 39700,train loss is: 0.0003061278825606433\n",
      "test loss is 0.000272684311010643\n",
      "Batch: 39800,train loss is: 0.00025618290867925216\n",
      "test loss is 0.00031035629584214535\n",
      "Batch: 39900,train loss is: 0.00015214270071525155\n",
      "test loss is 0.0002319870556558453\n",
      "Batch: 40000,train loss is: 0.00017267380671414541\n",
      "test loss is 0.0002677302887536829\n",
      "Batch: 40100,train loss is: 0.00022681482630936017\n",
      "test loss is 0.00027053063128688263\n",
      "Batch: 40200,train loss is: 0.00029416936325711595\n",
      "test loss is 0.00027759764992157487\n",
      "Batch: 40300,train loss is: 0.00015194112505043024\n",
      "test loss is 0.00023333296130657497\n",
      "Batch: 40400,train loss is: 0.0002574516447149297\n",
      "test loss is 0.0002628285440797901\n",
      "Batch: 40500,train loss is: 0.00025211728916214426\n",
      "test loss is 0.0002749976135201407\n",
      "Batch: 40600,train loss is: 0.00025902115395344267\n",
      "test loss is 0.0003052775812598177\n",
      "Batch: 40700,train loss is: 0.00022738624396063416\n",
      "test loss is 0.00024428096508315135\n",
      "Batch: 40800,train loss is: 0.0002592283356905564\n",
      "test loss is 0.0002438555457425568\n",
      "Batch: 40900,train loss is: 0.0002153406665387117\n",
      "test loss is 0.0002940142605911797\n",
      "Batch: 41000,train loss is: 0.00018925732300840135\n",
      "test loss is 0.00022123987192337432\n",
      "Batch: 41100,train loss is: 0.00027795547820250546\n",
      "test loss is 0.0003167381627021365\n",
      "Batch: 41200,train loss is: 0.0001337134950945342\n",
      "test loss is 0.0002245387223444156\n",
      "Batch: 41300,train loss is: 0.00016775055619430278\n",
      "test loss is 0.00023863288201958995\n",
      "Batch: 41400,train loss is: 0.00029999235748231687\n",
      "test loss is 0.00024296352877886475\n",
      "Batch: 41500,train loss is: 0.0002850195812026293\n",
      "test loss is 0.0002472781292551445\n",
      "Batch: 41600,train loss is: 0.00026824068541085556\n",
      "test loss is 0.00023885610932819902\n",
      "Batch: 41700,train loss is: 0.0003964100808783029\n",
      "test loss is 0.0002556530287515627\n",
      "Batch: 41800,train loss is: 0.0003198363139485224\n",
      "test loss is 0.0002991760063683344\n",
      "Batch: 41900,train loss is: 0.00015622567343776466\n",
      "test loss is 0.00022794096257609117\n",
      "Batch: 42000,train loss is: 0.0003550010266736577\n",
      "test loss is 0.0002286549937925581\n",
      "Batch: 42100,train loss is: 0.0001586422080464547\n",
      "test loss is 0.00022961368806259815\n",
      "Batch: 42200,train loss is: 0.00020478561823764733\n",
      "test loss is 0.00024630342358667156\n",
      "Batch: 42300,train loss is: 0.0002414968400957685\n",
      "test loss is 0.00025181951823936713\n",
      "Batch: 42400,train loss is: 0.0001865361259611643\n",
      "test loss is 0.0002541189906066402\n",
      "Batch: 42500,train loss is: 0.00035487242642559763\n",
      "test loss is 0.00025028499013704823\n",
      "Batch: 42600,train loss is: 0.00023355868598782078\n",
      "test loss is 0.00021893461870292063\n",
      "Batch: 42700,train loss is: 0.0001339802973643435\n",
      "test loss is 0.0002751026052695254\n",
      "Batch: 42800,train loss is: 0.00020334452774999892\n",
      "test loss is 0.00023189932845145983\n",
      "Batch: 42900,train loss is: 0.00031666967214253347\n",
      "test loss is 0.000242044267205144\n",
      "Batch: 43000,train loss is: 0.0001668422502288106\n",
      "test loss is 0.00022304245825258094\n",
      "Batch: 43100,train loss is: 0.00013706086041443235\n",
      "test loss is 0.00023946703390245416\n",
      "Batch: 43200,train loss is: 0.0002075304318660042\n",
      "test loss is 0.0002798541281417744\n",
      "Batch: 43300,train loss is: 0.0005259307653264354\n",
      "test loss is 0.0002786648846068381\n",
      "Batch: 43400,train loss is: 0.00016894974586546832\n",
      "test loss is 0.0002485906345318872\n",
      "Batch: 43500,train loss is: 0.0001565231891349006\n",
      "test loss is 0.00023939889016368862\n",
      "Batch: 43600,train loss is: 0.00026800747471084443\n",
      "test loss is 0.0002439615124233757\n",
      "Batch: 43700,train loss is: 0.0003586373658896961\n",
      "test loss is 0.0002418433509562705\n",
      "Batch: 43800,train loss is: 0.00028855535177066773\n",
      "test loss is 0.00025703986644901977\n",
      "Batch: 43900,train loss is: 0.0002426577651000225\n",
      "test loss is 0.00024023198714301225\n",
      "Batch: 44000,train loss is: 0.0002613140516633757\n",
      "test loss is 0.0002222723250311181\n",
      "Batch: 44100,train loss is: 0.00038291359056766966\n",
      "test loss is 0.0002814218921513778\n",
      "Batch: 44200,train loss is: 0.00015880326117740593\n",
      "test loss is 0.0002407772689244365\n",
      "Batch: 44300,train loss is: 0.00018835695872884562\n",
      "test loss is 0.00024163435032463002\n",
      "Batch: 44400,train loss is: 0.0003568266456341782\n",
      "test loss is 0.00025673623306450956\n",
      "Batch: 44500,train loss is: 0.00015656711335681317\n",
      "test loss is 0.0002287270502538417\n",
      "Batch: 44600,train loss is: 0.0002594732948609685\n",
      "test loss is 0.0002587451908651766\n",
      "Batch: 44700,train loss is: 0.0002802860288211112\n",
      "test loss is 0.00022953415044322473\n",
      "Batch: 44800,train loss is: 0.00014708856735418953\n",
      "test loss is 0.000250408143426545\n",
      "Batch: 44900,train loss is: 0.00025352211624400124\n",
      "test loss is 0.0002759291298022263\n",
      "Batch: 45000,train loss is: 0.00021965945241973208\n",
      "test loss is 0.00024002254773446746\n",
      "Batch: 45100,train loss is: 0.00016272979843344926\n",
      "test loss is 0.00023243689295288975\n",
      "Batch: 45200,train loss is: 0.0001362140301009179\n",
      "test loss is 0.00029754296181807547\n",
      "Batch: 45300,train loss is: 0.00017666317499337626\n",
      "test loss is 0.00023983167048874932\n",
      "Batch: 45400,train loss is: 0.00022368178100922492\n",
      "test loss is 0.0002187379467166479\n",
      "Batch: 45500,train loss is: 0.00043305705805269366\n",
      "test loss is 0.00030993989437257466\n",
      "Batch: 45600,train loss is: 0.00018418440873717708\n",
      "test loss is 0.00024112866055755702\n",
      "Batch: 45700,train loss is: 0.00015886973040025318\n",
      "test loss is 0.00024542854365101305\n",
      "Batch: 45800,train loss is: 0.00019031911068624545\n",
      "test loss is 0.00025790270909528765\n",
      "Batch: 45900,train loss is: 0.00020751744071236986\n",
      "test loss is 0.00023018423740406847\n",
      "Batch: 46000,train loss is: 0.00029356339769007737\n",
      "test loss is 0.0002273886628266509\n",
      "Batch: 46100,train loss is: 0.00018494793444936476\n",
      "test loss is 0.0002521223368971321\n",
      "Batch: 46200,train loss is: 0.00019530527560727092\n",
      "test loss is 0.00023216880442720038\n",
      "Batch: 46300,train loss is: 0.000198801633883882\n",
      "test loss is 0.0002934210647680656\n",
      "Batch: 46400,train loss is: 0.00023553084437601357\n",
      "test loss is 0.00023943897004806362\n",
      "Batch: 46500,train loss is: 0.00032689485973876536\n",
      "test loss is 0.00030932743979841804\n",
      "Batch: 46600,train loss is: 0.00014424145710181414\n",
      "test loss is 0.00024093312296705516\n",
      "Batch: 46700,train loss is: 0.0003667991352070932\n",
      "test loss is 0.00028955512011750236\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for param_group in optim_Adam.param_groups:\n",
    "    param_group['lr'] = 0.00035\n",
    "\n",
    "train_model(loss_MSE,optim_Adam,model,data_loader,train_data,test_data,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'../restruct_data_results/rflatBergomi_pointwise88_4-layer.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "initial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
