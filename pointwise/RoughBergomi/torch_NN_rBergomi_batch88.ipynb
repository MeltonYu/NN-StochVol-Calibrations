{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "f = gzip.GzipFile(r\"../../Data/rBergomiTrainSet.txt.gz\", \"r\")\n",
    "dat=np.load(f)\n",
    "xx=dat[:,:4]\n",
    "yy=dat[:,4:]\n",
    "strikes=np.array([0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4,1.5 ])\n",
    "maturities=np.array([0.1,0.3,0.6,0.9,1.2,1.5,1.8,2.0 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    xx, yy, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_and_expand(a,x,y):\n",
    "    # use choose and where !\n",
    "    n = len(x)*len(y)\n",
    "    a_index = np.arange(len(a))%n\n",
    "    \n",
    "    \n",
    "    x_index = a_index//len(y)\n",
    "    y_index = a_index%len(y)\n",
    "    \n",
    "    x_added = np.choose(x_index,x.reshape(-1,1)).reshape(-1,1)\n",
    "    y_added = np.choose(y_index,y.reshape(-1,1)).reshape(-1,1)\n",
    "    \n",
    "    return np.hstack([a,x_added,y_added])\n",
    "\n",
    "y_train,y_test = y_train.reshape(-1,8,11),y_test.reshape(-1,8,11)\n",
    "x_train,x_test = np.repeat(x_train, 8*11,axis=0),np.repeat(x_test, 8*11,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test=append_and_expand(x_train,maturities,strikes),append_and_expand(x_test,maturities,strikes)\n",
    "y_train,y_test = y_train.reshape(-1).reshape(-1,1), y_test.reshape(-1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "scale_y=  StandardScaler()\n",
    "\n",
    "def ytransform(y_train,y_test):\n",
    "    return [scale_y.fit_transform(y_train),scale_y.transform(y_test)]\n",
    "\n",
    "def yinversetransform(y):\n",
    "    return scale_y.inverse_transform(y)\n",
    "\n",
    "# Upper and lower bounds used in the training set\n",
    "ub=np.array([0.16,4,-0.1,0.5,2.0,1.5])\n",
    "lb=np.array([0.01,0.3,-0.95,0.025,0.1,0.5])\n",
    "\n",
    "def myscale(x):\n",
    "    return (x - (ub+lb)*0.5)*2/(ub-lb)\n",
    "def myinverse(x):\n",
    "    return x*(ub-lb)*0.5+(ub+lb)*0.5\n",
    "\n",
    "x_train_transform = myscale(x_train)\n",
    "x_test_transform = myscale(x_test)\n",
    "[y_train_transform,y_test_transform] = ytransform(y_train,y_test)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"device is {device}\")\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(x_train_transform).to(device=device),\n",
    "                                               torch.from_numpy(y_train_transform).to(device=device))\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.from_numpy(x_test_transform).to(device=device),\n",
    "                                              torch.from_numpy(y_test_transform).to(device=device))\n",
    "\n",
    "\n",
    "train_data = (torch.from_numpy(x_train_transform).to(device=device),torch.from_numpy(y_train_transform).to(device=device))\n",
    "test_data = (torch.from_numpy(x_test_transform).to(device=device),torch.from_numpy(y_test_transform).to(device=device))\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(train_dataset,batch_size =88,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')  # Add the parent directory to the Python path\n",
    "\n",
    "from torch_NN.nn import ResNN_pricing\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "hyperparas = {'input_dim':6,'hidden_dim':32,'hidden_nums':2,'output_dim':1,'block_layer_nums':2}\n",
    "\n",
    "model = ResNN_pricing(hyperparas=hyperparas).to(device=device,dtype=torch.float64)\n",
    "loss_MSE = nn.MSELoss()\n",
    "optim_Adam = torch.optim.Adam(model.parameters(),lr= 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Epoch: 0----------------------------------\n",
      "Batch: 0,train loss is: 0.9350224465108343\n",
      "test loss is 1.3191236531097355\n",
      "Batch: 100,train loss is: 0.7585126332330467\n",
      "test loss is 0.32679044017801256\n",
      "Batch: 200,train loss is: 0.06201622496455958\n",
      "test loss is 0.19031348747907373\n",
      "Batch: 300,train loss is: 0.04291416440674622\n",
      "test loss is 0.13472471227986357\n",
      "Batch: 400,train loss is: 0.04940528915663738\n",
      "test loss is 0.1013333044831383\n",
      "Batch: 500,train loss is: 0.020549839171460017\n",
      "test loss is 0.08798405824986696\n",
      "Batch: 600,train loss is: 0.031676135806809606\n",
      "test loss is 0.07191770857499215\n",
      "Batch: 700,train loss is: 0.024122471756715388\n",
      "test loss is 0.06357153232637401\n",
      "Batch: 800,train loss is: 0.024270533555906616\n",
      "test loss is 0.05510518458235628\n",
      "Batch: 900,train loss is: 0.06214446048635984\n",
      "test loss is 0.05024927521385726\n",
      "Batch: 1000,train loss is: 0.017333781559597212\n",
      "test loss is 0.042812890559569716\n",
      "Batch: 1100,train loss is: 0.15995411735969653\n",
      "test loss is 0.039620260310715756\n",
      "Batch: 1200,train loss is: 0.008831750977683992\n",
      "test loss is 0.04542367654808917\n",
      "Batch: 1300,train loss is: 0.0069113965107743145\n",
      "test loss is 0.03533926332812894\n",
      "Batch: 1400,train loss is: 0.0076278842676081055\n",
      "test loss is 0.03303382938689579\n",
      "Batch: 1500,train loss is: 0.02579896657973396\n",
      "test loss is 0.02805162058683292\n",
      "Batch: 1600,train loss is: 0.020674104388666577\n",
      "test loss is 0.02883967294193181\n",
      "Batch: 1700,train loss is: 0.08856386919794973\n",
      "test loss is 0.0300891468236862\n",
      "Batch: 1800,train loss is: 0.02967435395384251\n",
      "test loss is 0.023363194638491014\n",
      "Batch: 1900,train loss is: 0.0185366443080114\n",
      "test loss is 0.032240734238320135\n",
      "Batch: 2000,train loss is: 0.01489215493726403\n",
      "test loss is 0.02081283533360744\n",
      "Batch: 2100,train loss is: 0.024641909361837334\n",
      "test loss is 0.020319459005780915\n",
      "Batch: 2200,train loss is: 0.011751227408665497\n",
      "test loss is 0.019460651822366323\n",
      "Batch: 2300,train loss is: 0.019581243388137964\n",
      "test loss is 0.020661444273528753\n",
      "Batch: 2400,train loss is: 0.008273385755700684\n",
      "test loss is 0.019532093572475723\n",
      "Batch: 2500,train loss is: 0.020012243172924396\n",
      "test loss is 0.017021007525950968\n",
      "Batch: 2600,train loss is: 0.01827742445216158\n",
      "test loss is 0.03128623876032271\n",
      "Batch: 2700,train loss is: 0.008527560930744067\n",
      "test loss is 0.018178738138906792\n",
      "Batch: 2800,train loss is: 0.010735979207338564\n",
      "test loss is 0.02637021822937292\n",
      "Batch: 2900,train loss is: 0.012270428863657873\n",
      "test loss is 0.01528880921607141\n",
      "Batch: 3000,train loss is: 0.011570226793262056\n",
      "test loss is 0.015615582528554665\n",
      "Batch: 3100,train loss is: 0.008923896533334627\n",
      "test loss is 0.014304535128950446\n",
      "Batch: 3200,train loss is: 0.015120784743730478\n",
      "test loss is 0.012836838220200526\n",
      "Batch: 3300,train loss is: 0.005236546885529052\n",
      "test loss is 0.013151729229281399\n",
      "Batch: 3400,train loss is: 0.009660580150191857\n",
      "test loss is 0.012080037947430643\n",
      "Batch: 3500,train loss is: 0.004240750361577125\n",
      "test loss is 0.012139227089713132\n",
      "Batch: 3600,train loss is: 0.004303451857712165\n",
      "test loss is 0.013414162327785212\n",
      "Batch: 3700,train loss is: 0.007419886774002954\n",
      "test loss is 0.015543420691401024\n",
      "Batch: 3800,train loss is: 0.006919124271542389\n",
      "test loss is 0.01259835348746486\n",
      "Batch: 3900,train loss is: 0.01234920426526211\n",
      "test loss is 0.015041405430036323\n",
      "Batch: 4000,train loss is: 0.002985315872983882\n",
      "test loss is 0.012638550333395821\n",
      "Batch: 4100,train loss is: 0.007283317393029469\n",
      "test loss is 0.011770482229709797\n",
      "Batch: 4200,train loss is: 0.01618941021629104\n",
      "test loss is 0.01170944881504278\n",
      "Batch: 4300,train loss is: 0.005071327247648064\n",
      "test loss is 0.01772886225348142\n",
      "Batch: 4400,train loss is: 0.005469299695052789\n",
      "test loss is 0.01019138260732628\n",
      "Batch: 4500,train loss is: 0.004440497239837596\n",
      "test loss is 0.008991270272101625\n",
      "Batch: 4600,train loss is: 0.008100070870216452\n",
      "test loss is 0.011516345794728306\n",
      "Batch: 4700,train loss is: 0.02683427323945998\n",
      "test loss is 0.017764732825740783\n",
      "Batch: 4800,train loss is: 0.014437351076481945\n",
      "test loss is 0.04226516151420448\n",
      "Batch: 4900,train loss is: 0.005983876084362335\n",
      "test loss is 0.022882594157145334\n",
      "Batch: 5000,train loss is: 0.001860449851682584\n",
      "test loss is 0.00878326013840352\n",
      "Batch: 5100,train loss is: 0.012061061507026852\n",
      "test loss is 0.011367758322188257\n",
      "Batch: 5200,train loss is: 0.0009764976184956188\n",
      "test loss is 0.01054435153103067\n",
      "Batch: 5300,train loss is: 0.028951017970397647\n",
      "test loss is 0.02405407564861929\n",
      "Batch: 5400,train loss is: 0.028452031699177982\n",
      "test loss is 0.0114982806265393\n",
      "Batch: 5500,train loss is: 0.008584751304237581\n",
      "test loss is 0.010626311662882965\n",
      "Batch: 5600,train loss is: 0.002884197598192543\n",
      "test loss is 0.00781422293723525\n",
      "Batch: 5700,train loss is: 0.0068010938682991785\n",
      "test loss is 0.010154286496002878\n",
      "Batch: 5800,train loss is: 0.008676340015131663\n",
      "test loss is 0.0080993614662538\n",
      "Batch: 5900,train loss is: 0.03466128149507479\n",
      "test loss is 0.01436304937168093\n",
      "Batch: 6000,train loss is: 0.01705608815651953\n",
      "test loss is 0.009772201161588771\n",
      "Batch: 6100,train loss is: 0.006481212509422548\n",
      "test loss is 0.02278092335530776\n",
      "Batch: 6200,train loss is: 0.00527119828777551\n",
      "test loss is 0.007035928992696256\n",
      "Batch: 6300,train loss is: 0.014952825403002357\n",
      "test loss is 0.00834510393078488\n",
      "Batch: 6400,train loss is: 0.006561200319756521\n",
      "test loss is 0.02305871351473806\n",
      "Batch: 6500,train loss is: 0.010947663410028237\n",
      "test loss is 0.010477643255159316\n",
      "Batch: 6600,train loss is: 0.04095790798482013\n",
      "test loss is 0.007618285377710669\n",
      "Batch: 6700,train loss is: 0.0016961912396099758\n",
      "test loss is 0.008201177125270756\n",
      "Batch: 6800,train loss is: 0.05920843687590742\n",
      "test loss is 0.009238084244105601\n",
      "Batch: 6900,train loss is: 0.016402751885451705\n",
      "test loss is 0.007831280747961782\n",
      "Batch: 7000,train loss is: 0.012386150542940769\n",
      "test loss is 0.01846369346630658\n",
      "Batch: 7100,train loss is: 0.0030129032971274307\n",
      "test loss is 0.007711008112240948\n",
      "Batch: 7200,train loss is: 0.0017158626502196167\n",
      "test loss is 0.006657742771042308\n",
      "Batch: 7300,train loss is: 0.01002628461408581\n",
      "test loss is 0.008329123709515322\n",
      "Batch: 7400,train loss is: 0.0037723604479568624\n",
      "test loss is 0.009152750330156363\n",
      "Batch: 7500,train loss is: 0.007097978920296677\n",
      "test loss is 0.019957707597886266\n",
      "Batch: 7600,train loss is: 0.005809827728973807\n",
      "test loss is 0.007898958014568315\n",
      "Batch: 7700,train loss is: 0.027359199781616696\n",
      "test loss is 0.012655991223978714\n",
      "Batch: 7800,train loss is: 0.0019293397311535784\n",
      "test loss is 0.005561634260750691\n",
      "Batch: 7900,train loss is: 0.0043578522234212136\n",
      "test loss is 0.005402266899630296\n",
      "Batch: 8000,train loss is: 0.005360805407873418\n",
      "test loss is 0.007090620259061588\n",
      "Batch: 8100,train loss is: 0.008190605565755304\n",
      "test loss is 0.006677147184124305\n",
      "Batch: 8200,train loss is: 0.01249723253112896\n",
      "test loss is 0.008370143373803459\n",
      "Batch: 8300,train loss is: 0.0053091528509169655\n",
      "test loss is 0.014696605985848056\n",
      "Batch: 8400,train loss is: 0.015719917869294074\n",
      "test loss is 0.008416931218426776\n",
      "Batch: 8500,train loss is: 0.0083716583326923\n",
      "test loss is 0.006585963497179276\n",
      "Batch: 8600,train loss is: 0.00939164229234803\n",
      "test loss is 0.0067347592083760455\n",
      "Batch: 8700,train loss is: 0.006498848830755759\n",
      "test loss is 0.008301384131352648\n",
      "Batch: 8800,train loss is: 0.0032017367509101224\n",
      "test loss is 0.0063296499489882245\n",
      "Batch: 8900,train loss is: 0.006622491675144357\n",
      "test loss is 0.006752588506887472\n",
      "Batch: 9000,train loss is: 0.003401618282739799\n",
      "test loss is 0.007905786010550727\n",
      "Batch: 9100,train loss is: 0.0029962566901293826\n",
      "test loss is 0.006397607567173144\n",
      "Batch: 9200,train loss is: 0.020188431813203178\n",
      "test loss is 0.009101491629877114\n",
      "Batch: 9300,train loss is: 0.004756051801643351\n",
      "test loss is 0.00611928107820854\n",
      "Batch: 9400,train loss is: 0.0023624652212317345\n",
      "test loss is 0.005334494766395119\n",
      "Batch: 9500,train loss is: 0.0025409551108694197\n",
      "test loss is 0.006732456565287121\n",
      "Batch: 9600,train loss is: 0.004651345766261534\n",
      "test loss is 0.005646682070394082\n",
      "Batch: 9700,train loss is: 0.007509927319678041\n",
      "test loss is 0.01112446752894206\n",
      "Batch: 9800,train loss is: 0.003397967339783105\n",
      "test loss is 0.007506040510102783\n",
      "Batch: 9900,train loss is: 0.004673005346587488\n",
      "test loss is 0.006143532314081688\n",
      "Batch: 10000,train loss is: 0.0025350678768350093\n",
      "test loss is 0.014392871634321956\n",
      "Batch: 10100,train loss is: 0.013591364075060125\n",
      "test loss is 0.005220750960853514\n",
      "Batch: 10200,train loss is: 0.02236479091577829\n",
      "test loss is 0.006899479548479151\n",
      "Batch: 10300,train loss is: 0.0030039482919100493\n",
      "test loss is 0.007145100753848128\n",
      "Batch: 10400,train loss is: 0.002582947373100797\n",
      "test loss is 0.007260314468735438\n",
      "Batch: 10500,train loss is: 0.02418421895305872\n",
      "test loss is 0.017037815075007724\n",
      "Batch: 10600,train loss is: 0.010587275477182452\n",
      "test loss is 0.009578170086965796\n",
      "Batch: 10700,train loss is: 0.005292191830588163\n",
      "test loss is 0.0063394913391124075\n",
      "Batch: 10800,train loss is: 0.005406457599664458\n",
      "test loss is 0.005526656494352255\n",
      "Batch: 10900,train loss is: 0.0035121944629386372\n",
      "test loss is 0.005676097021866492\n",
      "Batch: 11000,train loss is: 0.003909837510994052\n",
      "test loss is 0.005599950257034159\n",
      "Batch: 11100,train loss is: 0.003930482475155789\n",
      "test loss is 0.004826474153325943\n",
      "Batch: 11200,train loss is: 0.005215745002898513\n",
      "test loss is 0.00609739289132075\n",
      "Batch: 11300,train loss is: 0.009772203016589149\n",
      "test loss is 0.008586141688912563\n",
      "Batch: 11400,train loss is: 0.004128103111996836\n",
      "test loss is 0.006303501014832237\n",
      "Batch: 11500,train loss is: 0.0016865964263556362\n",
      "test loss is 0.005929148685237822\n",
      "Batch: 11600,train loss is: 0.013399023923109028\n",
      "test loss is 0.00611746829300383\n",
      "Batch: 11700,train loss is: 0.007650795457465797\n",
      "test loss is 0.005761327760313956\n",
      "Batch: 11800,train loss is: 0.00233653736896658\n",
      "test loss is 0.0063935459801995245\n",
      "Batch: 11900,train loss is: 0.007507212567580251\n",
      "test loss is 0.005740616143922935\n",
      "Batch: 12000,train loss is: 0.007581774471989919\n",
      "test loss is 0.006647082691033928\n",
      "Batch: 12100,train loss is: 0.007597817503935312\n",
      "test loss is 0.005176450648224146\n",
      "Batch: 12200,train loss is: 0.004412841133176508\n",
      "test loss is 0.01096176658948265\n",
      "Batch: 12300,train loss is: 0.007074275464822523\n",
      "test loss is 0.007449633203508011\n",
      "Batch: 12400,train loss is: 0.003057501839808485\n",
      "test loss is 0.005858027267766869\n",
      "Batch: 12500,train loss is: 0.006468383661685287\n",
      "test loss is 0.005040243524884317\n",
      "Batch: 12600,train loss is: 0.007269352206452312\n",
      "test loss is 0.006322546491645926\n",
      "Batch: 12700,train loss is: 0.0026621103717369447\n",
      "test loss is 0.004795539809805592\n",
      "Batch: 12800,train loss is: 0.0013342318410817244\n",
      "test loss is 0.0046461495140697655\n",
      "Batch: 12900,train loss is: 0.025282711155841374\n",
      "test loss is 0.011361492320671301\n",
      "Batch: 13000,train loss is: 0.007545283757074036\n",
      "test loss is 0.007128679259644569\n",
      "Batch: 13100,train loss is: 0.003523822418110764\n",
      "test loss is 0.005410757951892236\n",
      "Batch: 13200,train loss is: 0.006631603895983856\n",
      "test loss is 0.005941399539216097\n",
      "Batch: 13300,train loss is: 0.008252286488777082\n",
      "test loss is 0.006193520352884304\n",
      "Batch: 13400,train loss is: 0.0022102752612645437\n",
      "test loss is 0.008406559974368661\n",
      "Batch: 13500,train loss is: 0.003828790178981751\n",
      "test loss is 0.004459402122006729\n",
      "Batch: 13600,train loss is: 0.0036419777868114214\n",
      "test loss is 0.0052199426428395975\n",
      "Batch: 13700,train loss is: 0.0030445367989401684\n",
      "test loss is 0.004875461443075971\n",
      "Batch: 13800,train loss is: 0.002961174470985287\n",
      "test loss is 0.005578698196381105\n",
      "Batch: 13900,train loss is: 0.001314868593031853\n",
      "test loss is 0.004443126981384685\n",
      "Batch: 14000,train loss is: 0.0015767899188319466\n",
      "test loss is 0.005423725655155115\n",
      "Batch: 14100,train loss is: 0.0023318050000304693\n",
      "test loss is 0.006170237020666044\n",
      "Batch: 14200,train loss is: 0.002677885260364555\n",
      "test loss is 0.005463552081981137\n",
      "Batch: 14300,train loss is: 0.0020736784239543658\n",
      "test loss is 0.004073969077937897\n",
      "Batch: 14400,train loss is: 0.0053149604266295\n",
      "test loss is 0.004090948846449181\n",
      "Batch: 14500,train loss is: 0.0019982523610025645\n",
      "test loss is 0.004147278964331577\n",
      "Batch: 14600,train loss is: 0.00929009925613954\n",
      "test loss is 0.004663509221844665\n",
      "Batch: 14700,train loss is: 0.0058757866027499776\n",
      "test loss is 0.008192389134592986\n",
      "Batch: 14800,train loss is: 0.007046207894420499\n",
      "test loss is 0.008035963020437888\n",
      "Batch: 14900,train loss is: 0.0065576419082808025\n",
      "test loss is 0.006253665069142457\n",
      "Batch: 15000,train loss is: 0.0032656737143715977\n",
      "test loss is 0.004023603557540582\n",
      "Batch: 15100,train loss is: 0.002812706286293007\n",
      "test loss is 0.003667853890797296\n",
      "Batch: 15200,train loss is: 0.010051028150651687\n",
      "test loss is 0.00405443052734519\n",
      "Batch: 15300,train loss is: 0.009715558310631468\n",
      "test loss is 0.00459200430607392\n",
      "Batch: 15400,train loss is: 0.004664683585254448\n",
      "test loss is 0.006160809004639813\n",
      "Batch: 15500,train loss is: 0.0026486030258135516\n",
      "test loss is 0.004919767033944022\n",
      "Batch: 15600,train loss is: 0.0026229694097572613\n",
      "test loss is 0.005494620493165099\n",
      "Batch: 15700,train loss is: 0.006311420377849083\n",
      "test loss is 0.004620218113888446\n",
      "Batch: 15800,train loss is: 0.0034810602736065205\n",
      "test loss is 0.004754820538509169\n",
      "Batch: 15900,train loss is: 0.004090806654991716\n",
      "test loss is 0.0061601297470808385\n",
      "Batch: 16000,train loss is: 0.003633945170357261\n",
      "test loss is 0.007773492302024682\n",
      "Batch: 16100,train loss is: 0.009260797262804139\n",
      "test loss is 0.005786144233127397\n",
      "Batch: 16200,train loss is: 0.005479918908020113\n",
      "test loss is 0.004465216483940616\n",
      "Batch: 16300,train loss is: 0.004820701510122561\n",
      "test loss is 0.004108039243080032\n",
      "Batch: 16400,train loss is: 0.001550076251297641\n",
      "test loss is 0.0035590871423301835\n",
      "Batch: 16500,train loss is: 0.011139144998247192\n",
      "test loss is 0.009309091270195208\n",
      "Batch: 16600,train loss is: 0.0027700904112672687\n",
      "test loss is 0.006490497883572699\n",
      "Batch: 16700,train loss is: 0.004416522800139236\n",
      "test loss is 0.00434715497210586\n",
      "Batch: 16800,train loss is: 0.0020336006102963907\n",
      "test loss is 0.00469112797029255\n",
      "Batch: 16900,train loss is: 0.023784441247577243\n",
      "test loss is 0.0057662397403677\n",
      "Batch: 17000,train loss is: 0.005615961499302376\n",
      "test loss is 0.004986563258680784\n",
      "Batch: 17100,train loss is: 0.0023636794829402625\n",
      "test loss is 0.004008607276156714\n",
      "Batch: 17200,train loss is: 0.009094566302807318\n",
      "test loss is 0.0038527043162851597\n",
      "Batch: 17300,train loss is: 0.006787207454949149\n",
      "test loss is 0.0033272336114915668\n",
      "Batch: 17400,train loss is: 0.0030630076303482107\n",
      "test loss is 0.005004361673807758\n",
      "Batch: 17500,train loss is: 0.006700427473793894\n",
      "test loss is 0.004850198145245973\n",
      "Batch: 17600,train loss is: 0.0015638271631006916\n",
      "test loss is 0.00410714885221357\n",
      "Batch: 17700,train loss is: 0.003677081460845535\n",
      "test loss is 0.005535263702838759\n",
      "Batch: 17800,train loss is: 0.007637702296504259\n",
      "test loss is 0.005311545080484865\n",
      "Batch: 17900,train loss is: 0.006513305449156093\n",
      "test loss is 0.003610898591460787\n",
      "Batch: 18000,train loss is: 0.0017872087437082872\n",
      "test loss is 0.0031407821713654504\n",
      "Batch: 18100,train loss is: 0.0018256753516292378\n",
      "test loss is 0.005185452372090583\n",
      "Batch: 18200,train loss is: 0.0024206802034815113\n",
      "test loss is 0.005189818461654253\n",
      "Batch: 18300,train loss is: 0.010655794599316747\n",
      "test loss is 0.00647020192384901\n",
      "Batch: 18400,train loss is: 0.007865220548481051\n",
      "test loss is 0.0038735496083064256\n",
      "Batch: 18500,train loss is: 0.004931795180226816\n",
      "test loss is 0.010664804176715444\n",
      "Batch: 18600,train loss is: 0.008547825066913357\n",
      "test loss is 0.010544283068320021\n",
      "Batch: 18700,train loss is: 0.0012401249806900897\n",
      "test loss is 0.005230995126680459\n",
      "Batch: 18800,train loss is: 0.006305619154480423\n",
      "test loss is 0.004367347829078972\n",
      "Batch: 18900,train loss is: 0.007704650118878363\n",
      "test loss is 0.004335011364977482\n",
      "Batch: 19000,train loss is: 0.0015500756968032726\n",
      "test loss is 0.003546799206115499\n",
      "Batch: 19100,train loss is: 0.0024137590378793075\n",
      "test loss is 0.004717994982269755\n",
      "Batch: 19200,train loss is: 0.007070997787210061\n",
      "test loss is 0.004703995864036774\n",
      "Batch: 19300,train loss is: 0.0051017047982165435\n",
      "test loss is 0.004866312239824173\n",
      "Batch: 19400,train loss is: 0.004255252373124221\n",
      "test loss is 0.004378921197368578\n",
      "Batch: 19500,train loss is: 0.0033312262116019687\n",
      "test loss is 0.005329015001925451\n",
      "Batch: 19600,train loss is: 0.002288946948532611\n",
      "test loss is 0.0035073172909015097\n",
      "Batch: 19700,train loss is: 0.009338329255700493\n",
      "test loss is 0.005531554041211851\n",
      "Batch: 19800,train loss is: 0.0018275469014898947\n",
      "test loss is 0.0037404947895182186\n",
      "Batch: 19900,train loss is: 0.0013480328815072442\n",
      "test loss is 0.007439816895097869\n",
      "Batch: 20000,train loss is: 0.006967600733796555\n",
      "test loss is 0.004914069568144665\n",
      "Batch: 20100,train loss is: 0.005037533020818336\n",
      "test loss is 0.0033022578592118634\n",
      "Batch: 20200,train loss is: 0.0018361624275454941\n",
      "test loss is 0.004191405069837381\n",
      "Batch: 20300,train loss is: 0.0017040151114911194\n",
      "test loss is 0.0033701662363586665\n",
      "Batch: 20400,train loss is: 0.0030311056745394582\n",
      "test loss is 0.007166767007615829\n",
      "Batch: 20500,train loss is: 0.0025705402403318894\n",
      "test loss is 0.004677547298662297\n",
      "Batch: 20600,train loss is: 0.004747161898508144\n",
      "test loss is 0.0049153833710429366\n",
      "Batch: 20700,train loss is: 0.0022670554281583213\n",
      "test loss is 0.0035506637405721183\n",
      "Batch: 20800,train loss is: 0.003933432709818005\n",
      "test loss is 0.004206980734641771\n",
      "Batch: 20900,train loss is: 0.004286008507111944\n",
      "test loss is 0.003709372342121547\n",
      "Batch: 21000,train loss is: 0.009080125104812794\n",
      "test loss is 0.005054430258437054\n",
      "Batch: 21100,train loss is: 0.004543086394744207\n",
      "test loss is 0.004434160452994657\n",
      "Batch: 21200,train loss is: 0.003681975248414058\n",
      "test loss is 0.005610434967557664\n",
      "Batch: 21300,train loss is: 0.002899702235105636\n",
      "test loss is 0.003296359117480542\n",
      "Batch: 21400,train loss is: 0.004433827015513122\n",
      "test loss is 0.003737054939105595\n",
      "Batch: 21500,train loss is: 0.009769499826978643\n",
      "test loss is 0.008000750936039915\n",
      "Batch: 21600,train loss is: 0.013999431871825447\n",
      "test loss is 0.0175182194281454\n",
      "Batch: 21700,train loss is: 0.0012609039199261434\n",
      "test loss is 0.003933078430221733\n",
      "Batch: 21800,train loss is: 0.0035451457984950864\n",
      "test loss is 0.002836288628689048\n",
      "Batch: 21900,train loss is: 0.003048467295346364\n",
      "test loss is 0.0041333920234353436\n",
      "Batch: 22000,train loss is: 0.0019176270369339616\n",
      "test loss is 0.004531413788448017\n",
      "Batch: 22100,train loss is: 0.006993535039947635\n",
      "test loss is 0.003928039279104594\n",
      "Batch: 22200,train loss is: 0.0024310074409785013\n",
      "test loss is 0.004385144792605709\n",
      "Batch: 22300,train loss is: 0.004979907952463478\n",
      "test loss is 0.0047721468357348985\n",
      "Batch: 22400,train loss is: 0.001242086012125667\n",
      "test loss is 0.0050761251681565684\n",
      "Batch: 22500,train loss is: 0.001892565825715102\n",
      "test loss is 0.0033841256165402247\n",
      "Batch: 22600,train loss is: 0.005602004902680444\n",
      "test loss is 0.010279608262114983\n",
      "Batch: 22700,train loss is: 0.0018608665998702555\n",
      "test loss is 0.0043213064944832135\n",
      "Batch: 22800,train loss is: 0.007928684140102392\n",
      "test loss is 0.0036106191784480075\n",
      "Batch: 22900,train loss is: 0.0014124825847545463\n",
      "test loss is 0.003455979228630541\n",
      "Batch: 23000,train loss is: 0.004759668607057488\n",
      "test loss is 0.0032351550093766123\n",
      "Batch: 23100,train loss is: 0.0031396708802082446\n",
      "test loss is 0.004171210346682612\n",
      "Batch: 23200,train loss is: 0.001376026409967797\n",
      "test loss is 0.004796043370660205\n",
      "Batch: 23300,train loss is: 0.0022168660786093777\n",
      "test loss is 0.0030177930459705958\n",
      "Batch: 23400,train loss is: 0.002480995226428068\n",
      "test loss is 0.003760429227911811\n",
      "Batch: 23500,train loss is: 0.004261387381636586\n",
      "test loss is 0.005436372631781463\n",
      "Batch: 23600,train loss is: 0.0010496063950806724\n",
      "test loss is 0.0029017905468270563\n",
      "Batch: 23700,train loss is: 0.00241088245448991\n",
      "test loss is 0.003040979960256841\n",
      "Batch: 23800,train loss is: 0.006619429334739071\n",
      "test loss is 0.003903136577272743\n",
      "Batch: 23900,train loss is: 0.008645388207891506\n",
      "test loss is 0.003305580389583239\n",
      "Batch: 24000,train loss is: 0.010379781264700537\n",
      "test loss is 0.004644883817418235\n",
      "Batch: 24100,train loss is: 0.011168851351918339\n",
      "test loss is 0.0033760661931292683\n",
      "Batch: 24200,train loss is: 0.008642441893644987\n",
      "test loss is 0.0034647583203278073\n",
      "Batch: 24300,train loss is: 0.0074134946892981885\n",
      "test loss is 0.004850491085093114\n",
      "Batch: 24400,train loss is: 0.01256867391932671\n",
      "test loss is 0.004660324736932041\n",
      "Batch: 24500,train loss is: 0.002540478940683172\n",
      "test loss is 0.003927303806436691\n",
      "Batch: 24600,train loss is: 0.003184887485482077\n",
      "test loss is 0.003528664626848427\n",
      "Batch: 24700,train loss is: 0.0015150964692275891\n",
      "test loss is 0.003105297065791003\n",
      "Batch: 24800,train loss is: 0.02076380625533437\n",
      "test loss is 0.005944684174165381\n",
      "Batch: 24900,train loss is: 0.012828532404469266\n",
      "test loss is 0.009786675429606937\n",
      "Batch: 25000,train loss is: 0.0010603914713592656\n",
      "test loss is 0.0030918116909421\n",
      "Batch: 25100,train loss is: 0.000844318695056834\n",
      "test loss is 0.00362615275341134\n",
      "Batch: 25200,train loss is: 0.00585626436220843\n",
      "test loss is 0.0026754808962307494\n",
      "Batch: 25300,train loss is: 0.0034871787807423684\n",
      "test loss is 0.0029195478875788687\n",
      "Batch: 25400,train loss is: 0.001528485893202563\n",
      "test loss is 0.002880184214743136\n",
      "Batch: 25500,train loss is: 0.0022703704843198866\n",
      "test loss is 0.002867273788641469\n",
      "Batch: 25600,train loss is: 0.0013950412165777794\n",
      "test loss is 0.0031941674284083513\n",
      "Batch: 25700,train loss is: 0.0024634834441959797\n",
      "test loss is 0.002792533882944636\n",
      "Batch: 25800,train loss is: 0.004843065914006833\n",
      "test loss is 0.004395874266293901\n",
      "Batch: 25900,train loss is: 0.0024790101491724468\n",
      "test loss is 0.007687161294380063\n",
      "Batch: 26000,train loss is: 0.020550344257172056\n",
      "test loss is 0.0045114580364083124\n",
      "Batch: 26100,train loss is: 0.0034288985882765517\n",
      "test loss is 0.003607711103592183\n",
      "Batch: 26200,train loss is: 0.0035980320832212542\n",
      "test loss is 0.005453724921315418\n",
      "Batch: 26300,train loss is: 0.005862457248926387\n",
      "test loss is 0.006199802938387795\n",
      "Batch: 26400,train loss is: 0.001482695834911311\n",
      "test loss is 0.003181468498889935\n",
      "Batch: 26500,train loss is: 0.0018742675740749336\n",
      "test loss is 0.0035229258228664817\n",
      "Batch: 26600,train loss is: 0.002286272652095294\n",
      "test loss is 0.004579337734209017\n",
      "Batch: 26700,train loss is: 0.002416223794242722\n",
      "test loss is 0.003484729586109737\n",
      "Batch: 26800,train loss is: 0.004297948239245885\n",
      "test loss is 0.003612702588811751\n",
      "Batch: 26900,train loss is: 0.011686839937527263\n",
      "test loss is 0.005381872408993138\n",
      "Batch: 27000,train loss is: 0.0016428373590630636\n",
      "test loss is 0.00567455576366189\n",
      "Batch: 27100,train loss is: 0.00844219234395705\n",
      "test loss is 0.0035594331224000123\n",
      "Batch: 27200,train loss is: 0.010984490253164439\n",
      "test loss is 0.00567653552109965\n",
      "Batch: 27300,train loss is: 0.0016680690753640396\n",
      "test loss is 0.0031698316020523517\n",
      "Batch: 27400,train loss is: 0.002294901668896172\n",
      "test loss is 0.0027484384925367073\n",
      "Batch: 27500,train loss is: 0.00950661504502009\n",
      "test loss is 0.004374402101078572\n",
      "Batch: 27600,train loss is: 0.005508912187286318\n",
      "test loss is 0.007239748157178932\n",
      "Batch: 27700,train loss is: 0.01614554583052783\n",
      "test loss is 0.0033499620615358846\n",
      "Batch: 27800,train loss is: 0.002963567433682053\n",
      "test loss is 0.007506827946817227\n",
      "Batch: 27900,train loss is: 0.00579446448556874\n",
      "test loss is 0.0033600349530039843\n",
      "Batch: 28000,train loss is: 0.002368490137301273\n",
      "test loss is 0.003679581158699345\n",
      "Batch: 28100,train loss is: 0.006213818992279904\n",
      "test loss is 0.004822036145306227\n",
      "Batch: 28200,train loss is: 0.016148554051464144\n",
      "test loss is 0.006665241135968716\n",
      "Batch: 28300,train loss is: 0.0047116456918111956\n",
      "test loss is 0.0030360909269784076\n",
      "Batch: 28400,train loss is: 0.0019162248611354848\n",
      "test loss is 0.003022752484610181\n",
      "Batch: 28500,train loss is: 0.0035604636097876473\n",
      "test loss is 0.0029548479658985808\n",
      "Batch: 28600,train loss is: 0.009798103290456111\n",
      "test loss is 0.0033403692288328213\n",
      "Batch: 28700,train loss is: 0.005086763328876337\n",
      "test loss is 0.00364148833035304\n",
      "Batch: 28800,train loss is: 0.0053854842149800245\n",
      "test loss is 0.0027845935791130905\n",
      "Batch: 28900,train loss is: 0.003650367207831037\n",
      "test loss is 0.0037256330879936825\n",
      "Batch: 29000,train loss is: 0.0021412458912767134\n",
      "test loss is 0.003327288651728451\n",
      "Batch: 29100,train loss is: 0.003464316054287571\n",
      "test loss is 0.003127441439750107\n",
      "Batch: 29200,train loss is: 0.004445800596223706\n",
      "test loss is 0.00932101973986453\n",
      "Batch: 29300,train loss is: 0.004579397072544242\n",
      "test loss is 0.002876152625005921\n",
      "Batch: 29400,train loss is: 0.0013892149142377373\n",
      "test loss is 0.0026823748395730872\n",
      "Batch: 29500,train loss is: 0.0016705724774048526\n",
      "test loss is 0.003776435337805348\n",
      "Batch: 29600,train loss is: 0.00432512160961484\n",
      "test loss is 0.0025029099254833802\n",
      "Batch: 29700,train loss is: 0.001434412904147944\n",
      "test loss is 0.004583127317159425\n",
      "Batch: 29800,train loss is: 0.005179540219531463\n",
      "test loss is 0.002684425288137009\n",
      "Batch: 29900,train loss is: 0.0079221618595888\n",
      "test loss is 0.0028467869247939615\n",
      "Batch: 30000,train loss is: 0.0018521476717847608\n",
      "test loss is 0.0028995542378597756\n",
      "Batch: 30100,train loss is: 0.002157838229818163\n",
      "test loss is 0.002829422302801982\n",
      "Batch: 30200,train loss is: 0.004500867595421203\n",
      "test loss is 0.003500480595177944\n",
      "Batch: 30300,train loss is: 0.0014481313084800048\n",
      "test loss is 0.0036278487844288833\n",
      "Batch: 30400,train loss is: 0.0026531723794925494\n",
      "test loss is 0.003491720957128097\n",
      "Batch: 30500,train loss is: 0.0013057284309105147\n",
      "test loss is 0.0031703635276977165\n",
      "Batch: 30600,train loss is: 0.0019830801992106375\n",
      "test loss is 0.0034641988862351516\n",
      "Batch: 30700,train loss is: 0.0015907484197710587\n",
      "test loss is 0.004115267716006711\n",
      "Batch: 30800,train loss is: 0.004930689001457758\n",
      "test loss is 0.003427498368321358\n",
      "Batch: 30900,train loss is: 0.00658274785229219\n",
      "test loss is 0.0034239947607871877\n",
      "Batch: 31000,train loss is: 0.0013837367269502706\n",
      "test loss is 0.003277077290623897\n",
      "Batch: 31100,train loss is: 0.0022197344217795537\n",
      "test loss is 0.004608929699781797\n",
      "Batch: 31200,train loss is: 0.0020483566362287093\n",
      "test loss is 0.002971858925360128\n",
      "Batch: 31300,train loss is: 0.003181805273529685\n",
      "test loss is 0.0028806532170110124\n",
      "Batch: 31400,train loss is: 0.005593881649175656\n",
      "test loss is 0.004168367524244383\n",
      "Batch: 31500,train loss is: 0.0026279567351247626\n",
      "test loss is 0.003735726810316928\n",
      "Batch: 31600,train loss is: 0.0023697192730934733\n",
      "test loss is 0.002607413721643773\n",
      "Batch: 31700,train loss is: 0.001585519330860987\n",
      "test loss is 0.002442242608677162\n",
      "Batch: 31800,train loss is: 0.003355600300297774\n",
      "test loss is 0.005265679661572354\n",
      "Batch: 31900,train loss is: 0.00197481704389538\n",
      "test loss is 0.002904714594053606\n",
      "Batch: 32000,train loss is: 0.017732387641636992\n",
      "test loss is 0.002730881105587415\n",
      "Batch: 32100,train loss is: 0.001457599817786067\n",
      "test loss is 0.0036329338425356954\n",
      "Batch: 32200,train loss is: 0.0013530291179715645\n",
      "test loss is 0.0035697143111375093\n",
      "Batch: 32300,train loss is: 0.001615544230669594\n",
      "test loss is 0.0029909997852560904\n",
      "Batch: 32400,train loss is: 0.0012442907967982067\n",
      "test loss is 0.004562450387537036\n",
      "Batch: 32500,train loss is: 0.01642289901843498\n",
      "test loss is 0.013230506489826592\n",
      "Batch: 32600,train loss is: 0.004921743994689779\n",
      "test loss is 0.003375597302022087\n",
      "Batch: 32700,train loss is: 0.0013338261018093858\n",
      "test loss is 0.003944189808497974\n",
      "Batch: 32800,train loss is: 0.0021307108512240063\n",
      "test loss is 0.0025379060201885183\n",
      "Batch: 32900,train loss is: 0.0031050026332098512\n",
      "test loss is 0.002448062250172132\n",
      "Batch: 33000,train loss is: 0.0015488654497689762\n",
      "test loss is 0.002609595384250297\n",
      "Batch: 33100,train loss is: 0.003121514358530504\n",
      "test loss is 0.005071824637078574\n",
      "Batch: 33200,train loss is: 0.0012447271862365457\n",
      "test loss is 0.002450127580491902\n",
      "Batch: 33300,train loss is: 0.004675412753110006\n",
      "test loss is 0.002883056160996438\n",
      "Batch: 33400,train loss is: 0.0019159832028123995\n",
      "test loss is 0.0032214212116355276\n",
      "Batch: 33500,train loss is: 0.003166367728292864\n",
      "test loss is 0.003152787063853207\n",
      "Batch: 33600,train loss is: 0.0006904766221138671\n",
      "test loss is 0.0028032795604693273\n",
      "Batch: 33700,train loss is: 0.005575540620710561\n",
      "test loss is 0.0048835973596467455\n",
      "Batch: 33800,train loss is: 0.01532507795347195\n",
      "test loss is 0.007499433177812725\n",
      "Batch: 33900,train loss is: 0.004702291031421454\n",
      "test loss is 0.0028644627353213714\n",
      "-----------------------Epoch: 1----------------------------------\n",
      "Batch: 0,train loss is: 0.0015779592706776464\n",
      "test loss is 0.0030415006013375188\n",
      "Batch: 100,train loss is: 0.0021352689636429285\n",
      "test loss is 0.002977853767057204\n",
      "Batch: 200,train loss is: 0.00466663244194393\n",
      "test loss is 0.004419576110974392\n",
      "Batch: 300,train loss is: 0.002150017096472774\n",
      "test loss is 0.0038259232491530183\n",
      "Batch: 400,train loss is: 0.0020473457689566344\n",
      "test loss is 0.004054905291561364\n",
      "Batch: 500,train loss is: 0.0009063293916212574\n",
      "test loss is 0.0024561941881400353\n",
      "Batch: 600,train loss is: 0.0007732922271545858\n",
      "test loss is 0.004035384023569875\n",
      "Batch: 700,train loss is: 0.0016535457785559933\n",
      "test loss is 0.00263988746078415\n",
      "Batch: 800,train loss is: 0.001744960166793729\n",
      "test loss is 0.0021599859663551336\n",
      "Batch: 900,train loss is: 0.0019955842824593184\n",
      "test loss is 0.005350820674125331\n",
      "Batch: 1000,train loss is: 0.0014026767379913016\n",
      "test loss is 0.0036683011008528457\n",
      "Batch: 1100,train loss is: 0.015823342962207963\n",
      "test loss is 0.004203305808789988\n",
      "Batch: 1200,train loss is: 0.0012784754331259423\n",
      "test loss is 0.005546276525791932\n",
      "Batch: 1300,train loss is: 0.00501904967657818\n",
      "test loss is 0.00319401068753307\n",
      "Batch: 1400,train loss is: 0.001543003408459357\n",
      "test loss is 0.002632959060636937\n",
      "Batch: 1500,train loss is: 0.0022743657448976644\n",
      "test loss is 0.00392729500414139\n",
      "Batch: 1600,train loss is: 0.0009225041487561424\n",
      "test loss is 0.004705939753246151\n",
      "Batch: 1700,train loss is: 0.012924574816736232\n",
      "test loss is 0.003358297648341231\n",
      "Batch: 1800,train loss is: 0.0007770253986401088\n",
      "test loss is 0.0026138063053284155\n",
      "Batch: 1900,train loss is: 0.0017986360712658786\n",
      "test loss is 0.003138685000080157\n",
      "Batch: 2000,train loss is: 0.005120478070024151\n",
      "test loss is 0.003536854680534227\n",
      "Batch: 2100,train loss is: 0.0016667150330393955\n",
      "test loss is 0.0034672767074364614\n",
      "Batch: 2200,train loss is: 0.003962319392165805\n",
      "test loss is 0.0028800589887159295\n",
      "Batch: 2300,train loss is: 0.0009040383910710859\n",
      "test loss is 0.0021279152707072676\n",
      "Batch: 2400,train loss is: 0.0016145688743721617\n",
      "test loss is 0.0023624444698819337\n",
      "Batch: 2500,train loss is: 0.0038002434507697132\n",
      "test loss is 0.0032789390673526033\n",
      "Batch: 2600,train loss is: 0.0033136058272143897\n",
      "test loss is 0.004841995937985554\n",
      "Batch: 2700,train loss is: 0.0014580196110469663\n",
      "test loss is 0.0026829910213389888\n",
      "Batch: 2800,train loss is: 0.003075520008005052\n",
      "test loss is 0.0035250563483503142\n",
      "Batch: 2900,train loss is: 0.00216129527989265\n",
      "test loss is 0.003246662591507153\n",
      "Batch: 3000,train loss is: 0.0026150271954126863\n",
      "test loss is 0.002901003446339387\n",
      "Batch: 3100,train loss is: 0.0020987371962293624\n",
      "test loss is 0.0025694110180626557\n",
      "Batch: 3200,train loss is: 0.007873043141277002\n",
      "test loss is 0.003400568413526092\n",
      "Batch: 3300,train loss is: 0.001416409236546233\n",
      "test loss is 0.002383560185316485\n",
      "Batch: 3400,train loss is: 0.0029643285618718494\n",
      "test loss is 0.0026168020201544394\n",
      "Batch: 3500,train loss is: 0.0037728514365973336\n",
      "test loss is 0.0026476790816120613\n",
      "Batch: 3600,train loss is: 0.0010467398701009844\n",
      "test loss is 0.005814597784430936\n",
      "Batch: 3700,train loss is: 0.0011686264345148825\n",
      "test loss is 0.0033549971990134036\n",
      "Batch: 3800,train loss is: 0.0008166410836234515\n",
      "test loss is 0.0031400372772459628\n",
      "Batch: 3900,train loss is: 0.005257257682067332\n",
      "test loss is 0.0025617447063235803\n",
      "Batch: 4000,train loss is: 0.0009168707099920017\n",
      "test loss is 0.003018903727535129\n",
      "Batch: 4100,train loss is: 0.00250093504964443\n",
      "test loss is 0.002769214386823895\n",
      "Batch: 4200,train loss is: 0.006610736950187087\n",
      "test loss is 0.0036672006004283635\n",
      "Batch: 4300,train loss is: 0.0012860449880476334\n",
      "test loss is 0.0027706669190546234\n",
      "Batch: 4400,train loss is: 0.002168883792977839\n",
      "test loss is 0.0035953301236951395\n",
      "Batch: 4500,train loss is: 0.014647270940315336\n",
      "test loss is 0.009729536186967322\n",
      "Batch: 4600,train loss is: 0.001158347157913577\n",
      "test loss is 0.004298899660431188\n",
      "Batch: 4700,train loss is: 0.00706486644231992\n",
      "test loss is 0.003867890656584188\n",
      "Batch: 4800,train loss is: 0.004980833166081258\n",
      "test loss is 0.004400519951945315\n",
      "Batch: 4900,train loss is: 0.0024220527963783405\n",
      "test loss is 0.00260875809127063\n",
      "Batch: 5000,train loss is: 0.0009091803288415964\n",
      "test loss is 0.00229303206302976\n",
      "Batch: 5100,train loss is: 0.003648079679744012\n",
      "test loss is 0.002726756862417642\n",
      "Batch: 5200,train loss is: 0.0016753755969999588\n",
      "test loss is 0.0022892747235314197\n",
      "Batch: 5300,train loss is: 0.0067372932862441535\n",
      "test loss is 0.007920834869245677\n",
      "Batch: 5400,train loss is: 0.003651561781827178\n",
      "test loss is 0.004097449656444851\n",
      "Batch: 5500,train loss is: 0.0037682531595774793\n",
      "test loss is 0.0037021571151334624\n",
      "Batch: 5600,train loss is: 0.001026359520695676\n",
      "test loss is 0.002130663502931437\n",
      "Batch: 5700,train loss is: 0.0012351545626812287\n",
      "test loss is 0.0031823949359795468\n",
      "Batch: 5800,train loss is: 0.0025417242158600356\n",
      "test loss is 0.0031194176210928176\n",
      "Batch: 5900,train loss is: 0.005822293550891109\n",
      "test loss is 0.0039595000116339355\n",
      "Batch: 6000,train loss is: 0.023109882360342216\n",
      "test loss is 0.006757832412750492\n",
      "Batch: 6100,train loss is: 0.0005975221923760933\n",
      "test loss is 0.004845945943507645\n",
      "Batch: 6200,train loss is: 0.00344259841690618\n",
      "test loss is 0.0023044063416672806\n",
      "Batch: 6300,train loss is: 0.002157322587386731\n",
      "test loss is 0.0027355557280300457\n",
      "Batch: 6400,train loss is: 0.002617711303691134\n",
      "test loss is 0.006841715202563473\n",
      "Batch: 6500,train loss is: 0.0038819384215899666\n",
      "test loss is 0.003285203204950135\n",
      "Batch: 6600,train loss is: 0.0063518450093780114\n",
      "test loss is 0.0035560321630667023\n",
      "Batch: 6700,train loss is: 0.001159751343434274\n",
      "test loss is 0.0038206583981946433\n",
      "Batch: 6800,train loss is: 0.0031399038097135777\n",
      "test loss is 0.002415173630218709\n",
      "Batch: 6900,train loss is: 0.0027246773648452773\n",
      "test loss is 0.0020515768711625707\n",
      "Batch: 7000,train loss is: 0.0016507653302382135\n",
      "test loss is 0.0055060683518281485\n",
      "Batch: 7100,train loss is: 0.002130566190645883\n",
      "test loss is 0.002851859354361288\n",
      "Batch: 7200,train loss is: 0.003284782407422572\n",
      "test loss is 0.0036000047853275065\n",
      "Batch: 7300,train loss is: 0.00352763631225214\n",
      "test loss is 0.0031285465116746313\n",
      "Batch: 7400,train loss is: 0.0018404361777093223\n",
      "test loss is 0.005759479701312848\n",
      "Batch: 7500,train loss is: 0.003544892292832699\n",
      "test loss is 0.006079925295189578\n",
      "Batch: 7600,train loss is: 0.0033086167101943546\n",
      "test loss is 0.003248706789675821\n",
      "Batch: 7700,train loss is: 0.0030369703342616136\n",
      "test loss is 0.002242123316380217\n",
      "Batch: 7800,train loss is: 0.0014283679101780831\n",
      "test loss is 0.002423296538825319\n",
      "Batch: 7900,train loss is: 0.0009016896937984099\n",
      "test loss is 0.0021970119774876212\n",
      "Batch: 8000,train loss is: 0.0010716403434317143\n",
      "test loss is 0.002290936531909271\n",
      "Batch: 8100,train loss is: 0.003891031600389841\n",
      "test loss is 0.0026032820619431786\n",
      "Batch: 8200,train loss is: 0.0027457977253334273\n",
      "test loss is 0.002399140140115947\n",
      "Batch: 8300,train loss is: 0.0009639021770782518\n",
      "test loss is 0.002374990306517082\n",
      "Batch: 8400,train loss is: 0.0031869791390656795\n",
      "test loss is 0.0026980823763660816\n",
      "Batch: 8500,train loss is: 0.0012664841382249192\n",
      "test loss is 0.002537130709742993\n",
      "Batch: 8600,train loss is: 0.001565074490203674\n",
      "test loss is 0.003759889901183439\n",
      "Batch: 8700,train loss is: 0.001370040163354563\n",
      "test loss is 0.0045307296785630944\n",
      "Batch: 8800,train loss is: 0.0021208429611267493\n",
      "test loss is 0.003999580779332779\n",
      "Batch: 8900,train loss is: 0.0038461246781828977\n",
      "test loss is 0.0020979856916122238\n",
      "Batch: 9000,train loss is: 0.0006226401994970574\n",
      "test loss is 0.002717999385386599\n",
      "Batch: 9100,train loss is: 0.0006446533602965212\n",
      "test loss is 0.003320562040951081\n",
      "Batch: 9200,train loss is: 0.008261060057768353\n",
      "test loss is 0.0050122852547340516\n",
      "Batch: 9300,train loss is: 0.0017832568754787084\n",
      "test loss is 0.0025144950290261294\n",
      "Batch: 9400,train loss is: 0.0030476114074799443\n",
      "test loss is 0.0034008976020457136\n",
      "Batch: 9500,train loss is: 0.0012506110736006278\n",
      "test loss is 0.003242942505099658\n",
      "Batch: 9600,train loss is: 0.0030420661918754196\n",
      "test loss is 0.002591611403722889\n",
      "Batch: 9700,train loss is: 0.0015368807369638983\n",
      "test loss is 0.0020764589020343654\n",
      "Batch: 9800,train loss is: 0.000753545932032682\n",
      "test loss is 0.003553995936157787\n",
      "Batch: 9900,train loss is: 0.0020009850609638688\n",
      "test loss is 0.0026354215452321977\n",
      "Batch: 10000,train loss is: 0.0010513576731369483\n",
      "test loss is 0.003968916088177346\n",
      "Batch: 10100,train loss is: 0.006103174144987935\n",
      "test loss is 0.0035620876370589557\n",
      "Batch: 10200,train loss is: 0.020577901807136405\n",
      "test loss is 0.003010039197552432\n",
      "Batch: 10300,train loss is: 0.0016095935610797015\n",
      "test loss is 0.0025495656120902333\n",
      "Batch: 10400,train loss is: 0.0006962399758925058\n",
      "test loss is 0.004142651474453364\n",
      "Batch: 10500,train loss is: 0.03142992730548597\n",
      "test loss is 0.014195628616564197\n",
      "Batch: 10600,train loss is: 0.0015929438267458114\n",
      "test loss is 0.003863295811764131\n",
      "Batch: 10700,train loss is: 0.0017800596015317455\n",
      "test loss is 0.002335707587139626\n",
      "Batch: 10800,train loss is: 0.0022547522427660534\n",
      "test loss is 0.0024476755186494006\n",
      "Batch: 10900,train loss is: 0.0021500916300461986\n",
      "test loss is 0.0023707019248323854\n",
      "Batch: 11000,train loss is: 0.0015318331749259331\n",
      "test loss is 0.0019711287425532163\n",
      "Batch: 11100,train loss is: 0.003816543773196308\n",
      "test loss is 0.0025628480540097536\n",
      "Batch: 11200,train loss is: 0.0017564141604798967\n",
      "test loss is 0.002272519536502938\n",
      "Batch: 11300,train loss is: 0.0024798587520988703\n",
      "test loss is 0.0035932497644525197\n",
      "Batch: 11400,train loss is: 0.001781277510107268\n",
      "test loss is 0.002703189537053826\n",
      "Batch: 11500,train loss is: 0.0012090875250819562\n",
      "test loss is 0.0022306284824133195\n",
      "Batch: 11600,train loss is: 0.010620944280996563\n",
      "test loss is 0.00838191865753071\n",
      "Batch: 11700,train loss is: 0.0022382090332861326\n",
      "test loss is 0.0030833278912790175\n",
      "Batch: 11800,train loss is: 0.0008046425152121547\n",
      "test loss is 0.00280638714272084\n",
      "Batch: 11900,train loss is: 0.004614666302806184\n",
      "test loss is 0.004529773264846089\n",
      "Batch: 12000,train loss is: 0.0034352153283029926\n",
      "test loss is 0.002753569192065315\n",
      "Batch: 12100,train loss is: 0.012178605407246378\n",
      "test loss is 0.0026728966795852723\n",
      "Batch: 12200,train loss is: 0.002527502600068954\n",
      "test loss is 0.007087546234329113\n",
      "Batch: 12300,train loss is: 0.001046007018738134\n",
      "test loss is 0.002449621856596973\n",
      "Batch: 12400,train loss is: 0.0014561719235728127\n",
      "test loss is 0.002043487680375734\n",
      "Batch: 12500,train loss is: 0.0027478009215022832\n",
      "test loss is 0.001879205621286083\n",
      "Batch: 12600,train loss is: 0.0013736318757028609\n",
      "test loss is 0.002725354236893131\n",
      "Batch: 12700,train loss is: 0.001061176686203004\n",
      "test loss is 0.001847386689629869\n",
      "Batch: 12800,train loss is: 0.0013168183830210623\n",
      "test loss is 0.0018478697540656988\n",
      "Batch: 12900,train loss is: 0.01549264102377939\n",
      "test loss is 0.0052311452639506794\n",
      "Batch: 13000,train loss is: 0.004877907263121794\n",
      "test loss is 0.003523948417369472\n",
      "Batch: 13100,train loss is: 0.00436374091262393\n",
      "test loss is 0.002839922134078051\n",
      "Batch: 13200,train loss is: 0.0025209068245785615\n",
      "test loss is 0.0020380641183832394\n",
      "Batch: 13300,train loss is: 0.006186959116096912\n",
      "test loss is 0.006377506329612565\n",
      "Batch: 13400,train loss is: 0.0030642444121037693\n",
      "test loss is 0.00625201955416565\n",
      "Batch: 13500,train loss is: 0.002210489539823621\n",
      "test loss is 0.0035615541754734433\n",
      "Batch: 13600,train loss is: 0.0010575068531151512\n",
      "test loss is 0.0023929815591846354\n",
      "Batch: 13700,train loss is: 0.0014281251551593098\n",
      "test loss is 0.0022912506752201254\n",
      "Batch: 13800,train loss is: 0.002890931850337325\n",
      "test loss is 0.0029625227660875376\n",
      "Batch: 13900,train loss is: 0.002862507302921248\n",
      "test loss is 0.003547063217860861\n",
      "Batch: 14000,train loss is: 0.0016036743971707585\n",
      "test loss is 0.0030430122720249294\n",
      "Batch: 14100,train loss is: 0.0015642606022537601\n",
      "test loss is 0.0029226420606271895\n",
      "Batch: 14200,train loss is: 0.0021854346735849365\n",
      "test loss is 0.002156531451389007\n",
      "Batch: 14300,train loss is: 0.0010610146424917817\n",
      "test loss is 0.003083563353960327\n",
      "Batch: 14400,train loss is: 0.0018995491109238677\n",
      "test loss is 0.0019388782297620787\n",
      "Batch: 14500,train loss is: 0.002230367584922091\n",
      "test loss is 0.0023255595026084918\n",
      "Batch: 14600,train loss is: 0.005519357636704283\n",
      "test loss is 0.003034828794637344\n",
      "Batch: 14700,train loss is: 0.0015905487432084345\n",
      "test loss is 0.003224985376570022\n",
      "Batch: 14800,train loss is: 0.002285841197326945\n",
      "test loss is 0.00307946929922599\n",
      "Batch: 14900,train loss is: 0.003230335239929499\n",
      "test loss is 0.002383432174528396\n",
      "Batch: 15000,train loss is: 0.004014804827606644\n",
      "test loss is 0.0026099293981053946\n",
      "Batch: 15100,train loss is: 0.0010299581964107085\n",
      "test loss is 0.0019069808230510374\n",
      "Batch: 15200,train loss is: 0.0035063745421470183\n",
      "test loss is 0.002349717908399369\n",
      "Batch: 15300,train loss is: 0.004399639483711397\n",
      "test loss is 0.0031773517392641023\n",
      "Batch: 15400,train loss is: 0.0013324683948541312\n",
      "test loss is 0.0028473707033300592\n",
      "Batch: 15500,train loss is: 0.0014093944601132828\n",
      "test loss is 0.0019724383149971846\n",
      "Batch: 15600,train loss is: 0.002426650681798294\n",
      "test loss is 0.0035400989252407653\n",
      "Batch: 15700,train loss is: 0.0015201209352470352\n",
      "test loss is 0.001997698865087431\n",
      "Batch: 15800,train loss is: 0.0024228600792747497\n",
      "test loss is 0.0022710840749145516\n",
      "Batch: 15900,train loss is: 0.001249769103872027\n",
      "test loss is 0.002856020417039223\n",
      "Batch: 16000,train loss is: 0.001069434405081906\n",
      "test loss is 0.0035023708007986746\n",
      "Batch: 16100,train loss is: 0.004106617375623711\n",
      "test loss is 0.004125687096477657\n",
      "Batch: 16200,train loss is: 0.0013370364872797515\n",
      "test loss is 0.0020906690374704177\n",
      "Batch: 16300,train loss is: 0.0023310239242308353\n",
      "test loss is 0.002300297677875095\n",
      "Batch: 16400,train loss is: 0.0009909760317610627\n",
      "test loss is 0.0021070500024587806\n",
      "Batch: 16500,train loss is: 0.0071580914422919835\n",
      "test loss is 0.006629177461579107\n",
      "Batch: 16600,train loss is: 0.0012194948009938019\n",
      "test loss is 0.002633175018356591\n",
      "Batch: 16700,train loss is: 0.0019026090764647388\n",
      "test loss is 0.002323885537583252\n",
      "Batch: 16800,train loss is: 0.0017229936521341047\n",
      "test loss is 0.002558494449817301\n",
      "Batch: 16900,train loss is: 0.0028676111259264927\n",
      "test loss is 0.0025698420158262614\n",
      "Batch: 17000,train loss is: 0.0037653287976184458\n",
      "test loss is 0.003176594771607176\n",
      "Batch: 17100,train loss is: 0.0012177566396983964\n",
      "test loss is 0.002970320194075435\n",
      "Batch: 17200,train loss is: 0.002209565536734981\n",
      "test loss is 0.0021879059556739167\n",
      "Batch: 17300,train loss is: 0.0012382054307593115\n",
      "test loss is 0.001854280428336248\n",
      "Batch: 17400,train loss is: 0.0010821917385671806\n",
      "test loss is 0.001998415234320566\n",
      "Batch: 17500,train loss is: 0.0032423348795119337\n",
      "test loss is 0.00195355895640539\n",
      "Batch: 17600,train loss is: 0.001475440478621854\n",
      "test loss is 0.00237545152510827\n",
      "Batch: 17700,train loss is: 0.002046608077835291\n",
      "test loss is 0.0025865143389557496\n",
      "Batch: 17800,train loss is: 0.0036512578851096685\n",
      "test loss is 0.00280653384301819\n",
      "Batch: 17900,train loss is: 0.0029417112445436085\n",
      "test loss is 0.002159964041142861\n",
      "Batch: 18000,train loss is: 0.0017148624609974695\n",
      "test loss is 0.0020050178323530767\n",
      "Batch: 18100,train loss is: 0.0017613232385868378\n",
      "test loss is 0.0035532357339093518\n",
      "Batch: 18200,train loss is: 0.0010614920601413867\n",
      "test loss is 0.0030266540077218554\n",
      "Batch: 18300,train loss is: 0.004208580075542592\n",
      "test loss is 0.0026172384707906977\n",
      "Batch: 18400,train loss is: 0.0013391161447221264\n",
      "test loss is 0.001986619090536444\n",
      "Batch: 18500,train loss is: 0.0018090625474796735\n",
      "test loss is 0.005165710504882053\n",
      "Batch: 18600,train loss is: 0.005696804986764892\n",
      "test loss is 0.0058782341472736616\n",
      "Batch: 18700,train loss is: 0.0009770585314238444\n",
      "test loss is 0.0023929562927485597\n",
      "Batch: 18800,train loss is: 0.0021559428349692184\n",
      "test loss is 0.0026893222854808495\n",
      "Batch: 18900,train loss is: 0.0038077659223251024\n",
      "test loss is 0.0023662475519969977\n",
      "Batch: 19000,train loss is: 0.0010818277322187301\n",
      "test loss is 0.0019592513058210924\n",
      "Batch: 19100,train loss is: 0.0011874308604652468\n",
      "test loss is 0.0023164326983631245\n",
      "Batch: 19200,train loss is: 0.0034237585787715726\n",
      "test loss is 0.0024340474320362093\n",
      "Batch: 19300,train loss is: 0.0016178326746621582\n",
      "test loss is 0.0025141152532547634\n",
      "Batch: 19400,train loss is: 0.002789106538254306\n",
      "test loss is 0.002227696129181491\n",
      "Batch: 19500,train loss is: 0.0017337778448921891\n",
      "test loss is 0.002252128860131763\n",
      "Batch: 19600,train loss is: 0.0014404062131623742\n",
      "test loss is 0.0020795123403511643\n",
      "Batch: 19700,train loss is: 0.0017660111477219285\n",
      "test loss is 0.0019051781875138842\n",
      "Batch: 19800,train loss is: 0.0008982622667426098\n",
      "test loss is 0.0022197229011739414\n",
      "Batch: 19900,train loss is: 0.002635241642286565\n",
      "test loss is 0.004187262091132449\n",
      "Batch: 20000,train loss is: 0.0016293698852766956\n",
      "test loss is 0.0020763566350697708\n",
      "Batch: 20100,train loss is: 0.004161874395290361\n",
      "test loss is 0.0020678088160264105\n",
      "Batch: 20200,train loss is: 0.001260637884586315\n",
      "test loss is 0.0019558223285261\n",
      "Batch: 20300,train loss is: 0.0012877771301981142\n",
      "test loss is 0.0019426214176694774\n",
      "Batch: 20400,train loss is: 0.002271064358615318\n",
      "test loss is 0.0024869816216391487\n",
      "Batch: 20500,train loss is: 0.0009537888635117188\n",
      "test loss is 0.003505603284069234\n",
      "Batch: 20600,train loss is: 0.0037630900787352936\n",
      "test loss is 0.0033744257856451283\n",
      "Batch: 20700,train loss is: 0.002485606228661536\n",
      "test loss is 0.0037971002990921337\n",
      "Batch: 20800,train loss is: 0.0029360440115736423\n",
      "test loss is 0.00242304042145179\n",
      "Batch: 20900,train loss is: 0.0029484396382307926\n",
      "test loss is 0.0019080961039821792\n",
      "Batch: 21000,train loss is: 0.0013273901999971026\n",
      "test loss is 0.004257845695321492\n",
      "Batch: 21100,train loss is: 0.001371978568825903\n",
      "test loss is 0.0024569775138332415\n",
      "Batch: 21200,train loss is: 0.003309797449760629\n",
      "test loss is 0.003952129357229085\n",
      "Batch: 21300,train loss is: 0.0009691586731238929\n",
      "test loss is 0.0020370387461961558\n",
      "Batch: 21400,train loss is: 0.0017742671956237765\n",
      "test loss is 0.00228625540037195\n",
      "Batch: 21500,train loss is: 0.00258222932006841\n",
      "test loss is 0.0032777885106544833\n",
      "Batch: 21600,train loss is: 0.006674049326015846\n",
      "test loss is 0.0037114863212561865\n",
      "Batch: 21700,train loss is: 0.0009116588365419858\n",
      "test loss is 0.0019210224659758626\n",
      "Batch: 21800,train loss is: 0.0018840638936253797\n",
      "test loss is 0.001855790017556928\n",
      "Batch: 21900,train loss is: 0.0017236999161015939\n",
      "test loss is 0.0030458342339845035\n",
      "Batch: 22000,train loss is: 0.0037858203917136243\n",
      "test loss is 0.005898776645623202\n",
      "Batch: 22100,train loss is: 0.0033471066842197055\n",
      "test loss is 0.0026133781103302883\n",
      "Batch: 22200,train loss is: 0.0009471878830599994\n",
      "test loss is 0.0023534329936307926\n",
      "Batch: 22300,train loss is: 0.003050548387979269\n",
      "test loss is 0.0024041365050593124\n",
      "Batch: 22400,train loss is: 0.0010888275860480687\n",
      "test loss is 0.002402471918697603\n",
      "Batch: 22500,train loss is: 0.0012936492494402129\n",
      "test loss is 0.0028988637460738826\n",
      "Batch: 22600,train loss is: 0.0020292421094358928\n",
      "test loss is 0.003031160261733867\n",
      "Batch: 22700,train loss is: 0.0017356938866888722\n",
      "test loss is 0.002440357755481692\n",
      "Batch: 22800,train loss is: 0.00148724227464486\n",
      "test loss is 0.0020549925940708913\n",
      "Batch: 22900,train loss is: 0.0006354243944283226\n",
      "test loss is 0.0019216085751475108\n",
      "Batch: 23000,train loss is: 0.002035976079540234\n",
      "test loss is 0.001984196031990228\n",
      "Batch: 23100,train loss is: 0.0013246723276681871\n",
      "test loss is 0.0021668472279668186\n",
      "Batch: 23200,train loss is: 0.0006284208114701655\n",
      "test loss is 0.0026801250440685095\n",
      "Batch: 23300,train loss is: 0.0021351939508643166\n",
      "test loss is 0.0023652982048804547\n",
      "Batch: 23400,train loss is: 0.004393341315283064\n",
      "test loss is 0.0034965657419837133\n",
      "Batch: 23500,train loss is: 0.0026172874045902717\n",
      "test loss is 0.0020659267285616555\n",
      "Batch: 23600,train loss is: 0.0011625502031351914\n",
      "test loss is 0.002430935490918995\n",
      "Batch: 23700,train loss is: 0.0025644360113967935\n",
      "test loss is 0.0020231690846046974\n",
      "Batch: 23800,train loss is: 0.006101696065006814\n",
      "test loss is 0.00268412857389611\n",
      "Batch: 23900,train loss is: 0.0039216402523315486\n",
      "test loss is 0.0048904531598144285\n",
      "Batch: 24000,train loss is: 0.0014429141712417673\n",
      "test loss is 0.0026268846288947764\n",
      "Batch: 24100,train loss is: 0.0021729804825189355\n",
      "test loss is 0.0017194538378454544\n",
      "Batch: 24200,train loss is: 0.004438858691768946\n",
      "test loss is 0.0018735414404653845\n",
      "Batch: 24300,train loss is: 0.006767802950994142\n",
      "test loss is 0.0030939736166016108\n",
      "Batch: 24400,train loss is: 0.004611156035997885\n",
      "test loss is 0.002699901136426817\n",
      "Batch: 24500,train loss is: 0.0027583985138612786\n",
      "test loss is 0.0020907653014263965\n",
      "Batch: 24600,train loss is: 0.0041635895936247115\n",
      "test loss is 0.0022732599423820883\n",
      "Batch: 24700,train loss is: 0.0006122058446324223\n",
      "test loss is 0.0022420572699960936\n",
      "Batch: 24800,train loss is: 0.005606775854446524\n",
      "test loss is 0.005519214879998312\n",
      "Batch: 24900,train loss is: 0.004005354215124098\n",
      "test loss is 0.0029133672679441773\n",
      "Batch: 25000,train loss is: 0.0007620212422903646\n",
      "test loss is 0.0023349522831503265\n",
      "Batch: 25100,train loss is: 0.001064904927787159\n",
      "test loss is 0.0020556768109960028\n",
      "Batch: 25200,train loss is: 0.004725156165271363\n",
      "test loss is 0.0018262724556080267\n",
      "Batch: 25300,train loss is: 0.003766402583386422\n",
      "test loss is 0.002476675120927782\n",
      "Batch: 25400,train loss is: 0.001466338899427076\n",
      "test loss is 0.0021543802600191746\n",
      "Batch: 25500,train loss is: 0.0022203059250110115\n",
      "test loss is 0.0020986283232482263\n",
      "Batch: 25600,train loss is: 0.0018236994677789733\n",
      "test loss is 0.0017161627810934603\n",
      "Batch: 25700,train loss is: 0.0014928181978847619\n",
      "test loss is 0.0023040514139294585\n",
      "Batch: 25800,train loss is: 0.00216535484859537\n",
      "test loss is 0.0024161598499336805\n",
      "Batch: 25900,train loss is: 0.0011213423319056525\n",
      "test loss is 0.0028100238422086084\n",
      "Batch: 26000,train loss is: 0.003082357383597253\n",
      "test loss is 0.0029079595738486963\n",
      "Batch: 26100,train loss is: 0.002468210897410226\n",
      "test loss is 0.0023646373333461863\n",
      "Batch: 26200,train loss is: 0.0030563190489170233\n",
      "test loss is 0.002342012680468644\n",
      "Batch: 26300,train loss is: 0.002229530433907319\n",
      "test loss is 0.0023064674627531097\n",
      "Batch: 26400,train loss is: 0.0015073982409792353\n",
      "test loss is 0.0020590799725305675\n",
      "Batch: 26500,train loss is: 0.0009889328127067787\n",
      "test loss is 0.001983536297929263\n",
      "Batch: 26600,train loss is: 0.0011894172173515514\n",
      "test loss is 0.0019076069344021141\n",
      "Batch: 26700,train loss is: 0.0017295076602467693\n",
      "test loss is 0.0033043279744767724\n",
      "Batch: 26800,train loss is: 0.0029801836178461245\n",
      "test loss is 0.0020746689321286074\n",
      "Batch: 26900,train loss is: 0.002223579497575299\n",
      "test loss is 0.0026010736857498083\n",
      "Batch: 27000,train loss is: 0.002943177070851665\n",
      "test loss is 0.002523096405566275\n",
      "Batch: 27100,train loss is: 0.0025980117460385516\n",
      "test loss is 0.003367936166458451\n",
      "Batch: 27200,train loss is: 0.008419496388474544\n",
      "test loss is 0.004678951193531162\n",
      "Batch: 27300,train loss is: 0.0010133797035893084\n",
      "test loss is 0.0030379796175779017\n",
      "Batch: 27400,train loss is: 0.001522327778291538\n",
      "test loss is 0.0018832009480567226\n",
      "Batch: 27500,train loss is: 0.005125242082759028\n",
      "test loss is 0.0025924294593131746\n",
      "Batch: 27600,train loss is: 0.0009479049633949153\n",
      "test loss is 0.0028957607871027253\n",
      "Batch: 27700,train loss is: 0.006533284281592141\n",
      "test loss is 0.002314368602833908\n",
      "Batch: 27800,train loss is: 0.0012779491769905084\n",
      "test loss is 0.006412495247607494\n",
      "Batch: 27900,train loss is: 0.0027849829356722464\n",
      "test loss is 0.002320862667721418\n",
      "Batch: 28000,train loss is: 0.0016124116678927635\n",
      "test loss is 0.0020134245419021653\n",
      "Batch: 28100,train loss is: 0.004501951023794661\n",
      "test loss is 0.0033346410519045867\n",
      "Batch: 28200,train loss is: 0.007320564704510303\n",
      "test loss is 0.0029909085007466317\n",
      "Batch: 28300,train loss is: 0.0029906861888981217\n",
      "test loss is 0.0019716793337164347\n",
      "Batch: 28400,train loss is: 0.0022533790759288485\n",
      "test loss is 0.0019141826232363774\n",
      "Batch: 28500,train loss is: 0.0029951876209030016\n",
      "test loss is 0.002463042656831942\n",
      "Batch: 28600,train loss is: 0.005304636791314778\n",
      "test loss is 0.0027680545260408513\n",
      "Batch: 28700,train loss is: 0.0016320393841460672\n",
      "test loss is 0.00246178233653398\n",
      "Batch: 28800,train loss is: 0.005134304380113467\n",
      "test loss is 0.001986876768035386\n",
      "Batch: 28900,train loss is: 0.0024869666405114062\n",
      "test loss is 0.0028831890670007188\n",
      "Batch: 29000,train loss is: 0.0008739035424598987\n",
      "test loss is 0.002216949408163367\n",
      "Batch: 29100,train loss is: 0.0029163699083115347\n",
      "test loss is 0.001901782536084906\n",
      "Batch: 29200,train loss is: 0.001644861840527791\n",
      "test loss is 0.00289820404100632\n",
      "Batch: 29300,train loss is: 0.0018707587329949882\n",
      "test loss is 0.0016540496733238323\n",
      "Batch: 29400,train loss is: 0.0013353248875494245\n",
      "test loss is 0.00282794686991591\n",
      "Batch: 29500,train loss is: 0.0009007827272186317\n",
      "test loss is 0.003216255408454562\n",
      "Batch: 29600,train loss is: 0.0013892780497421296\n",
      "test loss is 0.0015698803039785971\n",
      "Batch: 29700,train loss is: 0.0009979119153733472\n",
      "test loss is 0.0025288268851390377\n",
      "Batch: 29800,train loss is: 0.004249098967840224\n",
      "test loss is 0.0016794478419627497\n",
      "Batch: 29900,train loss is: 0.010406004880913188\n",
      "test loss is 0.0022877593044409467\n",
      "Batch: 30000,train loss is: 0.0015162295691674894\n",
      "test loss is 0.0022009361103395124\n",
      "Batch: 30100,train loss is: 0.0014316453458572851\n",
      "test loss is 0.0027963071047705573\n",
      "Batch: 30200,train loss is: 0.001622952578663415\n",
      "test loss is 0.0020435457268198343\n",
      "Batch: 30300,train loss is: 0.0016021351728532987\n",
      "test loss is 0.0019875244387795453\n",
      "Batch: 30400,train loss is: 0.001977083414581146\n",
      "test loss is 0.002522118620695842\n",
      "Batch: 30500,train loss is: 0.0007954397947632499\n",
      "test loss is 0.0019184717582639474\n",
      "Batch: 30600,train loss is: 0.001467170158378195\n",
      "test loss is 0.0028256269699726425\n",
      "Batch: 30700,train loss is: 0.001846692158971141\n",
      "test loss is 0.0031818352568735143\n",
      "Batch: 30800,train loss is: 0.0015485618223563958\n",
      "test loss is 0.0020897966380445127\n",
      "Batch: 30900,train loss is: 0.0020568967904766333\n",
      "test loss is 0.0024214900918633838\n",
      "Batch: 31000,train loss is: 0.0007151226644978837\n",
      "test loss is 0.0019102339027387857\n",
      "Batch: 31100,train loss is: 0.0026206041566211634\n",
      "test loss is 0.004878039397556175\n",
      "Batch: 31200,train loss is: 0.0026593120325656906\n",
      "test loss is 0.0020327568150524658\n",
      "Batch: 31300,train loss is: 0.0018856111914606717\n",
      "test loss is 0.002571491580380927\n",
      "Batch: 31400,train loss is: 0.0008505213694432564\n",
      "test loss is 0.001977490148439871\n",
      "Batch: 31500,train loss is: 0.0015641550980684022\n",
      "test loss is 0.0025660556497731166\n",
      "Batch: 31600,train loss is: 0.001419144114792973\n",
      "test loss is 0.0018655258615917839\n",
      "Batch: 31700,train loss is: 0.001046331490957886\n",
      "test loss is 0.0016413213976480413\n",
      "Batch: 31800,train loss is: 0.002603989739928292\n",
      "test loss is 0.002676219130215696\n",
      "Batch: 31900,train loss is: 0.004123914117090646\n",
      "test loss is 0.0032675175162028875\n",
      "Batch: 32000,train loss is: 0.02159295308282176\n",
      "test loss is 0.0021543191938263703\n",
      "Batch: 32100,train loss is: 0.0015983011077888407\n",
      "test loss is 0.0024003747824486816\n",
      "Batch: 32200,train loss is: 0.0008289287689695625\n",
      "test loss is 0.0027057485484344333\n",
      "Batch: 32300,train loss is: 0.0021028580727283706\n",
      "test loss is 0.0023028173688598727\n",
      "Batch: 32400,train loss is: 0.0007422518751004594\n",
      "test loss is 0.0030601145701847714\n",
      "Batch: 32500,train loss is: 0.014594330296577824\n",
      "test loss is 0.009072280228018646\n",
      "Batch: 32600,train loss is: 0.005088033509396661\n",
      "test loss is 0.0024263793947254023\n",
      "Batch: 32700,train loss is: 0.0012738699858004083\n",
      "test loss is 0.0022848260263336386\n",
      "Batch: 32800,train loss is: 0.0012297999712552296\n",
      "test loss is 0.0018925735482650144\n",
      "Batch: 32900,train loss is: 0.002900253636745291\n",
      "test loss is 0.0021544477981022426\n",
      "Batch: 33000,train loss is: 0.001066859091995829\n",
      "test loss is 0.002021427702526192\n",
      "Batch: 33100,train loss is: 0.001938988126049082\n",
      "test loss is 0.0032247244921715943\n",
      "Batch: 33200,train loss is: 0.0017268367021685506\n",
      "test loss is 0.0016594760273569\n",
      "Batch: 33300,train loss is: 0.001861665197267423\n",
      "test loss is 0.0018800455825510624\n",
      "Batch: 33400,train loss is: 0.003461818942649718\n",
      "test loss is 0.0025229456924479613\n",
      "Batch: 33500,train loss is: 0.0010643007599333003\n",
      "test loss is 0.0019536113620483146\n",
      "Batch: 33600,train loss is: 0.0007479746092731178\n",
      "test loss is 0.0019438512675886651\n",
      "Batch: 33700,train loss is: 0.002204878772032952\n",
      "test loss is 0.0029098591563658225\n",
      "Batch: 33800,train loss is: 0.005719625003492864\n",
      "test loss is 0.0028774731298522004\n",
      "Batch: 33900,train loss is: 0.00448556700341395\n",
      "test loss is 0.0021295592604915563\n",
      "-----------------------Epoch: 2----------------------------------\n",
      "Batch: 0,train loss is: 0.0019542853727337575\n",
      "test loss is 0.0024363442123640057\n",
      "Batch: 100,train loss is: 0.0018035249201563137\n",
      "test loss is 0.0020984295057044585\n",
      "Batch: 200,train loss is: 0.0009637821739591913\n",
      "test loss is 0.0017172912004319331\n",
      "Batch: 300,train loss is: 0.0009070115082002065\n",
      "test loss is 0.0018521179514739887\n",
      "Batch: 400,train loss is: 0.0015279029774620094\n",
      "test loss is 0.0020946886436948213\n",
      "Batch: 500,train loss is: 0.0010330042293349767\n",
      "test loss is 0.0019518969730767432\n",
      "Batch: 600,train loss is: 0.0004188812440626561\n",
      "test loss is 0.0029307817988801776\n",
      "Batch: 700,train loss is: 0.0013220268004280973\n",
      "test loss is 0.0016386831645114122\n",
      "Batch: 800,train loss is: 0.002794734792689417\n",
      "test loss is 0.002634259301463954\n",
      "Batch: 900,train loss is: 0.00210960241791327\n",
      "test loss is 0.004283539600281164\n",
      "Batch: 1000,train loss is: 0.0010312024850883913\n",
      "test loss is 0.0025531496289565112\n",
      "Batch: 1100,train loss is: 0.0074924567891796875\n",
      "test loss is 0.002663287215032831\n",
      "Batch: 1200,train loss is: 0.0007087864708358422\n",
      "test loss is 0.0039059614370339996\n",
      "Batch: 1300,train loss is: 0.0009058550322962551\n",
      "test loss is 0.0015927091847904088\n",
      "Batch: 1400,train loss is: 0.0014434005647582617\n",
      "test loss is 0.0018107123520667942\n",
      "Batch: 1500,train loss is: 0.0016687089879329966\n",
      "test loss is 0.002810175372032533\n",
      "Batch: 1600,train loss is: 0.0008226689031950375\n",
      "test loss is 0.0030355357026309805\n",
      "Batch: 1700,train loss is: 0.010630737564537144\n",
      "test loss is 0.0023568298551961257\n",
      "Batch: 1800,train loss is: 0.000449839084442258\n",
      "test loss is 0.0016840223673650642\n",
      "Batch: 1900,train loss is: 0.002102287979487919\n",
      "test loss is 0.0024097273436733675\n",
      "Batch: 2000,train loss is: 0.0031482383378412804\n",
      "test loss is 0.0027224404571498757\n",
      "Batch: 2100,train loss is: 0.0017228454601202862\n",
      "test loss is 0.002660742245395428\n",
      "Batch: 2200,train loss is: 0.002088197211345873\n",
      "test loss is 0.001839391624223213\n",
      "Batch: 2300,train loss is: 0.0011004707611304797\n",
      "test loss is 0.00179941287073656\n",
      "Batch: 2400,train loss is: 0.001614295887959776\n",
      "test loss is 0.0018853403116485838\n",
      "Batch: 2500,train loss is: 0.0033672927500166096\n",
      "test loss is 0.002071565092413414\n",
      "Batch: 2600,train loss is: 0.002195431104017368\n",
      "test loss is 0.0028519978720692534\n",
      "Batch: 2700,train loss is: 0.002657856534158502\n",
      "test loss is 0.0017541081266071454\n",
      "Batch: 2800,train loss is: 0.0010963759081594978\n",
      "test loss is 0.0022910263511855817\n",
      "Batch: 2900,train loss is: 0.0010127940962107057\n",
      "test loss is 0.0015919496331220496\n",
      "Batch: 3000,train loss is: 0.0013119310960194952\n",
      "test loss is 0.0017967968335578872\n",
      "Batch: 3100,train loss is: 0.0018009099471876996\n",
      "test loss is 0.0017810777567128305\n",
      "Batch: 3200,train loss is: 0.007675920902634778\n",
      "test loss is 0.0035581559505808835\n",
      "Batch: 3300,train loss is: 0.0007542801892262823\n",
      "test loss is 0.002106018216579401\n",
      "Batch: 3400,train loss is: 0.002695478120220523\n",
      "test loss is 0.0021645347312994096\n",
      "Batch: 3500,train loss is: 0.0011936883566288073\n",
      "test loss is 0.002957437841843272\n",
      "Batch: 3600,train loss is: 0.0009576131468308968\n",
      "test loss is 0.002757938756455785\n",
      "Batch: 3700,train loss is: 0.0027072332301255036\n",
      "test loss is 0.002447542376533909\n",
      "Batch: 3800,train loss is: 0.0008943551839042628\n",
      "test loss is 0.0023965024972766253\n",
      "Batch: 3900,train loss is: 0.002951494891677433\n",
      "test loss is 0.0019295092428512075\n",
      "Batch: 4000,train loss is: 0.0006513152993776786\n",
      "test loss is 0.0024634126079059915\n",
      "Batch: 4100,train loss is: 0.0014563656279559466\n",
      "test loss is 0.0016587430281205084\n",
      "Batch: 4200,train loss is: 0.0010799595639947909\n",
      "test loss is 0.0019895073204473745\n",
      "Batch: 4300,train loss is: 0.0015066508870666122\n",
      "test loss is 0.0028286101116140766\n",
      "Batch: 4400,train loss is: 0.001417256032087358\n",
      "test loss is 0.002233637450350393\n",
      "Batch: 4500,train loss is: 0.01532528110486565\n",
      "test loss is 0.007454847285972614\n",
      "Batch: 4600,train loss is: 0.0016199363521759052\n",
      "test loss is 0.0032843283714354718\n",
      "Batch: 4700,train loss is: 0.004075773660806742\n",
      "test loss is 0.0032175942549423904\n",
      "Batch: 4800,train loss is: 0.0036528381047072635\n",
      "test loss is 0.0034261104811692785\n",
      "Batch: 4900,train loss is: 0.0009600073676246483\n",
      "test loss is 0.001684149871740046\n",
      "Batch: 5000,train loss is: 0.000898359301821643\n",
      "test loss is 0.001909050875162281\n",
      "Batch: 5100,train loss is: 0.001960983695611376\n",
      "test loss is 0.002001211970271027\n",
      "Batch: 5200,train loss is: 0.0009179951023646011\n",
      "test loss is 0.0016152307982922288\n",
      "Batch: 5300,train loss is: 0.0021473240930854364\n",
      "test loss is 0.003473729942150648\n",
      "Batch: 5400,train loss is: 0.00246242892963029\n",
      "test loss is 0.0022330773786299754\n",
      "Batch: 5500,train loss is: 0.0019238916468115187\n",
      "test loss is 0.0019294588095337187\n",
      "Batch: 5600,train loss is: 0.0011034919146445871\n",
      "test loss is 0.0017073850530936044\n",
      "Batch: 5700,train loss is: 0.0008919262025241475\n",
      "test loss is 0.0023310133983249198\n",
      "Batch: 5800,train loss is: 0.0028545794981151827\n",
      "test loss is 0.0020676545235450037\n",
      "Batch: 5900,train loss is: 0.004975039991013921\n",
      "test loss is 0.0022038322998809514\n",
      "Batch: 6000,train loss is: 0.0036724437585771456\n",
      "test loss is 0.004873216999785051\n",
      "Batch: 6100,train loss is: 0.000691426128746059\n",
      "test loss is 0.00372428545588216\n",
      "Batch: 6200,train loss is: 0.0013615874893467047\n",
      "test loss is 0.001867147665733626\n",
      "Batch: 6300,train loss is: 0.0021703405241047646\n",
      "test loss is 0.0016886880254707998\n",
      "Batch: 6400,train loss is: 0.0017291213136682519\n",
      "test loss is 0.004406839332269173\n",
      "Batch: 6500,train loss is: 0.0002907911500331267\n",
      "test loss is 0.001902393631119798\n",
      "Batch: 6600,train loss is: 0.005934185883633274\n",
      "test loss is 0.0019882408993978687\n",
      "Batch: 6700,train loss is: 0.0008493399252480604\n",
      "test loss is 0.0022461264227435843\n",
      "Batch: 6800,train loss is: 0.002169716771065048\n",
      "test loss is 0.001713482937303859\n",
      "Batch: 6900,train loss is: 0.0033323208997161273\n",
      "test loss is 0.0016487382826791768\n",
      "Batch: 7000,train loss is: 0.0017477222693438063\n",
      "test loss is 0.005615089591069341\n",
      "Batch: 7100,train loss is: 0.0019566649234323916\n",
      "test loss is 0.0018846865174051964\n",
      "Batch: 7200,train loss is: 0.001687148507456941\n",
      "test loss is 0.0016873199430771518\n",
      "Batch: 7300,train loss is: 0.003960595774836349\n",
      "test loss is 0.0024134507953456224\n",
      "Batch: 7400,train loss is: 0.0014232497172570456\n",
      "test loss is 0.004502480820060087\n",
      "Batch: 7500,train loss is: 0.0029880853818948306\n",
      "test loss is 0.0037483884601587723\n",
      "Batch: 7600,train loss is: 0.0027813267024789506\n",
      "test loss is 0.0024963774700740315\n",
      "Batch: 7700,train loss is: 0.0016378904591055996\n",
      "test loss is 0.0014766356267087756\n",
      "Batch: 7800,train loss is: 0.0017974001752900826\n",
      "test loss is 0.0020088592800419497\n",
      "Batch: 7900,train loss is: 0.0007318639313993357\n",
      "test loss is 0.0018934305628279333\n",
      "Batch: 8000,train loss is: 0.0004643615290281768\n",
      "test loss is 0.0027970901462993666\n",
      "Batch: 8100,train loss is: 0.0027658385247861126\n",
      "test loss is 0.0018815385532262177\n",
      "Batch: 8200,train loss is: 0.0033848438230143422\n",
      "test loss is 0.00203688328825757\n",
      "Batch: 8300,train loss is: 0.0007501342931549358\n",
      "test loss is 0.0017308987526303203\n",
      "Batch: 8400,train loss is: 0.0037580837120516184\n",
      "test loss is 0.0026059512610318593\n",
      "Batch: 8500,train loss is: 0.0026357992959230407\n",
      "test loss is 0.004350637594275153\n",
      "Batch: 8600,train loss is: 0.004164988865009805\n",
      "test loss is 0.0036132822196658205\n",
      "Batch: 8700,train loss is: 0.0014035649106375282\n",
      "test loss is 0.0033874077966556913\n",
      "Batch: 8800,train loss is: 0.0006557353488542905\n",
      "test loss is 0.0019371421803429335\n",
      "Batch: 8900,train loss is: 0.002541975416920849\n",
      "test loss is 0.00147302443672368\n",
      "Batch: 9000,train loss is: 0.0004684858713061211\n",
      "test loss is 0.001976285415855712\n",
      "Batch: 9100,train loss is: 0.00048815565992042944\n",
      "test loss is 0.0019287239736686004\n",
      "Batch: 9200,train loss is: 0.008727942654449075\n",
      "test loss is 0.0050197977547640745\n",
      "Batch: 9300,train loss is: 0.0011977581298250452\n",
      "test loss is 0.0025847323434422392\n",
      "Batch: 9400,train loss is: 0.002032466881656274\n",
      "test loss is 0.002731141220736924\n",
      "Batch: 9500,train loss is: 0.0013058429957706515\n",
      "test loss is 0.0026912932678419374\n",
      "Batch: 9600,train loss is: 0.00398257964838651\n",
      "test loss is 0.0028497923118685907\n",
      "Batch: 9700,train loss is: 0.000984537315246944\n",
      "test loss is 0.002029465867391795\n",
      "Batch: 9800,train loss is: 0.001038977239935615\n",
      "test loss is 0.0021680638215754184\n",
      "Batch: 9900,train loss is: 0.0014679335284409627\n",
      "test loss is 0.0020821138224679865\n",
      "Batch: 10000,train loss is: 0.0009972615595103718\n",
      "test loss is 0.002107481309894129\n",
      "Batch: 10100,train loss is: 0.0022059890117580587\n",
      "test loss is 0.0018816697177818959\n",
      "Batch: 10200,train loss is: 0.009680718952979903\n",
      "test loss is 0.0018670976503924281\n",
      "Batch: 10300,train loss is: 0.0017794834599245042\n",
      "test loss is 0.002206899866332916\n",
      "Batch: 10400,train loss is: 0.0005778741799386773\n",
      "test loss is 0.002338078446881488\n",
      "Batch: 10500,train loss is: 0.03602635838824945\n",
      "test loss is 0.01617320848350469\n",
      "Batch: 10600,train loss is: 0.0013321919346162986\n",
      "test loss is 0.0031335432664873074\n",
      "Batch: 10700,train loss is: 0.0011216625470875275\n",
      "test loss is 0.001816067143268332\n",
      "Batch: 10800,train loss is: 0.0015766331573665777\n",
      "test loss is 0.00165230818951374\n",
      "Batch: 10900,train loss is: 0.0009840481344129243\n",
      "test loss is 0.001691476243420315\n",
      "Batch: 11000,train loss is: 0.0011677350544799358\n",
      "test loss is 0.001500753503099261\n",
      "Batch: 11100,train loss is: 0.0024536220843082\n",
      "test loss is 0.0016349669431103804\n",
      "Batch: 11200,train loss is: 0.0018771012787293156\n",
      "test loss is 0.0016990939009042102\n",
      "Batch: 11300,train loss is: 0.001223370720901167\n",
      "test loss is 0.002105931597881616\n",
      "Batch: 11400,train loss is: 0.0017082091679100548\n",
      "test loss is 0.0019298245233785095\n",
      "Batch: 11500,train loss is: 0.0011260406931277258\n",
      "test loss is 0.0017280784529644215\n",
      "Batch: 11600,train loss is: 0.00740835556025971\n",
      "test loss is 0.005939887534656319\n",
      "Batch: 11700,train loss is: 0.0016607490937980047\n",
      "test loss is 0.0023725443706490283\n",
      "Batch: 11800,train loss is: 0.000820033260996641\n",
      "test loss is 0.002245490494005196\n",
      "Batch: 11900,train loss is: 0.003539319899618774\n",
      "test loss is 0.003582670701202758\n",
      "Batch: 12000,train loss is: 0.0026229373885471325\n",
      "test loss is 0.0023618511665706153\n",
      "Batch: 12100,train loss is: 0.008380106101688475\n",
      "test loss is 0.0018998133399840851\n",
      "Batch: 12200,train loss is: 0.0013246906386486788\n",
      "test loss is 0.002581342147318433\n",
      "Batch: 12300,train loss is: 0.001006903108374377\n",
      "test loss is 0.0022297929580968436\n",
      "Batch: 12400,train loss is: 0.0012913603633429684\n",
      "test loss is 0.0018034770149203071\n",
      "Batch: 12500,train loss is: 0.0020759550747902664\n",
      "test loss is 0.0015136065070144788\n",
      "Batch: 12600,train loss is: 0.0016346815693291765\n",
      "test loss is 0.0023955532170467264\n",
      "Batch: 12700,train loss is: 0.001342608450470562\n",
      "test loss is 0.001461240045632464\n",
      "Batch: 12800,train loss is: 0.0012496118733980155\n",
      "test loss is 0.0017069820601763085\n",
      "Batch: 12900,train loss is: 0.0033953374139348816\n",
      "test loss is 0.0020830054963952246\n",
      "Batch: 13000,train loss is: 0.0020190331865315974\n",
      "test loss is 0.0023512126402683493\n",
      "Batch: 13100,train loss is: 0.0011892245420064532\n",
      "test loss is 0.0017287549001485617\n",
      "Batch: 13200,train loss is: 0.0035111028374931394\n",
      "test loss is 0.0025937099675822388\n",
      "Batch: 13300,train loss is: 0.0032615143047247286\n",
      "test loss is 0.004892833601635275\n",
      "Batch: 13400,train loss is: 0.0026065912120541283\n",
      "test loss is 0.0032309602146869108\n",
      "Batch: 13500,train loss is: 0.00285706689706041\n",
      "test loss is 0.0028236718182695787\n",
      "Batch: 13600,train loss is: 0.0009997703346949455\n",
      "test loss is 0.001615008032790683\n",
      "Batch: 13700,train loss is: 0.0009577083799592019\n",
      "test loss is 0.001901703864685304\n",
      "Batch: 13800,train loss is: 0.002194477506587339\n",
      "test loss is 0.002540937384424567\n",
      "Batch: 13900,train loss is: 0.0015544280882830678\n",
      "test loss is 0.002370716952394747\n",
      "Batch: 14000,train loss is: 0.000848543063390857\n",
      "test loss is 0.0023834592804274873\n",
      "Batch: 14100,train loss is: 0.0009539599590917396\n",
      "test loss is 0.001883320918475628\n",
      "Batch: 14200,train loss is: 0.0010174644166368707\n",
      "test loss is 0.0015471543357584482\n",
      "Batch: 14300,train loss is: 0.0010754977468297863\n",
      "test loss is 0.00274432109623561\n",
      "Batch: 14400,train loss is: 0.0023734582564031376\n",
      "test loss is 0.001660795738266857\n",
      "Batch: 14500,train loss is: 0.003265223007764733\n",
      "test loss is 0.00260531833845822\n",
      "Batch: 14600,train loss is: 0.004277260677825956\n",
      "test loss is 0.0015140682613125431\n",
      "Batch: 14700,train loss is: 0.0009433650327202298\n",
      "test loss is 0.002217860454398963\n",
      "Batch: 14800,train loss is: 0.001253189516626735\n",
      "test loss is 0.0022187341983748265\n",
      "Batch: 14900,train loss is: 0.0023215387763818288\n",
      "test loss is 0.0016983005396107086\n",
      "Batch: 15000,train loss is: 0.0027624005998256734\n",
      "test loss is 0.0024121597142740796\n",
      "Batch: 15100,train loss is: 0.0015709831172926077\n",
      "test loss is 0.0016694286677934823\n",
      "Batch: 15200,train loss is: 0.003381949502507082\n",
      "test loss is 0.0018815672875849222\n",
      "Batch: 15300,train loss is: 0.003384809617178681\n",
      "test loss is 0.002166482105708534\n",
      "Batch: 15400,train loss is: 0.0011830489701968928\n",
      "test loss is 0.002890794148952629\n",
      "Batch: 15500,train loss is: 0.0015995614137799996\n",
      "test loss is 0.0015203621699577668\n",
      "Batch: 15600,train loss is: 0.0033464439477213542\n",
      "test loss is 0.003136294734315642\n",
      "Batch: 15700,train loss is: 0.0010946389683563144\n",
      "test loss is 0.0016579864814084506\n",
      "Batch: 15800,train loss is: 0.0023508085159680745\n",
      "test loss is 0.0021753759376933798\n",
      "Batch: 15900,train loss is: 0.0009480250266428833\n",
      "test loss is 0.002135975814756349\n",
      "Batch: 16000,train loss is: 0.0015463375698636435\n",
      "test loss is 0.003033132087329309\n",
      "Batch: 16100,train loss is: 0.003690869928004671\n",
      "test loss is 0.004147505206888653\n",
      "Batch: 16200,train loss is: 0.0009654604911198264\n",
      "test loss is 0.0017708262106911933\n",
      "Batch: 16300,train loss is: 0.0014602435255047853\n",
      "test loss is 0.0017041133379055336\n",
      "Batch: 16400,train loss is: 0.0004435505514362887\n",
      "test loss is 0.0014473381993685983\n",
      "Batch: 16500,train loss is: 0.004798751181997201\n",
      "test loss is 0.0067714047250360945\n",
      "Batch: 16600,train loss is: 0.0008851592962643512\n",
      "test loss is 0.002004669633760458\n",
      "Batch: 16700,train loss is: 0.0012891701443540527\n",
      "test loss is 0.0019062392718324271\n",
      "Batch: 16800,train loss is: 0.0008357837450470873\n",
      "test loss is 0.0018388462522839764\n",
      "Batch: 16900,train loss is: 0.002830315750985851\n",
      "test loss is 0.002445597960640698\n",
      "Batch: 17000,train loss is: 0.0016927889090517822\n",
      "test loss is 0.0018508109435086382\n",
      "Batch: 17100,train loss is: 0.0010495040346547714\n",
      "test loss is 0.002144241591390962\n",
      "Batch: 17200,train loss is: 0.0015131784109978798\n",
      "test loss is 0.0016674732480501087\n",
      "Batch: 17300,train loss is: 0.0013665033527268777\n",
      "test loss is 0.001600095639782231\n",
      "Batch: 17400,train loss is: 0.0014444072665829176\n",
      "test loss is 0.0021289358242371453\n",
      "Batch: 17500,train loss is: 0.0028735459564747058\n",
      "test loss is 0.0019443995928456756\n",
      "Batch: 17600,train loss is: 0.0009053277263454838\n",
      "test loss is 0.0018555048486484384\n",
      "Batch: 17700,train loss is: 0.0015208905551119894\n",
      "test loss is 0.00210779143261236\n",
      "Batch: 17800,train loss is: 0.0027947390869087554\n",
      "test loss is 0.001866927837399472\n",
      "Batch: 17900,train loss is: 0.0020480145109729553\n",
      "test loss is 0.0016956176568188092\n",
      "Batch: 18000,train loss is: 0.0010249224330120402\n",
      "test loss is 0.001589738716100606\n",
      "Batch: 18100,train loss is: 0.0010533104496365288\n",
      "test loss is 0.0026351017292943145\n",
      "Batch: 18200,train loss is: 0.0008140457372479019\n",
      "test loss is 0.002121161294139258\n",
      "Batch: 18300,train loss is: 0.002734584805911484\n",
      "test loss is 0.001659702342202727\n",
      "Batch: 18400,train loss is: 0.0005583701242401536\n",
      "test loss is 0.001595196906323359\n",
      "Batch: 18500,train loss is: 0.0008692511996445072\n",
      "test loss is 0.003352343786271463\n",
      "Batch: 18600,train loss is: 0.0037379051087799604\n",
      "test loss is 0.003941932620739139\n",
      "Batch: 18700,train loss is: 0.0008798324712452009\n",
      "test loss is 0.0019287443348989094\n",
      "Batch: 18800,train loss is: 0.0018240902152637127\n",
      "test loss is 0.0021129637989220442\n",
      "Batch: 18900,train loss is: 0.00216714313775096\n",
      "test loss is 0.0018557562128931533\n",
      "Batch: 19000,train loss is: 0.0012029097139339318\n",
      "test loss is 0.0017040332729539723\n",
      "Batch: 19100,train loss is: 0.0010535014951440923\n",
      "test loss is 0.0017140461351073358\n",
      "Batch: 19200,train loss is: 0.0019386031371747753\n",
      "test loss is 0.001840323315574033\n",
      "Batch: 19300,train loss is: 0.0022734808933794932\n",
      "test loss is 0.002002806822694964\n",
      "Batch: 19400,train loss is: 0.0014335496365178774\n",
      "test loss is 0.001503834977098176\n",
      "Batch: 19500,train loss is: 0.001128459964390264\n",
      "test loss is 0.0018581071274643593\n",
      "Batch: 19600,train loss is: 0.0008184862216816714\n",
      "test loss is 0.0015229080483220342\n",
      "Batch: 19700,train loss is: 0.001243854476525436\n",
      "test loss is 0.0015356084226527806\n",
      "Batch: 19800,train loss is: 0.0006877016136587671\n",
      "test loss is 0.0017652162866925222\n",
      "Batch: 19900,train loss is: 0.001770612438280008\n",
      "test loss is 0.003208602525232145\n",
      "Batch: 20000,train loss is: 0.0014533323099146928\n",
      "test loss is 0.0019493516701846575\n",
      "Batch: 20100,train loss is: 0.001765431126307934\n",
      "test loss is 0.0017456838153588668\n",
      "Batch: 20200,train loss is: 0.0018373019575116305\n",
      "test loss is 0.0016629695180417222\n",
      "Batch: 20300,train loss is: 0.001511050725202731\n",
      "test loss is 0.0015904315817172282\n",
      "Batch: 20400,train loss is: 0.0021068125374701235\n",
      "test loss is 0.001993422956370449\n",
      "Batch: 20500,train loss is: 0.0010972672111472838\n",
      "test loss is 0.0022251787295336275\n",
      "Batch: 20600,train loss is: 0.002197338285531571\n",
      "test loss is 0.003596519556834242\n",
      "Batch: 20700,train loss is: 0.0029830672727798753\n",
      "test loss is 0.0042645041846421415\n",
      "Batch: 20800,train loss is: 0.0013984171759344746\n",
      "test loss is 0.0019353189693489647\n",
      "Batch: 20900,train loss is: 0.002358311360496972\n",
      "test loss is 0.0016177001195718994\n",
      "Batch: 21000,train loss is: 0.0006919153410688052\n",
      "test loss is 0.00405941059255215\n",
      "Batch: 21100,train loss is: 0.0008249698345827267\n",
      "test loss is 0.001936682994805295\n",
      "Batch: 21200,train loss is: 0.004412331347352615\n",
      "test loss is 0.0041441227494761635\n",
      "Batch: 21300,train loss is: 0.0005813555494650703\n",
      "test loss is 0.0016303470480211648\n",
      "Batch: 21400,train loss is: 0.0010804854895879194\n",
      "test loss is 0.001995557724733104\n",
      "Batch: 21500,train loss is: 0.0013092193384763079\n",
      "test loss is 0.0016872189477797686\n",
      "Batch: 21600,train loss is: 0.003324365345930899\n",
      "test loss is 0.001984297958360011\n",
      "Batch: 21700,train loss is: 0.0010823564535702896\n",
      "test loss is 0.0017797431478542344\n",
      "Batch: 21800,train loss is: 0.001241767411351533\n",
      "test loss is 0.0014104701438276501\n",
      "Batch: 21900,train loss is: 0.0009714620721455832\n",
      "test loss is 0.0024977412597385583\n",
      "Batch: 22000,train loss is: 0.004571457716106043\n",
      "test loss is 0.004306089947603664\n",
      "Batch: 22100,train loss is: 0.0016609561976513962\n",
      "test loss is 0.002179513429746516\n",
      "Batch: 22200,train loss is: 0.0008630410010757119\n",
      "test loss is 0.0019528453803483673\n",
      "Batch: 22300,train loss is: 0.0024264117417412815\n",
      "test loss is 0.0022376575511438624\n",
      "Batch: 22400,train loss is: 0.0018833431973975653\n",
      "test loss is 0.002012884545422978\n",
      "Batch: 22500,train loss is: 0.00171166656863498\n",
      "test loss is 0.002703818607524858\n",
      "Batch: 22600,train loss is: 0.0022897372028658027\n",
      "test loss is 0.002057462440729818\n",
      "Batch: 22700,train loss is: 0.0014546212781774972\n",
      "test loss is 0.00232579812769052\n",
      "Batch: 22800,train loss is: 0.000945454515748419\n",
      "test loss is 0.0015966351869741036\n",
      "Batch: 22900,train loss is: 0.001011972928808786\n",
      "test loss is 0.0015063146952963688\n",
      "Batch: 23000,train loss is: 0.0017181846433712627\n",
      "test loss is 0.0016680753171756893\n",
      "Batch: 23100,train loss is: 0.00083167636889611\n",
      "test loss is 0.0016215119356702442\n",
      "Batch: 23200,train loss is: 0.000661571759353693\n",
      "test loss is 0.002369713949206495\n",
      "Batch: 23300,train loss is: 0.0015323945517894968\n",
      "test loss is 0.001810572163342217\n",
      "Batch: 23400,train loss is: 0.002917928438743789\n",
      "test loss is 0.0025272855019355177\n",
      "Batch: 23500,train loss is: 0.0028163889361735445\n",
      "test loss is 0.0016481573277512354\n",
      "Batch: 23600,train loss is: 0.0013244627737892562\n",
      "test loss is 0.0026058075739945597\n",
      "Batch: 23700,train loss is: 0.0015930392587083565\n",
      "test loss is 0.0015350243334896296\n",
      "Batch: 23800,train loss is: 0.005413304045560003\n",
      "test loss is 0.0018957322459466734\n",
      "Batch: 23900,train loss is: 0.0033734689287393325\n",
      "test loss is 0.0032443078294270324\n",
      "Batch: 24000,train loss is: 0.0008393192402761332\n",
      "test loss is 0.0022044245212096323\n",
      "Batch: 24100,train loss is: 0.0015163541287112078\n",
      "test loss is 0.0018177277969529987\n",
      "Batch: 24200,train loss is: 0.0038543605822142337\n",
      "test loss is 0.0015452033962858056\n",
      "Batch: 24300,train loss is: 0.008212814600592236\n",
      "test loss is 0.003951584397504213\n",
      "Batch: 24400,train loss is: 0.004165083261739074\n",
      "test loss is 0.0022674710229833063\n",
      "Batch: 24500,train loss is: 0.0007959746820584482\n",
      "test loss is 0.0025449411533618686\n",
      "Batch: 24600,train loss is: 0.003721356118150291\n",
      "test loss is 0.001841156163530568\n",
      "Batch: 24700,train loss is: 0.0007520540296679956\n",
      "test loss is 0.0015113054301394773\n",
      "Batch: 24800,train loss is: 0.006055646669854347\n",
      "test loss is 0.006892688108106759\n",
      "Batch: 24900,train loss is: 0.004696883284470399\n",
      "test loss is 0.0028965795220761295\n",
      "Batch: 25000,train loss is: 0.0012079806912661543\n",
      "test loss is 0.001764663959500711\n",
      "Batch: 25100,train loss is: 0.0011672471631794291\n",
      "test loss is 0.001618589434843016\n",
      "Batch: 25200,train loss is: 0.003007185863907716\n",
      "test loss is 0.001428948989404389\n",
      "Batch: 25300,train loss is: 0.002868717081889191\n",
      "test loss is 0.0020041304299294595\n",
      "Batch: 25400,train loss is: 0.0016777700212788312\n",
      "test loss is 0.0017879133117472763\n",
      "Batch: 25500,train loss is: 0.0026203667095229476\n",
      "test loss is 0.001971977917364333\n",
      "Batch: 25600,train loss is: 0.0018606736027018435\n",
      "test loss is 0.001379452794724937\n",
      "Batch: 25700,train loss is: 0.0014225377035023438\n",
      "test loss is 0.001983372337554704\n",
      "Batch: 25800,train loss is: 0.002528024457510545\n",
      "test loss is 0.002369173569515592\n",
      "Batch: 25900,train loss is: 0.0009542304094435748\n",
      "test loss is 0.0021819101909162295\n",
      "Batch: 26000,train loss is: 0.003517582748709647\n",
      "test loss is 0.0030594991558424286\n",
      "Batch: 26100,train loss is: 0.002056873830378378\n",
      "test loss is 0.002873276806379804\n",
      "Batch: 26200,train loss is: 0.0037649049566250655\n",
      "test loss is 0.00278109181795422\n",
      "Batch: 26300,train loss is: 0.0014057180888979515\n",
      "test loss is 0.0015524774311103334\n",
      "Batch: 26400,train loss is: 0.0019480971911590098\n",
      "test loss is 0.0017780622985597947\n",
      "Batch: 26500,train loss is: 0.0009411094387545175\n",
      "test loss is 0.0016177971017867152\n",
      "Batch: 26600,train loss is: 0.0009530994402735726\n",
      "test loss is 0.0015979676975588356\n",
      "Batch: 26700,train loss is: 0.0009946651083731128\n",
      "test loss is 0.0021310791405863448\n",
      "Batch: 26800,train loss is: 0.0022649399019333393\n",
      "test loss is 0.001728703984104483\n",
      "Batch: 26900,train loss is: 0.0007627360462571424\n",
      "test loss is 0.0016538686013896952\n",
      "Batch: 27000,train loss is: 0.006200457801834479\n",
      "test loss is 0.003859268877693015\n",
      "Batch: 27100,train loss is: 0.001646142114593424\n",
      "test loss is 0.003461375002933425\n",
      "Batch: 27200,train loss is: 0.006259921399026658\n",
      "test loss is 0.004085606096666208\n",
      "Batch: 27300,train loss is: 0.0006892928522718184\n",
      "test loss is 0.0022045547869659655\n",
      "Batch: 27400,train loss is: 0.0014175687564426083\n",
      "test loss is 0.0014768117556754876\n",
      "Batch: 27500,train loss is: 0.0031950542095452615\n",
      "test loss is 0.0019059596688334277\n",
      "Batch: 27600,train loss is: 0.0013407177162004513\n",
      "test loss is 0.002329925680415227\n",
      "Batch: 27700,train loss is: 0.004912460342628278\n",
      "test loss is 0.0018066307299383468\n",
      "Batch: 27800,train loss is: 0.0008740456584788562\n",
      "test loss is 0.0041446229518189584\n",
      "Batch: 27900,train loss is: 0.0020971204215925244\n",
      "test loss is 0.0019443448849271949\n",
      "Batch: 28000,train loss is: 0.0014143398494868423\n",
      "test loss is 0.0019353607617595503\n",
      "Batch: 28100,train loss is: 0.004531752987594813\n",
      "test loss is 0.00285528269569634\n",
      "Batch: 28200,train loss is: 0.006116366156099314\n",
      "test loss is 0.0023129297722113665\n",
      "Batch: 28300,train loss is: 0.0050951267938834505\n",
      "test loss is 0.001560752811791463\n",
      "Batch: 28400,train loss is: 0.0031985425196778975\n",
      "test loss is 0.001588685355469145\n",
      "Batch: 28500,train loss is: 0.004510953603538931\n",
      "test loss is 0.0024143130475985704\n",
      "Batch: 28600,train loss is: 0.0038997526465245243\n",
      "test loss is 0.002248148710724918\n",
      "Batch: 28700,train loss is: 0.0007946482449472197\n",
      "test loss is 0.0019750944809615793\n",
      "Batch: 28800,train loss is: 0.004636798947770855\n",
      "test loss is 0.0016499097682923436\n",
      "Batch: 28900,train loss is: 0.001727191936883978\n",
      "test loss is 0.002122381758720909\n",
      "Batch: 29000,train loss is: 0.0009720924200322265\n",
      "test loss is 0.0017912217939404693\n",
      "Batch: 29100,train loss is: 0.0018075588707793396\n",
      "test loss is 0.001448347585147136\n",
      "Batch: 29200,train loss is: 0.0013352283650619862\n",
      "test loss is 0.002643884063054275\n",
      "Batch: 29300,train loss is: 0.0016386035268463287\n",
      "test loss is 0.001753126658127238\n",
      "Batch: 29400,train loss is: 0.0011552543022917887\n",
      "test loss is 0.0021881007642110227\n",
      "Batch: 29500,train loss is: 0.0008379149526016859\n",
      "test loss is 0.002571666001498439\n",
      "Batch: 29600,train loss is: 0.0013069915582440007\n",
      "test loss is 0.0013571767480562043\n",
      "Batch: 29700,train loss is: 0.0004779106589029552\n",
      "test loss is 0.0020114034984363683\n",
      "Batch: 29800,train loss is: 0.002738294085336579\n",
      "test loss is 0.0012809905003697375\n",
      "Batch: 29900,train loss is: 0.007657292150002326\n",
      "test loss is 0.0017741302942688119\n",
      "Batch: 30000,train loss is: 0.0010227355153350779\n",
      "test loss is 0.0015059982721322576\n",
      "Batch: 30100,train loss is: 0.0013664234665214242\n",
      "test loss is 0.0019195365674799906\n",
      "Batch: 30200,train loss is: 0.001264018071783873\n",
      "test loss is 0.0015132399257033274\n",
      "Batch: 30300,train loss is: 0.001097588914103997\n",
      "test loss is 0.0018543884078125927\n",
      "Batch: 30400,train loss is: 0.00222593032664667\n",
      "test loss is 0.002485273608280969\n",
      "Batch: 30500,train loss is: 0.0010308995456859038\n",
      "test loss is 0.0016829360872571038\n",
      "Batch: 30600,train loss is: 0.0012124945425195806\n",
      "test loss is 0.002353364429389559\n",
      "Batch: 30700,train loss is: 0.001067943803731612\n",
      "test loss is 0.002960438162431244\n",
      "Batch: 30800,train loss is: 0.0005331462566290635\n",
      "test loss is 0.001747864770951767\n",
      "Batch: 30900,train loss is: 0.0013097263931040428\n",
      "test loss is 0.0021016159553523843\n",
      "Batch: 31000,train loss is: 0.0007379146740284025\n",
      "test loss is 0.0016651139303507903\n",
      "Batch: 31100,train loss is: 0.0019612155017403633\n",
      "test loss is 0.004162056983669967\n",
      "Batch: 31200,train loss is: 0.0028110220768389383\n",
      "test loss is 0.0016942161466119508\n",
      "Batch: 31300,train loss is: 0.0015726338068437534\n",
      "test loss is 0.0017077585697257494\n",
      "Batch: 31400,train loss is: 0.0010317216555856374\n",
      "test loss is 0.001833274733713794\n",
      "Batch: 31500,train loss is: 0.0011820904289670119\n",
      "test loss is 0.0022241876356196106\n",
      "Batch: 31600,train loss is: 0.0007724998352532538\n",
      "test loss is 0.001862833049399445\n",
      "Batch: 31700,train loss is: 0.0006701896864274158\n",
      "test loss is 0.001379408598284082\n",
      "Batch: 31800,train loss is: 0.0016414397553132624\n",
      "test loss is 0.0017198219680214075\n",
      "Batch: 31900,train loss is: 0.006479748414915014\n",
      "test loss is 0.003245425544634691\n",
      "Batch: 32000,train loss is: 0.015610811615242016\n",
      "test loss is 0.0017742872185873035\n",
      "Batch: 32100,train loss is: 0.0014043491313807989\n",
      "test loss is 0.0025082590204304244\n",
      "Batch: 32200,train loss is: 0.0015381885014219337\n",
      "test loss is 0.002685727659880312\n",
      "Batch: 32300,train loss is: 0.0016958714046550554\n",
      "test loss is 0.0019087731885066598\n",
      "Batch: 32400,train loss is: 0.0006809302548338912\n",
      "test loss is 0.0030480076804264046\n",
      "Batch: 32500,train loss is: 0.009489067272969418\n",
      "test loss is 0.005018550668957084\n",
      "Batch: 32600,train loss is: 0.002442928295381225\n",
      "test loss is 0.0017835799696674164\n",
      "Batch: 32700,train loss is: 0.001103101788836948\n",
      "test loss is 0.0021309063020105577\n",
      "Batch: 32800,train loss is: 0.0008339063995181439\n",
      "test loss is 0.002005307558825765\n",
      "Batch: 32900,train loss is: 0.001905169041397571\n",
      "test loss is 0.001627154092419836\n",
      "Batch: 33000,train loss is: 0.001012667367560619\n",
      "test loss is 0.0017305510977437516\n",
      "Batch: 33100,train loss is: 0.001906071439805184\n",
      "test loss is 0.002517709958448332\n",
      "Batch: 33200,train loss is: 0.0009701080400653042\n",
      "test loss is 0.0015207938587205423\n",
      "Batch: 33300,train loss is: 0.0009297113421441211\n",
      "test loss is 0.0014298444906364251\n",
      "Batch: 33400,train loss is: 0.0035829647662900294\n",
      "test loss is 0.002495876882644273\n",
      "Batch: 33500,train loss is: 0.0008158165394846497\n",
      "test loss is 0.0020119818801822105\n",
      "Batch: 33600,train loss is: 0.0008572072648978318\n",
      "test loss is 0.00207957849240073\n",
      "Batch: 33700,train loss is: 0.0017848902203753117\n",
      "test loss is 0.0018581368064844435\n",
      "Batch: 33800,train loss is: 0.005908355845139001\n",
      "test loss is 0.002030795868226396\n",
      "Batch: 33900,train loss is: 0.0032178005731697823\n",
      "test loss is 0.0018473434171573234\n",
      "-----------------------Epoch: 3----------------------------------\n",
      "Batch: 0,train loss is: 0.0015656461342836575\n",
      "test loss is 0.0017035967805672872\n",
      "Batch: 100,train loss is: 0.0018854268277224734\n",
      "test loss is 0.0016599234801109608\n",
      "Batch: 200,train loss is: 0.0012632941261845347\n",
      "test loss is 0.001333644931337976\n",
      "Batch: 300,train loss is: 0.0006489302628085823\n",
      "test loss is 0.0015953338354973243\n",
      "Batch: 400,train loss is: 0.0012141217620875236\n",
      "test loss is 0.002134309479008444\n",
      "Batch: 500,train loss is: 0.0008659450020796814\n",
      "test loss is 0.001537659398979241\n",
      "Batch: 600,train loss is: 0.00030601699372730914\n",
      "test loss is 0.002035149354868492\n",
      "Batch: 700,train loss is: 0.0007395563431830804\n",
      "test loss is 0.0013778938600037345\n",
      "Batch: 800,train loss is: 0.0021826707054525495\n",
      "test loss is 0.002066035291167609\n",
      "Batch: 900,train loss is: 0.0015432270360300718\n",
      "test loss is 0.002931062806853252\n",
      "Batch: 1000,train loss is: 0.0005418802220958582\n",
      "test loss is 0.0023329021477557863\n",
      "Batch: 1100,train loss is: 0.0051277382679933625\n",
      "test loss is 0.0020856190208615164\n",
      "Batch: 1200,train loss is: 0.000704085080952855\n",
      "test loss is 0.0036233789033249014\n",
      "Batch: 1300,train loss is: 0.0008372590332013242\n",
      "test loss is 0.0014050209425536917\n",
      "Batch: 1400,train loss is: 0.0015327749394463228\n",
      "test loss is 0.0016908869783044697\n",
      "Batch: 1500,train loss is: 0.002061215515924226\n",
      "test loss is 0.002633290201229726\n",
      "Batch: 1600,train loss is: 0.0007126342847111933\n",
      "test loss is 0.0024197361808431165\n",
      "Batch: 1700,train loss is: 0.00869113444009859\n",
      "test loss is 0.001724315037049523\n",
      "Batch: 1800,train loss is: 0.0005476279349572674\n",
      "test loss is 0.0013902542978133155\n",
      "Batch: 1900,train loss is: 0.0026684016595191957\n",
      "test loss is 0.0029505738198687195\n",
      "Batch: 2000,train loss is: 0.0023304057297576877\n",
      "test loss is 0.0022945057281595995\n",
      "Batch: 2100,train loss is: 0.0014833106207451886\n",
      "test loss is 0.002158190376600103\n",
      "Batch: 2200,train loss is: 0.0016018703966149525\n",
      "test loss is 0.0014574501728138374\n",
      "Batch: 2300,train loss is: 0.0019810051443919932\n",
      "test loss is 0.0023577120298335\n",
      "Batch: 2400,train loss is: 0.001632999472530361\n",
      "test loss is 0.0014695471999842495\n",
      "Batch: 2500,train loss is: 0.002590899301458478\n",
      "test loss is 0.001810730214988941\n",
      "Batch: 2600,train loss is: 0.0013137179796909426\n",
      "test loss is 0.0020342746148475\n",
      "Batch: 2700,train loss is: 0.0030002233993496634\n",
      "test loss is 0.001437829134089761\n",
      "Batch: 2800,train loss is: 0.0011255297805089473\n",
      "test loss is 0.0022977945482412303\n",
      "Batch: 2900,train loss is: 0.0007452431066250512\n",
      "test loss is 0.0012533195631047383\n",
      "Batch: 3000,train loss is: 0.0012441742555087737\n",
      "test loss is 0.0015317851626736833\n",
      "Batch: 3100,train loss is: 0.0014679028874653138\n",
      "test loss is 0.0015066044218815834\n",
      "Batch: 3200,train loss is: 0.007010583180948615\n",
      "test loss is 0.002841076291078349\n",
      "Batch: 3300,train loss is: 0.0005804608803090377\n",
      "test loss is 0.001525933176147743\n",
      "Batch: 3400,train loss is: 0.0029675818305546437\n",
      "test loss is 0.001991187440042195\n",
      "Batch: 3500,train loss is: 0.0009369853187326335\n",
      "test loss is 0.0025295799706046645\n",
      "Batch: 3600,train loss is: 0.0009334023290589798\n",
      "test loss is 0.002674418386140701\n",
      "Batch: 3700,train loss is: 0.002412242550150481\n",
      "test loss is 0.0024914068292121315\n",
      "Batch: 3800,train loss is: 0.001396775492755935\n",
      "test loss is 0.0018480197689683792\n",
      "Batch: 3900,train loss is: 0.0022914832946372694\n",
      "test loss is 0.001538693351398556\n",
      "Batch: 4000,train loss is: 0.0005397579722785862\n",
      "test loss is 0.00199235111925042\n",
      "Batch: 4100,train loss is: 0.0010058630761324964\n",
      "test loss is 0.0014387618889152775\n",
      "Batch: 4200,train loss is: 0.0013033315533057665\n",
      "test loss is 0.0017412427084870445\n",
      "Batch: 4300,train loss is: 0.0013819571116441042\n",
      "test loss is 0.0026434236293957055\n",
      "Batch: 4400,train loss is: 0.0016082582216439775\n",
      "test loss is 0.0021137418093732352\n",
      "Batch: 4500,train loss is: 0.011091528237091117\n",
      "test loss is 0.0060162702705585745\n",
      "Batch: 4600,train loss is: 0.001587545100373483\n",
      "test loss is 0.002649293953357858\n",
      "Batch: 4700,train loss is: 0.002853729469465305\n",
      "test loss is 0.0023757880167814736\n",
      "Batch: 4800,train loss is: 0.0030172778046880263\n",
      "test loss is 0.002992869710080939\n",
      "Batch: 4900,train loss is: 0.0008361103466838799\n",
      "test loss is 0.0014026102620687136\n",
      "Batch: 5000,train loss is: 0.000563973112175195\n",
      "test loss is 0.0014139971840905448\n",
      "Batch: 5100,train loss is: 0.0011239624008258975\n",
      "test loss is 0.0014569079165847544\n",
      "Batch: 5200,train loss is: 0.0005256703701958822\n",
      "test loss is 0.0013099793481998834\n",
      "Batch: 5300,train loss is: 0.00210041080039903\n",
      "test loss is 0.0027552471869796073\n",
      "Batch: 5400,train loss is: 0.001760872560183759\n",
      "test loss is 0.0016509834766589217\n",
      "Batch: 5500,train loss is: 0.00198432156702427\n",
      "test loss is 0.0015667530427286205\n",
      "Batch: 5600,train loss is: 0.0010971897814444\n",
      "test loss is 0.001495699018524834\n",
      "Batch: 5700,train loss is: 0.0009287727664358934\n",
      "test loss is 0.0017114212931823747\n",
      "Batch: 5800,train loss is: 0.0021265871358395003\n",
      "test loss is 0.001678096733495777\n",
      "Batch: 5900,train loss is: 0.0048138979173867865\n",
      "test loss is 0.00169876535480867\n",
      "Batch: 6000,train loss is: 0.0018751161632598523\n",
      "test loss is 0.0041814689896551675\n",
      "Batch: 6100,train loss is: 0.000937873529862166\n",
      "test loss is 0.0026949851540403486\n",
      "Batch: 6200,train loss is: 0.0006908657912371183\n",
      "test loss is 0.0015327114255987862\n",
      "Batch: 6300,train loss is: 0.0019948555448563005\n",
      "test loss is 0.0017608571895427335\n",
      "Batch: 6400,train loss is: 0.0008964973500387842\n",
      "test loss is 0.0037926320495996735\n",
      "Batch: 6500,train loss is: 0.0006331326591958789\n",
      "test loss is 0.0018964603786775656\n",
      "Batch: 6600,train loss is: 0.005335979958254211\n",
      "test loss is 0.001553582645081467\n",
      "Batch: 6700,train loss is: 0.0009226534259805647\n",
      "test loss is 0.0016237421178552063\n",
      "Batch: 6800,train loss is: 0.003037905871260143\n",
      "test loss is 0.0015228138587860857\n",
      "Batch: 6900,train loss is: 0.002533746084651405\n",
      "test loss is 0.0013166705571659473\n",
      "Batch: 7000,train loss is: 0.0019576010335755375\n",
      "test loss is 0.005701474491639307\n",
      "Batch: 7100,train loss is: 0.001682282768933486\n",
      "test loss is 0.0017360738869042955\n",
      "Batch: 7200,train loss is: 0.0012848453884833002\n",
      "test loss is 0.0013663657020247185\n",
      "Batch: 7300,train loss is: 0.0032139321618091343\n",
      "test loss is 0.0022689356094671167\n",
      "Batch: 7400,train loss is: 0.0009141316614599661\n",
      "test loss is 0.003182788818070706\n",
      "Batch: 7500,train loss is: 0.002398224101797785\n",
      "test loss is 0.0022956782902076793\n",
      "Batch: 7600,train loss is: 0.0015601878173968347\n",
      "test loss is 0.001697891871420174\n",
      "Batch: 7700,train loss is: 0.0011683906058302899\n",
      "test loss is 0.001257915352506715\n",
      "Batch: 7800,train loss is: 0.001597662397214438\n",
      "test loss is 0.0017174523451217844\n",
      "Batch: 7900,train loss is: 0.0007042584050462073\n",
      "test loss is 0.001714409885289397\n",
      "Batch: 8000,train loss is: 0.0005498795677332223\n",
      "test loss is 0.0023883885542914066\n",
      "Batch: 8100,train loss is: 0.0021490851189377128\n",
      "test loss is 0.001561407179703146\n",
      "Batch: 8200,train loss is: 0.003201265443988679\n",
      "test loss is 0.0019076064570873563\n",
      "Batch: 8300,train loss is: 0.0005068050299577773\n",
      "test loss is 0.0016500516169182875\n",
      "Batch: 8400,train loss is: 0.0021601706101903742\n",
      "test loss is 0.0023304844458489054\n",
      "Batch: 8500,train loss is: 0.0014589580213719062\n",
      "test loss is 0.0023703238999774087\n",
      "Batch: 8600,train loss is: 0.0032516384044997356\n",
      "test loss is 0.003054876341506266\n",
      "Batch: 8700,train loss is: 0.0005582942004407201\n",
      "test loss is 0.002469858646318923\n",
      "Batch: 8800,train loss is: 0.0006538258003324435\n",
      "test loss is 0.001612362706690353\n",
      "Batch: 8900,train loss is: 0.002178486758746957\n",
      "test loss is 0.0012930426031130544\n",
      "Batch: 9000,train loss is: 0.0003838692678841578\n",
      "test loss is 0.001570041631080926\n",
      "Batch: 9100,train loss is: 0.0005473807446011764\n",
      "test loss is 0.0016847723123106886\n",
      "Batch: 9200,train loss is: 0.005605411842528313\n",
      "test loss is 0.003535767682405483\n",
      "Batch: 9300,train loss is: 0.0009235625612178021\n",
      "test loss is 0.0022354011628410017\n",
      "Batch: 9400,train loss is: 0.0013358987828403493\n",
      "test loss is 0.0024086760736932306\n",
      "Batch: 9500,train loss is: 0.0013345593715668818\n",
      "test loss is 0.0028360167733365253\n",
      "Batch: 9600,train loss is: 0.0030925221921677152\n",
      "test loss is 0.002211809496599347\n",
      "Batch: 9700,train loss is: 0.0009789621180004105\n",
      "test loss is 0.0020035193446004346\n",
      "Batch: 9800,train loss is: 0.0011154465437546528\n",
      "test loss is 0.0018557743096806573\n",
      "Batch: 9900,train loss is: 0.001025714203713258\n",
      "test loss is 0.0015916430889792933\n",
      "Batch: 10000,train loss is: 0.001006483757328324\n",
      "test loss is 0.0015145614443436474\n",
      "Batch: 10100,train loss is: 0.0019606046558528923\n",
      "test loss is 0.0015185560133146412\n",
      "Batch: 10200,train loss is: 0.0073614062184120775\n",
      "test loss is 0.0014426499706933088\n",
      "Batch: 10300,train loss is: 0.001480527320714323\n",
      "test loss is 0.002309443583307213\n",
      "Batch: 10400,train loss is: 0.0005852155666116176\n",
      "test loss is 0.002405862886976059\n",
      "Batch: 10500,train loss is: 0.03135639423394392\n",
      "test loss is 0.014114498203865912\n",
      "Batch: 10600,train loss is: 0.0025188154440655337\n",
      "test loss is 0.0026037850626584353\n",
      "Batch: 10700,train loss is: 0.0011335247265412372\n",
      "test loss is 0.0016751347787198557\n",
      "Batch: 10800,train loss is: 0.0008283082902024723\n",
      "test loss is 0.0014219015719298708\n",
      "Batch: 10900,train loss is: 0.0007753471820312582\n",
      "test loss is 0.0014170290832223307\n",
      "Batch: 11000,train loss is: 0.0008036295107590688\n",
      "test loss is 0.0013016674711820945\n",
      "Batch: 11100,train loss is: 0.00174482026975298\n",
      "test loss is 0.0013208610189208271\n",
      "Batch: 11200,train loss is: 0.0012122249032543908\n",
      "test loss is 0.0013941587837446516\n",
      "Batch: 11300,train loss is: 0.0008454507381159351\n",
      "test loss is 0.001647485056603364\n",
      "Batch: 11400,train loss is: 0.0013938934924914075\n",
      "test loss is 0.0015531579059043742\n",
      "Batch: 11500,train loss is: 0.0008016279590989647\n",
      "test loss is 0.001635063132533165\n",
      "Batch: 11600,train loss is: 0.006469454717256947\n",
      "test loss is 0.004713149269351538\n",
      "Batch: 11700,train loss is: 0.0015203830092411692\n",
      "test loss is 0.0019974423552961205\n",
      "Batch: 11800,train loss is: 0.0009548791007461968\n",
      "test loss is 0.0022537216567388714\n",
      "Batch: 11900,train loss is: 0.0021815390101580776\n",
      "test loss is 0.003133741048528294\n",
      "Batch: 12000,train loss is: 0.0017115676514953858\n",
      "test loss is 0.0022590777763529277\n",
      "Batch: 12100,train loss is: 0.006301373363018359\n",
      "test loss is 0.0015406892388981695\n",
      "Batch: 12200,train loss is: 0.0013200957864990725\n",
      "test loss is 0.00240232534321946\n",
      "Batch: 12300,train loss is: 0.0009539868974301059\n",
      "test loss is 0.0017289926209410722\n",
      "Batch: 12400,train loss is: 0.0010126178655666643\n",
      "test loss is 0.0015359720933947735\n",
      "Batch: 12500,train loss is: 0.000937552159076567\n",
      "test loss is 0.001309453500528094\n",
      "Batch: 12600,train loss is: 0.0007929764385740947\n",
      "test loss is 0.0017721036049554312\n",
      "Batch: 12700,train loss is: 0.001306891180995485\n",
      "test loss is 0.0012701081049650822\n",
      "Batch: 12800,train loss is: 0.0013219472302498946\n",
      "test loss is 0.0016136985316670103\n",
      "Batch: 12900,train loss is: 0.002271285627983158\n",
      "test loss is 0.001971573210689625\n",
      "Batch: 13000,train loss is: 0.001061963310618022\n",
      "test loss is 0.0015565541932191214\n",
      "Batch: 13100,train loss is: 0.001024172408775454\n",
      "test loss is 0.0016822436612476215\n",
      "Batch: 13200,train loss is: 0.0022277318896793082\n",
      "test loss is 0.0021082924085524765\n",
      "Batch: 13300,train loss is: 0.002016772895978591\n",
      "test loss is 0.0038292095043751377\n",
      "Batch: 13400,train loss is: 0.0018082793488736584\n",
      "test loss is 0.002073882397858346\n",
      "Batch: 13500,train loss is: 0.0038552843201695784\n",
      "test loss is 0.002923531672565546\n",
      "Batch: 13600,train loss is: 0.001043398639937972\n",
      "test loss is 0.0013518500845808507\n",
      "Batch: 13700,train loss is: 0.0008727195272090609\n",
      "test loss is 0.00166354428264301\n",
      "Batch: 13800,train loss is: 0.0017814766854329978\n",
      "test loss is 0.0024220797679024406\n",
      "Batch: 13900,train loss is: 0.0009886173402180965\n",
      "test loss is 0.001795321393847397\n",
      "Batch: 14000,train loss is: 0.0007117992003380042\n",
      "test loss is 0.0019647070598976333\n",
      "Batch: 14100,train loss is: 0.0010699464627500366\n",
      "test loss is 0.0016346713882231412\n",
      "Batch: 14200,train loss is: 0.0008079699111982953\n",
      "test loss is 0.0013339351093714028\n",
      "Batch: 14300,train loss is: 0.0007884697818497516\n",
      "test loss is 0.001768284508438151\n",
      "Batch: 14400,train loss is: 0.0021176969563448996\n",
      "test loss is 0.001470681944562099\n",
      "Batch: 14500,train loss is: 0.0036057853537478143\n",
      "test loss is 0.0022672115583600213\n",
      "Batch: 14600,train loss is: 0.003743697590509823\n",
      "test loss is 0.0013224862837027795\n",
      "Batch: 14700,train loss is: 0.001257583609027281\n",
      "test loss is 0.0018836211720476578\n",
      "Batch: 14800,train loss is: 0.0015298194603284665\n",
      "test loss is 0.0020512718598819603\n",
      "Batch: 14900,train loss is: 0.0012653696327903244\n",
      "test loss is 0.00149157413648803\n",
      "Batch: 15000,train loss is: 0.002687335023773004\n",
      "test loss is 0.0018047851298776062\n",
      "Batch: 15100,train loss is: 0.0018271782893496246\n",
      "test loss is 0.00152224124935222\n",
      "Batch: 15200,train loss is: 0.002547086150600367\n",
      "test loss is 0.0015375585935094467\n",
      "Batch: 15300,train loss is: 0.0027080141318292826\n",
      "test loss is 0.0018292804284891964\n",
      "Batch: 15400,train loss is: 0.0010304655535707209\n",
      "test loss is 0.0030053257115638357\n",
      "Batch: 15500,train loss is: 0.001777866206657231\n",
      "test loss is 0.0013883092579123205\n",
      "Batch: 15600,train loss is: 0.0029586289154884186\n",
      "test loss is 0.0024783911231011785\n",
      "Batch: 15700,train loss is: 0.0011944116563357152\n",
      "test loss is 0.0024389639036747754\n",
      "Batch: 15800,train loss is: 0.001355322443347326\n",
      "test loss is 0.002160405926722242\n",
      "Batch: 15900,train loss is: 0.0012270000820209976\n",
      "test loss is 0.0018653637458330783\n",
      "Batch: 16000,train loss is: 0.0016031931468757912\n",
      "test loss is 0.002636752786710643\n",
      "Batch: 16100,train loss is: 0.0028901344027805705\n",
      "test loss is 0.0030579838084373156\n",
      "Batch: 16200,train loss is: 0.0011609719656039753\n",
      "test loss is 0.00151228835927298\n",
      "Batch: 16300,train loss is: 0.0011873233421804534\n",
      "test loss is 0.0014033780098308226\n",
      "Batch: 16400,train loss is: 0.000508983483441838\n",
      "test loss is 0.0011954680879207316\n",
      "Batch: 16500,train loss is: 0.004793731992894373\n",
      "test loss is 0.006212185499515712\n",
      "Batch: 16600,train loss is: 0.0007535535314444011\n",
      "test loss is 0.0018987859581154384\n",
      "Batch: 16700,train loss is: 0.0014335874063489873\n",
      "test loss is 0.0016634541618789289\n",
      "Batch: 16800,train loss is: 0.0007397624739258926\n",
      "test loss is 0.0016256248289492376\n",
      "Batch: 16900,train loss is: 0.001962536148460819\n",
      "test loss is 0.0019519699505625597\n",
      "Batch: 17000,train loss is: 0.0016869780896331093\n",
      "test loss is 0.0018798913614999052\n",
      "Batch: 17100,train loss is: 0.0009819022043123494\n",
      "test loss is 0.0018541149447285556\n",
      "Batch: 17200,train loss is: 0.0009641034596741387\n",
      "test loss is 0.0014639972442599975\n",
      "Batch: 17300,train loss is: 0.001504744716272015\n",
      "test loss is 0.0014804830545892859\n",
      "Batch: 17400,train loss is: 0.001467253561857379\n",
      "test loss is 0.002012024166533102\n",
      "Batch: 17500,train loss is: 0.0016488143682451693\n",
      "test loss is 0.001638646122047641\n",
      "Batch: 17600,train loss is: 0.0007810154819793718\n",
      "test loss is 0.0018036882513697329\n",
      "Batch: 17700,train loss is: 0.0012970714564478486\n",
      "test loss is 0.0018497764435695533\n",
      "Batch: 17800,train loss is: 0.0023309386846483552\n",
      "test loss is 0.0014882150244938655\n",
      "Batch: 17900,train loss is: 0.0011758605548355095\n",
      "test loss is 0.001509678516857209\n",
      "Batch: 18000,train loss is: 0.0007743118524997583\n",
      "test loss is 0.001364603331574258\n",
      "Batch: 18100,train loss is: 0.0009285859420301607\n",
      "test loss is 0.0023663409389237115\n",
      "Batch: 18200,train loss is: 0.0008742276747348436\n",
      "test loss is 0.0019320841740964351\n",
      "Batch: 18300,train loss is: 0.0019149762393448314\n",
      "test loss is 0.001282499668379378\n",
      "Batch: 18400,train loss is: 0.0004977499278927912\n",
      "test loss is 0.001379537019462645\n",
      "Batch: 18500,train loss is: 0.0008231131303191298\n",
      "test loss is 0.0031201802326777267\n",
      "Batch: 18600,train loss is: 0.0027944362414198064\n",
      "test loss is 0.0031667114814608495\n",
      "Batch: 18700,train loss is: 0.001178835076899928\n",
      "test loss is 0.0019336867709268462\n",
      "Batch: 18800,train loss is: 0.0012490620007404947\n",
      "test loss is 0.0016169614383175054\n",
      "Batch: 18900,train loss is: 0.0014773995250239398\n",
      "test loss is 0.0015786867743738241\n",
      "Batch: 19000,train loss is: 0.0011034940484369303\n",
      "test loss is 0.0014863401240761348\n",
      "Batch: 19100,train loss is: 0.0009322312489035995\n",
      "test loss is 0.001510095541360412\n",
      "Batch: 19200,train loss is: 0.0010995923552858983\n",
      "test loss is 0.001407755129563091\n",
      "Batch: 19300,train loss is: 0.0024760360615170875\n",
      "test loss is 0.0014830168139922618\n",
      "Batch: 19400,train loss is: 0.0009495496049503196\n",
      "test loss is 0.0012982439704681264\n",
      "Batch: 19500,train loss is: 0.001112398168654936\n",
      "test loss is 0.0015641029558802876\n",
      "Batch: 19600,train loss is: 0.0005396099935108146\n",
      "test loss is 0.0013971364237739122\n",
      "Batch: 19700,train loss is: 0.0011509862786428702\n",
      "test loss is 0.001491742486327876\n",
      "Batch: 19800,train loss is: 0.0007061785506575349\n",
      "test loss is 0.0017055735645691856\n",
      "Batch: 19900,train loss is: 0.0012285874962796627\n",
      "test loss is 0.0028622134685588035\n",
      "Batch: 20000,train loss is: 0.0016591464097012007\n",
      "test loss is 0.0017691068836076288\n",
      "Batch: 20100,train loss is: 0.001464451850014175\n",
      "test loss is 0.0015853978586011265\n",
      "Batch: 20200,train loss is: 0.001181030339445217\n",
      "test loss is 0.0013807541593642968\n",
      "Batch: 20300,train loss is: 0.0016498438376002795\n",
      "test loss is 0.0012295932147922504\n",
      "Batch: 20400,train loss is: 0.002670064807336951\n",
      "test loss is 0.0022384437437640904\n",
      "Batch: 20500,train loss is: 0.001034558173611831\n",
      "test loss is 0.002530558303753269\n",
      "Batch: 20600,train loss is: 0.001996614219669685\n",
      "test loss is 0.0030904791081740337\n",
      "Batch: 20700,train loss is: 0.0013814395496116478\n",
      "test loss is 0.0033854240236950734\n",
      "Batch: 20800,train loss is: 0.0007976248448606003\n",
      "test loss is 0.001866818279522506\n",
      "Batch: 20900,train loss is: 0.001978368826385798\n",
      "test loss is 0.0014694597970850581\n",
      "Batch: 21000,train loss is: 0.0008391062257286796\n",
      "test loss is 0.0028343160323904457\n",
      "Batch: 21100,train loss is: 0.00075103358281512\n",
      "test loss is 0.0015921215913861884\n",
      "Batch: 21200,train loss is: 0.005601118578569256\n",
      "test loss is 0.004611093374415471\n",
      "Batch: 21300,train loss is: 0.0006155162804699657\n",
      "test loss is 0.0013359687806097057\n",
      "Batch: 21400,train loss is: 0.0009531195394689395\n",
      "test loss is 0.0017502065098619099\n",
      "Batch: 21500,train loss is: 0.0010499720019520485\n",
      "test loss is 0.001362082126374825\n",
      "Batch: 21600,train loss is: 0.002254508876253116\n",
      "test loss is 0.0013690714997105387\n",
      "Batch: 21700,train loss is: 0.001852424195283024\n",
      "test loss is 0.0023023863190369653\n",
      "Batch: 21800,train loss is: 0.0012179525155020068\n",
      "test loss is 0.001273264720313686\n",
      "Batch: 21900,train loss is: 0.0008768425100323596\n",
      "test loss is 0.0019471011129833075\n",
      "Batch: 22000,train loss is: 0.0027977685106960744\n",
      "test loss is 0.002561056241614518\n",
      "Batch: 22100,train loss is: 0.000970206911499814\n",
      "test loss is 0.0017837894861848754\n",
      "Batch: 22200,train loss is: 0.0009980378027663209\n",
      "test loss is 0.0014741806509761034\n",
      "Batch: 22300,train loss is: 0.002406210202451378\n",
      "test loss is 0.0024621882009962597\n",
      "Batch: 22400,train loss is: 0.0016858469399406069\n",
      "test loss is 0.001759614062682636\n",
      "Batch: 22500,train loss is: 0.0016151966166256784\n",
      "test loss is 0.002136727181291121\n",
      "Batch: 22600,train loss is: 0.0020465528637776662\n",
      "test loss is 0.001704555720001822\n",
      "Batch: 22700,train loss is: 0.0014673997387595558\n",
      "test loss is 0.0027895388526099013\n",
      "Batch: 22800,train loss is: 0.0006776156880859356\n",
      "test loss is 0.0013622491740510898\n",
      "Batch: 22900,train loss is: 0.001242771899076112\n",
      "test loss is 0.0014428021350395745\n",
      "Batch: 23000,train loss is: 0.001420891614662119\n",
      "test loss is 0.0013835033829625813\n",
      "Batch: 23100,train loss is: 0.0007898218556333143\n",
      "test loss is 0.0013837952439699752\n",
      "Batch: 23200,train loss is: 0.0009268260595437502\n",
      "test loss is 0.0021784550195572223\n",
      "Batch: 23300,train loss is: 0.002142475750995569\n",
      "test loss is 0.0016038683033922651\n",
      "Batch: 23400,train loss is: 0.0025590919938541334\n",
      "test loss is 0.001944074028963542\n",
      "Batch: 23500,train loss is: 0.003081309685432694\n",
      "test loss is 0.001512439354038091\n",
      "Batch: 23600,train loss is: 0.0013699040451015366\n",
      "test loss is 0.002765696186837632\n",
      "Batch: 23700,train loss is: 0.0011867701628193548\n",
      "test loss is 0.0012638251024882474\n",
      "Batch: 23800,train loss is: 0.00481351953734217\n",
      "test loss is 0.0016923591407744392\n",
      "Batch: 23900,train loss is: 0.0025198585358998642\n",
      "test loss is 0.003298280731112639\n",
      "Batch: 24000,train loss is: 0.0005844084918570704\n",
      "test loss is 0.0016701033171774328\n",
      "Batch: 24100,train loss is: 0.0010340691154527603\n",
      "test loss is 0.001400607434298587\n",
      "Batch: 24200,train loss is: 0.0032718557506366667\n",
      "test loss is 0.0014606299266563577\n",
      "Batch: 24300,train loss is: 0.008317867755225555\n",
      "test loss is 0.0040851335935689565\n",
      "Batch: 24400,train loss is: 0.0029912811869124032\n",
      "test loss is 0.0020613401775621517\n",
      "Batch: 24500,train loss is: 0.0005115420504837962\n",
      "test loss is 0.0027670725227211803\n",
      "Batch: 24600,train loss is: 0.0023271613373038107\n",
      "test loss is 0.001598878328485263\n",
      "Batch: 24700,train loss is: 0.0008030396542696574\n",
      "test loss is 0.0013704852951539031\n",
      "Batch: 24800,train loss is: 0.006111534369157058\n",
      "test loss is 0.006637320792627082\n",
      "Batch: 24900,train loss is: 0.003526369872430088\n",
      "test loss is 0.0023809665791526464\n",
      "Batch: 25000,train loss is: 0.0011340321970235796\n",
      "test loss is 0.0016052956263744027\n",
      "Batch: 25100,train loss is: 0.0011101173940421965\n",
      "test loss is 0.0013725073501015146\n",
      "Batch: 25200,train loss is: 0.0024540648522377007\n",
      "test loss is 0.0012868046676208099\n",
      "Batch: 25300,train loss is: 0.003256187919480073\n",
      "test loss is 0.0019101720900037984\n",
      "Batch: 25400,train loss is: 0.0017438116037301436\n",
      "test loss is 0.0015846983363758357\n",
      "Batch: 25500,train loss is: 0.0024250138625761366\n",
      "test loss is 0.001825688236907099\n",
      "Batch: 25600,train loss is: 0.0012977342253269887\n",
      "test loss is 0.0012600302029594696\n",
      "Batch: 25700,train loss is: 0.0011275301503869612\n",
      "test loss is 0.0018822645065198242\n",
      "Batch: 25800,train loss is: 0.002009585632944147\n",
      "test loss is 0.0022225201882216327\n",
      "Batch: 25900,train loss is: 0.0006408898201213937\n",
      "test loss is 0.002043172732325696\n",
      "Batch: 26000,train loss is: 0.004216358122441157\n",
      "test loss is 0.002775690969653612\n",
      "Batch: 26100,train loss is: 0.0021106903790266444\n",
      "test loss is 0.003111974484524337\n",
      "Batch: 26200,train loss is: 0.00393564226125135\n",
      "test loss is 0.0026691605043774103\n",
      "Batch: 26300,train loss is: 0.0013633374362596643\n",
      "test loss is 0.0013118261196899668\n",
      "Batch: 26400,train loss is: 0.0019033515629820947\n",
      "test loss is 0.0016518654712408356\n",
      "Batch: 26500,train loss is: 0.000851732743156599\n",
      "test loss is 0.0014321856517161887\n",
      "Batch: 26600,train loss is: 0.0010919054612986193\n",
      "test loss is 0.001338833794033064\n",
      "Batch: 26700,train loss is: 0.0005784488833132942\n",
      "test loss is 0.0015941273660598034\n",
      "Batch: 26800,train loss is: 0.0017693431524297095\n",
      "test loss is 0.0015176187171125938\n",
      "Batch: 26900,train loss is: 0.0006383701487702693\n",
      "test loss is 0.0014465658568218415\n",
      "Batch: 27000,train loss is: 0.006991992341200672\n",
      "test loss is 0.004462018077265983\n",
      "Batch: 27100,train loss is: 0.0015498257420331807\n",
      "test loss is 0.0027085710328887363\n",
      "Batch: 27200,train loss is: 0.006811191387776959\n",
      "test loss is 0.003939266586831087\n",
      "Batch: 27300,train loss is: 0.000668070885208977\n",
      "test loss is 0.0017617574417307213\n",
      "Batch: 27400,train loss is: 0.0011075494495504255\n",
      "test loss is 0.0011935679445263306\n",
      "Batch: 27500,train loss is: 0.0024315810146591345\n",
      "test loss is 0.0015958779166270103\n",
      "Batch: 27600,train loss is: 0.0016193955082494341\n",
      "test loss is 0.0021609670592557424\n",
      "Batch: 27700,train loss is: 0.0035881828638940857\n",
      "test loss is 0.0015293091291780168\n",
      "Batch: 27800,train loss is: 0.0008284398975756969\n",
      "test loss is 0.002916400093525256\n",
      "Batch: 27900,train loss is: 0.0017856072779273262\n",
      "test loss is 0.001926385997712133\n",
      "Batch: 28000,train loss is: 0.0015304980209825036\n",
      "test loss is 0.0019317980306472712\n",
      "Batch: 28100,train loss is: 0.0036981784850663073\n",
      "test loss is 0.002181110003154719\n",
      "Batch: 28200,train loss is: 0.004854498605533379\n",
      "test loss is 0.002054671869676287\n",
      "Batch: 28300,train loss is: 0.006246922245034647\n",
      "test loss is 0.0015419687588098259\n",
      "Batch: 28400,train loss is: 0.0033485759571850017\n",
      "test loss is 0.001527080122051012\n",
      "Batch: 28500,train loss is: 0.004679888381376644\n",
      "test loss is 0.002074225934963498\n",
      "Batch: 28600,train loss is: 0.00348689356198905\n",
      "test loss is 0.002169687820884482\n",
      "Batch: 28700,train loss is: 0.0007208642215163104\n",
      "test loss is 0.0016265210303387531\n",
      "Batch: 28800,train loss is: 0.003454825495799279\n",
      "test loss is 0.0014408368452733385\n",
      "Batch: 28900,train loss is: 0.0013307277562889723\n",
      "test loss is 0.0017768939510689951\n",
      "Batch: 29000,train loss is: 0.0010441835053771817\n",
      "test loss is 0.0015972761048553105\n",
      "Batch: 29100,train loss is: 0.0016410061679524924\n",
      "test loss is 0.001245717860054528\n",
      "Batch: 29200,train loss is: 0.0013532699119031756\n",
      "test loss is 0.0023819453303128694\n",
      "Batch: 29300,train loss is: 0.0016221886978088017\n",
      "test loss is 0.002024640651918587\n",
      "Batch: 29400,train loss is: 0.0009258344490700693\n",
      "test loss is 0.0015918827022653665\n",
      "Batch: 29500,train loss is: 0.0007398615409378919\n",
      "test loss is 0.0022699172073046793\n",
      "Batch: 29600,train loss is: 0.0009549927224785685\n",
      "test loss is 0.0012219120244351565\n",
      "Batch: 29700,train loss is: 0.00036659045915400813\n",
      "test loss is 0.0016702634851404503\n",
      "Batch: 29800,train loss is: 0.002222928755012202\n",
      "test loss is 0.0010835454705458692\n",
      "Batch: 29900,train loss is: 0.006196879721604029\n",
      "test loss is 0.001577128780647226\n",
      "Batch: 30000,train loss is: 0.0007397206232487486\n",
      "test loss is 0.001302073497996912\n",
      "Batch: 30100,train loss is: 0.0011812231181681872\n",
      "test loss is 0.0014848668314782364\n",
      "Batch: 30200,train loss is: 0.00108830619153862\n",
      "test loss is 0.0013304890591420945\n",
      "Batch: 30300,train loss is: 0.001364313773945036\n",
      "test loss is 0.0020980590700563242\n",
      "Batch: 30400,train loss is: 0.0019246867059419261\n",
      "test loss is 0.002410494980553096\n",
      "Batch: 30500,train loss is: 0.000971638215413018\n",
      "test loss is 0.001490860482725133\n",
      "Batch: 30600,train loss is: 0.0010481439359671027\n",
      "test loss is 0.001828536357599263\n",
      "Batch: 30700,train loss is: 0.0011553809927319226\n",
      "test loss is 0.0017744175219527743\n",
      "Batch: 30800,train loss is: 0.00048191578745318173\n",
      "test loss is 0.0015450830936396321\n",
      "Batch: 30900,train loss is: 0.0009303688412782048\n",
      "test loss is 0.001904234513557905\n",
      "Batch: 31000,train loss is: 0.0007519305750329459\n",
      "test loss is 0.001292201515715029\n",
      "Batch: 31100,train loss is: 0.002036163947439726\n",
      "test loss is 0.004219817401072276\n",
      "Batch: 31200,train loss is: 0.002403432461215375\n",
      "test loss is 0.001479018723164406\n",
      "Batch: 31300,train loss is: 0.0011778758854199443\n",
      "test loss is 0.0015908464030973924\n",
      "Batch: 31400,train loss is: 0.0020391546214965943\n",
      "test loss is 0.0022813403866023445\n",
      "Batch: 31500,train loss is: 0.0011372520957235973\n",
      "test loss is 0.0015478251195924617\n",
      "Batch: 31600,train loss is: 0.0007667273855503565\n",
      "test loss is 0.0019056386086746511\n",
      "Batch: 31700,train loss is: 0.0006113386517250536\n",
      "test loss is 0.0012271506090093792\n",
      "Batch: 31800,train loss is: 0.0014497368975053854\n",
      "test loss is 0.0017191063491811852\n",
      "Batch: 31900,train loss is: 0.0063264887716593865\n",
      "test loss is 0.003127338774273251\n",
      "Batch: 32000,train loss is: 0.013378712184523422\n",
      "test loss is 0.0017135080783090963\n",
      "Batch: 32100,train loss is: 0.0013419253147502045\n",
      "test loss is 0.0025205717472862794\n",
      "Batch: 32200,train loss is: 0.0012785036283234229\n",
      "test loss is 0.0020869263938210494\n",
      "Batch: 32300,train loss is: 0.0013638748629009876\n",
      "test loss is 0.001501248361609809\n",
      "Batch: 32400,train loss is: 0.0008566092782113897\n",
      "test loss is 0.0029203505567173805\n",
      "Batch: 32500,train loss is: 0.0047878660806787285\n",
      "test loss is 0.0030782831024166865\n",
      "Batch: 32600,train loss is: 0.0008642017309162975\n",
      "test loss is 0.0013155432644740314\n",
      "Batch: 32700,train loss is: 0.0007478261880774848\n",
      "test loss is 0.0017938115854837628\n",
      "Batch: 32800,train loss is: 0.0006591944671356034\n",
      "test loss is 0.002062874614539011\n",
      "Batch: 32900,train loss is: 0.001555542532598921\n",
      "test loss is 0.0015322714560601206\n",
      "Batch: 33000,train loss is: 0.0009270859674660832\n",
      "test loss is 0.0015308836678355957\n",
      "Batch: 33100,train loss is: 0.00163779655037041\n",
      "test loss is 0.0021904467693627996\n",
      "Batch: 33200,train loss is: 0.0008140676356137299\n",
      "test loss is 0.001613611267644726\n",
      "Batch: 33300,train loss is: 0.0008964683431850334\n",
      "test loss is 0.0012744766314391448\n",
      "Batch: 33400,train loss is: 0.0027440061894330836\n",
      "test loss is 0.0022228677034912122\n",
      "Batch: 33500,train loss is: 0.000801801617527086\n",
      "test loss is 0.0017624305463568846\n",
      "Batch: 33600,train loss is: 0.000823052075424754\n",
      "test loss is 0.0017978080406680615\n",
      "Batch: 33700,train loss is: 0.00175920167860389\n",
      "test loss is 0.0017602745808777136\n",
      "Batch: 33800,train loss is: 0.007012835865822327\n",
      "test loss is 0.002009335803207196\n",
      "Batch: 33900,train loss is: 0.0029058768205322966\n",
      "test loss is 0.001411274368848681\n",
      "-----------------------Epoch: 4----------------------------------\n",
      "Batch: 0,train loss is: 0.0011919151349146294\n",
      "test loss is 0.001393142583649961\n",
      "Batch: 100,train loss is: 0.00163956253338703\n",
      "test loss is 0.0015383062646226401\n",
      "Batch: 200,train loss is: 0.001931146222805851\n",
      "test loss is 0.0012019425417972222\n",
      "Batch: 300,train loss is: 0.0006122396473826989\n",
      "test loss is 0.0014390744895703845\n",
      "Batch: 400,train loss is: 0.0007979494185369608\n",
      "test loss is 0.0019619390781830093\n",
      "Batch: 500,train loss is: 0.0011191959732839252\n",
      "test loss is 0.0014237428732960787\n",
      "Batch: 600,train loss is: 0.000321851070327356\n",
      "test loss is 0.0019280972496583332\n",
      "Batch: 700,train loss is: 0.0004668388462059639\n",
      "test loss is 0.0012046211748882\n",
      "Batch: 800,train loss is: 0.0017379062486520497\n",
      "test loss is 0.0018978698682920213\n",
      "Batch: 900,train loss is: 0.0014623968353991606\n",
      "test loss is 0.002587108408681239\n",
      "Batch: 1000,train loss is: 0.0005207343748073206\n",
      "test loss is 0.0023085045619132133\n",
      "Batch: 1100,train loss is: 0.00400222583672897\n",
      "test loss is 0.0017586725397498466\n",
      "Batch: 1200,train loss is: 0.0005822665817124299\n",
      "test loss is 0.0033512266462551185\n",
      "Batch: 1300,train loss is: 0.0008782853037042963\n",
      "test loss is 0.001311419928653539\n",
      "Batch: 1400,train loss is: 0.0013456014782372641\n",
      "test loss is 0.0016464341562084084\n",
      "Batch: 1500,train loss is: 0.002094952067586291\n",
      "test loss is 0.002799614334265995\n",
      "Batch: 1600,train loss is: 0.0009325329285400818\n",
      "test loss is 0.0022528078924143923\n",
      "Batch: 1700,train loss is: 0.007209175965531061\n",
      "test loss is 0.001444553896491352\n",
      "Batch: 1800,train loss is: 0.0007315933279106534\n",
      "test loss is 0.0013047045282373164\n",
      "Batch: 1900,train loss is: 0.0016956280694703841\n",
      "test loss is 0.0024591032391841\n",
      "Batch: 2000,train loss is: 0.0015024830025470858\n",
      "test loss is 0.0019604845091688882\n",
      "Batch: 2100,train loss is: 0.0014544734945185177\n",
      "test loss is 0.0018831025643337874\n",
      "Batch: 2200,train loss is: 0.0012112600169573605\n",
      "test loss is 0.0013256882289827453\n",
      "Batch: 2300,train loss is: 0.0015053994191594048\n",
      "test loss is 0.0019297375096625393\n",
      "Batch: 2400,train loss is: 0.001557468457904259\n",
      "test loss is 0.0013059522017145745\n",
      "Batch: 2500,train loss is: 0.0020913068945488814\n",
      "test loss is 0.0017686229852049821\n",
      "Batch: 2600,train loss is: 0.0010956211862831516\n",
      "test loss is 0.0016229860906343296\n",
      "Batch: 2700,train loss is: 0.0025905856064822523\n",
      "test loss is 0.0012872579503993044\n",
      "Batch: 2800,train loss is: 0.0009979062823235291\n",
      "test loss is 0.0019145046193907236\n",
      "Batch: 2900,train loss is: 0.0004946862202613227\n",
      "test loss is 0.0011590469522609187\n",
      "Batch: 3000,train loss is: 0.0010848989280475522\n",
      "test loss is 0.0012958201510645155\n",
      "Batch: 3100,train loss is: 0.0014859056848487227\n",
      "test loss is 0.00134066301342683\n",
      "Batch: 3200,train loss is: 0.004563986807932805\n",
      "test loss is 0.001982214906817357\n",
      "Batch: 3300,train loss is: 0.0005116449870809336\n",
      "test loss is 0.0012081749697735824\n",
      "Batch: 3400,train loss is: 0.003225057441453137\n",
      "test loss is 0.001975013730067261\n",
      "Batch: 3500,train loss is: 0.0008236425640622185\n",
      "test loss is 0.0025104482604343934\n",
      "Batch: 3600,train loss is: 0.000981442864774094\n",
      "test loss is 0.0028663567534188894\n",
      "Batch: 3700,train loss is: 0.002741806519578997\n",
      "test loss is 0.0024382072697286365\n",
      "Batch: 3800,train loss is: 0.0012160747504515177\n",
      "test loss is 0.0017004281757946247\n",
      "Batch: 3900,train loss is: 0.0017550446758948482\n",
      "test loss is 0.0013388297707582085\n",
      "Batch: 4000,train loss is: 0.00037642342413626644\n",
      "test loss is 0.001512050519722828\n",
      "Batch: 4100,train loss is: 0.001003201240837006\n",
      "test loss is 0.0013417423537294146\n",
      "Batch: 4200,train loss is: 0.0011719952748123616\n",
      "test loss is 0.0015661619038701221\n",
      "Batch: 4300,train loss is: 0.0010888226728333415\n",
      "test loss is 0.002200666977618738\n",
      "Batch: 4400,train loss is: 0.0015704827334105192\n",
      "test loss is 0.0019145111354382587\n",
      "Batch: 4500,train loss is: 0.009766438420478397\n",
      "test loss is 0.004244639622394616\n",
      "Batch: 4600,train loss is: 0.00117864117549844\n",
      "test loss is 0.002227230239962977\n",
      "Batch: 4700,train loss is: 0.0020607375663724945\n",
      "test loss is 0.0019594745692215394\n",
      "Batch: 4800,train loss is: 0.002333303123415407\n",
      "test loss is 0.0024648679426242303\n",
      "Batch: 4900,train loss is: 0.0013332705229564055\n",
      "test loss is 0.0014655460958384074\n",
      "Batch: 5000,train loss is: 0.0005685431431345623\n",
      "test loss is 0.0012594370704813616\n",
      "Batch: 5100,train loss is: 0.0008043290002303096\n",
      "test loss is 0.0014639324476513668\n",
      "Batch: 5200,train loss is: 0.0005445835188053321\n",
      "test loss is 0.0011853004991435603\n",
      "Batch: 5300,train loss is: 0.0014770312260294672\n",
      "test loss is 0.0021826541065760804\n",
      "Batch: 5400,train loss is: 0.001542589253734325\n",
      "test loss is 0.0013983802761624888\n",
      "Batch: 5500,train loss is: 0.0027091658679897564\n",
      "test loss is 0.0015324743103988137\n",
      "Batch: 5600,train loss is: 0.0012223992504493532\n",
      "test loss is 0.0013982292161119755\n",
      "Batch: 5700,train loss is: 0.000813519876644289\n",
      "test loss is 0.001582733221293228\n",
      "Batch: 5800,train loss is: 0.0017271458662631196\n",
      "test loss is 0.0015885697048507673\n",
      "Batch: 5900,train loss is: 0.004379853760945259\n",
      "test loss is 0.0015612274725362255\n",
      "Batch: 6000,train loss is: 0.0018371072512224633\n",
      "test loss is 0.004236474073547042\n",
      "Batch: 6100,train loss is: 0.0007310626990047381\n",
      "test loss is 0.0018161940628877839\n",
      "Batch: 6200,train loss is: 0.0006428587548094471\n",
      "test loss is 0.0015172772312032905\n",
      "Batch: 6300,train loss is: 0.001985148270990336\n",
      "test loss is 0.002115592908524518\n",
      "Batch: 6400,train loss is: 0.000531914537258646\n",
      "test loss is 0.004309197035575792\n",
      "Batch: 6500,train loss is: 0.0028203541982653813\n",
      "test loss is 0.0020644635184681927\n",
      "Batch: 6600,train loss is: 0.00415948351209263\n",
      "test loss is 0.0013533771184734919\n",
      "Batch: 6700,train loss is: 0.0008525595793334041\n",
      "test loss is 0.001278091420903594\n",
      "Batch: 6800,train loss is: 0.0023109917071362502\n",
      "test loss is 0.0011982391547324788\n",
      "Batch: 6900,train loss is: 0.0021373644570886474\n",
      "test loss is 0.0012065772891377182\n",
      "Batch: 7000,train loss is: 0.0022327154020864645\n",
      "test loss is 0.0055529186555555166\n",
      "Batch: 7100,train loss is: 0.0018888095942073122\n",
      "test loss is 0.0015046839822707358\n",
      "Batch: 7200,train loss is: 0.0008535327692356312\n",
      "test loss is 0.0012054362899045078\n",
      "Batch: 7300,train loss is: 0.0019577149231747764\n",
      "test loss is 0.0015472516187410577\n",
      "Batch: 7400,train loss is: 0.0006578192755810584\n",
      "test loss is 0.0024267509874813446\n",
      "Batch: 7500,train loss is: 0.0016065414147426042\n",
      "test loss is 0.0017681660813843843\n",
      "Batch: 7600,train loss is: 0.0010408124765275424\n",
      "test loss is 0.0014164106934472416\n",
      "Batch: 7700,train loss is: 0.0009521582953878136\n",
      "test loss is 0.0012505412636376473\n",
      "Batch: 7800,train loss is: 0.0011816984788668664\n",
      "test loss is 0.0014982945413556737\n",
      "Batch: 7900,train loss is: 0.0005441391323117096\n",
      "test loss is 0.0014671024585866618\n",
      "Batch: 8000,train loss is: 0.0006070244917482628\n",
      "test loss is 0.002213495074843956\n",
      "Batch: 8100,train loss is: 0.0015728876993593745\n",
      "test loss is 0.001394336115866814\n",
      "Batch: 8200,train loss is: 0.0022624726103819175\n",
      "test loss is 0.0017897314700126908\n",
      "Batch: 8300,train loss is: 0.0004380946055602774\n",
      "test loss is 0.0013754008232496288\n",
      "Batch: 8400,train loss is: 0.0016364854555567492\n",
      "test loss is 0.001943837619094619\n",
      "Batch: 8500,train loss is: 0.0025692021027076727\n",
      "test loss is 0.003020695735043505\n",
      "Batch: 8600,train loss is: 0.003102462388492216\n",
      "test loss is 0.0031021189476774565\n",
      "Batch: 8700,train loss is: 0.0006175303523548672\n",
      "test loss is 0.001937442210910229\n",
      "Batch: 8800,train loss is: 0.000778567118421022\n",
      "test loss is 0.0013927764366087271\n",
      "Batch: 8900,train loss is: 0.0016615768556243018\n",
      "test loss is 0.0011304936931668286\n",
      "Batch: 9000,train loss is: 0.00035904774888357514\n",
      "test loss is 0.0013560250454964491\n",
      "Batch: 9100,train loss is: 0.0006389518095035063\n",
      "test loss is 0.0014941015193133086\n",
      "Batch: 9200,train loss is: 0.0038108615713561456\n",
      "test loss is 0.0025854078286424403\n",
      "Batch: 9300,train loss is: 0.0009146599556772674\n",
      "test loss is 0.0016876046298213501\n",
      "Batch: 9400,train loss is: 0.0011083016504652835\n",
      "test loss is 0.0021259991035229016\n",
      "Batch: 9500,train loss is: 0.0012002870638899332\n",
      "test loss is 0.0024563344581789172\n",
      "Batch: 9600,train loss is: 0.002205999879741485\n",
      "test loss is 0.0017999577630709012\n",
      "Batch: 9700,train loss is: 0.0009723343494035048\n",
      "test loss is 0.0018118032097606436\n",
      "Batch: 9800,train loss is: 0.0008780525731798279\n",
      "test loss is 0.0014706309057866526\n",
      "Batch: 9900,train loss is: 0.0012043422740590278\n",
      "test loss is 0.001427028906003516\n",
      "Batch: 10000,train loss is: 0.000898292025161006\n",
      "test loss is 0.0014736163574934774\n",
      "Batch: 10100,train loss is: 0.0014633779884605372\n",
      "test loss is 0.001392842933702823\n",
      "Batch: 10200,train loss is: 0.005996154310087757\n",
      "test loss is 0.0012619646981189877\n",
      "Batch: 10300,train loss is: 0.0016910669305213704\n",
      "test loss is 0.002306840328140821\n",
      "Batch: 10400,train loss is: 0.0005675213335459871\n",
      "test loss is 0.0022974537462376043\n",
      "Batch: 10500,train loss is: 0.027475074797448453\n",
      "test loss is 0.012266936726670254\n",
      "Batch: 10600,train loss is: 0.003688288224958762\n",
      "test loss is 0.002444963872102986\n",
      "Batch: 10700,train loss is: 0.0010039049099680344\n",
      "test loss is 0.001521868331782876\n",
      "Batch: 10800,train loss is: 0.0005187904530292455\n",
      "test loss is 0.00127623638788574\n",
      "Batch: 10900,train loss is: 0.0008601697526500761\n",
      "test loss is 0.0012763672269474453\n",
      "Batch: 11000,train loss is: 0.0006187149041237429\n",
      "test loss is 0.0011779783339492063\n",
      "Batch: 11100,train loss is: 0.0017867588501843915\n",
      "test loss is 0.0011942468717789257\n",
      "Batch: 11200,train loss is: 0.0011961281356311182\n",
      "test loss is 0.0012469563412772857\n",
      "Batch: 11300,train loss is: 0.0007358802937846295\n",
      "test loss is 0.0014494741144413963\n",
      "Batch: 11400,train loss is: 0.0013491474528429877\n",
      "test loss is 0.0014069785764761703\n",
      "Batch: 11500,train loss is: 0.0009368525059065876\n",
      "test loss is 0.0013249612674808915\n",
      "Batch: 11600,train loss is: 0.005464737388301991\n",
      "test loss is 0.003947506813310075\n",
      "Batch: 11700,train loss is: 0.0015561407960515941\n",
      "test loss is 0.0018269010076358253\n",
      "Batch: 11800,train loss is: 0.0010637160372881304\n",
      "test loss is 0.0023034172477955877\n",
      "Batch: 11900,train loss is: 0.0015156625776974895\n",
      "test loss is 0.0027987855313718955\n",
      "Batch: 12000,train loss is: 0.0008656191188810884\n",
      "test loss is 0.0020725485437664425\n",
      "Batch: 12100,train loss is: 0.005213824047388816\n",
      "test loss is 0.0013565433607224554\n",
      "Batch: 12200,train loss is: 0.0014941481056616746\n",
      "test loss is 0.002762757047077616\n",
      "Batch: 12300,train loss is: 0.0007218708436562848\n",
      "test loss is 0.0014930751795307956\n",
      "Batch: 12400,train loss is: 0.0009435442976431946\n",
      "test loss is 0.0013532335986159137\n",
      "Batch: 12500,train loss is: 0.0006921387370373506\n",
      "test loss is 0.0012877915411149722\n",
      "Batch: 12600,train loss is: 0.000554273081384665\n",
      "test loss is 0.001293300272652857\n",
      "Batch: 12700,train loss is: 0.0010556587266107613\n",
      "test loss is 0.001152784595859198\n",
      "Batch: 12800,train loss is: 0.0012349208512743488\n",
      "test loss is 0.001414635680348858\n",
      "Batch: 12900,train loss is: 0.0014767294323552148\n",
      "test loss is 0.002020487196622945\n",
      "Batch: 13000,train loss is: 0.0007801830150679942\n",
      "test loss is 0.00125293739013005\n",
      "Batch: 13100,train loss is: 0.0011271377469115323\n",
      "test loss is 0.0016797147048049188\n",
      "Batch: 13200,train loss is: 0.0019223783521341393\n",
      "test loss is 0.001977412157538223\n",
      "Batch: 13300,train loss is: 0.001569113208897207\n",
      "test loss is 0.0032297298728465847\n",
      "Batch: 13400,train loss is: 0.0010122593894943345\n",
      "test loss is 0.0017670062732201774\n",
      "Batch: 13500,train loss is: 0.003883977571592846\n",
      "test loss is 0.0028188323027369907\n",
      "Batch: 13600,train loss is: 0.0009486700520463208\n",
      "test loss is 0.0013013820482309086\n",
      "Batch: 13700,train loss is: 0.0007988648419313152\n",
      "test loss is 0.0015116797825434406\n",
      "Batch: 13800,train loss is: 0.0013583022584915717\n",
      "test loss is 0.002121598923903099\n",
      "Batch: 13900,train loss is: 0.0005683632753612566\n",
      "test loss is 0.001529783761048795\n",
      "Batch: 14000,train loss is: 0.0005874963617228809\n",
      "test loss is 0.0017109698797588742\n",
      "Batch: 14100,train loss is: 0.0010395288928193797\n",
      "test loss is 0.0016563676830998284\n",
      "Batch: 14200,train loss is: 0.0007305176204265722\n",
      "test loss is 0.0012350735344095645\n",
      "Batch: 14300,train loss is: 0.0005763330139619638\n",
      "test loss is 0.0013434440610768973\n",
      "Batch: 14400,train loss is: 0.0019038289262391192\n",
      "test loss is 0.0012722988530644499\n",
      "Batch: 14500,train loss is: 0.0022874324884294053\n",
      "test loss is 0.001716898104179918\n",
      "Batch: 14600,train loss is: 0.002598072220185822\n",
      "test loss is 0.0012865395415165022\n",
      "Batch: 14700,train loss is: 0.001300841606210241\n",
      "test loss is 0.0017762509324051737\n",
      "Batch: 14800,train loss is: 0.0016602295525196549\n",
      "test loss is 0.0021629861783147453\n",
      "Batch: 14900,train loss is: 0.0006823735928283103\n",
      "test loss is 0.0013608420630598637\n",
      "Batch: 15000,train loss is: 0.0025297576626981395\n",
      "test loss is 0.0014865717894293451\n",
      "Batch: 15100,train loss is: 0.0015840062259650967\n",
      "test loss is 0.0013875652902411374\n",
      "Batch: 15200,train loss is: 0.002105994228984889\n",
      "test loss is 0.001426481106437115\n",
      "Batch: 15300,train loss is: 0.0017524368381423925\n",
      "test loss is 0.0014236282477719901\n",
      "Batch: 15400,train loss is: 0.0011847117313040557\n",
      "test loss is 0.00362804458902874\n",
      "Batch: 15500,train loss is: 0.0018832821419782694\n",
      "test loss is 0.0013342588641845647\n",
      "Batch: 15600,train loss is: 0.002762002853677273\n",
      "test loss is 0.002055477089633361\n",
      "Batch: 15700,train loss is: 0.0010262753283251326\n",
      "test loss is 0.0024031555919860432\n",
      "Batch: 15800,train loss is: 0.0012106736990028057\n",
      "test loss is 0.0017651117830882429\n",
      "Batch: 15900,train loss is: 0.0012170993127040592\n",
      "test loss is 0.001642409034121106\n",
      "Batch: 16000,train loss is: 0.0013034500075204372\n",
      "test loss is 0.0024041356671609115\n",
      "Batch: 16100,train loss is: 0.0021964671054372775\n",
      "test loss is 0.0027654952375093626\n",
      "Batch: 16200,train loss is: 0.0011857273497722827\n",
      "test loss is 0.001289434267188323\n",
      "Batch: 16300,train loss is: 0.0012891000124545784\n",
      "test loss is 0.0012232188954864566\n",
      "Batch: 16400,train loss is: 0.00039430001476352335\n",
      "test loss is 0.001093326944120981\n",
      "Batch: 16500,train loss is: 0.004839693736754017\n",
      "test loss is 0.006403084755262715\n",
      "Batch: 16600,train loss is: 0.0006995521494256098\n",
      "test loss is 0.0016999917195784384\n",
      "Batch: 16700,train loss is: 0.0016414826339452078\n",
      "test loss is 0.001560377510302816\n",
      "Batch: 16800,train loss is: 0.0008582917368054216\n",
      "test loss is 0.0015037373866248166\n",
      "Batch: 16900,train loss is: 0.0017316107354899129\n",
      "test loss is 0.0017876032615015537\n",
      "Batch: 17000,train loss is: 0.0014945063919789522\n",
      "test loss is 0.0019499329547791257\n",
      "Batch: 17100,train loss is: 0.0008939062504021838\n",
      "test loss is 0.0016612487380899066\n",
      "Batch: 17200,train loss is: 0.0007880389889023852\n",
      "test loss is 0.0013883264550048197\n",
      "Batch: 17300,train loss is: 0.0017730032749748922\n",
      "test loss is 0.0012736760241406817\n",
      "Batch: 17400,train loss is: 0.0014138685312827634\n",
      "test loss is 0.0019421015909390617\n",
      "Batch: 17500,train loss is: 0.0012818644915139836\n",
      "test loss is 0.001385934482659869\n",
      "Batch: 17600,train loss is: 0.0008660612379712995\n",
      "test loss is 0.001808787008228647\n",
      "Batch: 17700,train loss is: 0.0010464760806855517\n",
      "test loss is 0.0014967483281068542\n",
      "Batch: 17800,train loss is: 0.002003231413910639\n",
      "test loss is 0.0013955532775815113\n",
      "Batch: 17900,train loss is: 0.0009136293744138425\n",
      "test loss is 0.0013078103783122172\n",
      "Batch: 18000,train loss is: 0.0006976197658313081\n",
      "test loss is 0.001273362462925779\n",
      "Batch: 18100,train loss is: 0.0009830443016963656\n",
      "test loss is 0.0021307462180876154\n",
      "Batch: 18200,train loss is: 0.0008749269887216321\n",
      "test loss is 0.0018661065426780444\n",
      "Batch: 18300,train loss is: 0.0014719360637595199\n",
      "test loss is 0.0011446156402290324\n",
      "Batch: 18400,train loss is: 0.0005602689195602628\n",
      "test loss is 0.0013616746086289005\n",
      "Batch: 18500,train loss is: 0.0007135826925144774\n",
      "test loss is 0.0026930238214362503\n",
      "Batch: 18600,train loss is: 0.0022834603145965846\n",
      "test loss is 0.0027823144416338316\n",
      "Batch: 18700,train loss is: 0.001234192206332556\n",
      "test loss is 0.0019126824329855617\n",
      "Batch: 18800,train loss is: 0.0008730125631345554\n",
      "test loss is 0.00136418084697489\n",
      "Batch: 18900,train loss is: 0.001264351491648813\n",
      "test loss is 0.0013567344221784498\n",
      "Batch: 19000,train loss is: 0.0010189124246197548\n",
      "test loss is 0.0013000674289307625\n",
      "Batch: 19100,train loss is: 0.0007076663238097409\n",
      "test loss is 0.0013561629392141224\n",
      "Batch: 19200,train loss is: 0.0008262609079249916\n",
      "test loss is 0.0013363983856259688\n",
      "Batch: 19300,train loss is: 0.001982462035558512\n",
      "test loss is 0.0012006228594135418\n",
      "Batch: 19400,train loss is: 0.0007384442837863929\n",
      "test loss is 0.0011539146700337663\n",
      "Batch: 19500,train loss is: 0.0010350302674327423\n",
      "test loss is 0.001163146628916182\n",
      "Batch: 19600,train loss is: 0.0005579957120773976\n",
      "test loss is 0.0013600519261468145\n",
      "Batch: 19700,train loss is: 0.0008663544210129267\n",
      "test loss is 0.0015794713222748947\n",
      "Batch: 19800,train loss is: 0.0007042244780413646\n",
      "test loss is 0.0015984999913280563\n",
      "Batch: 19900,train loss is: 0.0007414534690314796\n",
      "test loss is 0.002447409610318536\n",
      "Batch: 20000,train loss is: 0.0013470955576516356\n",
      "test loss is 0.0013597726738821135\n",
      "Batch: 20100,train loss is: 0.0010677428108188962\n",
      "test loss is 0.001354439056015186\n",
      "Batch: 20200,train loss is: 0.0010045431137755147\n",
      "test loss is 0.0013178210920876622\n",
      "Batch: 20300,train loss is: 0.0009638465902729546\n",
      "test loss is 0.0010666968606991337\n",
      "Batch: 20400,train loss is: 0.003579603434700901\n",
      "test loss is 0.002497740574810756\n",
      "Batch: 20500,train loss is: 0.0010669189382669524\n",
      "test loss is 0.0033856256767683616\n",
      "Batch: 20600,train loss is: 0.0022182553770483658\n",
      "test loss is 0.002254220335499148\n",
      "Batch: 20700,train loss is: 0.000900166207523443\n",
      "test loss is 0.002560945991677501\n",
      "Batch: 20800,train loss is: 0.0006820451940805635\n",
      "test loss is 0.0018174133233709526\n",
      "Batch: 20900,train loss is: 0.0017605737601407885\n",
      "test loss is 0.0013891311533000342\n",
      "Batch: 21000,train loss is: 0.000804196270433221\n",
      "test loss is 0.002243356930560763\n",
      "Batch: 21100,train loss is: 0.0007598811652855545\n",
      "test loss is 0.0013320063530516816\n",
      "Batch: 21200,train loss is: 0.006519148138149406\n",
      "test loss is 0.004948054680347633\n",
      "Batch: 21300,train loss is: 0.0008365920550150101\n",
      "test loss is 0.001277502337876254\n",
      "Batch: 21400,train loss is: 0.000808664336600652\n",
      "test loss is 0.0016371781718973355\n",
      "Batch: 21500,train loss is: 0.0010323469224583774\n",
      "test loss is 0.0012566435367758501\n",
      "Batch: 21600,train loss is: 0.0016436900747127373\n",
      "test loss is 0.0012828362602628736\n",
      "Batch: 21700,train loss is: 0.0017056037131319057\n",
      "test loss is 0.002175658104640824\n",
      "Batch: 21800,train loss is: 0.0009843141590218868\n",
      "test loss is 0.0011353269946904826\n",
      "Batch: 21900,train loss is: 0.0010284324303889592\n",
      "test loss is 0.0015364007312336212\n",
      "Batch: 22000,train loss is: 0.0023069149221943276\n",
      "test loss is 0.001976237660387846\n",
      "Batch: 22100,train loss is: 0.000756646959200874\n",
      "test loss is 0.0016697288993012878\n",
      "Batch: 22200,train loss is: 0.0007944797300923195\n",
      "test loss is 0.0013212678712324023\n",
      "Batch: 22300,train loss is: 0.002386536531657725\n",
      "test loss is 0.002418604927832268\n",
      "Batch: 22400,train loss is: 0.0013642829685560631\n",
      "test loss is 0.0015447772937422458\n",
      "Batch: 22500,train loss is: 0.0014899464620427548\n",
      "test loss is 0.0020041950466928213\n",
      "Batch: 22600,train loss is: 0.0018139809635451967\n",
      "test loss is 0.0016290399294073795\n",
      "Batch: 22700,train loss is: 0.001633464412400762\n",
      "test loss is 0.003137967349298779\n",
      "Batch: 22800,train loss is: 0.0006006729564954591\n",
      "test loss is 0.0012402806172621132\n",
      "Batch: 22900,train loss is: 0.0010821587580085814\n",
      "test loss is 0.0014866465033454622\n",
      "Batch: 23000,train loss is: 0.0011659808866392235\n",
      "test loss is 0.0013048787554005897\n",
      "Batch: 23100,train loss is: 0.0008810618048508013\n",
      "test loss is 0.0011849544736762803\n",
      "Batch: 23200,train loss is: 0.0008425803141960238\n",
      "test loss is 0.0018006841364817787\n",
      "Batch: 23300,train loss is: 0.0021818273368911316\n",
      "test loss is 0.0015207285029486808\n",
      "Batch: 23400,train loss is: 0.0023848291208462395\n",
      "test loss is 0.0016065773388629233\n",
      "Batch: 23500,train loss is: 0.0029412929816762146\n",
      "test loss is 0.0013342283181838007\n",
      "Batch: 23600,train loss is: 0.0012427765975632132\n",
      "test loss is 0.0024190912091615973\n",
      "Batch: 23700,train loss is: 0.0010941503628808123\n",
      "test loss is 0.0011174889340859495\n",
      "Batch: 23800,train loss is: 0.004987176704417367\n",
      "test loss is 0.0015948370991429128\n",
      "Batch: 23900,train loss is: 0.002371387376789033\n",
      "test loss is 0.0029164524400571773\n",
      "Batch: 24000,train loss is: 0.0007431066132872493\n",
      "test loss is 0.0014735854897650312\n",
      "Batch: 24100,train loss is: 0.000960533608484891\n",
      "test loss is 0.0012176346020829523\n",
      "Batch: 24200,train loss is: 0.0027050689673375748\n",
      "test loss is 0.001264713441681118\n",
      "Batch: 24300,train loss is: 0.006617358097907596\n",
      "test loss is 0.003277736132418273\n",
      "Batch: 24400,train loss is: 0.0024998644949126452\n",
      "test loss is 0.0020255344920467734\n",
      "Batch: 24500,train loss is: 0.0006871853522166068\n",
      "test loss is 0.003029216641913595\n",
      "Batch: 24600,train loss is: 0.0020304417309145377\n",
      "test loss is 0.0015047414084771106\n",
      "Batch: 24700,train loss is: 0.0008345950350334884\n",
      "test loss is 0.0013265526552706593\n",
      "Batch: 24800,train loss is: 0.006641573196330662\n",
      "test loss is 0.007056976313348648\n",
      "Batch: 24900,train loss is: 0.00312070747702844\n",
      "test loss is 0.002406165828038507\n",
      "Batch: 25000,train loss is: 0.0012659330456839984\n",
      "test loss is 0.00149242053728656\n",
      "Batch: 25100,train loss is: 0.0009875967917669548\n",
      "test loss is 0.0012629274704049217\n",
      "Batch: 25200,train loss is: 0.001975388554711491\n",
      "test loss is 0.0011429842715133122\n",
      "Batch: 25300,train loss is: 0.0026245071721283917\n",
      "test loss is 0.0016179758213166703\n",
      "Batch: 25400,train loss is: 0.0019015601608718617\n",
      "test loss is 0.0015055144572861578\n",
      "Batch: 25500,train loss is: 0.0022020072785205983\n",
      "test loss is 0.0016110358201688417\n",
      "Batch: 25600,train loss is: 0.0012643371413157923\n",
      "test loss is 0.001141985898497283\n",
      "Batch: 25700,train loss is: 0.0008934378727543076\n",
      "test loss is 0.0015893534522895174\n",
      "Batch: 25800,train loss is: 0.0014534105543287997\n",
      "test loss is 0.001832833806579743\n",
      "Batch: 25900,train loss is: 0.0006118112143659759\n",
      "test loss is 0.0016504162848892166\n",
      "Batch: 26000,train loss is: 0.0038975161722041864\n",
      "test loss is 0.002211855511163971\n",
      "Batch: 26100,train loss is: 0.0021721670083576085\n",
      "test loss is 0.0029075648411627628\n",
      "Batch: 26200,train loss is: 0.004931337550669945\n",
      "test loss is 0.0026635867019440514\n",
      "Batch: 26300,train loss is: 0.001397604126768376\n",
      "test loss is 0.0012997975675431864\n",
      "Batch: 26400,train loss is: 0.001923239216684566\n",
      "test loss is 0.001514066806922675\n",
      "Batch: 26500,train loss is: 0.0006801860064376212\n",
      "test loss is 0.0013127201321527144\n",
      "Batch: 26600,train loss is: 0.00112186138175433\n",
      "test loss is 0.0012692809830541643\n",
      "Batch: 26700,train loss is: 0.0004976701758986963\n",
      "test loss is 0.0013574964902315781\n",
      "Batch: 26800,train loss is: 0.001758298675286015\n",
      "test loss is 0.0013050999193200301\n",
      "Batch: 26900,train loss is: 0.0005856603801491434\n",
      "test loss is 0.0013720509129633086\n",
      "Batch: 27000,train loss is: 0.00728978891116757\n",
      "test loss is 0.004570552026668242\n",
      "Batch: 27100,train loss is: 0.0017799196435961471\n",
      "test loss is 0.0023705767615203022\n",
      "Batch: 27200,train loss is: 0.004584837942135999\n",
      "test loss is 0.0028690987866056795\n",
      "Batch: 27300,train loss is: 0.0006780325283926938\n",
      "test loss is 0.0013994073442628776\n",
      "Batch: 27400,train loss is: 0.0008918070162355156\n",
      "test loss is 0.0010934283554174494\n",
      "Batch: 27500,train loss is: 0.0018661120457558731\n",
      "test loss is 0.00151949930160212\n",
      "Batch: 27600,train loss is: 0.0015224518024183993\n",
      "test loss is 0.001994405474621477\n",
      "Batch: 27700,train loss is: 0.0033177571325118736\n",
      "test loss is 0.001353224531072948\n",
      "Batch: 27800,train loss is: 0.0007468170247235463\n",
      "test loss is 0.002362825421218642\n",
      "Batch: 27900,train loss is: 0.0017741153810650258\n",
      "test loss is 0.00173349698945655\n",
      "Batch: 28000,train loss is: 0.001615713182241712\n",
      "test loss is 0.0018765443099037647\n",
      "Batch: 28100,train loss is: 0.005127420515973662\n",
      "test loss is 0.002263119861214489\n",
      "Batch: 28200,train loss is: 0.005076260760541179\n",
      "test loss is 0.0019735075680666946\n",
      "Batch: 28300,train loss is: 0.005641097598713409\n",
      "test loss is 0.0014663954429567124\n",
      "Batch: 28400,train loss is: 0.002705242230855397\n",
      "test loss is 0.00149454060377372\n",
      "Batch: 28500,train loss is: 0.0025876208819854496\n",
      "test loss is 0.0015555882520906716\n",
      "Batch: 28600,train loss is: 0.003721770931293759\n",
      "test loss is 0.0021349436059817655\n",
      "Batch: 28700,train loss is: 0.0006195377248881983\n",
      "test loss is 0.0014893028384386016\n",
      "Batch: 28800,train loss is: 0.0029757481834507492\n",
      "test loss is 0.0013441492786258787\n",
      "Batch: 28900,train loss is: 0.001235961435538661\n",
      "test loss is 0.0015904603291366597\n",
      "Batch: 29000,train loss is: 0.001006412873669315\n",
      "test loss is 0.001449332379086446\n",
      "Batch: 29100,train loss is: 0.0014206626309540447\n",
      "test loss is 0.0013508990372109878\n",
      "Batch: 29200,train loss is: 0.0017307460515025637\n",
      "test loss is 0.002531953860158872\n",
      "Batch: 29300,train loss is: 0.001715980770648443\n",
      "test loss is 0.00205776604223065\n",
      "Batch: 29400,train loss is: 0.0007217964864740782\n",
      "test loss is 0.001530262225497001\n",
      "Batch: 29500,train loss is: 0.0006732490851214907\n",
      "test loss is 0.001995403161949702\n",
      "Batch: 29600,train loss is: 0.0010189180235813948\n",
      "test loss is 0.001161804309669428\n",
      "Batch: 29700,train loss is: 0.000302114780233563\n",
      "test loss is 0.0013994224132094283\n",
      "Batch: 29800,train loss is: 0.0021924108527801994\n",
      "test loss is 0.0010029572414248895\n",
      "Batch: 29900,train loss is: 0.005307574053338239\n",
      "test loss is 0.0014291790705551282\n",
      "Batch: 30000,train loss is: 0.0006806391856986861\n",
      "test loss is 0.0011346432850046937\n",
      "Batch: 30100,train loss is: 0.0010647871168144316\n",
      "test loss is 0.0013319945559427454\n",
      "Batch: 30200,train loss is: 0.0008499028458606974\n",
      "test loss is 0.0012207313051570778\n",
      "Batch: 30300,train loss is: 0.0014475448140835235\n",
      "test loss is 0.0022118438040495407\n",
      "Batch: 30400,train loss is: 0.0013432573499467166\n",
      "test loss is 0.0019445369799411933\n",
      "Batch: 30500,train loss is: 0.0008844480811498854\n",
      "test loss is 0.0013517190076735087\n",
      "Batch: 30600,train loss is: 0.0006733705268866202\n",
      "test loss is 0.0014468753604277456\n",
      "Batch: 30700,train loss is: 0.0009579362581606701\n",
      "test loss is 0.0015219064565724437\n",
      "Batch: 30800,train loss is: 0.00042730852282838854\n",
      "test loss is 0.0013086802220859534\n",
      "Batch: 30900,train loss is: 0.0008019439196959808\n",
      "test loss is 0.0017346919912440428\n",
      "Batch: 31000,train loss is: 0.000662840988725694\n",
      "test loss is 0.0012950531218918074\n",
      "Batch: 31100,train loss is: 0.0024756927426725917\n",
      "test loss is 0.004224290276727432\n",
      "Batch: 31200,train loss is: 0.0018669978278782755\n",
      "test loss is 0.0013329713637273237\n",
      "Batch: 31300,train loss is: 0.0008389372348512775\n",
      "test loss is 0.0013613099592987394\n",
      "Batch: 31400,train loss is: 0.0017851143080477865\n",
      "test loss is 0.0018994182298185203\n",
      "Batch: 31500,train loss is: 0.0008179729323735542\n",
      "test loss is 0.0013766286930457635\n",
      "Batch: 31600,train loss is: 0.0008356916423541773\n",
      "test loss is 0.0017970924700845877\n",
      "Batch: 31700,train loss is: 0.0006535342592777986\n",
      "test loss is 0.0011272063963247022\n",
      "Batch: 31800,train loss is: 0.001453592966696168\n",
      "test loss is 0.001760708137120944\n",
      "Batch: 31900,train loss is: 0.004998706423441786\n",
      "test loss is 0.002778895773762948\n",
      "Batch: 32000,train loss is: 0.010251158659677687\n",
      "test loss is 0.0014800733485112565\n",
      "Batch: 32100,train loss is: 0.0010920668911678507\n",
      "test loss is 0.0019368203354293513\n",
      "Batch: 32200,train loss is: 0.0010356448789002092\n",
      "test loss is 0.0017916158180702167\n",
      "Batch: 32300,train loss is: 0.0012098861160066128\n",
      "test loss is 0.0013338736265100278\n",
      "Batch: 32400,train loss is: 0.0008599151519682934\n",
      "test loss is 0.002450523319036831\n",
      "Batch: 32500,train loss is: 0.0029742379758487315\n",
      "test loss is 0.0024863421271702608\n",
      "Batch: 32600,train loss is: 0.0006784434797896451\n",
      "test loss is 0.0011625781462948738\n",
      "Batch: 32700,train loss is: 0.000707319323710726\n",
      "test loss is 0.0017542351309238662\n",
      "Batch: 32800,train loss is: 0.0004970191573050532\n",
      "test loss is 0.001870396328477351\n",
      "Batch: 32900,train loss is: 0.0013916984524458373\n",
      "test loss is 0.001306286431841244\n",
      "Batch: 33000,train loss is: 0.0007384798961161248\n",
      "test loss is 0.0013340444924134591\n",
      "Batch: 33100,train loss is: 0.0013688999729825665\n",
      "test loss is 0.0020263467209165843\n",
      "Batch: 33200,train loss is: 0.000873974324423376\n",
      "test loss is 0.001460603747143211\n",
      "Batch: 33300,train loss is: 0.0008734802585725059\n",
      "test loss is 0.0011848758104086056\n",
      "Batch: 33400,train loss is: 0.0027120950398852248\n",
      "test loss is 0.0019326459321177432\n",
      "Batch: 33500,train loss is: 0.0007676929551500888\n",
      "test loss is 0.0019311062351220574\n",
      "Batch: 33600,train loss is: 0.000772742265952188\n",
      "test loss is 0.0016517292573903935\n",
      "Batch: 33700,train loss is: 0.0016444465813527161\n",
      "test loss is 0.0017595851096493787\n",
      "Batch: 33800,train loss is: 0.008900065602934324\n",
      "test loss is 0.0021688993982296478\n",
      "Batch: 33900,train loss is: 0.0023949567867248968\n",
      "test loss is 0.001163629833554536\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch_NN.train import train_model\n",
    "\n",
    "train_model(loss_MSE,optim_Adam,model,data_loader,train_data,test_data,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Epoch: 0----------------------------------\n",
      "Batch: 0,train loss is: 0.0008136041264781578\n",
      "test loss is 0.0011932262283353896\n",
      "Batch: 100,train loss is: 0.001177657869277787\n",
      "test loss is 0.000990759981233816\n",
      "Batch: 200,train loss is: 0.001137975048906528\n",
      "test loss is 0.0009580303860115736\n",
      "Batch: 300,train loss is: 0.0004379209369922525\n",
      "test loss is 0.0009443038470637811\n",
      "Batch: 400,train loss is: 0.0006911707509453032\n",
      "test loss is 0.0009344677015693903\n",
      "Batch: 500,train loss is: 0.0006914656152541721\n",
      "test loss is 0.0009509406976755326\n",
      "Batch: 600,train loss is: 0.00048419083537505847\n",
      "test loss is 0.0009339807337114542\n",
      "Batch: 700,train loss is: 0.00039259703752652915\n",
      "test loss is 0.0009292155195822973\n",
      "Batch: 800,train loss is: 0.0005510201491311479\n",
      "test loss is 0.0009114298281759286\n",
      "Batch: 900,train loss is: 0.00043729376710663364\n",
      "test loss is 0.0009086317323654068\n",
      "Batch: 1000,train loss is: 0.0006141154500695981\n",
      "test loss is 0.0009204128226695306\n",
      "Batch: 1100,train loss is: 0.0029933260592356903\n",
      "test loss is 0.0009131596256208828\n",
      "Batch: 1200,train loss is: 0.00045615034643760685\n",
      "test loss is 0.0009230083654750324\n",
      "Batch: 1300,train loss is: 0.0007393772883751402\n",
      "test loss is 0.000909495570454325\n",
      "Batch: 1400,train loss is: 0.0007285615926897482\n",
      "test loss is 0.0009086829105798027\n",
      "Batch: 1500,train loss is: 0.000627849590375454\n",
      "test loss is 0.0009130151814871556\n",
      "Batch: 1600,train loss is: 0.00041271522858433426\n",
      "test loss is 0.0009130532998265161\n",
      "Batch: 1700,train loss is: 0.0012167356335519336\n",
      "test loss is 0.0009031987921291393\n",
      "Batch: 1800,train loss is: 0.0006004938352198662\n",
      "test loss is 0.0009290978523701261\n",
      "Batch: 1900,train loss is: 0.000493683470020935\n",
      "test loss is 0.0009582488329366368\n",
      "Batch: 2000,train loss is: 0.0009245419708392072\n",
      "test loss is 0.0009497583781988058\n",
      "Batch: 2100,train loss is: 0.0008695673981708813\n",
      "test loss is 0.0009581095094331355\n",
      "Batch: 2200,train loss is: 0.0011860879077937522\n",
      "test loss is 0.0009140788137732578\n",
      "Batch: 2300,train loss is: 0.00046388748845673715\n",
      "test loss is 0.0009447422613315466\n",
      "Batch: 2400,train loss is: 0.000901702685565971\n",
      "test loss is 0.0008977522251952379\n",
      "Batch: 2500,train loss is: 0.0008782093321171729\n",
      "test loss is 0.000899335553275432\n",
      "Batch: 2600,train loss is: 0.0003485752542328339\n",
      "test loss is 0.0009039304040132167\n",
      "Batch: 2700,train loss is: 0.0007895768125329224\n",
      "test loss is 0.0008955809090022017\n",
      "Batch: 2800,train loss is: 0.0006610986316629719\n",
      "test loss is 0.0009337957861396147\n",
      "Batch: 2900,train loss is: 0.0005263798797080966\n",
      "test loss is 0.0008920327962567311\n",
      "Batch: 3000,train loss is: 0.0006335300681530427\n",
      "test loss is 0.0008976564698231745\n",
      "Batch: 3100,train loss is: 0.0006095521512231212\n",
      "test loss is 0.0009074070144170243\n",
      "Batch: 3200,train loss is: 0.0009445249042349886\n",
      "test loss is 0.0009137413332115177\n",
      "Batch: 3300,train loss is: 0.0005463006103159831\n",
      "test loss is 0.0008986794223287916\n",
      "Batch: 3400,train loss is: 0.0006864238435797523\n",
      "test loss is 0.00091803838508749\n",
      "Batch: 3500,train loss is: 0.0004606475352298245\n",
      "test loss is 0.0009181644668011445\n",
      "Batch: 3600,train loss is: 0.0007043804070705552\n",
      "test loss is 0.0009377694692436443\n",
      "Batch: 3700,train loss is: 0.0005454035529347611\n",
      "test loss is 0.0008837401103708746\n",
      "Batch: 3800,train loss is: 0.00042370166573243935\n",
      "test loss is 0.0009381669104196752\n",
      "Batch: 3900,train loss is: 0.0006040097672423197\n",
      "test loss is 0.0008894200724507724\n",
      "Batch: 4000,train loss is: 0.0003967649586695536\n",
      "test loss is 0.0009008807597701142\n",
      "Batch: 4100,train loss is: 0.0008119363343454783\n",
      "test loss is 0.0009041099337474702\n",
      "Batch: 4200,train loss is: 0.00045631611000363225\n",
      "test loss is 0.0009254681074702162\n",
      "Batch: 4300,train loss is: 0.0006173870683627427\n",
      "test loss is 0.0009008905622986584\n",
      "Batch: 4400,train loss is: 0.0008824313859215629\n",
      "test loss is 0.0008987197702729917\n",
      "Batch: 4500,train loss is: 0.0007562198682236554\n",
      "test loss is 0.0009159865035878322\n",
      "Batch: 4600,train loss is: 0.000802352878464442\n",
      "test loss is 0.0008973724915963483\n",
      "Batch: 4700,train loss is: 0.000966866556741923\n",
      "test loss is 0.0009148759974988529\n",
      "Batch: 4800,train loss is: 0.0014139050329298442\n",
      "test loss is 0.0009019744890899394\n",
      "Batch: 4900,train loss is: 0.0006633926981675703\n",
      "test loss is 0.0008909726292799042\n",
      "Batch: 5000,train loss is: 0.0006662725762193033\n",
      "test loss is 0.0009026482609520072\n",
      "Batch: 5100,train loss is: 0.0007156434506124653\n",
      "test loss is 0.0009174342992596577\n",
      "Batch: 5200,train loss is: 0.0005060456212770733\n",
      "test loss is 0.0008932687108702016\n",
      "Batch: 5300,train loss is: 0.0009422268191376919\n",
      "test loss is 0.0009205202913714124\n",
      "Batch: 5400,train loss is: 0.0013175780094432422\n",
      "test loss is 0.0008883222969276517\n",
      "Batch: 5500,train loss is: 0.0007427938255218529\n",
      "test loss is 0.0009264298773132423\n",
      "Batch: 5600,train loss is: 0.0008120214564605821\n",
      "test loss is 0.0008999287603309367\n",
      "Batch: 5700,train loss is: 0.0009981601285899053\n",
      "test loss is 0.0008852154252657395\n",
      "Batch: 5800,train loss is: 0.0006141446247623682\n",
      "test loss is 0.0008989480982835179\n",
      "Batch: 5900,train loss is: 0.004109191370178219\n",
      "test loss is 0.0009024567295074209\n",
      "Batch: 6000,train loss is: 0.0017790912797429736\n",
      "test loss is 0.000927078906100747\n",
      "Batch: 6100,train loss is: 0.0005452064515045939\n",
      "test loss is 0.0008822435069694794\n",
      "Batch: 6200,train loss is: 0.0005804358068521369\n",
      "test loss is 0.0008987075849802002\n",
      "Batch: 6300,train loss is: 0.0017238042845959417\n",
      "test loss is 0.00090247850267852\n",
      "Batch: 6400,train loss is: 0.00047377687898828203\n",
      "test loss is 0.0009344159582865329\n",
      "Batch: 6500,train loss is: 0.0003383672660014845\n",
      "test loss is 0.0009127745417880265\n",
      "Batch: 6600,train loss is: 0.002519057883155878\n",
      "test loss is 0.0008950295988615554\n",
      "Batch: 6700,train loss is: 0.0006537512086468734\n",
      "test loss is 0.0009041297566920161\n",
      "Batch: 6800,train loss is: 0.0014257905208326983\n",
      "test loss is 0.0008803296120918091\n",
      "Batch: 6900,train loss is: 0.0006870089931799893\n",
      "test loss is 0.0008879214152448326\n",
      "Batch: 7000,train loss is: 0.0006800320376861581\n",
      "test loss is 0.0009458441272932076\n",
      "Batch: 7100,train loss is: 0.000909907750732479\n",
      "test loss is 0.0008856954830666378\n",
      "Batch: 7200,train loss is: 0.0005292586588117188\n",
      "test loss is 0.0009222193834523987\n",
      "Batch: 7300,train loss is: 0.0009202808476352956\n",
      "test loss is 0.0009358975862901285\n",
      "Batch: 7400,train loss is: 0.00050783421694179\n",
      "test loss is 0.0008791631399204228\n",
      "Batch: 7500,train loss is: 0.0010375522174729697\n",
      "test loss is 0.0009227649882038914\n",
      "Batch: 7600,train loss is: 0.0007528773141049129\n",
      "test loss is 0.0008914095193616563\n",
      "Batch: 7700,train loss is: 0.0008526872970242102\n",
      "test loss is 0.0008857186109631766\n",
      "Batch: 7800,train loss is: 0.0003500359848953888\n",
      "test loss is 0.0008981211902379454\n",
      "Batch: 7900,train loss is: 0.0007924282019531395\n",
      "test loss is 0.0008979810598228541\n",
      "Batch: 8000,train loss is: 0.0005468314820991079\n",
      "test loss is 0.0009188857897672899\n",
      "Batch: 8100,train loss is: 0.0007787922472480845\n",
      "test loss is 0.0009107795569820061\n",
      "Batch: 8200,train loss is: 0.0013604645877216964\n",
      "test loss is 0.0009038872929255933\n",
      "Batch: 8300,train loss is: 0.0005992218814862574\n",
      "test loss is 0.0009071336952941277\n",
      "Batch: 8400,train loss is: 0.0005713432818991942\n",
      "test loss is 0.0008861297630559397\n",
      "Batch: 8500,train loss is: 0.000894817950656751\n",
      "test loss is 0.0009114172688673295\n",
      "Batch: 8600,train loss is: 0.0008821016936769642\n",
      "test loss is 0.0008997515294938067\n",
      "Batch: 8700,train loss is: 0.0005005429021065987\n",
      "test loss is 0.0008946654590965554\n",
      "Batch: 8800,train loss is: 0.0003684607201076147\n",
      "test loss is 0.0008818464336806715\n",
      "Batch: 8900,train loss is: 0.0011073726035326177\n",
      "test loss is 0.0008819496751807011\n",
      "Batch: 9000,train loss is: 0.00035110746209334435\n",
      "test loss is 0.0008905842870557133\n",
      "Batch: 9100,train loss is: 0.0005943928409734359\n",
      "test loss is 0.0008958326876824381\n",
      "Batch: 9200,train loss is: 0.0009959937672276479\n",
      "test loss is 0.000931537814843955\n",
      "Batch: 9300,train loss is: 0.000758351938059971\n",
      "test loss is 0.0008839088999181809\n",
      "Batch: 9400,train loss is: 0.0006410249644312995\n",
      "test loss is 0.0009470591476791815\n",
      "Batch: 9500,train loss is: 0.00047017957844150556\n",
      "test loss is 0.0008962298164830101\n",
      "Batch: 9600,train loss is: 0.0006056651287314985\n",
      "test loss is 0.0008854076260749797\n",
      "Batch: 9700,train loss is: 0.0005784553732270363\n",
      "test loss is 0.0009084374367633755\n",
      "Batch: 9800,train loss is: 0.000711011337787248\n",
      "test loss is 0.0008871677225511756\n",
      "Batch: 9900,train loss is: 0.0006784296807742396\n",
      "test loss is 0.0009086762456523458\n",
      "Batch: 10000,train loss is: 0.0006348616261800039\n",
      "test loss is 0.0009012886676156139\n",
      "Batch: 10100,train loss is: 0.0012069367349025438\n",
      "test loss is 0.0008796698378091245\n",
      "Batch: 10200,train loss is: 0.0017321571891796403\n",
      "test loss is 0.0009849254292526493\n",
      "Batch: 10300,train loss is: 0.0006317655366202842\n",
      "test loss is 0.0008988708203518682\n",
      "Batch: 10400,train loss is: 0.0004891138477057252\n",
      "test loss is 0.0008791861704853017\n",
      "Batch: 10500,train loss is: 0.0007468484556095805\n",
      "test loss is 0.0008833944076085992\n",
      "Batch: 10600,train loss is: 0.00036260427543076916\n",
      "test loss is 0.0009232452183274716\n",
      "Batch: 10700,train loss is: 0.0006893270871900783\n",
      "test loss is 0.0008818294175329054\n",
      "Batch: 10800,train loss is: 0.0004666329650411379\n",
      "test loss is 0.0009049946397484572\n",
      "Batch: 10900,train loss is: 0.0007821523920334332\n",
      "test loss is 0.0008814101545171793\n",
      "Batch: 11000,train loss is: 0.0005798198023755993\n",
      "test loss is 0.0009125532918448235\n",
      "Batch: 11100,train loss is: 0.001140463018655543\n",
      "test loss is 0.0008826178533404036\n",
      "Batch: 11200,train loss is: 0.0006774705566137794\n",
      "test loss is 0.0008872762014320024\n",
      "Batch: 11300,train loss is: 0.0004744852325168062\n",
      "test loss is 0.001010402816214525\n",
      "Batch: 11400,train loss is: 0.0008179650530799828\n",
      "test loss is 0.0009045716321490095\n",
      "Batch: 11500,train loss is: 0.0006455338387955102\n",
      "test loss is 0.0008856651977666156\n",
      "Batch: 11600,train loss is: 0.0022126087131023155\n",
      "test loss is 0.0009042760078403344\n",
      "Batch: 11700,train loss is: 0.0004329136745444767\n",
      "test loss is 0.000864246488364803\n",
      "Batch: 11800,train loss is: 0.0006797829907503482\n",
      "test loss is 0.0008843846582246023\n",
      "Batch: 11900,train loss is: 0.0007320583603101148\n",
      "test loss is 0.0008894770283780268\n",
      "Batch: 12000,train loss is: 0.0004763095097937328\n",
      "test loss is 0.000895304303966071\n",
      "Batch: 12100,train loss is: 0.002007230712106481\n",
      "test loss is 0.0008874014275543188\n",
      "Batch: 12200,train loss is: 0.0007726837612171397\n",
      "test loss is 0.0008971613136166334\n",
      "Batch: 12300,train loss is: 0.0005486459433939842\n",
      "test loss is 0.0009216504045643568\n",
      "Batch: 12400,train loss is: 0.0009226846894830429\n",
      "test loss is 0.0008632512465522222\n",
      "Batch: 12500,train loss is: 0.00047410153047493687\n",
      "test loss is 0.0009028645991276303\n",
      "Batch: 12600,train loss is: 0.000451994236435591\n",
      "test loss is 0.0008861090421712159\n",
      "Batch: 12700,train loss is: 0.0005913384240039611\n",
      "test loss is 0.0008684414399313368\n",
      "Batch: 12800,train loss is: 0.0005352152794655167\n",
      "test loss is 0.0008785239020941177\n",
      "Batch: 12900,train loss is: 0.0009117989385620549\n",
      "test loss is 0.0008880334733245916\n",
      "Batch: 13000,train loss is: 0.00034679544067521675\n",
      "test loss is 0.0008661821443693852\n",
      "Batch: 13100,train loss is: 0.0023514921475663556\n",
      "test loss is 0.000866821060950546\n",
      "Batch: 13200,train loss is: 0.0008283793788924843\n",
      "test loss is 0.0009380228538154948\n",
      "Batch: 13300,train loss is: 0.0004420639117929729\n",
      "test loss is 0.0009296218477679139\n",
      "Batch: 13400,train loss is: 0.000432720696801857\n",
      "test loss is 0.0008916308220027218\n",
      "Batch: 13500,train loss is: 0.0006787489207662876\n",
      "test loss is 0.0008984259591843166\n",
      "Batch: 13600,train loss is: 0.000678793043059571\n",
      "test loss is 0.0008763953058266742\n",
      "Batch: 13700,train loss is: 0.0007914280769720339\n",
      "test loss is 0.0008840314258105564\n",
      "Batch: 13800,train loss is: 0.0009569936490030461\n",
      "test loss is 0.0008923387534666513\n",
      "Batch: 13900,train loss is: 0.0009221656433638676\n",
      "test loss is 0.0008824685539063009\n",
      "Batch: 14000,train loss is: 0.0004424266777289591\n",
      "test loss is 0.0008729899659370043\n",
      "Batch: 14100,train loss is: 0.0009506156927330999\n",
      "test loss is 0.0009335594745351382\n",
      "Batch: 14200,train loss is: 0.000563563964923003\n",
      "test loss is 0.0008603930873515089\n",
      "Batch: 14300,train loss is: 0.0006295202575027633\n",
      "test loss is 0.0008666607770388623\n",
      "Batch: 14400,train loss is: 0.0016411640477270778\n",
      "test loss is 0.0008673239466698765\n",
      "Batch: 14500,train loss is: 0.0005561965688686835\n",
      "test loss is 0.000874053755636874\n",
      "Batch: 14600,train loss is: 0.002362612719193593\n",
      "test loss is 0.0008719902374807895\n",
      "Batch: 14700,train loss is: 0.00047245306300019443\n",
      "test loss is 0.0008815395271128785\n",
      "Batch: 14800,train loss is: 0.0009964327967552677\n",
      "test loss is 0.000876343104208563\n",
      "Batch: 14900,train loss is: 0.0004268157358168682\n",
      "test loss is 0.0008818594939865357\n",
      "Batch: 15000,train loss is: 0.0009187789145355132\n",
      "test loss is 0.0008703082566111151\n",
      "Batch: 15100,train loss is: 0.0007742805934427562\n",
      "test loss is 0.0008979500363812064\n",
      "Batch: 15200,train loss is: 0.0014290166096013197\n",
      "test loss is 0.0009121257216314092\n",
      "Batch: 15300,train loss is: 0.0005869430298680335\n",
      "test loss is 0.0008713620400371434\n",
      "Batch: 15400,train loss is: 0.0005507989636028799\n",
      "test loss is 0.0009016079208624268\n",
      "Batch: 15500,train loss is: 0.0008451083733390891\n",
      "test loss is 0.000863537877894908\n",
      "Batch: 15600,train loss is: 0.0011712805651822344\n",
      "test loss is 0.0008957183655388884\n",
      "Batch: 15700,train loss is: 0.000745761994289381\n",
      "test loss is 0.0008684317226955908\n",
      "Batch: 15800,train loss is: 0.0007031201865666865\n",
      "test loss is 0.0008809304884480817\n",
      "Batch: 15900,train loss is: 0.001304679012823598\n",
      "test loss is 0.0008919291444873642\n",
      "Batch: 16000,train loss is: 0.0005889492686646123\n",
      "test loss is 0.0009067373277498507\n",
      "Batch: 16100,train loss is: 0.0007504660655371233\n",
      "test loss is 0.0008938382265466154\n",
      "Batch: 16200,train loss is: 0.0005513128135670143\n",
      "test loss is 0.0008759310131584215\n",
      "Batch: 16300,train loss is: 0.0013492053025293967\n",
      "test loss is 0.0008616521013514852\n",
      "Batch: 16400,train loss is: 0.00047876826975855563\n",
      "test loss is 0.0008624824596975846\n",
      "Batch: 16500,train loss is: 0.0006612603179038982\n",
      "test loss is 0.000907993296118905\n",
      "Batch: 16600,train loss is: 0.0003629459836078285\n",
      "test loss is 0.0008866576545951868\n",
      "Batch: 16700,train loss is: 0.0013781961872474432\n",
      "test loss is 0.0008881151724422275\n",
      "Batch: 16800,train loss is: 0.0007281798034078443\n",
      "test loss is 0.0008914476011551407\n",
      "Batch: 16900,train loss is: 0.0007775696325171263\n",
      "test loss is 0.000890382416701076\n",
      "Batch: 17000,train loss is: 0.0007395042238822098\n",
      "test loss is 0.0008659308786546365\n",
      "Batch: 17100,train loss is: 0.0006838879062041191\n",
      "test loss is 0.0008778727094717472\n",
      "Batch: 17200,train loss is: 0.0008219640120926263\n",
      "test loss is 0.0008759942127387912\n",
      "Batch: 17300,train loss is: 0.000793090038304535\n",
      "test loss is 0.0008725975218867703\n",
      "Batch: 17400,train loss is: 0.0006072190355077641\n",
      "test loss is 0.0008829185027251023\n",
      "Batch: 17500,train loss is: 0.0005852175478979093\n",
      "test loss is 0.0008808123745802305\n",
      "Batch: 17600,train loss is: 0.000556258352825419\n",
      "test loss is 0.0008971755932825264\n",
      "Batch: 17700,train loss is: 0.0003058094279480032\n",
      "test loss is 0.0009022887941936622\n",
      "Batch: 17800,train loss is: 0.001261500979961568\n",
      "test loss is 0.0008742810517042852\n",
      "Batch: 17900,train loss is: 0.000551136371176778\n",
      "test loss is 0.0009173101350649328\n",
      "Batch: 18000,train loss is: 0.00040896550576428414\n",
      "test loss is 0.0008631359355131221\n",
      "Batch: 18100,train loss is: 0.0005498208969085789\n",
      "test loss is 0.0008681991397064919\n",
      "Batch: 18200,train loss is: 0.0005148379782732246\n",
      "test loss is 0.0008740419915619034\n",
      "Batch: 18300,train loss is: 0.0009596498491819986\n",
      "test loss is 0.0008702429535186873\n",
      "Batch: 18400,train loss is: 0.0006370362736693125\n",
      "test loss is 0.0008697441113913604\n",
      "Batch: 18500,train loss is: 0.0007314669454259524\n",
      "test loss is 0.0009043203084102264\n",
      "Batch: 18600,train loss is: 0.00038139353828749026\n",
      "test loss is 0.0008726698170563092\n",
      "Batch: 18700,train loss is: 0.000714033861494964\n",
      "test loss is 0.000867475860070145\n",
      "Batch: 18800,train loss is: 0.0008876637165529988\n",
      "test loss is 0.0008755242745756737\n",
      "Batch: 18900,train loss is: 0.0008542909298905074\n",
      "test loss is 0.0009129749666814841\n",
      "Batch: 19000,train loss is: 0.0008435127710145189\n",
      "test loss is 0.0008743272740869127\n",
      "Batch: 19100,train loss is: 0.0004950199314346839\n",
      "test loss is 0.0008796597149263517\n",
      "Batch: 19200,train loss is: 0.0007989646536101312\n",
      "test loss is 0.000885044684961718\n",
      "Batch: 19300,train loss is: 0.0009833751281531197\n",
      "test loss is 0.0008677745229015731\n",
      "Batch: 19400,train loss is: 0.000393724011628839\n",
      "test loss is 0.0008600984547154583\n",
      "Batch: 19500,train loss is: 0.00074670996572129\n",
      "test loss is 0.0009277969661917174\n",
      "Batch: 19600,train loss is: 0.0005125376895012907\n",
      "test loss is 0.0008789346084112047\n",
      "Batch: 19700,train loss is: 0.0007955341344050848\n",
      "test loss is 0.0009226971719832098\n",
      "Batch: 19800,train loss is: 0.0004768670647595268\n",
      "test loss is 0.000874819926526861\n",
      "Batch: 19900,train loss is: 0.0004048191265569699\n",
      "test loss is 0.0008913847307090671\n",
      "Batch: 20000,train loss is: 0.0008161891819580752\n",
      "test loss is 0.0008692693125158436\n",
      "Batch: 20100,train loss is: 0.000779716345272291\n",
      "test loss is 0.0008751132094545855\n",
      "Batch: 20200,train loss is: 0.0009664440295777456\n",
      "test loss is 0.000902902095531284\n",
      "Batch: 20300,train loss is: 0.00045705241498348817\n",
      "test loss is 0.0008772210418874447\n",
      "Batch: 20400,train loss is: 0.0006654932143508572\n",
      "test loss is 0.0009197332119232189\n",
      "Batch: 20500,train loss is: 0.00067467718610364\n",
      "test loss is 0.0008760238765530038\n",
      "Batch: 20600,train loss is: 0.0009086856418557447\n",
      "test loss is 0.0009197247513336238\n",
      "Batch: 20700,train loss is: 0.0005577405206394503\n",
      "test loss is 0.0008894630266968064\n",
      "Batch: 20800,train loss is: 0.0003979083415920702\n",
      "test loss is 0.0008723428163929857\n",
      "Batch: 20900,train loss is: 0.0008449074788898285\n",
      "test loss is 0.0008996402311536909\n",
      "Batch: 21000,train loss is: 0.0004633805280753524\n",
      "test loss is 0.0008705112528797557\n",
      "Batch: 21100,train loss is: 0.0006645225027505866\n",
      "test loss is 0.0008957022940830245\n",
      "Batch: 21200,train loss is: 0.00034164133886608374\n",
      "test loss is 0.0008676644917675633\n",
      "Batch: 21300,train loss is: 0.0004516990334084129\n",
      "test loss is 0.0008910249036479617\n",
      "Batch: 21400,train loss is: 0.0009358648152748237\n",
      "test loss is 0.0008870333250636478\n",
      "Batch: 21500,train loss is: 0.0003095507854252596\n",
      "test loss is 0.0008844544913184828\n",
      "Batch: 21600,train loss is: 0.001034598735515997\n",
      "test loss is 0.0008755840013123597\n",
      "Batch: 21700,train loss is: 0.0004043369816052104\n",
      "test loss is 0.0008608555998802918\n",
      "Batch: 21800,train loss is: 0.000769764782156834\n",
      "test loss is 0.00090902637593021\n",
      "Batch: 21900,train loss is: 0.0007351628297373668\n",
      "test loss is 0.0009548757551565086\n",
      "Batch: 22000,train loss is: 0.000893189890619347\n",
      "test loss is 0.0008666793009174706\n",
      "Batch: 22100,train loss is: 0.00032298688126704126\n",
      "test loss is 0.0008667269468921379\n",
      "Batch: 22200,train loss is: 0.0006302103541957789\n",
      "test loss is 0.0008735158323870712\n",
      "Batch: 22300,train loss is: 0.00102229281450935\n",
      "test loss is 0.0008678804928567217\n",
      "Batch: 22400,train loss is: 0.0006795923186287858\n",
      "test loss is 0.000857352304808381\n",
      "Batch: 22500,train loss is: 0.000497856843340065\n",
      "test loss is 0.0008621465318777204\n",
      "Batch: 22600,train loss is: 0.0012686249807592508\n",
      "test loss is 0.0009312777591179262\n",
      "Batch: 22700,train loss is: 0.0004463348135079789\n",
      "test loss is 0.0008705627364763048\n",
      "Batch: 22800,train loss is: 0.0005424993700881309\n",
      "test loss is 0.0008759486808442593\n",
      "Batch: 22900,train loss is: 0.0004214046479579552\n",
      "test loss is 0.0009412828331823032\n",
      "Batch: 23000,train loss is: 0.000531201919640991\n",
      "test loss is 0.0008669126066808339\n",
      "Batch: 23100,train loss is: 0.0007621705363981244\n",
      "test loss is 0.000846965037766402\n",
      "Batch: 23200,train loss is: 0.00040323761144095947\n",
      "test loss is 0.0008498458729105248\n",
      "Batch: 23300,train loss is: 0.0006515204992957536\n",
      "test loss is 0.0008570682160657455\n",
      "Batch: 23400,train loss is: 0.0009113389429490918\n",
      "test loss is 0.0008727212569691617\n",
      "Batch: 23500,train loss is: 0.003547986950143507\n",
      "test loss is 0.0008671003562042345\n",
      "Batch: 23600,train loss is: 0.00045078274880665323\n",
      "test loss is 0.000852960373423301\n",
      "Batch: 23700,train loss is: 0.0009995741371667537\n",
      "test loss is 0.0008836503553010342\n",
      "Batch: 23800,train loss is: 0.0011379930435888451\n",
      "test loss is 0.000857025604534524\n",
      "Batch: 23900,train loss is: 0.0009848883415367914\n",
      "test loss is 0.0008514744069388758\n",
      "Batch: 24000,train loss is: 0.000667001438642411\n",
      "test loss is 0.0008992805982630411\n",
      "Batch: 24100,train loss is: 0.0006437558895453652\n",
      "test loss is 0.0008559996798896771\n",
      "Batch: 24200,train loss is: 0.001860959679565515\n",
      "test loss is 0.0008768068703630074\n",
      "Batch: 24300,train loss is: 0.000564746231154772\n",
      "test loss is 0.000986114078154526\n",
      "Batch: 24400,train loss is: 0.001497899003352084\n",
      "test loss is 0.0008636808963311346\n",
      "Batch: 24500,train loss is: 0.0005666153053570328\n",
      "test loss is 0.0008698815722814951\n",
      "Batch: 24600,train loss is: 0.0005883067789368981\n",
      "test loss is 0.0008928126727114395\n",
      "Batch: 24700,train loss is: 0.0003749641953516045\n",
      "test loss is 0.0008539291155578846\n",
      "Batch: 24800,train loss is: 0.001138294891728052\n",
      "test loss is 0.000862428825013535\n",
      "Batch: 24900,train loss is: 0.000221476475275725\n",
      "test loss is 0.0008936843574683672\n",
      "Batch: 25000,train loss is: 0.0007291176859278213\n",
      "test loss is 0.000861036963817128\n",
      "Batch: 25100,train loss is: 0.0007208274222580698\n",
      "test loss is 0.0008702457091616079\n",
      "Batch: 25200,train loss is: 0.0009234612800435773\n",
      "test loss is 0.0008765035725833857\n",
      "Batch: 25300,train loss is: 0.0009598224689393086\n",
      "test loss is 0.000884503814715441\n",
      "Batch: 25400,train loss is: 0.0004938356292653219\n",
      "test loss is 0.0008552850917268705\n",
      "Batch: 25500,train loss is: 0.0005973102919817925\n",
      "test loss is 0.0008543866148915939\n",
      "Batch: 25600,train loss is: 0.0007684609499071657\n",
      "test loss is 0.0008718264545687032\n",
      "Batch: 25700,train loss is: 0.0007751480101425842\n",
      "test loss is 0.0008820551564421886\n",
      "Batch: 25800,train loss is: 0.00031277101224009906\n",
      "test loss is 0.0008562962932079683\n",
      "Batch: 25900,train loss is: 0.0005290691606277\n",
      "test loss is 0.0009034515521146076\n",
      "Batch: 26000,train loss is: 0.0019837759119293555\n",
      "test loss is 0.0009184238898896099\n",
      "Batch: 26100,train loss is: 0.0010038070661957715\n",
      "test loss is 0.0008749785021141454\n",
      "Batch: 26200,train loss is: 0.0010202034976829468\n",
      "test loss is 0.0008426310148635218\n",
      "Batch: 26300,train loss is: 0.0014220894838550175\n",
      "test loss is 0.000896052964447613\n",
      "Batch: 26400,train loss is: 0.0008131091759835948\n",
      "test loss is 0.0008589072922900479\n",
      "Batch: 26500,train loss is: 0.0005319555377010523\n",
      "test loss is 0.0008807049127361195\n",
      "Batch: 26600,train loss is: 0.001070158708030613\n",
      "test loss is 0.0008916129379985143\n",
      "Batch: 26700,train loss is: 0.00046497293854280697\n",
      "test loss is 0.0009733722236113386\n",
      "Batch: 26800,train loss is: 0.0015131902336596012\n",
      "test loss is 0.0008684635170419573\n",
      "Batch: 26900,train loss is: 0.0006749714556039977\n",
      "test loss is 0.0008554483446102687\n",
      "Batch: 27000,train loss is: 0.0007528918567434956\n",
      "test loss is 0.000991654953117365\n",
      "Batch: 27100,train loss is: 0.00048616787057969283\n",
      "test loss is 0.0008686098679479063\n",
      "Batch: 27200,train loss is: 0.0006039796465959872\n",
      "test loss is 0.0008943123059425918\n",
      "Batch: 27300,train loss is: 0.0006413892043632597\n",
      "test loss is 0.0008749130608235159\n",
      "Batch: 27400,train loss is: 0.000829937018765215\n",
      "test loss is 0.000842781095437035\n",
      "Batch: 27500,train loss is: 0.000662980631227987\n",
      "test loss is 0.0009443624306763324\n",
      "Batch: 27600,train loss is: 0.0007563191420999673\n",
      "test loss is 0.0009187278335069726\n",
      "Batch: 27700,train loss is: 0.0022755857506256932\n",
      "test loss is 0.0008645315165303551\n",
      "Batch: 27800,train loss is: 0.0007721888667742883\n",
      "test loss is 0.0008782366851872113\n",
      "Batch: 27900,train loss is: 0.0012839142739451564\n",
      "test loss is 0.0008477749265099208\n",
      "Batch: 28000,train loss is: 0.0004935170734255226\n",
      "test loss is 0.0008767230087603261\n",
      "Batch: 28100,train loss is: 0.0007870016758666835\n",
      "test loss is 0.0008662485645222269\n",
      "Batch: 28200,train loss is: 0.0008504122752788984\n",
      "test loss is 0.0008382260747163586\n",
      "Batch: 28300,train loss is: 0.0021723493494540977\n",
      "test loss is 0.0008486872351588884\n",
      "Batch: 28400,train loss is: 0.0006546008835093605\n",
      "test loss is 0.0008647098979015109\n",
      "Batch: 28500,train loss is: 0.00132145469558662\n",
      "test loss is 0.000866352910321513\n",
      "Batch: 28600,train loss is: 0.0005678341739531114\n",
      "test loss is 0.0008661311872411216\n",
      "Batch: 28700,train loss is: 0.0005123470472668615\n",
      "test loss is 0.0008525930369660045\n",
      "Batch: 28800,train loss is: 0.0017518894554549233\n",
      "test loss is 0.0008602913928381449\n",
      "Batch: 28900,train loss is: 0.000785127120478254\n",
      "test loss is 0.0008593729258699572\n",
      "Batch: 29000,train loss is: 0.000637075294732678\n",
      "test loss is 0.0008468721520610736\n",
      "Batch: 29100,train loss is: 0.0007623476140729879\n",
      "test loss is 0.0008854447815261222\n",
      "Batch: 29200,train loss is: 0.0005494656862764689\n",
      "test loss is 0.0008725631569303619\n",
      "Batch: 29300,train loss is: 0.0009911381707158543\n",
      "test loss is 0.0008588284306359632\n",
      "Batch: 29400,train loss is: 0.0007063845704901739\n",
      "test loss is 0.000893875487079465\n",
      "Batch: 29500,train loss is: 0.000517168461506901\n",
      "test loss is 0.0008613047305449812\n",
      "Batch: 29600,train loss is: 0.0007014509700799451\n",
      "test loss is 0.0008679790561768649\n",
      "Batch: 29700,train loss is: 0.00032783235213775755\n",
      "test loss is 0.0008829222084218947\n",
      "Batch: 29800,train loss is: 0.001283278167373961\n",
      "test loss is 0.0008541226073311505\n",
      "Batch: 29900,train loss is: 0.0037071639558969526\n",
      "test loss is 0.0008776708954601666\n",
      "Batch: 30000,train loss is: 0.0004149191938704319\n",
      "test loss is 0.0008854053216785743\n",
      "Batch: 30100,train loss is: 0.000528990975717967\n",
      "test loss is 0.0008902193280438788\n",
      "Batch: 30200,train loss is: 0.0004548570546423823\n",
      "test loss is 0.0008497541138293371\n",
      "Batch: 30300,train loss is: 0.00048089509338825244\n",
      "test loss is 0.0008513011284269469\n",
      "Batch: 30400,train loss is: 0.0004902243207459551\n",
      "test loss is 0.0008494185020934598\n",
      "Batch: 30500,train loss is: 0.0005140526475817004\n",
      "test loss is 0.0008823084583355142\n",
      "Batch: 30600,train loss is: 0.0003282738752014026\n",
      "test loss is 0.0008594974035757506\n",
      "Batch: 30700,train loss is: 0.0006290108255869885\n",
      "test loss is 0.0009749392055063911\n",
      "Batch: 30800,train loss is: 0.00036984384071719764\n",
      "test loss is 0.0009125811636309285\n",
      "Batch: 30900,train loss is: 0.0004882248875403725\n",
      "test loss is 0.0008585149005401278\n",
      "Batch: 31000,train loss is: 0.0006009246711758543\n",
      "test loss is 0.0008634170164939236\n",
      "Batch: 31100,train loss is: 0.000403347797229675\n",
      "test loss is 0.000910857270306879\n",
      "Batch: 31200,train loss is: 0.000748782964644605\n",
      "test loss is 0.0008585453401871486\n",
      "Batch: 31300,train loss is: 0.0007464372819678544\n",
      "test loss is 0.0008794022944298243\n",
      "Batch: 31400,train loss is: 0.000862127012983855\n",
      "test loss is 0.0008474092073398958\n",
      "Batch: 31500,train loss is: 0.0006982807015369271\n",
      "test loss is 0.0008691583519394612\n",
      "Batch: 31600,train loss is: 0.000617225463828154\n",
      "test loss is 0.0008564290341102007\n",
      "Batch: 31700,train loss is: 0.0006063778126120606\n",
      "test loss is 0.0009165588467744708\n",
      "Batch: 31800,train loss is: 0.0011208204210491882\n",
      "test loss is 0.0008598043257793956\n",
      "Batch: 31900,train loss is: 0.0010396496643523682\n",
      "test loss is 0.0008910268780692167\n",
      "Batch: 32000,train loss is: 0.005191769670961976\n",
      "test loss is 0.0008508780145633214\n",
      "Batch: 32100,train loss is: 0.00043784237202912823\n",
      "test loss is 0.0008452166903430606\n",
      "Batch: 32200,train loss is: 0.0005814363471555603\n",
      "test loss is 0.0008586344005860422\n",
      "Batch: 32300,train loss is: 0.0004722028030788174\n",
      "test loss is 0.0008586563491482667\n",
      "Batch: 32400,train loss is: 0.000535978900304201\n",
      "test loss is 0.0008411040759595018\n",
      "Batch: 32500,train loss is: 0.0011694800027588102\n",
      "test loss is 0.0008854388807001481\n",
      "Batch: 32600,train loss is: 0.00044234469082166436\n",
      "test loss is 0.0008593110965412568\n",
      "Batch: 32700,train loss is: 0.0005175761242937078\n",
      "test loss is 0.0008495054259177903\n",
      "Batch: 32800,train loss is: 0.0004675331051902075\n",
      "test loss is 0.0008559901008220387\n",
      "Batch: 32900,train loss is: 0.0007297198689720114\n",
      "test loss is 0.0008570153377863565\n",
      "Batch: 33000,train loss is: 0.0006859555116150551\n",
      "test loss is 0.0008361877655839614\n",
      "Batch: 33100,train loss is: 0.0005803040543792005\n",
      "test loss is 0.0008795670608589546\n",
      "Batch: 33200,train loss is: 0.0006768392375924216\n",
      "test loss is 0.0009536379314153225\n",
      "Batch: 33300,train loss is: 0.0003211448270363158\n",
      "test loss is 0.0008375506604510648\n",
      "Batch: 33400,train loss is: 0.0005624009060438497\n",
      "test loss is 0.0008690906370343952\n",
      "Batch: 33500,train loss is: 0.0006245660226107186\n",
      "test loss is 0.0008464547222492958\n",
      "Batch: 33600,train loss is: 0.0006308939572396379\n",
      "test loss is 0.0008646337559569438\n",
      "Batch: 33700,train loss is: 0.0007161303338147952\n",
      "test loss is 0.000861109327498045\n",
      "Batch: 33800,train loss is: 0.0008043963745286018\n",
      "test loss is 0.000849276223025741\n",
      "Batch: 33900,train loss is: 0.0007022721398190983\n",
      "test loss is 0.0008559910831333543\n",
      "-----------------------Epoch: 1----------------------------------\n",
      "Batch: 0,train loss is: 0.000589111557965298\n",
      "test loss is 0.0008549046506772305\n",
      "Batch: 100,train loss is: 0.0015887528470120247\n",
      "test loss is 0.0008544800585735036\n",
      "Batch: 200,train loss is: 0.0008209049464547233\n",
      "test loss is 0.0008507822613605668\n",
      "Batch: 300,train loss is: 0.00040009638772222684\n",
      "test loss is 0.0008541308016113036\n",
      "Batch: 400,train loss is: 0.0007093523583205978\n",
      "test loss is 0.0008581984326325659\n",
      "Batch: 500,train loss is: 0.0007193917503356343\n",
      "test loss is 0.000897924011638616\n",
      "Batch: 600,train loss is: 0.00035746969405500115\n",
      "test loss is 0.0008780737948538822\n",
      "Batch: 700,train loss is: 0.00037258384613517745\n",
      "test loss is 0.0008597780273489526\n",
      "Batch: 800,train loss is: 0.0005029899723999305\n",
      "test loss is 0.0008425094476777974\n",
      "Batch: 900,train loss is: 0.0004503016238207367\n",
      "test loss is 0.0008427541566317476\n",
      "Batch: 1000,train loss is: 0.000625604421831546\n",
      "test loss is 0.000876400227961898\n",
      "Batch: 1100,train loss is: 0.0022329936908794474\n",
      "test loss is 0.0008480564470990098\n",
      "Batch: 1200,train loss is: 0.0004075719131579048\n",
      "test loss is 0.0008396654872752455\n",
      "Batch: 1300,train loss is: 0.0006248904822933286\n",
      "test loss is 0.0008520975827735273\n",
      "Batch: 1400,train loss is: 0.0006899599544988536\n",
      "test loss is 0.0008585878535149441\n",
      "Batch: 1500,train loss is: 0.0005765254647096418\n",
      "test loss is 0.0008646379351995632\n",
      "Batch: 1600,train loss is: 0.00046779998506802224\n",
      "test loss is 0.0008760215597818145\n",
      "Batch: 1700,train loss is: 0.0013269740807590384\n",
      "test loss is 0.0008418102725383079\n",
      "Batch: 1800,train loss is: 0.0005136624451777922\n",
      "test loss is 0.0008714296386111703\n",
      "Batch: 1900,train loss is: 0.0005217374772978557\n",
      "test loss is 0.0009026074718229529\n",
      "Batch: 2000,train loss is: 0.000810806884448786\n",
      "test loss is 0.0009319929845398713\n",
      "Batch: 2100,train loss is: 0.0006873603055348217\n",
      "test loss is 0.0009628353635618654\n",
      "Batch: 2200,train loss is: 0.001054061201159201\n",
      "test loss is 0.0008529843502344716\n",
      "Batch: 2300,train loss is: 0.0004179171803024024\n",
      "test loss is 0.0008948452018249085\n",
      "Batch: 2400,train loss is: 0.0007922808595380966\n",
      "test loss is 0.0008495922178429575\n",
      "Batch: 2500,train loss is: 0.0008380915637135037\n",
      "test loss is 0.0008482531651204194\n",
      "Batch: 2600,train loss is: 0.0003749304854189871\n",
      "test loss is 0.0008579286165320819\n",
      "Batch: 2700,train loss is: 0.0007727455301525479\n",
      "test loss is 0.0008399484474694151\n",
      "Batch: 2800,train loss is: 0.0005646977668753164\n",
      "test loss is 0.0008860454875544572\n",
      "Batch: 2900,train loss is: 0.00052987529133545\n",
      "test loss is 0.0008375320857307044\n",
      "Batch: 3000,train loss is: 0.0005540253256656457\n",
      "test loss is 0.0008441263792567181\n",
      "Batch: 3100,train loss is: 0.0006521097110521157\n",
      "test loss is 0.0008731125049358473\n",
      "Batch: 3200,train loss is: 0.0009752886932766234\n",
      "test loss is 0.0008589825400603523\n",
      "Batch: 3300,train loss is: 0.0005152255528281472\n",
      "test loss is 0.0008470342210900016\n",
      "Batch: 3400,train loss is: 0.0005943385833226886\n",
      "test loss is 0.0008604018831380626\n",
      "Batch: 3500,train loss is: 0.0003958729831024635\n",
      "test loss is 0.000858603575524761\n",
      "Batch: 3600,train loss is: 0.0006867314346334\n",
      "test loss is 0.0008861803567969366\n",
      "Batch: 3700,train loss is: 0.0005308969626793187\n",
      "test loss is 0.0008319228580764553\n",
      "Batch: 3800,train loss is: 0.0004247474033185827\n",
      "test loss is 0.0008825146478336178\n",
      "Batch: 3900,train loss is: 0.0006190501782100484\n",
      "test loss is 0.0008379709389331534\n",
      "Batch: 4000,train loss is: 0.0004099413375003198\n",
      "test loss is 0.0008465039793474315\n",
      "Batch: 4100,train loss is: 0.0007808393741317257\n",
      "test loss is 0.0008571268904166187\n",
      "Batch: 4200,train loss is: 0.00043750072602781676\n",
      "test loss is 0.0008790248591186279\n",
      "Batch: 4300,train loss is: 0.000549622697147252\n",
      "test loss is 0.000848618538204348\n",
      "Batch: 4400,train loss is: 0.0007326139844108265\n",
      "test loss is 0.0008444291987626605\n",
      "Batch: 4500,train loss is: 0.0007165042093570704\n",
      "test loss is 0.0008710398332158859\n",
      "Batch: 4600,train loss is: 0.0007127553065577159\n",
      "test loss is 0.0008456073598355311\n",
      "Batch: 4700,train loss is: 0.0009879089924462027\n",
      "test loss is 0.0008703981882340248\n",
      "Batch: 4800,train loss is: 0.0014172091588955666\n",
      "test loss is 0.0008553323638429482\n",
      "Batch: 4900,train loss is: 0.0006577674350893092\n",
      "test loss is 0.0008467888800212091\n",
      "Batch: 5000,train loss is: 0.0006635541818743012\n",
      "test loss is 0.0008515680471457326\n",
      "Batch: 5100,train loss is: 0.0006888268982983599\n",
      "test loss is 0.0008783818810635736\n",
      "Batch: 5200,train loss is: 0.00045972703674018056\n",
      "test loss is 0.0008449842401459246\n",
      "Batch: 5300,train loss is: 0.0008809233931454723\n",
      "test loss is 0.0008736877334468687\n",
      "Batch: 5400,train loss is: 0.0010698010419642\n",
      "test loss is 0.0008437000696237375\n",
      "Batch: 5500,train loss is: 0.0006766838804643738\n",
      "test loss is 0.0008710740586105279\n",
      "Batch: 5600,train loss is: 0.0007811824021927052\n",
      "test loss is 0.0008578655250145276\n",
      "Batch: 5700,train loss is: 0.0009447195920156186\n",
      "test loss is 0.0008400700572159095\n",
      "Batch: 5800,train loss is: 0.0006222406790930264\n",
      "test loss is 0.0008501788343001638\n",
      "Batch: 5900,train loss is: 0.003989336817673882\n",
      "test loss is 0.0008558813954926915\n",
      "Batch: 6000,train loss is: 0.0016819597196320567\n",
      "test loss is 0.0008732215621096996\n",
      "Batch: 6100,train loss is: 0.0005503365891820147\n",
      "test loss is 0.0008354477749899245\n",
      "Batch: 6200,train loss is: 0.0006200068179022851\n",
      "test loss is 0.0008523326145029348\n",
      "Batch: 6300,train loss is: 0.0017622377030803867\n",
      "test loss is 0.0008625338204889578\n",
      "Batch: 6400,train loss is: 0.00045687170637247437\n",
      "test loss is 0.0008943185497481724\n",
      "Batch: 6500,train loss is: 0.000377453678639855\n",
      "test loss is 0.000875329740279581\n",
      "Batch: 6600,train loss is: 0.002476990931850277\n",
      "test loss is 0.0008498093951107971\n",
      "Batch: 6700,train loss is: 0.000618116673502783\n",
      "test loss is 0.0008626503008543148\n",
      "Batch: 6800,train loss is: 0.0013466452241619718\n",
      "test loss is 0.0008376300154787986\n",
      "Batch: 6900,train loss is: 0.0006114779014793303\n",
      "test loss is 0.000843154734173533\n",
      "Batch: 7000,train loss is: 0.0006505717743364107\n",
      "test loss is 0.0008938616500059419\n",
      "Batch: 7100,train loss is: 0.0009162565645499974\n",
      "test loss is 0.0008430394158800637\n",
      "Batch: 7200,train loss is: 0.00047202859973359643\n",
      "test loss is 0.0008743102444528167\n",
      "Batch: 7300,train loss is: 0.0008875754696813355\n",
      "test loss is 0.0008921753939127837\n",
      "Batch: 7400,train loss is: 0.0004570409301968961\n",
      "test loss is 0.0008350438634844684\n",
      "Batch: 7500,train loss is: 0.0009805777099952718\n",
      "test loss is 0.0008703313378471386\n",
      "Batch: 7600,train loss is: 0.0006917476861063454\n",
      "test loss is 0.0008456925653507859\n",
      "Batch: 7700,train loss is: 0.0007330660035672055\n",
      "test loss is 0.0008412593251521366\n",
      "Batch: 7800,train loss is: 0.0003313807865014983\n",
      "test loss is 0.0008528972850482978\n",
      "Batch: 7900,train loss is: 0.0007072666019931113\n",
      "test loss is 0.0008492861900543725\n",
      "Batch: 8000,train loss is: 0.0005215780676159267\n",
      "test loss is 0.0008827114132355982\n",
      "Batch: 8100,train loss is: 0.0007364903175437846\n",
      "test loss is 0.0008620989090047153\n",
      "Batch: 8200,train loss is: 0.0013541971706960212\n",
      "test loss is 0.0008614484983701994\n",
      "Batch: 8300,train loss is: 0.0006141931403402132\n",
      "test loss is 0.000861536464611167\n",
      "Batch: 8400,train loss is: 0.0005586321248045981\n",
      "test loss is 0.000843252330568156\n",
      "Batch: 8500,train loss is: 0.0010753370329410553\n",
      "test loss is 0.0008673227767193659\n",
      "Batch: 8600,train loss is: 0.0008050142332771299\n",
      "test loss is 0.0008589705055285612\n",
      "Batch: 8700,train loss is: 0.00048410199107427264\n",
      "test loss is 0.0008560338837220338\n",
      "Batch: 8800,train loss is: 0.0003597382306737192\n",
      "test loss is 0.0008403910371008185\n",
      "Batch: 8900,train loss is: 0.0011276308182899424\n",
      "test loss is 0.0008385293623984626\n",
      "Batch: 9000,train loss is: 0.00033642826374564295\n",
      "test loss is 0.0008482697422625391\n",
      "Batch: 9100,train loss is: 0.0005376887175185709\n",
      "test loss is 0.0008467785296474088\n",
      "Batch: 9200,train loss is: 0.0010354445409112738\n",
      "test loss is 0.0008889613840983564\n",
      "Batch: 9300,train loss is: 0.0006568401811104179\n",
      "test loss is 0.0008411429578913639\n",
      "Batch: 9400,train loss is: 0.0005817706521569597\n",
      "test loss is 0.0008954483684654345\n",
      "Batch: 9500,train loss is: 0.0004725973645304998\n",
      "test loss is 0.0008552602340632363\n",
      "Batch: 9600,train loss is: 0.0005530459876463197\n",
      "test loss is 0.0008415669022194994\n",
      "Batch: 9700,train loss is: 0.0005693543131736265\n",
      "test loss is 0.00086429084723605\n",
      "Batch: 9800,train loss is: 0.0006638676243018593\n",
      "test loss is 0.0008469334957521401\n",
      "Batch: 9900,train loss is: 0.000614529238228432\n",
      "test loss is 0.0008726209342529225\n",
      "Batch: 10000,train loss is: 0.000648151950346819\n",
      "test loss is 0.0008532011036251438\n",
      "Batch: 10100,train loss is: 0.0011344223101900054\n",
      "test loss is 0.0008360223619661317\n",
      "Batch: 10200,train loss is: 0.00161422544005844\n",
      "test loss is 0.0009457362871076284\n",
      "Batch: 10300,train loss is: 0.0005933613758663406\n",
      "test loss is 0.0008562492640167438\n",
      "Batch: 10400,train loss is: 0.0004749285691766559\n",
      "test loss is 0.0008404257529832751\n",
      "Batch: 10500,train loss is: 0.0007443496441866471\n",
      "test loss is 0.0008435674325289566\n",
      "Batch: 10600,train loss is: 0.0003472275639165065\n",
      "test loss is 0.000885548269636126\n",
      "Batch: 10700,train loss is: 0.0006347081284790256\n",
      "test loss is 0.0008410047855831222\n",
      "Batch: 10800,train loss is: 0.00047387290141112144\n",
      "test loss is 0.0008617873483868407\n",
      "Batch: 10900,train loss is: 0.0007151669720903647\n",
      "test loss is 0.0008417787217990739\n",
      "Batch: 11000,train loss is: 0.0006247486280015953\n",
      "test loss is 0.0008744217564599481\n",
      "Batch: 11100,train loss is: 0.0010774003065280686\n",
      "test loss is 0.0008403376291000889\n",
      "Batch: 11200,train loss is: 0.0006850869911174311\n",
      "test loss is 0.0008551100819682993\n",
      "Batch: 11300,train loss is: 0.0004232324737699591\n",
      "test loss is 0.0009511371033079561\n",
      "Batch: 11400,train loss is: 0.0007207335970477585\n",
      "test loss is 0.0008643431205325327\n",
      "Batch: 11500,train loss is: 0.0006235308210222704\n",
      "test loss is 0.0008504934466270672\n",
      "Batch: 11600,train loss is: 0.002127706972422116\n",
      "test loss is 0.0008648077515100391\n",
      "Batch: 11700,train loss is: 0.00040187237867590384\n",
      "test loss is 0.0008246406137965095\n",
      "Batch: 11800,train loss is: 0.0006678156175199016\n",
      "test loss is 0.0008454190470054774\n",
      "Batch: 11900,train loss is: 0.0007403435146445921\n",
      "test loss is 0.0008527649541972235\n",
      "Batch: 12000,train loss is: 0.00048564135957721797\n",
      "test loss is 0.0008588940305296337\n",
      "Batch: 12100,train loss is: 0.0018997924873296343\n",
      "test loss is 0.0008478573286402184\n",
      "Batch: 12200,train loss is: 0.0008060644219774921\n",
      "test loss is 0.0008646295130515785\n",
      "Batch: 12300,train loss is: 0.000523044334754684\n",
      "test loss is 0.0008766878983871307\n",
      "Batch: 12400,train loss is: 0.0008485975329486011\n",
      "test loss is 0.0008277585198408811\n",
      "Batch: 12500,train loss is: 0.00043137895639961885\n",
      "test loss is 0.0008646254305529871\n",
      "Batch: 12600,train loss is: 0.00043169216912229615\n",
      "test loss is 0.0008492337863726187\n",
      "Batch: 12700,train loss is: 0.0005884200408231015\n",
      "test loss is 0.0008313518445265157\n",
      "Batch: 12800,train loss is: 0.0004973470312188337\n",
      "test loss is 0.0008383663833464003\n",
      "Batch: 12900,train loss is: 0.0008253235642258091\n",
      "test loss is 0.0008494714721283557\n",
      "Batch: 13000,train loss is: 0.000321076854233634\n",
      "test loss is 0.0008275665061013204\n",
      "Batch: 13100,train loss is: 0.002415585853337533\n",
      "test loss is 0.0008330279979840362\n",
      "Batch: 13200,train loss is: 0.0008411136585627001\n",
      "test loss is 0.0009114479434088192\n",
      "Batch: 13300,train loss is: 0.0005006513106850382\n",
      "test loss is 0.0008901748656786013\n",
      "Batch: 13400,train loss is: 0.0004053985179081493\n",
      "test loss is 0.0008586055542650375\n",
      "Batch: 13500,train loss is: 0.0006898040252283343\n",
      "test loss is 0.0008620880399837597\n",
      "Batch: 13600,train loss is: 0.0006267454984146281\n",
      "test loss is 0.0008405207627834779\n",
      "Batch: 13700,train loss is: 0.0007416314617093739\n",
      "test loss is 0.0008425313584955801\n",
      "Batch: 13800,train loss is: 0.0008534020877841653\n",
      "test loss is 0.0008529825513782048\n",
      "Batch: 13900,train loss is: 0.0008474630044594571\n",
      "test loss is 0.0008521510497821194\n",
      "Batch: 14000,train loss is: 0.0004413661782196612\n",
      "test loss is 0.0008398631438836737\n",
      "Batch: 14100,train loss is: 0.0009138967420265238\n",
      "test loss is 0.0008957391060919677\n",
      "Batch: 14200,train loss is: 0.0005469087046648381\n",
      "test loss is 0.000823996711209186\n",
      "Batch: 14300,train loss is: 0.0005675266416698893\n",
      "test loss is 0.0008319239139598401\n",
      "Batch: 14400,train loss is: 0.001533155939025605\n",
      "test loss is 0.0008316449648642915\n",
      "Batch: 14500,train loss is: 0.0005064042374848534\n",
      "test loss is 0.0008379631381853621\n",
      "Batch: 14600,train loss is: 0.0021159268160355436\n",
      "test loss is 0.0008376662713059824\n",
      "Batch: 14700,train loss is: 0.0004524605071193694\n",
      "test loss is 0.0008529266943161419\n",
      "Batch: 14800,train loss is: 0.0009629702615639723\n",
      "test loss is 0.0008418474789840699\n",
      "Batch: 14900,train loss is: 0.00044445932381797766\n",
      "test loss is 0.0008516972163545215\n",
      "Batch: 15000,train loss is: 0.0008983703736923076\n",
      "test loss is 0.0008338812258681833\n",
      "Batch: 15100,train loss is: 0.0007493976462409635\n",
      "test loss is 0.0008612359253412653\n",
      "Batch: 15200,train loss is: 0.0013676984317962735\n",
      "test loss is 0.0008701333775752065\n",
      "Batch: 15300,train loss is: 0.0005431094517559062\n",
      "test loss is 0.0008325783915816225\n",
      "Batch: 15400,train loss is: 0.0005367953693651325\n",
      "test loss is 0.0008661723688903376\n",
      "Batch: 15500,train loss is: 0.0008681744808883308\n",
      "test loss is 0.0008267467760752435\n",
      "Batch: 15600,train loss is: 0.0011192823950779208\n",
      "test loss is 0.0008636094635003299\n",
      "Batch: 15700,train loss is: 0.0007043090555452438\n",
      "test loss is 0.0008348778591814992\n",
      "Batch: 15800,train loss is: 0.0006951202551653061\n",
      "test loss is 0.0008390030429808574\n",
      "Batch: 15900,train loss is: 0.0012976555926743694\n",
      "test loss is 0.0008561148452330148\n",
      "Batch: 16000,train loss is: 0.0005253039402618527\n",
      "test loss is 0.0008659133304566235\n",
      "Batch: 16100,train loss is: 0.0006793738760463461\n",
      "test loss is 0.0008536705692331067\n",
      "Batch: 16200,train loss is: 0.000502634544222497\n",
      "test loss is 0.0008399041939556352\n",
      "Batch: 16300,train loss is: 0.001302069314185057\n",
      "test loss is 0.0008331726820553202\n",
      "Batch: 16400,train loss is: 0.00046549519360749713\n",
      "test loss is 0.0008270449241462111\n",
      "Batch: 16500,train loss is: 0.0006343939840627278\n",
      "test loss is 0.000871299504339429\n",
      "Batch: 16600,train loss is: 0.00035628409164872274\n",
      "test loss is 0.0008471917399164777\n",
      "Batch: 16700,train loss is: 0.001263489335977925\n",
      "test loss is 0.0008521497139191228\n",
      "Batch: 16800,train loss is: 0.0006625753255488003\n",
      "test loss is 0.0008561276428288729\n",
      "Batch: 16900,train loss is: 0.0008041996183709648\n",
      "test loss is 0.0008538962354428186\n",
      "Batch: 17000,train loss is: 0.0007600866991164574\n",
      "test loss is 0.0008302060995521886\n",
      "Batch: 17100,train loss is: 0.0006137844720899784\n",
      "test loss is 0.0008411057771892387\n",
      "Batch: 17200,train loss is: 0.0007187693956311416\n",
      "test loss is 0.000843743059234204\n",
      "Batch: 17300,train loss is: 0.0008137762288413653\n",
      "test loss is 0.0008421949014918228\n",
      "Batch: 17400,train loss is: 0.0005474791679480896\n",
      "test loss is 0.0008429183732771413\n",
      "Batch: 17500,train loss is: 0.0005442706699975759\n",
      "test loss is 0.0008462286132834536\n",
      "Batch: 17600,train loss is: 0.0005138567196119341\n",
      "test loss is 0.0008617040867118688\n",
      "Batch: 17700,train loss is: 0.0003112349198600062\n",
      "test loss is 0.000869650255061958\n",
      "Batch: 17800,train loss is: 0.0012009084939728254\n",
      "test loss is 0.0008435494847598178\n",
      "Batch: 17900,train loss is: 0.0005311389187526173\n",
      "test loss is 0.0008778065306146352\n",
      "Batch: 18000,train loss is: 0.00041500091788075865\n",
      "test loss is 0.0008294770325765989\n",
      "Batch: 18100,train loss is: 0.0005211016527198203\n",
      "test loss is 0.0008349174927920209\n",
      "Batch: 18200,train loss is: 0.0005549795463387138\n",
      "test loss is 0.0008387344075552616\n",
      "Batch: 18300,train loss is: 0.000947578729422813\n",
      "test loss is 0.0008350119770729492\n",
      "Batch: 18400,train loss is: 0.0006043685291881987\n",
      "test loss is 0.0008379965021641943\n",
      "Batch: 18500,train loss is: 0.0007493416173834587\n",
      "test loss is 0.0008623369792166376\n",
      "Batch: 18600,train loss is: 0.0004195467920714308\n",
      "test loss is 0.0008348060935274618\n",
      "Batch: 18700,train loss is: 0.0006292970185499726\n",
      "test loss is 0.0008345127858990399\n",
      "Batch: 18800,train loss is: 0.0008408437437160828\n",
      "test loss is 0.0008413908343660264\n",
      "Batch: 18900,train loss is: 0.0008557758302715964\n",
      "test loss is 0.0008770778173226843\n",
      "Batch: 19000,train loss is: 0.0007969083442228149\n",
      "test loss is 0.0008408771825133101\n",
      "Batch: 19100,train loss is: 0.00044483594025252637\n",
      "test loss is 0.0008491221070025486\n",
      "Batch: 19200,train loss is: 0.0007547890501428435\n",
      "test loss is 0.0008509299348770258\n",
      "Batch: 19300,train loss is: 0.0009408857770555054\n",
      "test loss is 0.000833859090693227\n",
      "Batch: 19400,train loss is: 0.00037727582015882534\n",
      "test loss is 0.0008272313547457966\n",
      "Batch: 19500,train loss is: 0.0006844103615809086\n",
      "test loss is 0.0008867860219620141\n",
      "Batch: 19600,train loss is: 0.0005012749623143676\n",
      "test loss is 0.0008460277045864961\n",
      "Batch: 19700,train loss is: 0.000805888010223059\n",
      "test loss is 0.0008960328309860316\n",
      "Batch: 19800,train loss is: 0.0005182166345107231\n",
      "test loss is 0.000843198933087143\n",
      "Batch: 19900,train loss is: 0.00035845769666488725\n",
      "test loss is 0.000863653203512155\n",
      "Batch: 20000,train loss is: 0.0008469187586192629\n",
      "test loss is 0.0008373014517255887\n",
      "Batch: 20100,train loss is: 0.0007346641685503672\n",
      "test loss is 0.000842691878245738\n",
      "Batch: 20200,train loss is: 0.0010440460342356385\n",
      "test loss is 0.0008650820310809209\n",
      "Batch: 20300,train loss is: 0.00043554391552592996\n",
      "test loss is 0.0008486781808040327\n",
      "Batch: 20400,train loss is: 0.0006287197917834296\n",
      "test loss is 0.0008862612393216777\n",
      "Batch: 20500,train loss is: 0.0006620913892912688\n",
      "test loss is 0.0008439272698610914\n",
      "Batch: 20600,train loss is: 0.0008244231019363695\n",
      "test loss is 0.0008871172532239846\n",
      "Batch: 20700,train loss is: 0.0005960374130397989\n",
      "test loss is 0.0008579154962371607\n",
      "Batch: 20800,train loss is: 0.0003929527830058503\n",
      "test loss is 0.0008408040607971002\n",
      "Batch: 20900,train loss is: 0.0008046264316034379\n",
      "test loss is 0.0008712848242232661\n",
      "Batch: 21000,train loss is: 0.0004309600116879589\n",
      "test loss is 0.0008334967724320566\n",
      "Batch: 21100,train loss is: 0.0006668697061668223\n",
      "test loss is 0.0008650115605511496\n",
      "Batch: 21200,train loss is: 0.00033473040339578893\n",
      "test loss is 0.0008374796307781114\n",
      "Batch: 21300,train loss is: 0.0004460739628901686\n",
      "test loss is 0.0008497946641279873\n",
      "Batch: 21400,train loss is: 0.0009567049207634067\n",
      "test loss is 0.0008517584584648314\n",
      "Batch: 21500,train loss is: 0.00029418923727629707\n",
      "test loss is 0.000858765126803545\n",
      "Batch: 21600,train loss is: 0.0009353824496545987\n",
      "test loss is 0.0008474211202026175\n",
      "Batch: 21700,train loss is: 0.0003734566137843195\n",
      "test loss is 0.0008310586530966393\n",
      "Batch: 21800,train loss is: 0.0007208030781209829\n",
      "test loss is 0.0008840851316029506\n",
      "Batch: 21900,train loss is: 0.0006791135017078263\n",
      "test loss is 0.0009227532485080294\n",
      "Batch: 22000,train loss is: 0.0008328000222861421\n",
      "test loss is 0.0008349596316508256\n",
      "Batch: 22100,train loss is: 0.00032614195439175774\n",
      "test loss is 0.0008358078428643332\n",
      "Batch: 22200,train loss is: 0.0006252216922348017\n",
      "test loss is 0.0008425769066741469\n",
      "Batch: 22300,train loss is: 0.0009658650143461473\n",
      "test loss is 0.0008357569117062441\n",
      "Batch: 22400,train loss is: 0.0006359433524228013\n",
      "test loss is 0.0008258198797068004\n",
      "Batch: 22500,train loss is: 0.00045093804974906913\n",
      "test loss is 0.0008294169059526447\n",
      "Batch: 22600,train loss is: 0.0012197011177144494\n",
      "test loss is 0.0009001097788182524\n",
      "Batch: 22700,train loss is: 0.00044212360820978776\n",
      "test loss is 0.0008376489209623088\n",
      "Batch: 22800,train loss is: 0.0005530611361918757\n",
      "test loss is 0.0008475849591590824\n",
      "Batch: 22900,train loss is: 0.0004152497576783726\n",
      "test loss is 0.0009072019479484004\n",
      "Batch: 23000,train loss is: 0.0004913935603346543\n",
      "test loss is 0.0008352804338469982\n",
      "Batch: 23100,train loss is: 0.0007092932477102283\n",
      "test loss is 0.0008170034856617797\n",
      "Batch: 23200,train loss is: 0.0003972636862000292\n",
      "test loss is 0.0008166862021929282\n",
      "Batch: 23300,train loss is: 0.0006142614420506433\n",
      "test loss is 0.0008266804651177091\n",
      "Batch: 23400,train loss is: 0.0008495243397692057\n",
      "test loss is 0.0008422164933444085\n",
      "Batch: 23500,train loss is: 0.003211090522903618\n",
      "test loss is 0.0008354651923316287\n",
      "Batch: 23600,train loss is: 0.00040375385073552133\n",
      "test loss is 0.0008212436861355782\n",
      "Batch: 23700,train loss is: 0.0009319365906263365\n",
      "test loss is 0.0008544968917638354\n",
      "Batch: 23800,train loss is: 0.0010651695331682807\n",
      "test loss is 0.0008240032158892539\n",
      "Batch: 23900,train loss is: 0.0009512579364681806\n",
      "test loss is 0.0008241815535391286\n",
      "Batch: 24000,train loss is: 0.0007014496516639057\n",
      "test loss is 0.0008719524929960752\n",
      "Batch: 24100,train loss is: 0.0005874863955098104\n",
      "test loss is 0.0008211594698694599\n",
      "Batch: 24200,train loss is: 0.0018393320150253214\n",
      "test loss is 0.0008463294499604661\n",
      "Batch: 24300,train loss is: 0.0005290750227644864\n",
      "test loss is 0.0009583168588165437\n",
      "Batch: 24400,train loss is: 0.001427138321468667\n",
      "test loss is 0.000833186709791264\n",
      "Batch: 24500,train loss is: 0.000604332342290228\n",
      "test loss is 0.0008353832961765781\n",
      "Batch: 24600,train loss is: 0.0005826859082927036\n",
      "test loss is 0.0008650300156652058\n",
      "Batch: 24700,train loss is: 0.00038006026443413614\n",
      "test loss is 0.0008257390336072389\n",
      "Batch: 24800,train loss is: 0.0011318077743115653\n",
      "test loss is 0.0008316557717584948\n",
      "Batch: 24900,train loss is: 0.00019541076826818009\n",
      "test loss is 0.00085467514237747\n",
      "Batch: 25000,train loss is: 0.000792520786973214\n",
      "test loss is 0.0008288101744058968\n",
      "Batch: 25100,train loss is: 0.0007069762276863507\n",
      "test loss is 0.0008401710919052347\n",
      "Batch: 25200,train loss is: 0.0008780173488703142\n",
      "test loss is 0.0008451799596553942\n",
      "Batch: 25300,train loss is: 0.0009253531419267771\n",
      "test loss is 0.0008548058247188717\n",
      "Batch: 25400,train loss is: 0.0004658556039063284\n",
      "test loss is 0.0008235460585637185\n",
      "Batch: 25500,train loss is: 0.000570269052489493\n",
      "test loss is 0.0008241060134357157\n",
      "Batch: 25600,train loss is: 0.0007574319898852463\n",
      "test loss is 0.0008414520994215386\n",
      "Batch: 25700,train loss is: 0.0007346824351849592\n",
      "test loss is 0.0008540202037866251\n",
      "Batch: 25800,train loss is: 0.000309354747348242\n",
      "test loss is 0.0008266735361904702\n",
      "Batch: 25900,train loss is: 0.0004784726903268145\n",
      "test loss is 0.0008705467163942547\n",
      "Batch: 26000,train loss is: 0.0018792251365462639\n",
      "test loss is 0.0008890709090030244\n",
      "Batch: 26100,train loss is: 0.000995551113804853\n",
      "test loss is 0.0008388561229237177\n",
      "Batch: 26200,train loss is: 0.0009518154225206653\n",
      "test loss is 0.0008135152271206642\n",
      "Batch: 26300,train loss is: 0.0013735333086459642\n",
      "test loss is 0.0008580389885375011\n",
      "Batch: 26400,train loss is: 0.0007651815672755705\n",
      "test loss is 0.0008327152269497035\n",
      "Batch: 26500,train loss is: 0.00047839124616846153\n",
      "test loss is 0.0008469627716868028\n",
      "Batch: 26600,train loss is: 0.00112088318126184\n",
      "test loss is 0.000873716663564996\n",
      "Batch: 26700,train loss is: 0.00045699752102784023\n",
      "test loss is 0.0009484064962073401\n",
      "Batch: 26800,train loss is: 0.0014146614559068117\n",
      "test loss is 0.0008352120155371136\n",
      "Batch: 26900,train loss is: 0.0006352964342581902\n",
      "test loss is 0.0008255081710210796\n",
      "Batch: 27000,train loss is: 0.0007380312367639297\n",
      "test loss is 0.000963228810150746\n",
      "Batch: 27100,train loss is: 0.00047341115673793266\n",
      "test loss is 0.0008481250462735007\n",
      "Batch: 27200,train loss is: 0.0006345299188615796\n",
      "test loss is 0.0008681928876663844\n",
      "Batch: 27300,train loss is: 0.00062730057207995\n",
      "test loss is 0.0008423947500682865\n",
      "Batch: 27400,train loss is: 0.0008326821155744031\n",
      "test loss is 0.000814081343322987\n",
      "Batch: 27500,train loss is: 0.0006299059027699902\n",
      "test loss is 0.0009175881867737119\n",
      "Batch: 27600,train loss is: 0.0007209953395364894\n",
      "test loss is 0.0008856885445896041\n",
      "Batch: 27700,train loss is: 0.002102106117533366\n",
      "test loss is 0.0008376304606371443\n",
      "Batch: 27800,train loss is: 0.0007244351100910987\n",
      "test loss is 0.0008524335594579095\n",
      "Batch: 27900,train loss is: 0.0012747699928239065\n",
      "test loss is 0.0008199618456504596\n",
      "Batch: 28000,train loss is: 0.000461142183257158\n",
      "test loss is 0.0008464363262979705\n",
      "Batch: 28100,train loss is: 0.0008255419685119599\n",
      "test loss is 0.0008416799287598139\n",
      "Batch: 28200,train loss is: 0.0008032163617571465\n",
      "test loss is 0.000809377000183675\n",
      "Batch: 28300,train loss is: 0.0021536159174383148\n",
      "test loss is 0.0008220726182134866\n",
      "Batch: 28400,train loss is: 0.0006671237531273959\n",
      "test loss is 0.0008310022968758171\n",
      "Batch: 28500,train loss is: 0.0013623021830936604\n",
      "test loss is 0.0008424181056856989\n",
      "Batch: 28600,train loss is: 0.0005161395444673577\n",
      "test loss is 0.0008380189343229791\n",
      "Batch: 28700,train loss is: 0.00048324631044069035\n",
      "test loss is 0.0008235382649834912\n",
      "Batch: 28800,train loss is: 0.0015444833454807488\n",
      "test loss is 0.0008311354414293515\n",
      "Batch: 28900,train loss is: 0.00070401492829481\n",
      "test loss is 0.0008297572274752887\n",
      "Batch: 29000,train loss is: 0.0006267095840870872\n",
      "test loss is 0.000817250836738363\n",
      "Batch: 29100,train loss is: 0.0007598823588640432\n",
      "test loss is 0.0008526701949148935\n",
      "Batch: 29200,train loss is: 0.0005420996609498847\n",
      "test loss is 0.0008420259026081684\n",
      "Batch: 29300,train loss is: 0.0010292858207792347\n",
      "test loss is 0.0008294611522929468\n",
      "Batch: 29400,train loss is: 0.0007018952301602494\n",
      "test loss is 0.0008712660048434208\n",
      "Batch: 29500,train loss is: 0.000507046203231162\n",
      "test loss is 0.0008345192612591802\n",
      "Batch: 29600,train loss is: 0.0006654391917180182\n",
      "test loss is 0.0008360962937843393\n",
      "Batch: 29700,train loss is: 0.0003415780902713561\n",
      "test loss is 0.00085057544462017\n",
      "Batch: 29800,train loss is: 0.0011066199620312475\n",
      "test loss is 0.0008256097248605062\n",
      "Batch: 29900,train loss is: 0.0035116705436593824\n",
      "test loss is 0.0008420198241939986\n",
      "Batch: 30000,train loss is: 0.00038830219791142866\n",
      "test loss is 0.0008627761863486088\n",
      "Batch: 30100,train loss is: 0.0005129748546576907\n",
      "test loss is 0.0008617722921505134\n",
      "Batch: 30200,train loss is: 0.00043550698357863705\n",
      "test loss is 0.0008225126993427099\n",
      "Batch: 30300,train loss is: 0.0004470126690949698\n",
      "test loss is 0.000820332939189706\n",
      "Batch: 30400,train loss is: 0.0004595901750222194\n",
      "test loss is 0.0008229149263808966\n",
      "Batch: 30500,train loss is: 0.0004866316137380205\n",
      "test loss is 0.0008613000206515344\n",
      "Batch: 30600,train loss is: 0.0003179786115437083\n",
      "test loss is 0.0008281967580570083\n",
      "Batch: 30700,train loss is: 0.0005982974264858304\n",
      "test loss is 0.0009581495437811808\n",
      "Batch: 30800,train loss is: 0.000342502649464379\n",
      "test loss is 0.0008862254643729758\n",
      "Batch: 30900,train loss is: 0.00045715293278877754\n",
      "test loss is 0.0008285434226958547\n",
      "Batch: 31000,train loss is: 0.0006088541935142564\n",
      "test loss is 0.0008359592600643149\n",
      "Batch: 31100,train loss is: 0.0004231901061290977\n",
      "test loss is 0.0008901951569174691\n",
      "Batch: 31200,train loss is: 0.0007265554950505167\n",
      "test loss is 0.0008322335092774343\n",
      "Batch: 31300,train loss is: 0.0007300273661922401\n",
      "test loss is 0.0008499611078080147\n",
      "Batch: 31400,train loss is: 0.0008406451370910502\n",
      "test loss is 0.0008204466784819147\n",
      "Batch: 31500,train loss is: 0.0007024000738966227\n",
      "test loss is 0.0008403692221620028\n",
      "Batch: 31600,train loss is: 0.0005781757759179536\n",
      "test loss is 0.0008315486650859703\n",
      "Batch: 31700,train loss is: 0.0005870836285207701\n",
      "test loss is 0.0008797852156283474\n",
      "Batch: 31800,train loss is: 0.0011319219937414827\n",
      "test loss is 0.0008305237890287565\n",
      "Batch: 31900,train loss is: 0.0010738216875904574\n",
      "test loss is 0.0008603955229049465\n",
      "Batch: 32000,train loss is: 0.004892742845363368\n",
      "test loss is 0.0008228630548889893\n",
      "Batch: 32100,train loss is: 0.00043732751729762205\n",
      "test loss is 0.0008179680880452775\n",
      "Batch: 32200,train loss is: 0.0005420614377536043\n",
      "test loss is 0.0008332703842117829\n",
      "Batch: 32300,train loss is: 0.00044063058538728245\n",
      "test loss is 0.0008290898916988697\n",
      "Batch: 32400,train loss is: 0.0005348445742791024\n",
      "test loss is 0.0008138110357561799\n",
      "Batch: 32500,train loss is: 0.0011493036807709814\n",
      "test loss is 0.0008572297159277397\n",
      "Batch: 32600,train loss is: 0.00045161436309914166\n",
      "test loss is 0.0008337891730508252\n",
      "Batch: 32700,train loss is: 0.000509849783518242\n",
      "test loss is 0.0008236693531986465\n",
      "Batch: 32800,train loss is: 0.00044830528066988823\n",
      "test loss is 0.000830837485362533\n",
      "Batch: 32900,train loss is: 0.0007212213745724163\n",
      "test loss is 0.0008291337522838227\n",
      "Batch: 33000,train loss is: 0.0006475572028792211\n",
      "test loss is 0.0008094883983682386\n",
      "Batch: 33100,train loss is: 0.0005451502141513193\n",
      "test loss is 0.0008537066278709724\n",
      "Batch: 33200,train loss is: 0.0006542571969470773\n",
      "test loss is 0.000927840775281621\n",
      "Batch: 33300,train loss is: 0.000342418796443251\n",
      "test loss is 0.0008107479550986473\n",
      "Batch: 33400,train loss is: 0.0005635964634078935\n",
      "test loss is 0.0008413956239696406\n",
      "Batch: 33500,train loss is: 0.0005891385386428751\n",
      "test loss is 0.0008198909589529797\n",
      "Batch: 33600,train loss is: 0.0006264510391097411\n",
      "test loss is 0.0008334863080428417\n",
      "Batch: 33700,train loss is: 0.0007142507956013152\n",
      "test loss is 0.00083659083738241\n",
      "Batch: 33800,train loss is: 0.0008027841365636607\n",
      "test loss is 0.0008209317058994859\n",
      "Batch: 33900,train loss is: 0.0007055524595430305\n",
      "test loss is 0.0008253241961798305\n",
      "-----------------------Epoch: 2----------------------------------\n",
      "Batch: 0,train loss is: 0.0005314539779658346\n",
      "test loss is 0.0008267782455565266\n",
      "Batch: 100,train loss is: 0.001583378220098386\n",
      "test loss is 0.000827162778777213\n",
      "Batch: 200,train loss is: 0.000788729833875534\n",
      "test loss is 0.0008250745859911742\n",
      "Batch: 300,train loss is: 0.00039079793690253295\n",
      "test loss is 0.0008251042208650979\n",
      "Batch: 400,train loss is: 0.0006701589433665927\n",
      "test loss is 0.0008271947457584941\n",
      "Batch: 500,train loss is: 0.0006912714476565196\n",
      "test loss is 0.0008651507373521345\n",
      "Batch: 600,train loss is: 0.00033546679547929135\n",
      "test loss is 0.0008523822751923001\n",
      "Batch: 700,train loss is: 0.00036478087249345\n",
      "test loss is 0.0008286169173456234\n",
      "Batch: 800,train loss is: 0.0005051359704783357\n",
      "test loss is 0.0008170842375408529\n",
      "Batch: 900,train loss is: 0.00042723429162133794\n",
      "test loss is 0.0008155902972887662\n",
      "Batch: 1000,train loss is: 0.0006189445747686436\n",
      "test loss is 0.0008510252119113581\n",
      "Batch: 1100,train loss is: 0.0020835273000653245\n",
      "test loss is 0.0008187998797394539\n",
      "Batch: 1200,train loss is: 0.00040011587260263846\n",
      "test loss is 0.0008131616740815756\n",
      "Batch: 1300,train loss is: 0.0006020885073114072\n",
      "test loss is 0.0008240654226426861\n",
      "Batch: 1400,train loss is: 0.0006431739686135736\n",
      "test loss is 0.0008334128297434799\n",
      "Batch: 1500,train loss is: 0.0005555188548619397\n",
      "test loss is 0.0008403500363715981\n",
      "Batch: 1600,train loss is: 0.0004905060536892959\n",
      "test loss is 0.0008634481095866624\n",
      "Batch: 1700,train loss is: 0.0013238074575893236\n",
      "test loss is 0.0008127894801533765\n",
      "Batch: 1800,train loss is: 0.0005149888908653562\n",
      "test loss is 0.0008423733474784521\n",
      "Batch: 1900,train loss is: 0.000493749936696292\n",
      "test loss is 0.0008750529276092553\n",
      "Batch: 2000,train loss is: 0.0007501132344403547\n",
      "test loss is 0.0009162242653975606\n",
      "Batch: 2100,train loss is: 0.0006822060104704416\n",
      "test loss is 0.00094205053380288\n",
      "Batch: 2200,train loss is: 0.0010271022932435445\n",
      "test loss is 0.0008256299828532402\n",
      "Batch: 2300,train loss is: 0.0004101730957212652\n",
      "test loss is 0.0008675222580806993\n",
      "Batch: 2400,train loss is: 0.000745939062407776\n",
      "test loss is 0.0008242673749753829\n",
      "Batch: 2500,train loss is: 0.0008056100341236958\n",
      "test loss is 0.0008205674682418576\n",
      "Batch: 2600,train loss is: 0.00037816202133722894\n",
      "test loss is 0.0008339060496007185\n",
      "Batch: 2700,train loss is: 0.0007677975098464041\n",
      "test loss is 0.0008115417138330527\n",
      "Batch: 2800,train loss is: 0.0005383685363190846\n",
      "test loss is 0.0008587679196735992\n",
      "Batch: 2900,train loss is: 0.0005324441676403274\n",
      "test loss is 0.0008096809878453154\n",
      "Batch: 3000,train loss is: 0.0005291318399782577\n",
      "test loss is 0.0008137561900591419\n",
      "Batch: 3100,train loss is: 0.0006512939081922163\n",
      "test loss is 0.0008524570125627459\n",
      "Batch: 3200,train loss is: 0.000987128882970603\n",
      "test loss is 0.0008313422250351877\n",
      "Batch: 3300,train loss is: 0.000478551945928857\n",
      "test loss is 0.0008205678642826332\n",
      "Batch: 3400,train loss is: 0.0005558431283634962\n",
      "test loss is 0.0008313875798122642\n",
      "Batch: 3500,train loss is: 0.00035955895728511023\n",
      "test loss is 0.0008305969518748745\n",
      "Batch: 3600,train loss is: 0.0006492447875055235\n",
      "test loss is 0.0008600197043655843\n",
      "Batch: 3700,train loss is: 0.0005153613343085471\n",
      "test loss is 0.0008066671678658699\n",
      "Batch: 3800,train loss is: 0.00042543535973845175\n",
      "test loss is 0.0008532614237280459\n",
      "Batch: 3900,train loss is: 0.0006259908197444028\n",
      "test loss is 0.0008140511436462418\n",
      "Batch: 4000,train loss is: 0.00041933073598195115\n",
      "test loss is 0.0008206006632274068\n",
      "Batch: 4100,train loss is: 0.0007716122086611532\n",
      "test loss is 0.0008330793143928151\n",
      "Batch: 4200,train loss is: 0.00043639559735984145\n",
      "test loss is 0.0008552198257103944\n",
      "Batch: 4300,train loss is: 0.0004899753760524198\n",
      "test loss is 0.0008199720280821132\n",
      "Batch: 4400,train loss is: 0.0006702359033584139\n",
      "test loss is 0.0008171589692085393\n",
      "Batch: 4500,train loss is: 0.0006713002247940533\n",
      "test loss is 0.0008472746024781165\n",
      "Batch: 4600,train loss is: 0.0006798676330351285\n",
      "test loss is 0.000818167122058488\n",
      "Batch: 4700,train loss is: 0.0009271255566948715\n",
      "test loss is 0.0008483997355915708\n",
      "Batch: 4800,train loss is: 0.0013800524850870626\n",
      "test loss is 0.0008304839877833316\n",
      "Batch: 4900,train loss is: 0.0006281813685146006\n",
      "test loss is 0.0008208336617151153\n",
      "Batch: 5000,train loss is: 0.0006412642503441063\n",
      "test loss is 0.0008250363775415077\n",
      "Batch: 5100,train loss is: 0.0006871171724877608\n",
      "test loss is 0.0008628524373668415\n",
      "Batch: 5200,train loss is: 0.0004304939333621168\n",
      "test loss is 0.0008200532925572926\n",
      "Batch: 5300,train loss is: 0.0008287792429619023\n",
      "test loss is 0.0008490635697316629\n",
      "Batch: 5400,train loss is: 0.0009592847872675676\n",
      "test loss is 0.0008178032374824786\n",
      "Batch: 5500,train loss is: 0.0006177809598894663\n",
      "test loss is 0.0008423385594045731\n",
      "Batch: 5600,train loss is: 0.0007396995071017782\n",
      "test loss is 0.000834867978876066\n",
      "Batch: 5700,train loss is: 0.0009031675500767937\n",
      "test loss is 0.0008143240489738365\n",
      "Batch: 5800,train loss is: 0.0006205392541729903\n",
      "test loss is 0.0008241347012394882\n",
      "Batch: 5900,train loss is: 0.0038158846436384783\n",
      "test loss is 0.0008276606580804141\n",
      "Batch: 6000,train loss is: 0.0016910578917311966\n",
      "test loss is 0.0008471046225963574\n",
      "Batch: 6100,train loss is: 0.0005350093034768753\n",
      "test loss is 0.0008096114769169669\n",
      "Batch: 6200,train loss is: 0.0006269651770262676\n",
      "test loss is 0.0008257106461161079\n",
      "Batch: 6300,train loss is: 0.0016509435414535695\n",
      "test loss is 0.0008378499269750518\n",
      "Batch: 6400,train loss is: 0.0004615453995334906\n",
      "test loss is 0.000872960341408119\n",
      "Batch: 6500,train loss is: 0.00039867336752942117\n",
      "test loss is 0.0008517568925748602\n",
      "Batch: 6600,train loss is: 0.0024150457985650854\n",
      "test loss is 0.0008228700571260535\n",
      "Batch: 6700,train loss is: 0.0005822566902665782\n",
      "test loss is 0.0008404002007968191\n",
      "Batch: 6800,train loss is: 0.0012445997036650825\n",
      "test loss is 0.0008130501306275261\n",
      "Batch: 6900,train loss is: 0.0006092973225445764\n",
      "test loss is 0.0008196160020980841\n",
      "Batch: 7000,train loss is: 0.0006262197190539581\n",
      "test loss is 0.0008634422515394159\n",
      "Batch: 7100,train loss is: 0.0009313966224463512\n",
      "test loss is 0.0008195826197322096\n",
      "Batch: 7200,train loss is: 0.0004544406689435277\n",
      "test loss is 0.000844896586457594\n",
      "Batch: 7300,train loss is: 0.0008772002090237914\n",
      "test loss is 0.000869951453127263\n",
      "Batch: 7400,train loss is: 0.00044476486001841553\n",
      "test loss is 0.0008108531092280988\n",
      "Batch: 7500,train loss is: 0.0009217812447514965\n",
      "test loss is 0.0008411343919926752\n",
      "Batch: 7600,train loss is: 0.0006375469377025523\n",
      "test loss is 0.00081820096777777\n",
      "Batch: 7700,train loss is: 0.0007274526479624326\n",
      "test loss is 0.0008163158063535403\n",
      "Batch: 7800,train loss is: 0.0003229860705068785\n",
      "test loss is 0.0008253419897607657\n",
      "Batch: 7900,train loss is: 0.0006630700531633874\n",
      "test loss is 0.0008227702208343618\n",
      "Batch: 8000,train loss is: 0.0005146959732533977\n",
      "test loss is 0.0008582604821817493\n",
      "Batch: 8100,train loss is: 0.0006852253385821353\n",
      "test loss is 0.0008325649078471574\n",
      "Batch: 8200,train loss is: 0.0013189372676462574\n",
      "test loss is 0.0008375094646943535\n",
      "Batch: 8300,train loss is: 0.0006191479607850374\n",
      "test loss is 0.0008370187461708419\n",
      "Batch: 8400,train loss is: 0.0005584008090979396\n",
      "test loss is 0.0008195576785249619\n",
      "Batch: 8500,train loss is: 0.0010980607850927255\n",
      "test loss is 0.0008421079197432505\n",
      "Batch: 8600,train loss is: 0.0007724295898608763\n",
      "test loss is 0.0008347019066417816\n",
      "Batch: 8700,train loss is: 0.0004950848087766763\n",
      "test loss is 0.0008357664198794582\n",
      "Batch: 8800,train loss is: 0.00035555903170297415\n",
      "test loss is 0.0008161043685843234\n",
      "Batch: 8900,train loss is: 0.0011213803205253018\n",
      "test loss is 0.000813221246946843\n",
      "Batch: 9000,train loss is: 0.00032305450625105776\n",
      "test loss is 0.0008242329303562445\n",
      "Batch: 9100,train loss is: 0.0005001037104322693\n",
      "test loss is 0.0008183085318655097\n",
      "Batch: 9200,train loss is: 0.0010455809314836146\n",
      "test loss is 0.0008679385248191497\n",
      "Batch: 9300,train loss is: 0.0006236401403801175\n",
      "test loss is 0.0008171116221230851\n",
      "Batch: 9400,train loss is: 0.0005493005321462627\n",
      "test loss is 0.0008659169416357341\n",
      "Batch: 9500,train loss is: 0.0004741817662154312\n",
      "test loss is 0.0008327575075347704\n",
      "Batch: 9600,train loss is: 0.0005322192299575485\n",
      "test loss is 0.0008163447659029164\n",
      "Batch: 9700,train loss is: 0.0005359739484449936\n",
      "test loss is 0.0008382676292032679\n",
      "Batch: 9800,train loss is: 0.0006199854904987637\n",
      "test loss is 0.0008223367728013851\n",
      "Batch: 9900,train loss is: 0.0005811559397353457\n",
      "test loss is 0.0008531656410405581\n",
      "Batch: 10000,train loss is: 0.0006238973078821581\n",
      "test loss is 0.0008257234694063305\n",
      "Batch: 10100,train loss is: 0.00110970491336524\n",
      "test loss is 0.0008120972877691006\n",
      "Batch: 10200,train loss is: 0.0016183066077682293\n",
      "test loss is 0.0009253704343147555\n",
      "Batch: 10300,train loss is: 0.0005719610096427442\n",
      "test loss is 0.0008287932361652524\n",
      "Batch: 10400,train loss is: 0.00045744567961188154\n",
      "test loss is 0.0008172359231924159\n",
      "Batch: 10500,train loss is: 0.0007564785707301717\n",
      "test loss is 0.0008191845716744338\n",
      "Batch: 10600,train loss is: 0.000360393562381346\n",
      "test loss is 0.0008628572835866458\n",
      "Batch: 10700,train loss is: 0.0006221587064173083\n",
      "test loss is 0.0008167436896619932\n",
      "Batch: 10800,train loss is: 0.0004989292016713769\n",
      "test loss is 0.0008343554341349098\n",
      "Batch: 10900,train loss is: 0.0007025267066627824\n",
      "test loss is 0.0008179950186517756\n",
      "Batch: 11000,train loss is: 0.0006738540516091157\n",
      "test loss is 0.0008494801577509994\n",
      "Batch: 11100,train loss is: 0.0010593360071063551\n",
      "test loss is 0.0008149250326426489\n",
      "Batch: 11200,train loss is: 0.0006989841176353243\n",
      "test loss is 0.0008360115808113039\n",
      "Batch: 11300,train loss is: 0.00038761085319237047\n",
      "test loss is 0.0009025337043216284\n",
      "Batch: 11400,train loss is: 0.0006745715564323945\n",
      "test loss is 0.0008379550081721917\n",
      "Batch: 11500,train loss is: 0.0006088670803178202\n",
      "test loss is 0.0008280426244851564\n",
      "Batch: 11600,train loss is: 0.0020616199826546\n",
      "test loss is 0.0008401443600362768\n",
      "Batch: 11700,train loss is: 0.0003839087737970391\n",
      "test loss is 0.0008003267834047507\n",
      "Batch: 11800,train loss is: 0.0006597993219736441\n",
      "test loss is 0.0008210817273735695\n",
      "Batch: 11900,train loss is: 0.0007352281785271196\n",
      "test loss is 0.0008273689245822453\n",
      "Batch: 12000,train loss is: 0.0004816073921252528\n",
      "test loss is 0.0008340314229082407\n",
      "Batch: 12100,train loss is: 0.001851119101576317\n",
      "test loss is 0.0008234268726930445\n",
      "Batch: 12200,train loss is: 0.0007979295638272469\n",
      "test loss is 0.0008418833366832345\n",
      "Batch: 12300,train loss is: 0.0005031147257844285\n",
      "test loss is 0.0008491721969728444\n",
      "Batch: 12400,train loss is: 0.0007879514732057465\n",
      "test loss is 0.0008067012805853814\n",
      "Batch: 12500,train loss is: 0.0003909123771457152\n",
      "test loss is 0.0008408453811228011\n",
      "Batch: 12600,train loss is: 0.00042670110596331064\n",
      "test loss is 0.0008267255204538833\n",
      "Batch: 12700,train loss is: 0.0005710474743315803\n",
      "test loss is 0.0008081456932380244\n",
      "Batch: 12800,train loss is: 0.0004817563593542286\n",
      "test loss is 0.0008145619981767422\n",
      "Batch: 12900,train loss is: 0.0007836065588499398\n",
      "test loss is 0.0008268212002911493\n",
      "Batch: 13000,train loss is: 0.0003010341809539935\n",
      "test loss is 0.00080326180560754\n",
      "Batch: 13100,train loss is: 0.002401150940376571\n",
      "test loss is 0.0008102338719934181\n",
      "Batch: 13200,train loss is: 0.000829265967155256\n",
      "test loss is 0.0008911676718062523\n",
      "Batch: 13300,train loss is: 0.0005324822798557452\n",
      "test loss is 0.0008656823302637123\n",
      "Batch: 13400,train loss is: 0.00039024772743895297\n",
      "test loss is 0.0008374950270363897\n",
      "Batch: 13500,train loss is: 0.0006826672483213334\n",
      "test loss is 0.0008438151292269806\n",
      "Batch: 13600,train loss is: 0.0006076772940696624\n",
      "test loss is 0.000818989985421798\n",
      "Batch: 13700,train loss is: 0.0007280191137805748\n",
      "test loss is 0.0008178477286007172\n",
      "Batch: 13800,train loss is: 0.0007702250923037685\n",
      "test loss is 0.0008259946564625268\n",
      "Batch: 13900,train loss is: 0.0007789233209906188\n",
      "test loss is 0.0008323016191720474\n",
      "Batch: 14000,train loss is: 0.0004490020692213744\n",
      "test loss is 0.0008179726747227818\n",
      "Batch: 14100,train loss is: 0.0008870469166624924\n",
      "test loss is 0.0008707495089136669\n",
      "Batch: 14200,train loss is: 0.0005379761053586597\n",
      "test loss is 0.0008004146376732437\n",
      "Batch: 14300,train loss is: 0.0005318040574984733\n",
      "test loss is 0.0008086714370505449\n",
      "Batch: 14400,train loss is: 0.0014772912759726448\n",
      "test loss is 0.0008088260234750338\n",
      "Batch: 14500,train loss is: 0.00047649066035482723\n",
      "test loss is 0.0008147097097778247\n",
      "Batch: 14600,train loss is: 0.0019482975663899348\n",
      "test loss is 0.0008128913145717268\n",
      "Batch: 14700,train loss is: 0.00044929730561454445\n",
      "test loss is 0.0008362764670389299\n",
      "Batch: 14800,train loss is: 0.0009448694697938516\n",
      "test loss is 0.0008184209795306318\n",
      "Batch: 14900,train loss is: 0.0004568670725802232\n",
      "test loss is 0.00083358640281444\n",
      "Batch: 15000,train loss is: 0.0008766369438830558\n",
      "test loss is 0.0008098518035154892\n",
      "Batch: 15100,train loss is: 0.0007230308519712844\n",
      "test loss is 0.0008354116269598802\n",
      "Batch: 15200,train loss is: 0.0013345153814119256\n",
      "test loss is 0.0008402302993133311\n",
      "Batch: 15300,train loss is: 0.0005322387870565201\n",
      "test loss is 0.0008081096111960957\n",
      "Batch: 15400,train loss is: 0.000528151755304133\n",
      "test loss is 0.0008419939496668388\n",
      "Batch: 15500,train loss is: 0.0008679262903636775\n",
      "test loss is 0.0008030725582247217\n",
      "Batch: 15600,train loss is: 0.001084358007640508\n",
      "test loss is 0.0008444334202448094\n",
      "Batch: 15700,train loss is: 0.000673581994477004\n",
      "test loss is 0.0008127672834150708\n",
      "Batch: 15800,train loss is: 0.0006932341843106034\n",
      "test loss is 0.0008140229129374373\n",
      "Batch: 15900,train loss is: 0.0012833926534403409\n",
      "test loss is 0.0008332114223892679\n",
      "Batch: 16000,train loss is: 0.00048625166799025425\n",
      "test loss is 0.0008406663034491508\n",
      "Batch: 16100,train loss is: 0.000654697178485089\n",
      "test loss is 0.0008286996271172069\n",
      "Batch: 16200,train loss is: 0.0004842717639573909\n",
      "test loss is 0.0008173791317671731\n",
      "Batch: 16300,train loss is: 0.001280723526526656\n",
      "test loss is 0.0008156362611542128\n",
      "Batch: 16400,train loss is: 0.000453393588531072\n",
      "test loss is 0.0008040667784923863\n",
      "Batch: 16500,train loss is: 0.0006209543929678259\n",
      "test loss is 0.0008484179667635224\n",
      "Batch: 16600,train loss is: 0.0003477246749000862\n",
      "test loss is 0.0008202481621735375\n",
      "Batch: 16700,train loss is: 0.0011972278333488835\n",
      "test loss is 0.0008288766004119906\n",
      "Batch: 16800,train loss is: 0.0006336057483314882\n",
      "test loss is 0.0008333187634336137\n",
      "Batch: 16900,train loss is: 0.0008031613348646876\n",
      "test loss is 0.0008295930886819045\n",
      "Batch: 17000,train loss is: 0.0007806220948178571\n",
      "test loss is 0.0008063324888527493\n",
      "Batch: 17100,train loss is: 0.0005780973670953261\n",
      "test loss is 0.0008170028196149261\n",
      "Batch: 17200,train loss is: 0.000659233435458814\n",
      "test loss is 0.0008211391871304508\n",
      "Batch: 17300,train loss is: 0.0008335770190278066\n",
      "test loss is 0.0008219956306190139\n",
      "Batch: 17400,train loss is: 0.0005183924906930201\n",
      "test loss is 0.0008182906179914991\n",
      "Batch: 17500,train loss is: 0.0005297175862433221\n",
      "test loss is 0.0008221150602299941\n",
      "Batch: 17600,train loss is: 0.0004823756173626656\n",
      "test loss is 0.0008381482472479293\n",
      "Batch: 17700,train loss is: 0.0003159024427893065\n",
      "test loss is 0.0008463893424008661\n",
      "Batch: 17800,train loss is: 0.0011677382771707672\n",
      "test loss is 0.0008208880949054906\n",
      "Batch: 17900,train loss is: 0.0005123857033823855\n",
      "test loss is 0.0008563033302163901\n",
      "Batch: 18000,train loss is: 0.0004057557916274141\n",
      "test loss is 0.0008052528632502098\n",
      "Batch: 18100,train loss is: 0.0005102221263225932\n",
      "test loss is 0.0008128977786034641\n",
      "Batch: 18200,train loss is: 0.0005741779278328158\n",
      "test loss is 0.0008153490544731788\n",
      "Batch: 18300,train loss is: 0.0009519963979578678\n",
      "test loss is 0.0008125044911986949\n",
      "Batch: 18400,train loss is: 0.0005808723306566506\n",
      "test loss is 0.0008150615231423686\n",
      "Batch: 18500,train loss is: 0.0007808357890158233\n",
      "test loss is 0.0008360846823705526\n",
      "Batch: 18600,train loss is: 0.00044960593015010944\n",
      "test loss is 0.000810779869823959\n",
      "Batch: 18700,train loss is: 0.0005931900017098666\n",
      "test loss is 0.0008141914979369968\n",
      "Batch: 18800,train loss is: 0.0008055252492855752\n",
      "test loss is 0.0008165242691264521\n",
      "Batch: 18900,train loss is: 0.000824647586603561\n",
      "test loss is 0.0008538272405396761\n",
      "Batch: 19000,train loss is: 0.0007550821540207285\n",
      "test loss is 0.0008168695255813003\n",
      "Batch: 19100,train loss is: 0.00039707533317964743\n",
      "test loss is 0.0008291148877607252\n",
      "Batch: 19200,train loss is: 0.000711762643926491\n",
      "test loss is 0.0008263148967807453\n",
      "Batch: 19300,train loss is: 0.0009196254942975468\n",
      "test loss is 0.0008097121427571459\n",
      "Batch: 19400,train loss is: 0.0003550064690983331\n",
      "test loss is 0.000802720557373472\n",
      "Batch: 19500,train loss is: 0.0006487834698309183\n",
      "test loss is 0.0008597652061266636\n",
      "Batch: 19600,train loss is: 0.0005001038602095878\n",
      "test loss is 0.0008235104819346573\n",
      "Batch: 19700,train loss is: 0.000810900895006172\n",
      "test loss is 0.0008730849438013192\n",
      "Batch: 19800,train loss is: 0.0005452607397603867\n",
      "test loss is 0.0008210146293006208\n",
      "Batch: 19900,train loss is: 0.0003179623076803399\n",
      "test loss is 0.0008450452933830245\n",
      "Batch: 20000,train loss is: 0.0008635472533049145\n",
      "test loss is 0.0008156826449392048\n",
      "Batch: 20100,train loss is: 0.0006958603643180927\n",
      "test loss is 0.000820818565921899\n",
      "Batch: 20200,train loss is: 0.0010482001221068992\n",
      "test loss is 0.0008378677984557414\n",
      "Batch: 20300,train loss is: 0.0004217358931702747\n",
      "test loss is 0.0008284912345857697\n",
      "Batch: 20400,train loss is: 0.0005933516067268531\n",
      "test loss is 0.0008614132046895012\n",
      "Batch: 20500,train loss is: 0.0006479005041794618\n",
      "test loss is 0.0008217609005203299\n",
      "Batch: 20600,train loss is: 0.0007572023580291056\n",
      "test loss is 0.0008614934228556454\n",
      "Batch: 20700,train loss is: 0.000604410676808483\n",
      "test loss is 0.0008315351079088667\n",
      "Batch: 20800,train loss is: 0.0003869726355458175\n",
      "test loss is 0.0008190816058416153\n",
      "Batch: 20900,train loss is: 0.0007785988457868049\n",
      "test loss is 0.0008525620151489358\n",
      "Batch: 21000,train loss is: 0.00040344889138526344\n",
      "test loss is 0.0008087146520106565\n",
      "Batch: 21100,train loss is: 0.0006583845856437509\n",
      "test loss is 0.0008412509564237237\n",
      "Batch: 21200,train loss is: 0.00033288379487429677\n",
      "test loss is 0.0008175498059221418\n",
      "Batch: 21300,train loss is: 0.00043191229891873103\n",
      "test loss is 0.0008205758267565884\n",
      "Batch: 21400,train loss is: 0.0009420046972389801\n",
      "test loss is 0.0008284922108430517\n",
      "Batch: 21500,train loss is: 0.00028457876050546374\n",
      "test loss is 0.0008398064697000212\n",
      "Batch: 21600,train loss is: 0.0008658500351541965\n",
      "test loss is 0.0008289075484416761\n",
      "Batch: 21700,train loss is: 0.00035951133144094123\n",
      "test loss is 0.0008098017308335488\n",
      "Batch: 21800,train loss is: 0.0006828889724094584\n",
      "test loss is 0.0008652423734685688\n",
      "Batch: 21900,train loss is: 0.0006310741146627979\n",
      "test loss is 0.000900121861081958\n",
      "Batch: 22000,train loss is: 0.0007989688482208298\n",
      "test loss is 0.0008131441217352828\n",
      "Batch: 22100,train loss is: 0.0003409860632761713\n",
      "test loss is 0.0008145303850743273\n",
      "Batch: 22200,train loss is: 0.0006046065848645485\n",
      "test loss is 0.0008198590983629592\n",
      "Batch: 22300,train loss is: 0.0009005981539347569\n",
      "test loss is 0.0008161199931769771\n",
      "Batch: 22400,train loss is: 0.0006119252099606164\n",
      "test loss is 0.0008038784364063483\n",
      "Batch: 22500,train loss is: 0.00044024307021246577\n",
      "test loss is 0.0008061771570449572\n",
      "Batch: 22600,train loss is: 0.0011819972708956468\n",
      "test loss is 0.0008759092236031313\n",
      "Batch: 22700,train loss is: 0.0004459262000509264\n",
      "test loss is 0.0008137506921107387\n",
      "Batch: 22800,train loss is: 0.0005558114602203384\n",
      "test loss is 0.0008267805179929572\n",
      "Batch: 22900,train loss is: 0.0004047413115747291\n",
      "test loss is 0.0008784394345943351\n",
      "Batch: 23000,train loss is: 0.0004539029357281215\n",
      "test loss is 0.0008134265884932135\n",
      "Batch: 23100,train loss is: 0.0006754940996048789\n",
      "test loss is 0.0007963400846458593\n",
      "Batch: 23200,train loss is: 0.0003917039737666492\n",
      "test loss is 0.0007943430389654638\n",
      "Batch: 23300,train loss is: 0.0005975017818742877\n",
      "test loss is 0.0008049546333399565\n",
      "Batch: 23400,train loss is: 0.0008078723020576418\n",
      "test loss is 0.0008227948440754665\n",
      "Batch: 23500,train loss is: 0.003013720025958473\n",
      "test loss is 0.0008120187568600938\n",
      "Batch: 23600,train loss is: 0.00036737074442183673\n",
      "test loss is 0.0007991643205330026\n",
      "Batch: 23700,train loss is: 0.0009038106007803115\n",
      "test loss is 0.0008339210633473995\n",
      "Batch: 23800,train loss is: 0.0010198853616527307\n",
      "test loss is 0.000801739243476349\n",
      "Batch: 23900,train loss is: 0.0009294930344427544\n",
      "test loss is 0.0008058531383652888\n",
      "Batch: 24000,train loss is: 0.000727030221027673\n",
      "test loss is 0.0008514707097293545\n",
      "Batch: 24100,train loss is: 0.0005430269847293456\n",
      "test loss is 0.0007987678026007434\n",
      "Batch: 24200,train loss is: 0.0018182671540352806\n",
      "test loss is 0.0008246043283534081\n",
      "Batch: 24300,train loss is: 0.0005098922885643584\n",
      "test loss is 0.0009421289535367309\n",
      "Batch: 24400,train loss is: 0.0013856933903122393\n",
      "test loss is 0.0008121512533751693\n",
      "Batch: 24500,train loss is: 0.0006241721918144492\n",
      "test loss is 0.00081068023394681\n",
      "Batch: 24600,train loss is: 0.0005790361114100767\n",
      "test loss is 0.0008430638739603608\n",
      "Batch: 24700,train loss is: 0.00038089639463765695\n",
      "test loss is 0.0008054291498120035\n",
      "Batch: 24800,train loss is: 0.0011029121546055555\n",
      "test loss is 0.0008100268826310336\n",
      "Batch: 24900,train loss is: 0.00018755728233401807\n",
      "test loss is 0.0008257111097375361\n",
      "Batch: 25000,train loss is: 0.0008260007986589092\n",
      "test loss is 0.000806237981349326\n",
      "Batch: 25100,train loss is: 0.0006921743745184047\n",
      "test loss is 0.0008191956069894689\n",
      "Batch: 25200,train loss is: 0.0008413544524520351\n",
      "test loss is 0.00082528217549232\n",
      "Batch: 25300,train loss is: 0.0009113970234494107\n",
      "test loss is 0.0008323000245346564\n",
      "Batch: 25400,train loss is: 0.0004475996223232546\n",
      "test loss is 0.0008011892816546115\n",
      "Batch: 25500,train loss is: 0.0005678495903567606\n",
      "test loss is 0.0008033603784077503\n",
      "Batch: 25600,train loss is: 0.0007468686027789535\n",
      "test loss is 0.0008177968641005519\n",
      "Batch: 25700,train loss is: 0.0007053764187187533\n",
      "test loss is 0.000831125162803473\n",
      "Batch: 25800,train loss is: 0.00031411072757541434\n",
      "test loss is 0.0008055221557946735\n",
      "Batch: 25900,train loss is: 0.00043210778411992507\n",
      "test loss is 0.0008481944085579363\n",
      "Batch: 26000,train loss is: 0.0018461618526417589\n",
      "test loss is 0.0008683297217951476\n",
      "Batch: 26100,train loss is: 0.0009823204446209662\n",
      "test loss is 0.0008125571378542506\n",
      "Batch: 26200,train loss is: 0.0008997437154831676\n",
      "test loss is 0.0007926992658673682\n",
      "Batch: 26300,train loss is: 0.0013335788027917957\n",
      "test loss is 0.0008337613643462591\n",
      "Batch: 26400,train loss is: 0.0007369377154749823\n",
      "test loss is 0.0008123229598765005\n",
      "Batch: 26500,train loss is: 0.0004291647930846826\n",
      "test loss is 0.0008223331391727284\n",
      "Batch: 26600,train loss is: 0.0011517861284255583\n",
      "test loss is 0.0008636320906823727\n",
      "Batch: 26700,train loss is: 0.00045158981885182653\n",
      "test loss is 0.0009248944240529554\n",
      "Batch: 26800,train loss is: 0.001346731685511969\n",
      "test loss is 0.0008121640067913405\n",
      "Batch: 26900,train loss is: 0.0006018961891823551\n",
      "test loss is 0.0008047574556010399\n",
      "Batch: 27000,train loss is: 0.0007315112632058678\n",
      "test loss is 0.0009400738831908052\n",
      "Batch: 27100,train loss is: 0.00046498612714895296\n",
      "test loss is 0.0008345789939101177\n",
      "Batch: 27200,train loss is: 0.0006550793747730928\n",
      "test loss is 0.0008483098730065284\n",
      "Batch: 27300,train loss is: 0.0006182889709860955\n",
      "test loss is 0.000817083471203395\n",
      "Batch: 27400,train loss is: 0.000848519069863957\n",
      "test loss is 0.0007946084109773638\n",
      "Batch: 27500,train loss is: 0.0006081245243756216\n",
      "test loss is 0.000893239732708408\n",
      "Batch: 27600,train loss is: 0.0006968355532748378\n",
      "test loss is 0.0008656468263015588\n",
      "Batch: 27700,train loss is: 0.0020004037439788963\n",
      "test loss is 0.0008178050224702129\n",
      "Batch: 27800,train loss is: 0.0006994079231212506\n",
      "test loss is 0.0008340617265292056\n",
      "Batch: 27900,train loss is: 0.0012863040135640247\n",
      "test loss is 0.0008017429995808376\n",
      "Batch: 28000,train loss is: 0.0004458020221976177\n",
      "test loss is 0.0008238137106647804\n",
      "Batch: 28100,train loss is: 0.0008547237706832334\n",
      "test loss is 0.0008239243269048209\n",
      "Batch: 28200,train loss is: 0.0007706090350515549\n",
      "test loss is 0.0007890143503840341\n",
      "Batch: 28300,train loss is: 0.0021424461892398275\n",
      "test loss is 0.0008030757456157326\n",
      "Batch: 28400,train loss is: 0.0006615069970847857\n",
      "test loss is 0.0008068074352890917\n",
      "Batch: 28500,train loss is: 0.0013936293569792309\n",
      "test loss is 0.000824145177988695\n",
      "Batch: 28600,train loss is: 0.0004746364068121995\n",
      "test loss is 0.0008180125687296487\n",
      "Batch: 28700,train loss is: 0.00047177034006239176\n",
      "test loss is 0.0008030878767683723\n",
      "Batch: 28800,train loss is: 0.0014213744567304308\n",
      "test loss is 0.0008096617607156322\n",
      "Batch: 28900,train loss is: 0.0006481608690088501\n",
      "test loss is 0.0008076093464254921\n",
      "Batch: 29000,train loss is: 0.0006186555003664913\n",
      "test loss is 0.0007955213614333765\n",
      "Batch: 29100,train loss is: 0.0007428545058013665\n",
      "test loss is 0.0008273176551833729\n",
      "Batch: 29200,train loss is: 0.0005281577958778362\n",
      "test loss is 0.0008219468758801976\n",
      "Batch: 29300,train loss is: 0.001031492525502628\n",
      "test loss is 0.0008075943811164741\n",
      "Batch: 29400,train loss is: 0.0006900131295735286\n",
      "test loss is 0.0008543678872353728\n",
      "Batch: 29500,train loss is: 0.0004939526748434939\n",
      "test loss is 0.0008150612645271891\n",
      "Batch: 29600,train loss is: 0.0006278897864289498\n",
      "test loss is 0.0008124472865445926\n",
      "Batch: 29700,train loss is: 0.0003515681018149311\n",
      "test loss is 0.0008272259463264214\n",
      "Batch: 29800,train loss is: 0.0010140948732455576\n",
      "test loss is 0.0008039841535044428\n",
      "Batch: 29900,train loss is: 0.0034481290009002945\n",
      "test loss is 0.0008164812396495474\n",
      "Batch: 30000,train loss is: 0.00036792726070457445\n",
      "test loss is 0.0008455986143218319\n",
      "Batch: 30100,train loss is: 0.0005059311346829208\n",
      "test loss is 0.0008405057731253069\n",
      "Batch: 30200,train loss is: 0.0004161352527614777\n",
      "test loss is 0.0008025429000814173\n",
      "Batch: 30300,train loss is: 0.00041859476442396504\n",
      "test loss is 0.0007987534237271311\n",
      "Batch: 30400,train loss is: 0.00044135971900425614\n",
      "test loss is 0.000804215028722257\n",
      "Batch: 30500,train loss is: 0.0004700426553187028\n",
      "test loss is 0.0008471711183915946\n",
      "Batch: 30600,train loss is: 0.0003146918009443648\n",
      "test loss is 0.0008069454380783219\n",
      "Batch: 30700,train loss is: 0.0005764217603383556\n",
      "test loss is 0.0009449809026520709\n",
      "Batch: 30800,train loss is: 0.0003255318347386488\n",
      "test loss is 0.0008670815828382225\n",
      "Batch: 30900,train loss is: 0.00042663891244795326\n",
      "test loss is 0.000807326933782532\n",
      "Batch: 31000,train loss is: 0.0006088108760408629\n",
      "test loss is 0.0008161539092984933\n",
      "Batch: 31100,train loss is: 0.0004333912003201141\n",
      "test loss is 0.0008740966887913163\n",
      "Batch: 31200,train loss is: 0.0006842731312546264\n",
      "test loss is 0.0008135935817872553\n",
      "Batch: 31300,train loss is: 0.0007106609566924171\n",
      "test loss is 0.000827494693615418\n",
      "Batch: 31400,train loss is: 0.0008396115106114959\n",
      "test loss is 0.0008021673696156224\n",
      "Batch: 31500,train loss is: 0.000680337759593329\n",
      "test loss is 0.000819012693817813\n",
      "Batch: 31600,train loss is: 0.0005653578446542531\n",
      "test loss is 0.0008120120588813313\n",
      "Batch: 31700,train loss is: 0.0005725593685432946\n",
      "test loss is 0.0008540481769578009\n",
      "Batch: 31800,train loss is: 0.0011454514874382387\n",
      "test loss is 0.0008086896090530337\n",
      "Batch: 31900,train loss is: 0.0010578942605864808\n",
      "test loss is 0.0008380058264674056\n",
      "Batch: 32000,train loss is: 0.004749864468148263\n",
      "test loss is 0.0008035949535469162\n",
      "Batch: 32100,train loss is: 0.00044331316141141465\n",
      "test loss is 0.0007981127266827448\n",
      "Batch: 32200,train loss is: 0.0005283988530608617\n",
      "test loss is 0.0008146466828960798\n",
      "Batch: 32300,train loss is: 0.00042384955666311444\n",
      "test loss is 0.0008085228494815345\n",
      "Batch: 32400,train loss is: 0.0005239984263745606\n",
      "test loss is 0.0007937358060992792\n",
      "Batch: 32500,train loss is: 0.001135792178396851\n",
      "test loss is 0.0008373033027604359\n",
      "Batch: 32600,train loss is: 0.00046586423610699433\n",
      "test loss is 0.0008145884938645388\n",
      "Batch: 32700,train loss is: 0.0004933026327536974\n",
      "test loss is 0.0008051495389304301\n",
      "Batch: 32800,train loss is: 0.0004424423102514679\n",
      "test loss is 0.0008143817420943755\n",
      "Batch: 32900,train loss is: 0.0006978978622032395\n",
      "test loss is 0.0008063642612859362\n",
      "Batch: 33000,train loss is: 0.000616190037225163\n",
      "test loss is 0.0007899436386704323\n",
      "Batch: 33100,train loss is: 0.0005271926468567438\n",
      "test loss is 0.0008343140103492855\n",
      "Batch: 33200,train loss is: 0.0006384579965036471\n",
      "test loss is 0.0009102483157066895\n",
      "Batch: 33300,train loss is: 0.00035784718342036365\n",
      "test loss is 0.000790433618536678\n",
      "Batch: 33400,train loss is: 0.0005652200485375032\n",
      "test loss is 0.0008186402802236151\n",
      "Batch: 33500,train loss is: 0.0005675257377433961\n",
      "test loss is 0.0008017951830102815\n",
      "Batch: 33600,train loss is: 0.0006186984015043869\n",
      "test loss is 0.0008107895845343797\n",
      "Batch: 33700,train loss is: 0.000707699680013114\n",
      "test loss is 0.0008173120871462939\n",
      "Batch: 33800,train loss is: 0.0008104289902934931\n",
      "test loss is 0.0007992116578506155\n",
      "Batch: 33900,train loss is: 0.0006997020389311898\n",
      "test loss is 0.0008042700985934933\n",
      "-----------------------Epoch: 3----------------------------------\n",
      "Batch: 0,train loss is: 0.0004849831018824049\n",
      "test loss is 0.0008068916618587397\n",
      "Batch: 100,train loss is: 0.001565584950624091\n",
      "test loss is 0.0008074144516088882\n",
      "Batch: 200,train loss is: 0.0007588021079209885\n",
      "test loss is 0.0008068337738816522\n",
      "Batch: 300,train loss is: 0.00037754103221785995\n",
      "test loss is 0.0008051017982047553\n",
      "Batch: 400,train loss is: 0.0006335483965426096\n",
      "test loss is 0.0008045971224799927\n",
      "Batch: 500,train loss is: 0.0006856070035622656\n",
      "test loss is 0.0008417688087704478\n",
      "Batch: 600,train loss is: 0.00031677968729678806\n",
      "test loss is 0.0008328510612990703\n",
      "Batch: 700,train loss is: 0.00035194476477665154\n",
      "test loss is 0.0008060071796823865\n",
      "Batch: 800,train loss is: 0.0005001698593467524\n",
      "test loss is 0.0007975910343154453\n",
      "Batch: 900,train loss is: 0.00040582608437668123\n",
      "test loss is 0.0007958020281339889\n",
      "Batch: 1000,train loss is: 0.0006085386630386461\n",
      "test loss is 0.0008331952229666209\n",
      "Batch: 1100,train loss is: 0.002005688052132774\n",
      "test loss is 0.0007976438067561635\n",
      "Batch: 1200,train loss is: 0.0004004022957912167\n",
      "test loss is 0.0007937589680871358\n",
      "Batch: 1300,train loss is: 0.0005911741653624624\n",
      "test loss is 0.000802453323848295\n",
      "Batch: 1400,train loss is: 0.0006036200696474957\n",
      "test loss is 0.0008153233639716581\n",
      "Batch: 1500,train loss is: 0.0005462015228510024\n",
      "test loss is 0.0008228539262438907\n",
      "Batch: 1600,train loss is: 0.0005155986472496025\n",
      "test loss is 0.0008584244733545504\n",
      "Batch: 1700,train loss is: 0.0013227402851370533\n",
      "test loss is 0.0007920311293670358\n",
      "Batch: 1800,train loss is: 0.000519431862381002\n",
      "test loss is 0.0008191985686705188\n",
      "Batch: 1900,train loss is: 0.0004673667610359896\n",
      "test loss is 0.0008509405335645796\n",
      "Batch: 2000,train loss is: 0.0007090664862715295\n",
      "test loss is 0.0009036429356855393\n",
      "Batch: 2100,train loss is: 0.0006831131999949978\n",
      "test loss is 0.000924888028570876\n",
      "Batch: 2200,train loss is: 0.0010126835841899843\n",
      "test loss is 0.0008052693455725332\n",
      "Batch: 2300,train loss is: 0.00041885600538842484\n",
      "test loss is 0.0008465536262124148\n",
      "Batch: 2400,train loss is: 0.0007154700894573326\n",
      "test loss is 0.0008042481625561116\n",
      "Batch: 2500,train loss is: 0.0007882973919831979\n",
      "test loss is 0.0007995058998850384\n",
      "Batch: 2600,train loss is: 0.0003882333162797926\n",
      "test loss is 0.0008161176634264093\n",
      "Batch: 2700,train loss is: 0.0007538805975310369\n",
      "test loss is 0.0007908379189043538\n",
      "Batch: 2800,train loss is: 0.0005257232412199319\n",
      "test loss is 0.0008384962486588768\n",
      "Batch: 2900,train loss is: 0.0005488846798024753\n",
      "test loss is 0.0007890393534482611\n",
      "Batch: 3000,train loss is: 0.0005146485444390758\n",
      "test loss is 0.0007914537535950757\n",
      "Batch: 3100,train loss is: 0.0006520521176266831\n",
      "test loss is 0.0008356601219208276\n",
      "Batch: 3200,train loss is: 0.000972211173866877\n",
      "test loss is 0.0008105430846462406\n",
      "Batch: 3300,train loss is: 0.0004548005403158318\n",
      "test loss is 0.0008007675598985762\n",
      "Batch: 3400,train loss is: 0.0005287676892057689\n",
      "test loss is 0.0008085513137362066\n",
      "Batch: 3500,train loss is: 0.0003295485110270418\n",
      "test loss is 0.0008102763480473859\n",
      "Batch: 3600,train loss is: 0.0006215467673807178\n",
      "test loss is 0.0008391548958718864\n",
      "Batch: 3700,train loss is: 0.0004945833331492314\n",
      "test loss is 0.0007877934978514465\n",
      "Batch: 3800,train loss is: 0.0004287026147295788\n",
      "test loss is 0.0008322606372044429\n",
      "Batch: 3900,train loss is: 0.000637751929896026\n",
      "test loss is 0.000796122417513074\n",
      "Batch: 4000,train loss is: 0.0004240526177984619\n",
      "test loss is 0.000802055802900758\n",
      "Batch: 4100,train loss is: 0.0007651168577679904\n",
      "test loss is 0.0008147983816772334\n",
      "Batch: 4200,train loss is: 0.0004368269011630359\n",
      "test loss is 0.0008363198515703497\n",
      "Batch: 4300,train loss is: 0.0004591167759373713\n",
      "test loss is 0.0007987673318695072\n",
      "Batch: 4400,train loss is: 0.0006267207914690732\n",
      "test loss is 0.0007975933483181682\n",
      "Batch: 4500,train loss is: 0.0006423103012193707\n",
      "test loss is 0.0008293555900540028\n",
      "Batch: 4600,train loss is: 0.0006526512945414319\n",
      "test loss is 0.0007973150452591505\n",
      "Batch: 4700,train loss is: 0.0008613968740208516\n",
      "test loss is 0.0008311823324635676\n",
      "Batch: 4800,train loss is: 0.0013466808652944668\n",
      "test loss is 0.0008105028053474579\n",
      "Batch: 4900,train loss is: 0.0006054054516888577\n",
      "test loss is 0.000802104092290882\n",
      "Batch: 5000,train loss is: 0.0006186340085624333\n",
      "test loss is 0.0008067717074146125\n",
      "Batch: 5100,train loss is: 0.0006775934433537877\n",
      "test loss is 0.0008511858037717878\n",
      "Batch: 5200,train loss is: 0.0004118066273740761\n",
      "test loss is 0.0008017467590360853\n",
      "Batch: 5300,train loss is: 0.0007905715000881725\n",
      "test loss is 0.0008309962884781637\n",
      "Batch: 5400,train loss is: 0.0008948048080595981\n",
      "test loss is 0.0007971842080719906\n",
      "Batch: 5500,train loss is: 0.0005812833788969793\n",
      "test loss is 0.000820824313716717\n",
      "Batch: 5600,train loss is: 0.0007141229940714329\n",
      "test loss is 0.0008149307835656815\n",
      "Batch: 5700,train loss is: 0.0008748225691355614\n",
      "test loss is 0.0007957269817949942\n",
      "Batch: 5800,train loss is: 0.0006120501428908961\n",
      "test loss is 0.0008029942428841558\n",
      "Batch: 5900,train loss is: 0.0036904623665115753\n",
      "test loss is 0.0008063649614084178\n",
      "Batch: 6000,train loss is: 0.0016567922938703812\n",
      "test loss is 0.0008210213434162311\n",
      "Batch: 6100,train loss is: 0.0005232935728456925\n",
      "test loss is 0.0007893567892549645\n",
      "Batch: 6200,train loss is: 0.000628941171585999\n",
      "test loss is 0.0008052816720082905\n",
      "Batch: 6300,train loss is: 0.001549830611965417\n",
      "test loss is 0.0008187240776929616\n",
      "Batch: 6400,train loss is: 0.00046715978540939516\n",
      "test loss is 0.0008569481211059151\n",
      "Batch: 6500,train loss is: 0.00040603656040602977\n",
      "test loss is 0.0008321574583227704\n",
      "Batch: 6600,train loss is: 0.002364841810369261\n",
      "test loss is 0.0008027332517041353\n",
      "Batch: 6700,train loss is: 0.0005466370855066717\n",
      "test loss is 0.0008218681750900594\n",
      "Batch: 6800,train loss is: 0.0011550361196957892\n",
      "test loss is 0.00079431493695686\n",
      "Batch: 6900,train loss is: 0.0006114731594356382\n",
      "test loss is 0.0008010753000425113\n",
      "Batch: 7000,train loss is: 0.0006201396082217934\n",
      "test loss is 0.0008394961651038795\n",
      "Batch: 7100,train loss is: 0.0009571399323414511\n",
      "test loss is 0.0008023041927238762\n",
      "Batch: 7200,train loss is: 0.0004467145240456917\n",
      "test loss is 0.0008236941085258174\n",
      "Batch: 7300,train loss is: 0.0008650504915281589\n",
      "test loss is 0.0008527098488851819\n",
      "Batch: 7400,train loss is: 0.0004352476824811135\n",
      "test loss is 0.000792686256712877\n",
      "Batch: 7500,train loss is: 0.0008690259738625188\n",
      "test loss is 0.0008200025961948813\n",
      "Batch: 7600,train loss is: 0.0005890324926893427\n",
      "test loss is 0.0007966585948926716\n",
      "Batch: 7700,train loss is: 0.0007192636282159668\n",
      "test loss is 0.000797208059536631\n",
      "Batch: 7800,train loss is: 0.000317780501511055\n",
      "test loss is 0.000803487867548863\n",
      "Batch: 7900,train loss is: 0.0006447502894948967\n",
      "test loss is 0.0008037420052938993\n",
      "Batch: 8000,train loss is: 0.0005039234566455417\n",
      "test loss is 0.0008365061677232714\n",
      "Batch: 8100,train loss is: 0.0006667881941252842\n",
      "test loss is 0.0008122564292669096\n",
      "Batch: 8200,train loss is: 0.0013050108499051136\n",
      "test loss is 0.0008189023115801196\n",
      "Batch: 8300,train loss is: 0.0006277650721652508\n",
      "test loss is 0.0008190154631530216\n",
      "Batch: 8400,train loss is: 0.000560056366373382\n",
      "test loss is 0.000800559820725264\n",
      "Batch: 8500,train loss is: 0.001072999251472216\n",
      "test loss is 0.0008219672729971138\n",
      "Batch: 8600,train loss is: 0.0007499037248575153\n",
      "test loss is 0.0008158339239689822\n",
      "Batch: 8700,train loss is: 0.0005016456120788298\n",
      "test loss is 0.0008210282238328181\n",
      "Batch: 8800,train loss is: 0.0003454987495487152\n",
      "test loss is 0.0007955700086586895\n",
      "Batch: 8900,train loss is: 0.001110271900680614\n",
      "test loss is 0.000793780719195835\n",
      "Batch: 9000,train loss is: 0.0003099298423959314\n",
      "test loss is 0.0008048014525756182\n",
      "Batch: 9100,train loss is: 0.00047234098472041623\n",
      "test loss is 0.0007975285811938447\n",
      "Batch: 9200,train loss is: 0.0010221074209890786\n",
      "test loss is 0.0008523445759550047\n",
      "Batch: 9300,train loss is: 0.0006096436524739804\n",
      "test loss is 0.0007992095431794819\n",
      "Batch: 9400,train loss is: 0.0005262496546540158\n",
      "test loss is 0.0008433980258421653\n",
      "Batch: 9500,train loss is: 0.0004731970733264034\n",
      "test loss is 0.000815321900369031\n",
      "Batch: 9600,train loss is: 0.0005203688518904104\n",
      "test loss is 0.0007969048087763102\n",
      "Batch: 9700,train loss is: 0.0004990755482608629\n",
      "test loss is 0.0008189930575068474\n",
      "Batch: 9800,train loss is: 0.0005860944432681442\n",
      "test loss is 0.0008025092019251773\n",
      "Batch: 9900,train loss is: 0.0005642897051696389\n",
      "test loss is 0.0008370301821254537\n",
      "Batch: 10000,train loss is: 0.0006034835446687874\n",
      "test loss is 0.0008028633992829793\n",
      "Batch: 10100,train loss is: 0.0010935304047946224\n",
      "test loss is 0.0007938776395919704\n",
      "Batch: 10200,train loss is: 0.001625630626323156\n",
      "test loss is 0.0009080790524308458\n",
      "Batch: 10300,train loss is: 0.0005534866667346878\n",
      "test loss is 0.0008079465769614299\n",
      "Batch: 10400,train loss is: 0.00044521143086384633\n",
      "test loss is 0.0007982171741375593\n",
      "Batch: 10500,train loss is: 0.0007472507044116773\n",
      "test loss is 0.000799920976898093\n",
      "Batch: 10600,train loss is: 0.0003663878819416606\n",
      "test loss is 0.0008448548256544467\n",
      "Batch: 10700,train loss is: 0.0006256025911546298\n",
      "test loss is 0.0007980518457285183\n",
      "Batch: 10800,train loss is: 0.0005150828349814707\n",
      "test loss is 0.0008120762805564685\n",
      "Batch: 10900,train loss is: 0.0007032141773886394\n",
      "test loss is 0.0008006717204912223\n",
      "Batch: 11000,train loss is: 0.0006977725298218123\n",
      "test loss is 0.0008313577658127243\n",
      "Batch: 11100,train loss is: 0.0010459553988304184\n",
      "test loss is 0.0007951725312207538\n",
      "Batch: 11200,train loss is: 0.0007015683729070598\n",
      "test loss is 0.0008221750464858268\n",
      "Batch: 11300,train loss is: 0.00036920341969219785\n",
      "test loss is 0.0008644152368979719\n",
      "Batch: 11400,train loss is: 0.0006397807997905637\n",
      "test loss is 0.0008171863585627989\n",
      "Batch: 11500,train loss is: 0.0006047370075948517\n",
      "test loss is 0.0008076885771166955\n",
      "Batch: 11600,train loss is: 0.0020036260316270145\n",
      "test loss is 0.0008175122431295313\n",
      "Batch: 11700,train loss is: 0.0003643056932743783\n",
      "test loss is 0.0007817780227056051\n",
      "Batch: 11800,train loss is: 0.0006473592884075771\n",
      "test loss is 0.0008012854749183195\n",
      "Batch: 11900,train loss is: 0.0007278293846243955\n",
      "test loss is 0.000809261379819159\n",
      "Batch: 12000,train loss is: 0.000485951899251831\n",
      "test loss is 0.0008112328247456906\n",
      "Batch: 12100,train loss is: 0.0018211333346887914\n",
      "test loss is 0.0008042171490067098\n",
      "Batch: 12200,train loss is: 0.0007974340886618202\n",
      "test loss is 0.0008238537494216669\n",
      "Batch: 12300,train loss is: 0.0004935767554488706\n",
      "test loss is 0.000825081139557487\n",
      "Batch: 12400,train loss is: 0.0007410463995803243\n",
      "test loss is 0.0007907760586270764\n",
      "Batch: 12500,train loss is: 0.0003725238370465484\n",
      "test loss is 0.0008217498454726391\n",
      "Batch: 12600,train loss is: 0.0004193444995074812\n",
      "test loss is 0.0008088188727010526\n",
      "Batch: 12700,train loss is: 0.0005527360935938105\n",
      "test loss is 0.0007893427770506903\n",
      "Batch: 12800,train loss is: 0.0004671388157397572\n",
      "test loss is 0.0007954040218122197\n",
      "Batch: 12900,train loss is: 0.0007474117065648207\n",
      "test loss is 0.0008103223115668945\n",
      "Batch: 13000,train loss is: 0.00028585655818038496\n",
      "test loss is 0.0007840183097302764\n",
      "Batch: 13100,train loss is: 0.0024097007256340124\n",
      "test loss is 0.0007917957486672275\n",
      "Batch: 13200,train loss is: 0.0008148500002551354\n",
      "test loss is 0.0008767794459709952\n",
      "Batch: 13300,train loss is: 0.000555465109165398\n",
      "test loss is 0.0008450775217401623\n",
      "Batch: 13400,train loss is: 0.0003844606420466033\n",
      "test loss is 0.0008198733475107648\n",
      "Batch: 13500,train loss is: 0.0006762691777363651\n",
      "test loss is 0.000826499609869381\n",
      "Batch: 13600,train loss is: 0.0005792371039743494\n",
      "test loss is 0.0008013603545340028\n",
      "Batch: 13700,train loss is: 0.0007144942999536155\n",
      "test loss is 0.0007979560586133534\n",
      "Batch: 13800,train loss is: 0.0007155173295735779\n",
      "test loss is 0.0008046786611286106\n",
      "Batch: 13900,train loss is: 0.0007277999380182033\n",
      "test loss is 0.000818709482180211\n",
      "Batch: 14000,train loss is: 0.00044346255927326864\n",
      "test loss is 0.0008001725129092221\n",
      "Batch: 14100,train loss is: 0.0008651522523122369\n",
      "test loss is 0.0008505159721114416\n",
      "Batch: 14200,train loss is: 0.0005418863406649931\n",
      "test loss is 0.0007817201243016209\n",
      "Batch: 14300,train loss is: 0.0004930609201692277\n",
      "test loss is 0.0007901314078268844\n",
      "Batch: 14400,train loss is: 0.0014496320436236311\n",
      "test loss is 0.0007916282301417015\n",
      "Batch: 14500,train loss is: 0.0004587986713574238\n",
      "test loss is 0.0007960500661093562\n",
      "Batch: 14600,train loss is: 0.001837737407449093\n",
      "test loss is 0.0007936174796662979\n",
      "Batch: 14700,train loss is: 0.00045452408106450606\n",
      "test loss is 0.000823367592795869\n",
      "Batch: 14800,train loss is: 0.0009225265552611501\n",
      "test loss is 0.0008003409379601553\n",
      "Batch: 14900,train loss is: 0.00045829410243373005\n",
      "test loss is 0.0008182981474678908\n",
      "Batch: 15000,train loss is: 0.0008586198019509878\n",
      "test loss is 0.0007910471113421623\n",
      "Batch: 15100,train loss is: 0.000708118905959254\n",
      "test loss is 0.0008178265359676371\n",
      "Batch: 15200,train loss is: 0.00130394748266375\n",
      "test loss is 0.000817918350040809\n",
      "Batch: 15300,train loss is: 0.0005140791459087527\n",
      "test loss is 0.000788986229953497\n",
      "Batch: 15400,train loss is: 0.0005206348101854799\n",
      "test loss is 0.0008218633803523097\n",
      "Batch: 15500,train loss is: 0.0008643193329246466\n",
      "test loss is 0.0007838847715498548\n",
      "Batch: 15600,train loss is: 0.0010442832100532084\n",
      "test loss is 0.000829201140084246\n",
      "Batch: 15700,train loss is: 0.0006479164633056369\n",
      "test loss is 0.0007947750677015722\n",
      "Batch: 15800,train loss is: 0.0006903034493076061\n",
      "test loss is 0.0007933866072995108\n",
      "Batch: 15900,train loss is: 0.001238567510623918\n",
      "test loss is 0.0008140641037939234\n",
      "Batch: 16000,train loss is: 0.0004575264164134316\n",
      "test loss is 0.0008205118263976341\n",
      "Batch: 16100,train loss is: 0.0006378179751493845\n",
      "test loss is 0.0008082218012561054\n",
      "Batch: 16200,train loss is: 0.00047450580082345476\n",
      "test loss is 0.0007988325322919777\n",
      "Batch: 16300,train loss is: 0.0012442145987975211\n",
      "test loss is 0.0008017323992883192\n",
      "Batch: 16400,train loss is: 0.00045248435955601394\n",
      "test loss is 0.0007849249713407769\n",
      "Batch: 16500,train loss is: 0.000613105174898302\n",
      "test loss is 0.0008306203824111876\n",
      "Batch: 16600,train loss is: 0.0003376258793511523\n",
      "test loss is 0.0007985362496011232\n",
      "Batch: 16700,train loss is: 0.0011333457687047294\n",
      "test loss is 0.0008093499153543873\n",
      "Batch: 16800,train loss is: 0.0006107942702901904\n",
      "test loss is 0.0008152546529081584\n",
      "Batch: 16900,train loss is: 0.0007844001340224634\n",
      "test loss is 0.0008097943745062268\n",
      "Batch: 17000,train loss is: 0.0007998211362886951\n",
      "test loss is 0.0007859279463209285\n",
      "Batch: 17100,train loss is: 0.0005716501860927873\n",
      "test loss is 0.0007961676040172027\n",
      "Batch: 17200,train loss is: 0.0006174563876676038\n",
      "test loss is 0.0008023187429153585\n",
      "Batch: 17300,train loss is: 0.0008530165670668075\n",
      "test loss is 0.0008059538314743732\n",
      "Batch: 17400,train loss is: 0.0004984363887260876\n",
      "test loss is 0.0007982941371105347\n",
      "Batch: 17500,train loss is: 0.0005278329480027018\n",
      "test loss is 0.0007995777583024147\n",
      "Batch: 17600,train loss is: 0.00046362801035044614\n",
      "test loss is 0.0008188180416793151\n",
      "Batch: 17700,train loss is: 0.0003263461368209983\n",
      "test loss is 0.0008287736224196623\n",
      "Batch: 17800,train loss is: 0.0011864094940393108\n",
      "test loss is 0.0008030688641401447\n",
      "Batch: 17900,train loss is: 0.0004996975385641437\n",
      "test loss is 0.0008366776873047698\n",
      "Batch: 18000,train loss is: 0.0003994826615124678\n",
      "test loss is 0.0007853757143945789\n",
      "Batch: 18100,train loss is: 0.0005046214639695032\n",
      "test loss is 0.0007948334248334909\n",
      "Batch: 18200,train loss is: 0.0005813825592411788\n",
      "test loss is 0.0007974792057885762\n",
      "Batch: 18300,train loss is: 0.0009592262272572497\n",
      "test loss is 0.0007936455708022605\n",
      "Batch: 18400,train loss is: 0.0005586708393628292\n",
      "test loss is 0.000795666571072183\n",
      "Batch: 18500,train loss is: 0.0008066434649233029\n",
      "test loss is 0.000814412682737484\n",
      "Batch: 18600,train loss is: 0.0004703318296555601\n",
      "test loss is 0.000790996588785828\n",
      "Batch: 18700,train loss is: 0.0005762673495695348\n",
      "test loss is 0.0007979262882647951\n",
      "Batch: 18800,train loss is: 0.0007660762263029097\n",
      "test loss is 0.0007967293147709391\n",
      "Batch: 18900,train loss is: 0.0008041128909741658\n",
      "test loss is 0.0008362660069742281\n",
      "Batch: 19000,train loss is: 0.0007346498521480979\n",
      "test loss is 0.0007996376265413444\n",
      "Batch: 19100,train loss is: 0.0003657009542273878\n",
      "test loss is 0.0008141264593577643\n",
      "Batch: 19200,train loss is: 0.0006752875896677519\n",
      "test loss is 0.0008066217054104003\n",
      "Batch: 19300,train loss is: 0.0009012085355005985\n",
      "test loss is 0.0007903910247480844\n",
      "Batch: 19400,train loss is: 0.0003393235788078863\n",
      "test loss is 0.0007841238939633642\n",
      "Batch: 19500,train loss is: 0.000628863864149883\n",
      "test loss is 0.0008385340533561742\n",
      "Batch: 19600,train loss is: 0.0004982730371288343\n",
      "test loss is 0.000805942100788586\n",
      "Batch: 19700,train loss is: 0.000816796372464736\n",
      "test loss is 0.0008565553390135336\n",
      "Batch: 19800,train loss is: 0.0005586189979032682\n",
      "test loss is 0.0008023857789173126\n",
      "Batch: 19900,train loss is: 0.00029233619257907614\n",
      "test loss is 0.000828980581810286\n",
      "Batch: 20000,train loss is: 0.0008519823075551341\n",
      "test loss is 0.0007999113650613044\n",
      "Batch: 20100,train loss is: 0.0006535878604852611\n",
      "test loss is 0.0008031975731838243\n",
      "Batch: 20200,train loss is: 0.0010315542297474356\n",
      "test loss is 0.0008171789469073779\n",
      "Batch: 20300,train loss is: 0.00041200747664161435\n",
      "test loss is 0.0008135282911146124\n",
      "Batch: 20400,train loss is: 0.0005720334416642402\n",
      "test loss is 0.0008427443313006849\n",
      "Batch: 20500,train loss is: 0.0006411924474168918\n",
      "test loss is 0.0008041569633927985\n",
      "Batch: 20600,train loss is: 0.0007152045882400992\n",
      "test loss is 0.0008405041056075487\n",
      "Batch: 20700,train loss is: 0.0006070864096054795\n",
      "test loss is 0.0008114214527246674\n",
      "Batch: 20800,train loss is: 0.00038621093584540443\n",
      "test loss is 0.0008026224815280324\n",
      "Batch: 20900,train loss is: 0.0007546219868069273\n",
      "test loss is 0.0008368927568233886\n",
      "Batch: 21000,train loss is: 0.00038161170123018873\n",
      "test loss is 0.0007890084971590902\n",
      "Batch: 21100,train loss is: 0.0006580107166667191\n",
      "test loss is 0.0008223462208773773\n",
      "Batch: 21200,train loss is: 0.0003327634634871606\n",
      "test loss is 0.0008024023934611771\n",
      "Batch: 21300,train loss is: 0.00041562885316444954\n",
      "test loss is 0.0007983964109578015\n",
      "Batch: 21400,train loss is: 0.0009340632504014037\n",
      "test loss is 0.0008081368568460847\n",
      "Batch: 21500,train loss is: 0.00027394577984822565\n",
      "test loss is 0.0008237190731817555\n",
      "Batch: 21600,train loss is: 0.0008242752704604671\n",
      "test loss is 0.0008121225672993999\n",
      "Batch: 21700,train loss is: 0.0003492746492681418\n",
      "test loss is 0.0007920467299676375\n",
      "Batch: 21800,train loss is: 0.0006676181689583884\n",
      "test loss is 0.000849427186704411\n",
      "Batch: 21900,train loss is: 0.0005980401027032074\n",
      "test loss is 0.0008803542862579603\n",
      "Batch: 22000,train loss is: 0.0007713378582932614\n",
      "test loss is 0.0007959271542029574\n",
      "Batch: 22100,train loss is: 0.00034578272206670503\n",
      "test loss is 0.0007968057387046619\n",
      "Batch: 22200,train loss is: 0.0005823592493063338\n",
      "test loss is 0.0008006388792728223\n",
      "Batch: 22300,train loss is: 0.0008388751127217256\n",
      "test loss is 0.0007985224611083308\n",
      "Batch: 22400,train loss is: 0.0005927143366616032\n",
      "test loss is 0.0007874964643892477\n",
      "Batch: 22500,train loss is: 0.0004410641054577218\n",
      "test loss is 0.0007883171091054742\n",
      "Batch: 22600,train loss is: 0.0011436685310187182\n",
      "test loss is 0.0008558312168751196\n",
      "Batch: 22700,train loss is: 0.0004450093803531266\n",
      "test loss is 0.0007953202030804729\n",
      "Batch: 22800,train loss is: 0.0005518437346806392\n",
      "test loss is 0.000809593837373868\n",
      "Batch: 22900,train loss is: 0.0003912865477736491\n",
      "test loss is 0.0008576267425167584\n",
      "Batch: 23000,train loss is: 0.00043553100632031056\n",
      "test loss is 0.0007948034135582476\n",
      "Batch: 23100,train loss is: 0.0006390653667984044\n",
      "test loss is 0.0007789075198232242\n",
      "Batch: 23200,train loss is: 0.00038443336999307074\n",
      "test loss is 0.0007759392677014297\n",
      "Batch: 23300,train loss is: 0.0005861969016354401\n",
      "test loss is 0.0007872586304484918\n",
      "Batch: 23400,train loss is: 0.0007778392933290689\n",
      "test loss is 0.0008061427995237541\n",
      "Batch: 23500,train loss is: 0.0028732182312731767\n",
      "test loss is 0.0007922918288449164\n",
      "Batch: 23600,train loss is: 0.0003415511832185638\n",
      "test loss is 0.0007816247839924823\n",
      "Batch: 23700,train loss is: 0.000892958548764387\n",
      "test loss is 0.0008162279112070651\n",
      "Batch: 23800,train loss is: 0.0009923687849976376\n",
      "test loss is 0.000784574214638778\n",
      "Batch: 23900,train loss is: 0.0009254291553409731\n",
      "test loss is 0.0007904625485036849\n",
      "Batch: 24000,train loss is: 0.0007329009871898927\n",
      "test loss is 0.0008342726320429366\n",
      "Batch: 24100,train loss is: 0.0005174390537961211\n",
      "test loss is 0.0007816711941356392\n",
      "Batch: 24200,train loss is: 0.0018005258607264798\n",
      "test loss is 0.0008073917577855547\n",
      "Batch: 24300,train loss is: 0.0004998247361700839\n",
      "test loss is 0.0009293921377499654\n",
      "Batch: 24400,train loss is: 0.0013763522080222092\n",
      "test loss is 0.0007938291353152638\n",
      "Batch: 24500,train loss is: 0.0006360048965360295\n",
      "test loss is 0.0007914689859530538\n",
      "Batch: 24600,train loss is: 0.0005822280368419128\n",
      "test loss is 0.0008247262541998434\n",
      "Batch: 24700,train loss is: 0.0003766965914068563\n",
      "test loss is 0.0007886842043638649\n",
      "Batch: 24800,train loss is: 0.0010801858723652987\n",
      "test loss is 0.000792073298346596\n",
      "Batch: 24900,train loss is: 0.0001827081387690315\n",
      "test loss is 0.0008034949675731202\n",
      "Batch: 25000,train loss is: 0.0008320444745440965\n",
      "test loss is 0.0007883555058236085\n",
      "Batch: 25100,train loss is: 0.0006765211279816496\n",
      "test loss is 0.0008023730551591252\n",
      "Batch: 25200,train loss is: 0.0008115578880608605\n",
      "test loss is 0.0008076030860908955\n",
      "Batch: 25300,train loss is: 0.0008947422969557497\n",
      "test loss is 0.0008150444961570186\n",
      "Batch: 25400,train loss is: 0.00043630820453909323\n",
      "test loss is 0.0007833574914381614\n",
      "Batch: 25500,train loss is: 0.0005681500281670668\n",
      "test loss is 0.000786384088724658\n",
      "Batch: 25600,train loss is: 0.0007412860275551817\n",
      "test loss is 0.0007978376239938993\n",
      "Batch: 25700,train loss is: 0.0006765089624001647\n",
      "test loss is 0.0008128119915998416\n",
      "Batch: 25800,train loss is: 0.0003197165277949522\n",
      "test loss is 0.0007881209683117487\n",
      "Batch: 25900,train loss is: 0.00040254775350972164\n",
      "test loss is 0.0008311548779642674\n",
      "Batch: 26000,train loss is: 0.0018034940509262859\n",
      "test loss is 0.0008520953308709285\n",
      "Batch: 26100,train loss is: 0.0009733225335827294\n",
      "test loss is 0.0007912002913411542\n",
      "Batch: 26200,train loss is: 0.0008603853858865199\n",
      "test loss is 0.000776003761428254\n",
      "Batch: 26300,train loss is: 0.001297877225072265\n",
      "test loss is 0.0008141690715300277\n",
      "Batch: 26400,train loss is: 0.0007173100167033947\n",
      "test loss is 0.0007952571538554132\n",
      "Batch: 26500,train loss is: 0.00040012107300143094\n",
      "test loss is 0.0008022521485410301\n",
      "Batch: 26600,train loss is: 0.0011751664585564665\n",
      "test loss is 0.0008554120764509664\n",
      "Batch: 26700,train loss is: 0.00044766566574524873\n",
      "test loss is 0.0009065715874697507\n",
      "Batch: 26800,train loss is: 0.0013028462616159664\n",
      "test loss is 0.0007937048811131575\n",
      "Batch: 26900,train loss is: 0.0005712341353260503\n",
      "test loss is 0.0007887606204558696\n",
      "Batch: 27000,train loss is: 0.0007150187131838777\n",
      "test loss is 0.0009194418442823507\n",
      "Batch: 27100,train loss is: 0.00046583488776772537\n",
      "test loss is 0.0008237856252006013\n",
      "Batch: 27200,train loss is: 0.0006712703377011966\n",
      "test loss is 0.0008319751349770744\n",
      "Batch: 27300,train loss is: 0.0006105429037434359\n",
      "test loss is 0.000796735711607571\n",
      "Batch: 27400,train loss is: 0.000882287705080727\n",
      "test loss is 0.0007797571477849719\n",
      "Batch: 27500,train loss is: 0.0005878035362461347\n",
      "test loss is 0.0008768538854616948\n",
      "Batch: 27600,train loss is: 0.0006721277982713685\n",
      "test loss is 0.000847837109245604\n",
      "Batch: 27700,train loss is: 0.0019209563005202087\n",
      "test loss is 0.0008006144355493123\n",
      "Batch: 27800,train loss is: 0.0006949220478502322\n",
      "test loss is 0.000818027067960076\n",
      "Batch: 27900,train loss is: 0.0013085503619679369\n",
      "test loss is 0.0007857948863298668\n",
      "Batch: 28000,train loss is: 0.0004412204423820372\n",
      "test loss is 0.0008030458963753967\n",
      "Batch: 28100,train loss is: 0.0008617281463087677\n",
      "test loss is 0.0008086399894168698\n",
      "Batch: 28200,train loss is: 0.0007393262065002682\n",
      "test loss is 0.000772436490239797\n",
      "Batch: 28300,train loss is: 0.0021408494242735865\n",
      "test loss is 0.000787937118145005\n",
      "Batch: 28400,train loss is: 0.000646432418267177\n",
      "test loss is 0.0007873728177610681\n",
      "Batch: 28500,train loss is: 0.0014166989521301918\n",
      "test loss is 0.0008098930247406393\n",
      "Batch: 28600,train loss is: 0.0004474580282034144\n",
      "test loss is 0.0008011868880751828\n",
      "Batch: 28700,train loss is: 0.00047121126951172053\n",
      "test loss is 0.0007858794268233424\n",
      "Batch: 28800,train loss is: 0.0013382697798108654\n",
      "test loss is 0.0007921451330929237\n",
      "Batch: 28900,train loss is: 0.0005986975333998806\n",
      "test loss is 0.0007901351012387807\n",
      "Batch: 29000,train loss is: 0.000600949454006731\n",
      "test loss is 0.0007777786796083829\n",
      "Batch: 29100,train loss is: 0.0007256166842480527\n",
      "test loss is 0.0008061267310374318\n",
      "Batch: 29200,train loss is: 0.0005095242632826235\n",
      "test loss is 0.0008049775364373082\n",
      "Batch: 29300,train loss is: 0.0010398978337538862\n",
      "test loss is 0.0007888627206842117\n",
      "Batch: 29400,train loss is: 0.0006840488865730108\n",
      "test loss is 0.0008414140177317699\n",
      "Batch: 29500,train loss is: 0.0004817613297877821\n",
      "test loss is 0.0007992129606657656\n",
      "Batch: 29600,train loss is: 0.0005903026915541263\n",
      "test loss is 0.000793751737985465\n",
      "Batch: 29700,train loss is: 0.0003575871404938389\n",
      "test loss is 0.0008082133524607895\n",
      "Batch: 29800,train loss is: 0.000952378033034019\n",
      "test loss is 0.0007849125181153843\n",
      "Batch: 29900,train loss is: 0.003421819696377638\n",
      "test loss is 0.0007963117084252352\n",
      "Batch: 30000,train loss is: 0.00035177948833137223\n",
      "test loss is 0.0008334193958743027\n",
      "Batch: 30100,train loss is: 0.0005168184211983483\n",
      "test loss is 0.0008249653606252322\n",
      "Batch: 30200,train loss is: 0.0003964987598194322\n",
      "test loss is 0.0007859905013436826\n",
      "Batch: 30300,train loss is: 0.0003901508091308696\n",
      "test loss is 0.0007809074793672107\n",
      "Batch: 30400,train loss is: 0.00042907460329842206\n",
      "test loss is 0.0007886059562707909\n",
      "Batch: 30500,train loss is: 0.0004588609992746815\n",
      "test loss is 0.0008354871844356674\n",
      "Batch: 30600,train loss is: 0.00030448525746154775\n",
      "test loss is 0.0007876936810553945\n",
      "Batch: 30700,train loss is: 0.0005619176561889244\n",
      "test loss is 0.0009324364617572467\n",
      "Batch: 30800,train loss is: 0.0003087147286795063\n",
      "test loss is 0.0008512374095207238\n",
      "Batch: 30900,train loss is: 0.00040589765167883016\n",
      "test loss is 0.0007902163114514855\n",
      "Batch: 31000,train loss is: 0.0005962861307666822\n",
      "test loss is 0.0007998059004357868\n",
      "Batch: 31100,train loss is: 0.0004383229814981447\n",
      "test loss is 0.0008605387370256679\n",
      "Batch: 31200,train loss is: 0.000662712857446369\n",
      "test loss is 0.0007993249104664795\n",
      "Batch: 31300,train loss is: 0.0006894229396123587\n",
      "test loss is 0.0008095399289162487\n",
      "Batch: 31400,train loss is: 0.0008427071036831758\n",
      "test loss is 0.0007869002490514094\n",
      "Batch: 31500,train loss is: 0.0006526520799609754\n",
      "test loss is 0.0008010221914036454\n",
      "Batch: 31600,train loss is: 0.0005534315712369917\n",
      "test loss is 0.0007959553349430681\n",
      "Batch: 31700,train loss is: 0.0005665173298006022\n",
      "test loss is 0.0008309633614263287\n",
      "Batch: 31800,train loss is: 0.0011570976429918446\n",
      "test loss is 0.0007917797138709248\n",
      "Batch: 31900,train loss is: 0.0010284262680200266\n",
      "test loss is 0.0008233364015870872\n",
      "Batch: 32000,train loss is: 0.004643999511618323\n",
      "test loss is 0.0007873544092959505\n",
      "Batch: 32100,train loss is: 0.0004513080543564411\n",
      "test loss is 0.0007822655553895656\n",
      "Batch: 32200,train loss is: 0.0005053294782382449\n",
      "test loss is 0.0007990290522909088\n",
      "Batch: 32300,train loss is: 0.0004180416632102787\n",
      "test loss is 0.0007912325741982084\n",
      "Batch: 32400,train loss is: 0.0005084059776576133\n",
      "test loss is 0.0007769184540163931\n",
      "Batch: 32500,train loss is: 0.0011309834385939647\n",
      "test loss is 0.0008204399963398113\n",
      "Batch: 32600,train loss is: 0.0004625652335532044\n",
      "test loss is 0.0007969627735309231\n",
      "Batch: 32700,train loss is: 0.00048303367379094845\n",
      "test loss is 0.0007906017421344056\n",
      "Batch: 32800,train loss is: 0.00043138689740282004\n",
      "test loss is 0.0008010551290535745\n",
      "Batch: 32900,train loss is: 0.0006832996924782078\n",
      "test loss is 0.0007869869118071343\n",
      "Batch: 33000,train loss is: 0.0005969050193661496\n",
      "test loss is 0.0007740873958891625\n",
      "Batch: 33100,train loss is: 0.0005156096410979212\n",
      "test loss is 0.0008169217476255482\n",
      "Batch: 33200,train loss is: 0.0006192371705115706\n",
      "test loss is 0.000890052148408247\n",
      "Batch: 33300,train loss is: 0.00037231307758633025\n",
      "test loss is 0.000773152540250538\n",
      "Batch: 33400,train loss is: 0.0005604082431074891\n",
      "test loss is 0.0008007168926241164\n",
      "Batch: 33500,train loss is: 0.0005396970390689482\n",
      "test loss is 0.0007859506355630762\n",
      "Batch: 33600,train loss is: 0.00061367210466805\n",
      "test loss is 0.0007914582113662639\n",
      "Batch: 33700,train loss is: 0.0006979018355857226\n",
      "test loss is 0.0008010181333903381\n",
      "Batch: 33800,train loss is: 0.0008157870497693092\n",
      "test loss is 0.000781598990659111\n",
      "Batch: 33900,train loss is: 0.0007005113463196794\n",
      "test loss is 0.0007875841540261494\n",
      "-----------------------Epoch: 4----------------------------------\n",
      "Batch: 0,train loss is: 0.00044489180736740285\n",
      "test loss is 0.0007909011766965983\n",
      "Batch: 100,train loss is: 0.0015307907214510974\n",
      "test loss is 0.0007906819136766774\n",
      "Batch: 200,train loss is: 0.0007458197996399532\n",
      "test loss is 0.0007911029959156919\n",
      "Batch: 300,train loss is: 0.000359072166373557\n",
      "test loss is 0.0007876995048276779\n",
      "Batch: 400,train loss is: 0.0006013961918607182\n",
      "test loss is 0.0007864188781368232\n",
      "Batch: 500,train loss is: 0.0006771605905981676\n",
      "test loss is 0.000823150461756077\n",
      "Batch: 600,train loss is: 0.00030211200333739167\n",
      "test loss is 0.0008157277310909958\n",
      "Batch: 700,train loss is: 0.0003403735868128467\n",
      "test loss is 0.00078783051293252\n",
      "Batch: 800,train loss is: 0.0004941030834780682\n",
      "test loss is 0.0007819916521034894\n",
      "Batch: 900,train loss is: 0.0003918244621385007\n",
      "test loss is 0.0007799216574021977\n",
      "Batch: 1000,train loss is: 0.0005939062210120215\n",
      "test loss is 0.0008167564780158706\n",
      "Batch: 1100,train loss is: 0.0019407649952070173\n",
      "test loss is 0.0007804430080983641\n",
      "Batch: 1200,train loss is: 0.00039697753616639846\n",
      "test loss is 0.0007795827948975437\n",
      "Batch: 1300,train loss is: 0.0005737283865369143\n",
      "test loss is 0.000784468189291987\n",
      "Batch: 1400,train loss is: 0.0005828521964140178\n",
      "test loss is 0.0007990579538997285\n",
      "Batch: 1500,train loss is: 0.0005459354171756751\n",
      "test loss is 0.0008081899092536699\n",
      "Batch: 1600,train loss is: 0.0005450216201945304\n",
      "test loss is 0.0008528210200525499\n",
      "Batch: 1700,train loss is: 0.0013318965236922828\n",
      "test loss is 0.0007752490946523118\n",
      "Batch: 1800,train loss is: 0.0005136555937433989\n",
      "test loss is 0.0008004622742329957\n",
      "Batch: 1900,train loss is: 0.00044558749901643516\n",
      "test loss is 0.0008317248674965229\n",
      "Batch: 2000,train loss is: 0.0006869546179303097\n",
      "test loss is 0.0008956602207269288\n",
      "Batch: 2100,train loss is: 0.00067680618676279\n",
      "test loss is 0.000909381445794998\n",
      "Batch: 2200,train loss is: 0.0009965937840567462\n",
      "test loss is 0.0007880909854719525\n",
      "Batch: 2300,train loss is: 0.00043049958402720717\n",
      "test loss is 0.0008288879473888353\n",
      "Batch: 2400,train loss is: 0.0006853122978738929\n",
      "test loss is 0.0007885063682475222\n",
      "Batch: 2500,train loss is: 0.0007795951238815591\n",
      "test loss is 0.0007823471949566683\n",
      "Batch: 2600,train loss is: 0.0003934291355658954\n",
      "test loss is 0.0008009495131515496\n",
      "Batch: 2700,train loss is: 0.0007376588521669003\n",
      "test loss is 0.0007740276353781687\n",
      "Batch: 2800,train loss is: 0.0005118766501989568\n",
      "test loss is 0.0008225532335670782\n",
      "Batch: 2900,train loss is: 0.0005461462637945863\n",
      "test loss is 0.0007726092179584481\n",
      "Batch: 3000,train loss is: 0.0005100300038826341\n",
      "test loss is 0.0007736419269884327\n",
      "Batch: 3100,train loss is: 0.0006546507344967429\n",
      "test loss is 0.000821227588911212\n",
      "Batch: 3200,train loss is: 0.0009696958197578688\n",
      "test loss is 0.0007929455605363908\n",
      "Batch: 3300,train loss is: 0.0004403599717813457\n",
      "test loss is 0.0007844772423659116\n",
      "Batch: 3400,train loss is: 0.0005078990045836213\n",
      "test loss is 0.0007904357317134199\n",
      "Batch: 3500,train loss is: 0.0003050430193637458\n",
      "test loss is 0.0007936024653327901\n",
      "Batch: 3600,train loss is: 0.0005951265156629717\n",
      "test loss is 0.0008209545894882601\n",
      "Batch: 3700,train loss is: 0.0004729582521885281\n",
      "test loss is 0.0007713222838836923\n",
      "Batch: 3800,train loss is: 0.00043380767974511564\n",
      "test loss is 0.0008143873647759787\n",
      "Batch: 3900,train loss is: 0.0006450577724970825\n",
      "test loss is 0.0007811663768798954\n",
      "Batch: 4000,train loss is: 0.00042978206946942747\n",
      "test loss is 0.0007868173287159715\n",
      "Batch: 4100,train loss is: 0.0007497690346086831\n",
      "test loss is 0.0007998508881105716\n",
      "Batch: 4200,train loss is: 0.00043528978429046623\n",
      "test loss is 0.0008201235335677093\n",
      "Batch: 4300,train loss is: 0.0004482679404129229\n",
      "test loss is 0.000781350439048821\n",
      "Batch: 4400,train loss is: 0.0005825567728782403\n",
      "test loss is 0.0007806253315569303\n",
      "Batch: 4500,train loss is: 0.0006219180907447091\n",
      "test loss is 0.0008124515482186265\n",
      "Batch: 4600,train loss is: 0.0006331151558587333\n",
      "test loss is 0.000780893865490192\n",
      "Batch: 4700,train loss is: 0.0008184061192797739\n",
      "test loss is 0.0008155194923754355\n",
      "Batch: 4800,train loss is: 0.0013075970005872448\n",
      "test loss is 0.0007931265416084949\n",
      "Batch: 4900,train loss is: 0.0005936292448075629\n",
      "test loss is 0.0007863671087375931\n",
      "Batch: 5000,train loss is: 0.0006092426496604686\n",
      "test loss is 0.0007912062489055506\n",
      "Batch: 5100,train loss is: 0.000673419319137216\n",
      "test loss is 0.0008412388725711146\n",
      "Batch: 5200,train loss is: 0.0003974175865324216\n",
      "test loss is 0.0007873777482586178\n",
      "Batch: 5300,train loss is: 0.0007620897115223706\n",
      "test loss is 0.000815776259516952\n",
      "Batch: 5400,train loss is: 0.0008598831935818419\n",
      "test loss is 0.0007800418771201166\n",
      "Batch: 5500,train loss is: 0.0005545496736515037\n",
      "test loss is 0.00080279886551582\n",
      "Batch: 5600,train loss is: 0.0006980384662811993\n",
      "test loss is 0.0007984986270442022\n",
      "Batch: 5700,train loss is: 0.0008449139374605916\n",
      "test loss is 0.0007812962910469455\n",
      "Batch: 5800,train loss is: 0.0005869878581026115\n",
      "test loss is 0.000785812618735068\n",
      "Batch: 5900,train loss is: 0.003574351449332308\n",
      "test loss is 0.0007892379948253791\n",
      "Batch: 6000,train loss is: 0.001664893088937314\n",
      "test loss is 0.0008025999399516757\n",
      "Batch: 6100,train loss is: 0.0005094714241647633\n",
      "test loss is 0.0007729594626857371\n",
      "Batch: 6200,train loss is: 0.000623869467130038\n",
      "test loss is 0.000788600641005942\n",
      "Batch: 6300,train loss is: 0.001476635770718524\n",
      "test loss is 0.0008027971731139969\n",
      "Batch: 6400,train loss is: 0.00047179017467814275\n",
      "test loss is 0.0008420280588387092\n",
      "Batch: 6500,train loss is: 0.00040785523572760843\n",
      "test loss is 0.000813671432571273\n",
      "Batch: 6600,train loss is: 0.002292572779843241\n",
      "test loss is 0.0007858516156614329\n",
      "Batch: 6700,train loss is: 0.0005150699103715092\n",
      "test loss is 0.0008060578841150627\n",
      "Batch: 6800,train loss is: 0.001079351845791147\n",
      "test loss is 0.0007785138258269597\n",
      "Batch: 6900,train loss is: 0.0006134556321520699\n",
      "test loss is 0.000785561772467815\n",
      "Batch: 7000,train loss is: 0.0006121117961886035\n",
      "test loss is 0.0008204685667692675\n",
      "Batch: 7100,train loss is: 0.0009878883244558018\n",
      "test loss is 0.000788297388342111\n",
      "Batch: 7200,train loss is: 0.0004392761494407183\n",
      "test loss is 0.0008064882148481972\n",
      "Batch: 7300,train loss is: 0.0008598421963431766\n",
      "test loss is 0.0008397437676188042\n",
      "Batch: 7400,train loss is: 0.00042636582511422597\n",
      "test loss is 0.0007784645199945587\n",
      "Batch: 7500,train loss is: 0.0008242975941504482\n",
      "test loss is 0.0008027050812545622\n",
      "Batch: 7600,train loss is: 0.0005533331653684377\n",
      "test loss is 0.0007798421889258355\n",
      "Batch: 7700,train loss is: 0.0007048506603692149\n",
      "test loss is 0.0007815439842594431\n",
      "Batch: 7800,train loss is: 0.00031420802215814057\n",
      "test loss is 0.0007843873538182733\n",
      "Batch: 7900,train loss is: 0.000638274252052019\n",
      "test loss is 0.0007883465766241825\n",
      "Batch: 8000,train loss is: 0.0004937692408009754\n",
      "test loss is 0.0008180736584131746\n",
      "Batch: 8100,train loss is: 0.0006438302916241235\n",
      "test loss is 0.0007961131707206709\n",
      "Batch: 8200,train loss is: 0.001305598654941817\n",
      "test loss is 0.0008029933227273532\n",
      "Batch: 8300,train loss is: 0.000629757922444556\n",
      "test loss is 0.0008038507632442622\n",
      "Batch: 8400,train loss is: 0.0005639545047031417\n",
      "test loss is 0.0007853900808255196\n",
      "Batch: 8500,train loss is: 0.0010450923939024303\n",
      "test loss is 0.0008048523497606314\n",
      "Batch: 8600,train loss is: 0.0007398395713608053\n",
      "test loss is 0.0008008083965109824\n",
      "Batch: 8700,train loss is: 0.0005017667290261179\n",
      "test loss is 0.0008089283077606053\n",
      "Batch: 8800,train loss is: 0.00033639258042702066\n",
      "test loss is 0.0007780602618668236\n",
      "Batch: 8900,train loss is: 0.0011070084869996004\n",
      "test loss is 0.000777469002602481\n",
      "Batch: 9000,train loss is: 0.00029945302968808894\n",
      "test loss is 0.0007883860852014392\n",
      "Batch: 9100,train loss is: 0.0004523468824702015\n",
      "test loss is 0.0007795906550704716\n",
      "Batch: 9200,train loss is: 0.0009902497601935027\n",
      "test loss is 0.0008398576513097536\n",
      "Batch: 9300,train loss is: 0.0006065805505347\n",
      "test loss is 0.0007849062191610376\n",
      "Batch: 9400,train loss is: 0.000508927138117426\n",
      "test loss is 0.0008255765566109238\n",
      "Batch: 9500,train loss is: 0.0004675065801652726\n",
      "test loss is 0.0008006833953113694\n",
      "Batch: 9600,train loss is: 0.0005095548163825751\n",
      "test loss is 0.0007809418458055463\n",
      "Batch: 9700,train loss is: 0.00046120846876756806\n",
      "test loss is 0.0008030435211327584\n",
      "Batch: 9800,train loss is: 0.0005580033426797212\n",
      "test loss is 0.0007857669913544221\n",
      "Batch: 9900,train loss is: 0.0005564116632727199\n",
      "test loss is 0.000822976340548465\n",
      "Batch: 10000,train loss is: 0.000582442551348326\n",
      "test loss is 0.0007855054293712504\n",
      "Batch: 10100,train loss is: 0.001082988563199517\n",
      "test loss is 0.0007797451077228124\n",
      "Batch: 10200,train loss is: 0.0016549711702548342\n",
      "test loss is 0.0008907001607540039\n",
      "Batch: 10300,train loss is: 0.0005352515070782615\n",
      "test loss is 0.0007911129161705255\n",
      "Batch: 10400,train loss is: 0.0004297360652567715\n",
      "test loss is 0.0007826532785037397\n",
      "Batch: 10500,train loss is: 0.0007431602938765832\n",
      "test loss is 0.0007835666260398495\n",
      "Batch: 10600,train loss is: 0.0003753218566034559\n",
      "test loss is 0.0008298055830995897\n",
      "Batch: 10700,train loss is: 0.0006314622164748353\n",
      "test loss is 0.0007826424510591881\n",
      "Batch: 10800,train loss is: 0.0005287216466332858\n",
      "test loss is 0.0007943486943405291\n",
      "Batch: 10900,train loss is: 0.000699593230320004\n",
      "test loss is 0.0007852204215941531\n",
      "Batch: 11000,train loss is: 0.0007222151806052254\n",
      "test loss is 0.000814323725975607\n",
      "Batch: 11100,train loss is: 0.001049749784393141\n",
      "test loss is 0.000778983830659764\n",
      "Batch: 11200,train loss is: 0.0006977984127041284\n",
      "test loss is 0.0008105749603986307\n",
      "Batch: 11300,train loss is: 0.00035480728910229663\n",
      "test loss is 0.0008344435987402282\n",
      "Batch: 11400,train loss is: 0.0006136128993868537\n",
      "test loss is 0.0007997661824865182\n",
      "Batch: 11500,train loss is: 0.0005961023190683989\n",
      "test loss is 0.0007911606797272952\n",
      "Batch: 11600,train loss is: 0.001973697250683518\n",
      "test loss is 0.0008015069182042081\n",
      "Batch: 11700,train loss is: 0.0003516993377134443\n",
      "test loss is 0.000766606984596334\n",
      "Batch: 11800,train loss is: 0.0006371052877811829\n",
      "test loss is 0.0007846072657453764\n",
      "Batch: 11900,train loss is: 0.0007129390938762914\n",
      "test loss is 0.0007957000141094954\n",
      "Batch: 12000,train loss is: 0.0004838055483863945\n",
      "test loss is 0.0007929961223191615\n",
      "Batch: 12100,train loss is: 0.001778405502192764\n",
      "test loss is 0.0007883726443306882\n",
      "Batch: 12200,train loss is: 0.0007825674091291936\n",
      "test loss is 0.0008080904380202427\n",
      "Batch: 12300,train loss is: 0.0004812095443735743\n",
      "test loss is 0.0008080167994933067\n",
      "Batch: 12400,train loss is: 0.00069690678067732\n",
      "test loss is 0.0007777862928354717\n",
      "Batch: 12500,train loss is: 0.00036888849716202265\n",
      "test loss is 0.0008069906169741683\n",
      "Batch: 12600,train loss is: 0.00041839012786222275\n",
      "test loss is 0.0007947326651504554\n",
      "Batch: 12700,train loss is: 0.000535402520360179\n",
      "test loss is 0.0007746528304754204\n",
      "Batch: 12800,train loss is: 0.0004619149003431534\n",
      "test loss is 0.0007806590979542988\n",
      "Batch: 12900,train loss is: 0.0007220859406489637\n",
      "test loss is 0.000797403734764782\n",
      "Batch: 13000,train loss is: 0.0002741906447421257\n",
      "test loss is 0.0007678250938863543\n",
      "Batch: 13100,train loss is: 0.0023654667198250144\n",
      "test loss is 0.0007758481139477853\n",
      "Batch: 13200,train loss is: 0.0008006543668552818\n",
      "test loss is 0.0008662798906362594\n",
      "Batch: 13300,train loss is: 0.0005670727301839685\n",
      "test loss is 0.0008286722816633023\n",
      "Batch: 13400,train loss is: 0.00038622388936776275\n",
      "test loss is 0.0008048602358962535\n",
      "Batch: 13500,train loss is: 0.0006609423134520946\n",
      "test loss is 0.0008123400653316957\n",
      "Batch: 13600,train loss is: 0.0005614777918500748\n",
      "test loss is 0.0007877661874621161\n",
      "Batch: 13700,train loss is: 0.0007150118790977474\n",
      "test loss is 0.00078160524379851\n",
      "Batch: 13800,train loss is: 0.0006703652492378561\n",
      "test loss is 0.0007875344170059857\n",
      "Batch: 13900,train loss is: 0.0006919757299400976\n",
      "test loss is 0.000809174051956886\n",
      "Batch: 14000,train loss is: 0.00044104145661452173\n",
      "test loss is 0.0007871686407177695\n",
      "Batch: 14100,train loss is: 0.0008461105667262981\n",
      "test loss is 0.0008325250849720103\n",
      "Batch: 14200,train loss is: 0.0005485476536339697\n",
      "test loss is 0.00076593783611475\n",
      "Batch: 14300,train loss is: 0.000460193294091375\n",
      "test loss is 0.00077449536086377\n",
      "Batch: 14400,train loss is: 0.0014346819524887163\n",
      "test loss is 0.0007778492863035387\n",
      "Batch: 14500,train loss is: 0.00045008261876108374\n",
      "test loss is 0.0007806669680022339\n",
      "Batch: 14600,train loss is: 0.0017541977371352856\n",
      "test loss is 0.000778085215221318\n",
      "Batch: 14700,train loss is: 0.0004614975083426958\n",
      "test loss is 0.0008125662061685925\n",
      "Batch: 14800,train loss is: 0.0009024850064691495\n",
      "test loss is 0.0007849972306808498\n",
      "Batch: 14900,train loss is: 0.0004484787953984275\n",
      "test loss is 0.000804534687820401\n",
      "Batch: 15000,train loss is: 0.0008508997159725536\n",
      "test loss is 0.000774933823891024\n",
      "Batch: 15100,train loss is: 0.000701013580324591\n",
      "test loss is 0.0008026480268635155\n",
      "Batch: 15200,train loss is: 0.0012598691422279022\n",
      "test loss is 0.000796837971249251\n",
      "Batch: 15300,train loss is: 0.0005071596647560129\n",
      "test loss is 0.0007735971580187375\n",
      "Batch: 15400,train loss is: 0.0005158162097325672\n",
      "test loss is 0.0008052108948052673\n",
      "Batch: 15500,train loss is: 0.0008590138618768634\n",
      "test loss is 0.0007679362025964624\n",
      "Batch: 15600,train loss is: 0.0010139703598744238\n",
      "test loss is 0.0008161862216140807\n",
      "Batch: 15700,train loss is: 0.0006282422823663043\n",
      "test loss is 0.0007789873741090998\n",
      "Batch: 15800,train loss is: 0.0006905849597194951\n",
      "test loss is 0.000776558315257235\n",
      "Batch: 15900,train loss is: 0.0011803758186175175\n",
      "test loss is 0.000797130510975606\n",
      "Batch: 16000,train loss is: 0.00043527941667375036\n",
      "test loss is 0.0008040472173886338\n",
      "Batch: 16100,train loss is: 0.0006191519423251954\n",
      "test loss is 0.0007902216911646557\n",
      "Batch: 16200,train loss is: 0.0004565255676327611\n",
      "test loss is 0.0007836890973421568\n",
      "Batch: 16300,train loss is: 0.0012062678760552147\n",
      "test loss is 0.0007906526448727681\n",
      "Batch: 16400,train loss is: 0.00044916319551679035\n",
      "test loss is 0.0007696489434210753\n",
      "Batch: 16500,train loss is: 0.0006096866695291306\n",
      "test loss is 0.0008150054010400605\n",
      "Batch: 16600,train loss is: 0.00033069640940971534\n",
      "test loss is 0.0007799196780959999\n",
      "Batch: 16700,train loss is: 0.0010845542984469472\n",
      "test loss is 0.000793680177079067\n",
      "Batch: 16800,train loss is: 0.0006065258438374322\n",
      "test loss is 0.0008007052537458727\n",
      "Batch: 16900,train loss is: 0.0007781525932125911\n",
      "test loss is 0.00079381245403431\n",
      "Batch: 17000,train loss is: 0.0008085757591799943\n",
      "test loss is 0.0007689526293131469\n",
      "Batch: 17100,train loss is: 0.0005837577250409813\n",
      "test loss is 0.0007796081838609027\n",
      "Batch: 17200,train loss is: 0.0005888754777049379\n",
      "test loss is 0.0007852462295190776\n",
      "Batch: 17300,train loss is: 0.0008658667252590888\n",
      "test loss is 0.0007924808063099657\n",
      "Batch: 17400,train loss is: 0.00048536555200027234\n",
      "test loss is 0.0007821269182255157\n",
      "Batch: 17500,train loss is: 0.0005259870882081747\n",
      "test loss is 0.0007813871953751315\n",
      "Batch: 17600,train loss is: 0.0004455716861209278\n",
      "test loss is 0.0008027628652422767\n",
      "Batch: 17700,train loss is: 0.0003352530032647723\n",
      "test loss is 0.0008137923948226624\n",
      "Batch: 17800,train loss is: 0.0011830818424706816\n",
      "test loss is 0.0007872395486868254\n",
      "Batch: 17900,train loss is: 0.0004905152805125306\n",
      "test loss is 0.0008208024840014211\n",
      "Batch: 18000,train loss is: 0.00039219259546151487\n",
      "test loss is 0.0007690669596895695\n",
      "Batch: 18100,train loss is: 0.0005006454237972635\n",
      "test loss is 0.0007795948696363316\n",
      "Batch: 18200,train loss is: 0.0005874875104568172\n",
      "test loss is 0.0007830315598811308\n",
      "Batch: 18300,train loss is: 0.000980649303490938\n",
      "test loss is 0.0007789503643232475\n",
      "Batch: 18400,train loss is: 0.0005445102462037704\n",
      "test loss is 0.0007793707819013337\n",
      "Batch: 18500,train loss is: 0.0008267798698959607\n",
      "test loss is 0.0007956686637875854\n",
      "Batch: 18600,train loss is: 0.0004905330118169011\n",
      "test loss is 0.0007757848230053966\n",
      "Batch: 18700,train loss is: 0.0005697188100421846\n",
      "test loss is 0.0007834020946734135\n",
      "Batch: 18800,train loss is: 0.0007334597378261677\n",
      "test loss is 0.0007800010646566828\n",
      "Batch: 18900,train loss is: 0.000778390120858828\n",
      "test loss is 0.0008204309091501067\n",
      "Batch: 19000,train loss is: 0.0007162910260989197\n",
      "test loss is 0.0007833892238628393\n",
      "Batch: 19100,train loss is: 0.00034284356240539966\n",
      "test loss is 0.0008002352603569931\n",
      "Batch: 19200,train loss is: 0.0006474050654321676\n",
      "test loss is 0.0007888878816542143\n",
      "Batch: 19300,train loss is: 0.0008969263284211821\n",
      "test loss is 0.0007741391213329507\n",
      "Batch: 19400,train loss is: 0.00032809409635952834\n",
      "test loss is 0.0007688835005824177\n",
      "Batch: 19500,train loss is: 0.0006147320708263588\n",
      "test loss is 0.0008214342519640083\n",
      "Batch: 19600,train loss is: 0.0004961946870188643\n",
      "test loss is 0.0007924040699982278\n",
      "Batch: 19700,train loss is: 0.0008223797971277637\n",
      "test loss is 0.000843671026311743\n",
      "Batch: 19800,train loss is: 0.0005640126795645797\n",
      "test loss is 0.0007867642418721642\n",
      "Batch: 19900,train loss is: 0.0002772650444082689\n",
      "test loss is 0.0008167459867167683\n",
      "Batch: 20000,train loss is: 0.000840026225436253\n",
      "test loss is 0.0007855228550145009\n",
      "Batch: 20100,train loss is: 0.0006079632994141596\n",
      "test loss is 0.0007873570932838025\n",
      "Batch: 20200,train loss is: 0.0010170435952961796\n",
      "test loss is 0.000800127232134078\n",
      "Batch: 20300,train loss is: 0.0004035215138724337\n",
      "test loss is 0.0007997451394079738\n",
      "Batch: 20400,train loss is: 0.000547826605476809\n",
      "test loss is 0.0008262949004942305\n",
      "Batch: 20500,train loss is: 0.0006322231951653622\n",
      "test loss is 0.0007884052547048709\n",
      "Batch: 20600,train loss is: 0.0006835242563254865\n",
      "test loss is 0.0008230114888781156\n",
      "Batch: 20700,train loss is: 0.0006186768522099155\n",
      "test loss is 0.0007947548822066885\n",
      "Batch: 20800,train loss is: 0.00038162724499685375\n",
      "test loss is 0.0007885724833135052\n",
      "Batch: 20900,train loss is: 0.0007370096594462363\n",
      "test loss is 0.0008245869750841444\n",
      "Batch: 21000,train loss is: 0.0003564331487578287\n",
      "test loss is 0.0007728770232300236\n",
      "Batch: 21100,train loss is: 0.0006611792578518643\n",
      "test loss is 0.0008061986890579118\n",
      "Batch: 21200,train loss is: 0.00033194861854418857\n",
      "test loss is 0.0007895474815525945\n",
      "Batch: 21300,train loss is: 0.00039850847213653245\n",
      "test loss is 0.0007799480458745713\n",
      "Batch: 21400,train loss is: 0.0009243099956351207\n",
      "test loss is 0.0007916228012399654\n",
      "Batch: 21500,train loss is: 0.0002647882630635066\n",
      "test loss is 0.0008095310009635003\n",
      "Batch: 21600,train loss is: 0.0008050893172619744\n",
      "test loss is 0.0007966180007145856\n",
      "Batch: 21700,train loss is: 0.00033899568629791073\n",
      "test loss is 0.0007762925239981864\n",
      "Batch: 21800,train loss is: 0.0006515516393431235\n",
      "test loss is 0.0008348688378464499\n",
      "Batch: 21900,train loss is: 0.0005815674887468795\n",
      "test loss is 0.0008645380071395127\n",
      "Batch: 22000,train loss is: 0.0007513063742848894\n",
      "test loss is 0.0007804168769841332\n",
      "Batch: 22100,train loss is: 0.0003488595023064397\n",
      "test loss is 0.0007821274882904059\n",
      "Batch: 22200,train loss is: 0.0005782825117865081\n",
      "test loss is 0.0007846335265240775\n",
      "Batch: 22300,train loss is: 0.0007942269585468584\n",
      "test loss is 0.0007837226318237739\n",
      "Batch: 22400,train loss is: 0.0005738066415291693\n",
      "test loss is 0.0007737622959721648\n",
      "Batch: 22500,train loss is: 0.00044474200488980035\n",
      "test loss is 0.0007729833470427833\n",
      "Batch: 22600,train loss is: 0.0011163393411559273\n",
      "test loss is 0.0008376379710790756\n",
      "Batch: 22700,train loss is: 0.00044596644895976157\n",
      "test loss is 0.0007794990987799742\n",
      "Batch: 22800,train loss is: 0.000552611462434163\n",
      "test loss is 0.0007958004355119719\n",
      "Batch: 22900,train loss is: 0.000377297961571757\n",
      "test loss is 0.0008425027988724635\n",
      "Batch: 23000,train loss is: 0.00042535288264344687\n",
      "test loss is 0.0007789668347777518\n",
      "Batch: 23100,train loss is: 0.000605202200873515\n",
      "test loss is 0.0007640267704796208\n",
      "Batch: 23200,train loss is: 0.0003778450256699187\n",
      "test loss is 0.0007606378511481705\n",
      "Batch: 23300,train loss is: 0.0005732342118121462\n",
      "test loss is 0.0007710763024135933\n",
      "Batch: 23400,train loss is: 0.000753762314664493\n",
      "test loss is 0.0007936753691190486\n",
      "Batch: 23500,train loss is: 0.0027437199488750657\n",
      "test loss is 0.0007772457404457571\n",
      "Batch: 23600,train loss is: 0.00032562142475893506\n",
      "test loss is 0.0007679683329147378\n",
      "Batch: 23700,train loss is: 0.0008942729015257826\n",
      "test loss is 0.0008006371590177219\n",
      "Batch: 23800,train loss is: 0.0009715515303828744\n",
      "test loss is 0.0007704664694930518\n",
      "Batch: 23900,train loss is: 0.0009262572682865591\n",
      "test loss is 0.0007772234830760755\n",
      "Batch: 24000,train loss is: 0.0007424348209358653\n",
      "test loss is 0.00082095253706616\n",
      "Batch: 24100,train loss is: 0.00048324325598632866\n",
      "test loss is 0.0007663455763513252\n",
      "Batch: 24200,train loss is: 0.001748381828378067\n",
      "test loss is 0.0007932579435202176\n",
      "Batch: 24300,train loss is: 0.0004905169132725259\n",
      "test loss is 0.00091620343109379\n",
      "Batch: 24400,train loss is: 0.0013570939688959897\n",
      "test loss is 0.0007785838708547024\n",
      "Batch: 24500,train loss is: 0.0006457987717395996\n",
      "test loss is 0.0007746759600846054\n",
      "Batch: 24600,train loss is: 0.0005777453719771334\n",
      "test loss is 0.000809913596089904\n",
      "Batch: 24700,train loss is: 0.0003611272419142114\n",
      "test loss is 0.0007750358730236433\n",
      "Batch: 24800,train loss is: 0.001038974072616612\n",
      "test loss is 0.0007767612697166006\n",
      "Batch: 24900,train loss is: 0.00017984395551323137\n",
      "test loss is 0.0007842431634604856\n",
      "Batch: 25000,train loss is: 0.0008273164917819996\n",
      "test loss is 0.000773670509860756\n",
      "Batch: 25100,train loss is: 0.000657248423076254\n",
      "test loss is 0.0007879240844837334\n",
      "Batch: 25200,train loss is: 0.0007922146046681884\n",
      "test loss is 0.0007922257633786662\n",
      "Batch: 25300,train loss is: 0.000888879108629582\n",
      "test loss is 0.0008017204637907705\n",
      "Batch: 25400,train loss is: 0.0004235192517778071\n",
      "test loss is 0.0007682660829386689\n",
      "Batch: 25500,train loss is: 0.0005692753522938375\n",
      "test loss is 0.0007727113085440266\n",
      "Batch: 25600,train loss is: 0.0007390606330096596\n",
      "test loss is 0.0007817036568941481\n",
      "Batch: 25700,train loss is: 0.0006416186126277035\n",
      "test loss is 0.0007966545839990606\n",
      "Batch: 25800,train loss is: 0.0003279856761820478\n",
      "test loss is 0.000773325379256275\n",
      "Batch: 25900,train loss is: 0.00037696074738202006\n",
      "test loss is 0.0008153062886160958\n",
      "Batch: 26000,train loss is: 0.0017947904552052096\n",
      "test loss is 0.0008386059027206374\n",
      "Batch: 26100,train loss is: 0.0009550034769789058\n",
      "test loss is 0.0007744265318765336\n",
      "Batch: 26200,train loss is: 0.0008222300552419455\n",
      "test loss is 0.0007619628181485652\n",
      "Batch: 26300,train loss is: 0.0012569226626528785\n",
      "test loss is 0.0007969729737997303\n",
      "Batch: 26400,train loss is: 0.0006874937513003106\n",
      "test loss is 0.0007801671562553157\n",
      "Batch: 26500,train loss is: 0.00038205074872466364\n",
      "test loss is 0.0007852536152432605\n",
      "Batch: 26600,train loss is: 0.0012178272306617448\n",
      "test loss is 0.0008475607629741598\n",
      "Batch: 26700,train loss is: 0.00045008119569009837\n",
      "test loss is 0.0008930817016405836\n",
      "Batch: 26800,train loss is: 0.001266505696837535\n",
      "test loss is 0.0007794038772194479\n",
      "Batch: 26900,train loss is: 0.0005486787028791135\n",
      "test loss is 0.0007742896012383852\n",
      "Batch: 27000,train loss is: 0.0006866777033653443\n",
      "test loss is 0.0009041135418477962\n",
      "Batch: 27100,train loss is: 0.0004747108024521608\n",
      "test loss is 0.0008154532151505418\n",
      "Batch: 27200,train loss is: 0.0006793302723066668\n",
      "test loss is 0.0008185420472189709\n",
      "Batch: 27300,train loss is: 0.00060579155873731\n",
      "test loss is 0.0007804479595613534\n",
      "Batch: 27400,train loss is: 0.000900008493969364\n",
      "test loss is 0.000766532896747051\n",
      "Batch: 27500,train loss is: 0.0005763444153836438\n",
      "test loss is 0.0008609834042022344\n",
      "Batch: 27600,train loss is: 0.0006598390240777303\n",
      "test loss is 0.0008307977643615093\n",
      "Batch: 27700,train loss is: 0.0018502581017266185\n",
      "test loss is 0.0007864489561654213\n",
      "Batch: 27800,train loss is: 0.0007026673380541974\n",
      "test loss is 0.000804734352907002\n",
      "Batch: 27900,train loss is: 0.001322185467776446\n",
      "test loss is 0.0007720527675151651\n",
      "Batch: 28000,train loss is: 0.0004363662908612706\n",
      "test loss is 0.000784779786250024\n",
      "Batch: 28100,train loss is: 0.0008809758378730138\n",
      "test loss is 0.0007958666030271955\n",
      "Batch: 28200,train loss is: 0.0007266776870904483\n",
      "test loss is 0.0007580174476410544\n",
      "Batch: 28300,train loss is: 0.002148569826774833\n",
      "test loss is 0.0007745992608230703\n",
      "Batch: 28400,train loss is: 0.0006283769127226653\n",
      "test loss is 0.0007710215227258818\n",
      "Batch: 28500,train loss is: 0.0014267990255734714\n",
      "test loss is 0.0007967829166582815\n",
      "Batch: 28600,train loss is: 0.00042413702481690346\n",
      "test loss is 0.0007893466030309861\n",
      "Batch: 28700,train loss is: 0.0004777447018394623\n",
      "test loss is 0.000771537693997763\n",
      "Batch: 28800,train loss is: 0.0012590857312426043\n",
      "test loss is 0.0007774704090467758\n",
      "Batch: 28900,train loss is: 0.0005628735635797975\n",
      "test loss is 0.0007743552795190612\n",
      "Batch: 29000,train loss is: 0.0005910319153246316\n",
      "test loss is 0.0007635335968833346\n",
      "Batch: 29100,train loss is: 0.0007096184427968186\n",
      "test loss is 0.0007884616468175503\n",
      "Batch: 29200,train loss is: 0.0004934216091866408\n",
      "test loss is 0.0007896004871155653\n",
      "Batch: 29300,train loss is: 0.001043416138660111\n",
      "test loss is 0.0007726229368404614\n",
      "Batch: 29400,train loss is: 0.0006800912491978123\n",
      "test loss is 0.0008308372886613151\n",
      "Batch: 29500,train loss is: 0.0004729946401027298\n",
      "test loss is 0.0007860978328888357\n",
      "Batch: 29600,train loss is: 0.0005661260867088378\n",
      "test loss is 0.000777166568789139\n",
      "Batch: 29700,train loss is: 0.00036520350936466994\n",
      "test loss is 0.000792806797734385\n",
      "Batch: 29800,train loss is: 0.0009113606975168119\n",
      "test loss is 0.0007684040119828245\n",
      "Batch: 29900,train loss is: 0.0033700271516702456\n",
      "test loss is 0.0007796470685331064\n",
      "Batch: 30000,train loss is: 0.000343461524684938\n",
      "test loss is 0.0008220848255609239\n",
      "Batch: 30100,train loss is: 0.00051188182633185\n",
      "test loss is 0.0008114943676070253\n",
      "Batch: 30200,train loss is: 0.00038041903991409086\n",
      "test loss is 0.0007727974113943458\n",
      "Batch: 30300,train loss is: 0.00035977788086387406\n",
      "test loss is 0.0007653966759591247\n",
      "Batch: 30400,train loss is: 0.00042078660026523225\n",
      "test loss is 0.0007739246301288966\n",
      "Batch: 30500,train loss is: 0.00044519426162669616\n",
      "test loss is 0.000824965126344996\n",
      "Batch: 30600,train loss is: 0.00028974799992602835\n",
      "test loss is 0.0007722544143417873\n",
      "Batch: 30700,train loss is: 0.0005179615614841177\n",
      "test loss is 0.0009165965873709848\n",
      "Batch: 30800,train loss is: 0.00029685129701254243\n",
      "test loss is 0.0008399242279222658\n",
      "Batch: 30900,train loss is: 0.0003932095128741554\n",
      "test loss is 0.0007750612534612045\n",
      "Batch: 31000,train loss is: 0.0005822028970081987\n",
      "test loss is 0.000785280347263672\n",
      "Batch: 31100,train loss is: 0.00043154803431049213\n",
      "test loss is 0.0008476724446452229\n",
      "Batch: 31200,train loss is: 0.0006478809863231652\n",
      "test loss is 0.0007859798834169412\n",
      "Batch: 31300,train loss is: 0.0006725385087424596\n",
      "test loss is 0.0007946185857963987\n",
      "Batch: 31400,train loss is: 0.0008382494237063786\n",
      "test loss is 0.0007740068477386034\n",
      "Batch: 31500,train loss is: 0.0006217349907403131\n",
      "test loss is 0.000785832263433104\n",
      "Batch: 31600,train loss is: 0.0005419042897648583\n",
      "test loss is 0.000781635904195688\n",
      "Batch: 31700,train loss is: 0.0005634010773571864\n",
      "test loss is 0.0008135150344714275\n",
      "Batch: 31800,train loss is: 0.001181623646808081\n",
      "test loss is 0.0007769097397881404\n",
      "Batch: 31900,train loss is: 0.0009903013403386924\n",
      "test loss is 0.0008073564457671059\n",
      "Batch: 32000,train loss is: 0.004514350934258231\n",
      "test loss is 0.0007730947669566209\n",
      "Batch: 32100,train loss is: 0.0004581953801102666\n",
      "test loss is 0.0007691142283924922\n",
      "Batch: 32200,train loss is: 0.00048239486415457253\n",
      "test loss is 0.0007850739424710141\n",
      "Batch: 32300,train loss is: 0.000408814683628747\n",
      "test loss is 0.0007769529407258399\n",
      "Batch: 32400,train loss is: 0.0004902692199302749\n",
      "test loss is 0.000761822069351571\n",
      "Batch: 32500,train loss is: 0.0011157950175416016\n",
      "test loss is 0.0008050604286689165\n",
      "Batch: 32600,train loss is: 0.00046737716613902636\n",
      "test loss is 0.0007827374852293177\n",
      "Batch: 32700,train loss is: 0.00047633083230832113\n",
      "test loss is 0.0007777960999495854\n",
      "Batch: 32800,train loss is: 0.00042547538533114354\n",
      "test loss is 0.0007903395614638348\n",
      "Batch: 32900,train loss is: 0.0006749794458637745\n",
      "test loss is 0.0007701315508127967\n",
      "Batch: 33000,train loss is: 0.0005790653915727812\n",
      "test loss is 0.0007601903196554994\n",
      "Batch: 33100,train loss is: 0.0004973109879795266\n",
      "test loss is 0.0008014500201165058\n",
      "Batch: 33200,train loss is: 0.0006020068142226935\n",
      "test loss is 0.0008722704818216626\n",
      "Batch: 33300,train loss is: 0.00037874278440026376\n",
      "test loss is 0.0007592680223034693\n",
      "Batch: 33400,train loss is: 0.0005524995531188988\n",
      "test loss is 0.0007859059084440542\n",
      "Batch: 33500,train loss is: 0.0005139401125297393\n",
      "test loss is 0.000772389279081712\n",
      "Batch: 33600,train loss is: 0.0006130405102368107\n",
      "test loss is 0.0007751830895068491\n",
      "Batch: 33700,train loss is: 0.0006821348342397048\n",
      "test loss is 0.0007862549851738623\n",
      "Batch: 33800,train loss is: 0.0008157507557917887\n",
      "test loss is 0.0007661971714179128\n",
      "Batch: 33900,train loss is: 0.0007043134398357496\n",
      "test loss is 0.0007735603580468502\n",
      "-----------------------Epoch: 5----------------------------------\n",
      "Batch: 0,train loss is: 0.0004177027528789618\n",
      "test loss is 0.0007778201602542862\n",
      "Batch: 100,train loss is: 0.0015099501777612475\n",
      "test loss is 0.0007761282049939905\n",
      "Batch: 200,train loss is: 0.0007223423580618354\n",
      "test loss is 0.000778340899653799\n",
      "Batch: 300,train loss is: 0.00034649780017624777\n",
      "test loss is 0.0007734085968223257\n",
      "Batch: 400,train loss is: 0.0005775066800692251\n",
      "test loss is 0.0007706317094393071\n",
      "Batch: 500,train loss is: 0.0006729511341378945\n",
      "test loss is 0.0008072926954673966\n",
      "Batch: 600,train loss is: 0.0002842146641681689\n",
      "test loss is 0.0008021244836112811\n",
      "Batch: 700,train loss is: 0.00033476943877716944\n",
      "test loss is 0.0007716661712478293\n",
      "Batch: 800,train loss is: 0.00048411460745250573\n",
      "test loss is 0.0007689210005611777\n",
      "Batch: 900,train loss is: 0.0003799220653473286\n",
      "test loss is 0.0007656942115178402\n",
      "Batch: 1000,train loss is: 0.0005749744035248793\n",
      "test loss is 0.0008035507515114196\n",
      "Batch: 1100,train loss is: 0.0018998349966316247\n",
      "test loss is 0.0007655184317423237\n",
      "Batch: 1200,train loss is: 0.00039258205481298834\n",
      "test loss is 0.0007684874569293936\n",
      "Batch: 1300,train loss is: 0.0005596101511276222\n",
      "test loss is 0.0007693452335622676\n",
      "Batch: 1400,train loss is: 0.0005600138059759548\n",
      "test loss is 0.0007851651944664361\n",
      "Batch: 1500,train loss is: 0.0005408980634852712\n",
      "test loss is 0.0007950393480481297\n",
      "Batch: 1600,train loss is: 0.0005731702855046796\n",
      "test loss is 0.0008467472793846283\n",
      "Batch: 1700,train loss is: 0.0013305678778163723\n",
      "test loss is 0.0007610095637011602\n",
      "Batch: 1800,train loss is: 0.0005155418370420936\n",
      "test loss is 0.0007852900271609395\n",
      "Batch: 1900,train loss is: 0.00042381528394255954\n",
      "test loss is 0.000814740325734597\n",
      "Batch: 2000,train loss is: 0.0006725389769334726\n",
      "test loss is 0.0008865771509788399\n",
      "Batch: 2100,train loss is: 0.0006812988997916069\n",
      "test loss is 0.0008994288422632126\n",
      "Batch: 2200,train loss is: 0.0009769143269349697\n",
      "test loss is 0.000774241950381745\n",
      "Batch: 2300,train loss is: 0.000434671647858698\n",
      "test loss is 0.0008132231189801771\n",
      "Batch: 2400,train loss is: 0.0006562547886533011\n",
      "test loss is 0.0007749377499086732\n",
      "Batch: 2500,train loss is: 0.000768775447789847\n",
      "test loss is 0.0007675252689727998\n",
      "Batch: 2600,train loss is: 0.0003925712502105455\n",
      "test loss is 0.0007875967932941208\n",
      "Batch: 2700,train loss is: 0.000720356648872296\n",
      "test loss is 0.0007594323447706112\n",
      "Batch: 2800,train loss is: 0.0005059823512450088\n",
      "test loss is 0.0008095833449045998\n",
      "Batch: 2900,train loss is: 0.0005381106216521248\n",
      "test loss is 0.0007583107878849437\n",
      "Batch: 3000,train loss is: 0.0005124965196031554\n",
      "test loss is 0.0007589768930066189\n",
      "Batch: 3100,train loss is: 0.0006607990979060546\n",
      "test loss is 0.0008095097634859663\n",
      "Batch: 3200,train loss is: 0.0009651604805577063\n",
      "test loss is 0.0007796305628800209\n",
      "Batch: 3300,train loss is: 0.00043681540865209243\n",
      "test loss is 0.000770412837079759\n",
      "Batch: 3400,train loss is: 0.0004975124577375908\n",
      "test loss is 0.0007751593078222923\n",
      "Batch: 3500,train loss is: 0.0002849248307129381\n",
      "test loss is 0.0007786616107732132\n",
      "Batch: 3600,train loss is: 0.0005697645061070504\n",
      "test loss is 0.0008068818190581766\n",
      "Batch: 3700,train loss is: 0.0004608899115240228\n",
      "test loss is 0.0007577746152571892\n",
      "Batch: 3800,train loss is: 0.0004392420418909519\n",
      "test loss is 0.0008009583149444944\n",
      "Batch: 3900,train loss is: 0.0006482334347604348\n",
      "test loss is 0.0007685395606507404\n",
      "Batch: 4000,train loss is: 0.0004328017140315632\n",
      "test loss is 0.0007736255962794021\n",
      "Batch: 4100,train loss is: 0.0007402880980487841\n",
      "test loss is 0.0007877548225899301\n",
      "Batch: 4200,train loss is: 0.00041915960073051487\n",
      "test loss is 0.0008060786241245238\n",
      "Batch: 4300,train loss is: 0.0004381272757253024\n",
      "test loss is 0.0007663006639252317\n",
      "Batch: 4400,train loss is: 0.0005419587083169044\n",
      "test loss is 0.0007665432011130743\n",
      "Batch: 4500,train loss is: 0.00061814169555541\n",
      "test loss is 0.0007974115685176816\n",
      "Batch: 4600,train loss is: 0.0006217163022951038\n",
      "test loss is 0.0007675473972465958\n",
      "Batch: 4700,train loss is: 0.0007771413913912086\n",
      "test loss is 0.0008020245162790741\n",
      "Batch: 4800,train loss is: 0.0012790066242654933\n",
      "test loss is 0.0007778339776077158\n",
      "Batch: 4900,train loss is: 0.0005921629021741964\n",
      "test loss is 0.0007725493081940131\n",
      "Batch: 5000,train loss is: 0.0006049603506328715\n",
      "test loss is 0.000778385520266644\n",
      "Batch: 5100,train loss is: 0.0006721802166480043\n",
      "test loss is 0.000832412557820843\n",
      "Batch: 5200,train loss is: 0.000382187068908742\n",
      "test loss is 0.0007747642794635234\n",
      "Batch: 5300,train loss is: 0.0007332289631890534\n",
      "test loss is 0.0008015561353071897\n",
      "Batch: 5400,train loss is: 0.0008332576316389316\n",
      "test loss is 0.0007656033252915013\n",
      "Batch: 5500,train loss is: 0.0005327422546295254\n",
      "test loss is 0.0007890155571547866\n",
      "Batch: 5600,train loss is: 0.0006904134038714307\n",
      "test loss is 0.0007846732331124386\n",
      "Batch: 5700,train loss is: 0.0008151126382006373\n",
      "test loss is 0.0007671420225657567\n",
      "Batch: 5800,train loss is: 0.0005680562409071978\n",
      "test loss is 0.0007706865861196152\n",
      "Batch: 5900,train loss is: 0.003500071792904402\n",
      "test loss is 0.0007750816372869274\n",
      "Batch: 6000,train loss is: 0.001688859148712442\n",
      "test loss is 0.0007880331913297242\n",
      "Batch: 6100,train loss is: 0.0004953398314775666\n",
      "test loss is 0.0007591115157824675\n",
      "Batch: 6200,train loss is: 0.0006219015510725522\n",
      "test loss is 0.0007747738941080634\n",
      "Batch: 6300,train loss is: 0.0014089843002469114\n",
      "test loss is 0.0007891876394706378\n",
      "Batch: 6400,train loss is: 0.00047153696919001204\n",
      "test loss is 0.0008274123389089014\n",
      "Batch: 6500,train loss is: 0.00040339192374303043\n",
      "test loss is 0.0007978156852102501\n",
      "Batch: 6600,train loss is: 0.0022539058897283167\n",
      "test loss is 0.0007710445938346746\n",
      "Batch: 6700,train loss is: 0.00048541610217533787\n",
      "test loss is 0.000792906097313717\n",
      "Batch: 6800,train loss is: 0.0010194045355745222\n",
      "test loss is 0.000765603847210304\n",
      "Batch: 6900,train loss is: 0.0006250858423119083\n",
      "test loss is 0.0007717735163180317\n",
      "Batch: 7000,train loss is: 0.000606324807341218\n",
      "test loss is 0.0008050425367422757\n",
      "Batch: 7100,train loss is: 0.001014535721886054\n",
      "test loss is 0.0007765209749551585\n",
      "Batch: 7200,train loss is: 0.0004288021311605598\n",
      "test loss is 0.0007900643303168666\n",
      "Batch: 7300,train loss is: 0.000857712339117726\n",
      "test loss is 0.0008273721867795614\n",
      "Batch: 7400,train loss is: 0.0004211501966596391\n",
      "test loss is 0.0007661815169545794\n",
      "Batch: 7500,train loss is: 0.0007813009620251794\n",
      "test loss is 0.0007877790460214377\n",
      "Batch: 7600,train loss is: 0.0005194168980464817\n",
      "test loss is 0.0007642507485577749\n",
      "Batch: 7700,train loss is: 0.0007092750063747985\n",
      "test loss is 0.0007678495456882232\n",
      "Batch: 7800,train loss is: 0.00031165048870929836\n",
      "test loss is 0.0007681065001018411\n",
      "Batch: 7900,train loss is: 0.0006369938989104703\n",
      "test loss is 0.0007752321899183313\n",
      "Batch: 8000,train loss is: 0.0004917828514621934\n",
      "test loss is 0.0008021612234506058\n",
      "Batch: 8100,train loss is: 0.0006296159806494499\n",
      "test loss is 0.0007844050427420541\n",
      "Batch: 8200,train loss is: 0.0013070797226779252\n",
      "test loss is 0.0007885781485778472\n",
      "Batch: 8300,train loss is: 0.0006226056419488327\n",
      "test loss is 0.0007901369940073485\n",
      "Batch: 8400,train loss is: 0.0005697311839889509\n",
      "test loss is 0.0007716418723889372\n",
      "Batch: 8500,train loss is: 0.0010150669543966912\n",
      "test loss is 0.0007912065480360111\n",
      "Batch: 8600,train loss is: 0.0007269102934501128\n",
      "test loss is 0.0007888305493896603\n",
      "Batch: 8700,train loss is: 0.0005050871617812366\n",
      "test loss is 0.0007981914300721402\n",
      "Batch: 8800,train loss is: 0.0003400564119957322\n",
      "test loss is 0.0007636706209261946\n",
      "Batch: 8900,train loss is: 0.001105295641036341\n",
      "test loss is 0.0007635473498837405\n",
      "Batch: 9000,train loss is: 0.00029402808704427906\n",
      "test loss is 0.0007749780646592426\n",
      "Batch: 9100,train loss is: 0.000435544451958277\n",
      "test loss is 0.0007653244425958185\n",
      "Batch: 9200,train loss is: 0.0009552245967101765\n",
      "test loss is 0.0008287582024879005\n",
      "Batch: 9300,train loss is: 0.000608087075309762\n",
      "test loss is 0.0007734552848997464\n",
      "Batch: 9400,train loss is: 0.0004905762521806147\n",
      "test loss is 0.0008087602527429748\n",
      "Batch: 9500,train loss is: 0.0004580011797777332\n",
      "test loss is 0.0007885373487098788\n",
      "Batch: 9600,train loss is: 0.0004961919348999623\n",
      "test loss is 0.0007665851608920317\n",
      "Batch: 9700,train loss is: 0.00042548698303901763\n",
      "test loss is 0.0007886585097625837\n",
      "Batch: 9800,train loss is: 0.0005272066569012743\n",
      "test loss is 0.0007714241617710868\n",
      "Batch: 9900,train loss is: 0.0005534721915871163\n",
      "test loss is 0.0008103505098267757\n",
      "Batch: 10000,train loss is: 0.000558383361501532\n",
      "test loss is 0.0007700674635453805\n",
      "Batch: 10100,train loss is: 0.0010707002836295454\n",
      "test loss is 0.0007670060924630536\n",
      "Batch: 10200,train loss is: 0.0016985511505498616\n",
      "test loss is 0.000876050295705989\n",
      "Batch: 10300,train loss is: 0.0005213282345454595\n",
      "test loss is 0.0007762118100293639\n",
      "Batch: 10400,train loss is: 0.00041393665426710274\n",
      "test loss is 0.0007687604172969228\n",
      "Batch: 10500,train loss is: 0.0007444887237552285\n",
      "test loss is 0.000769894399820863\n",
      "Batch: 10600,train loss is: 0.00038326262959932747\n",
      "test loss is 0.0008170643606729555\n",
      "Batch: 10700,train loss is: 0.0006353600811378333\n",
      "test loss is 0.0007695471815752006\n",
      "Batch: 10800,train loss is: 0.0005291322114175319\n",
      "test loss is 0.0007795753492416532\n",
      "Batch: 10900,train loss is: 0.000695855702140265\n",
      "test loss is 0.0007720453056294175\n",
      "Batch: 11000,train loss is: 0.0007432127154772024\n",
      "test loss is 0.0008013609057501349\n",
      "Batch: 11100,train loss is: 0.0010494757432599692\n",
      "test loss is 0.0007652961537141028\n",
      "Batch: 11200,train loss is: 0.0006911276279561357\n",
      "test loss is 0.0007996649543952903\n",
      "Batch: 11300,train loss is: 0.0003482274491369459\n",
      "test loss is 0.000809672841833382\n",
      "Batch: 11400,train loss is: 0.0005946257984637808\n",
      "test loss is 0.0007841043951445928\n",
      "Batch: 11500,train loss is: 0.0005836572431882247\n",
      "test loss is 0.0007768532269267972\n",
      "Batch: 11600,train loss is: 0.001975139414489203\n",
      "test loss is 0.0007854358329117605\n",
      "Batch: 11700,train loss is: 0.00033387233557116426\n",
      "test loss is 0.0007536441489067325\n",
      "Batch: 11800,train loss is: 0.0006282760899686983\n",
      "test loss is 0.0007706059693554636\n",
      "Batch: 11900,train loss is: 0.0006981639170268839\n",
      "test loss is 0.0007839875913681156\n",
      "Batch: 12000,train loss is: 0.00047628563219019013\n",
      "test loss is 0.0007768218458519237\n",
      "Batch: 12100,train loss is: 0.0017352315589445554\n",
      "test loss is 0.0007745674147648705\n",
      "Batch: 12200,train loss is: 0.0007687120703908079\n",
      "test loss is 0.0007955974537691082\n",
      "Batch: 12300,train loss is: 0.00048061460691120226\n",
      "test loss is 0.0007928104703510588\n",
      "Batch: 12400,train loss is: 0.0006633714986086929\n",
      "test loss is 0.0007660198406527969\n",
      "Batch: 12500,train loss is: 0.0003692715489304093\n",
      "test loss is 0.0007936550158426556\n",
      "Batch: 12600,train loss is: 0.0004214218768124099\n",
      "test loss is 0.00078252869056085\n",
      "Batch: 12700,train loss is: 0.0005182382954917785\n",
      "test loss is 0.0007603265269154799\n",
      "Batch: 12800,train loss is: 0.0004447333692630381\n",
      "test loss is 0.0007676668383221834\n",
      "Batch: 12900,train loss is: 0.0007098562879467157\n",
      "test loss is 0.0007853136189004719\n",
      "Batch: 13000,train loss is: 0.0002675148813614499\n",
      "test loss is 0.0007547675413500114\n",
      "Batch: 13100,train loss is: 0.0023393123911458896\n",
      "test loss is 0.0007618278818469662\n",
      "Batch: 13200,train loss is: 0.0007854494784312116\n",
      "test loss is 0.0008579264238027639\n",
      "Batch: 13300,train loss is: 0.0005748851968590498\n",
      "test loss is 0.0008111555242568241\n",
      "Batch: 13400,train loss is: 0.0003880063994926132\n",
      "test loss is 0.0007928172621048045\n",
      "Batch: 13500,train loss is: 0.0006548072388712949\n",
      "test loss is 0.0008004214442573491\n",
      "Batch: 13600,train loss is: 0.0005501637308869191\n",
      "test loss is 0.00077538900274078\n",
      "Batch: 13700,train loss is: 0.000711037956515666\n",
      "test loss is 0.0007675046641007148\n",
      "Batch: 13800,train loss is: 0.0006434325341525846\n",
      "test loss is 0.0007730482590159516\n",
      "Batch: 13900,train loss is: 0.0006530459196225574\n",
      "test loss is 0.0008008726647793237\n",
      "Batch: 14000,train loss is: 0.00043989436669449614\n",
      "test loss is 0.0007764320930157795\n",
      "Batch: 14100,train loss is: 0.0008339522520509567\n",
      "test loss is 0.0008197427989913049\n",
      "Batch: 14200,train loss is: 0.0005522718259846872\n",
      "test loss is 0.00075251466782797\n",
      "Batch: 14300,train loss is: 0.00042902206940243946\n",
      "test loss is 0.0007610092425041371\n",
      "Batch: 14400,train loss is: 0.0014101412748366154\n",
      "test loss is 0.000764923982065261\n",
      "Batch: 14500,train loss is: 0.0004444794836361707\n",
      "test loss is 0.0007669645817684546\n",
      "Batch: 14600,train loss is: 0.0016746693286369143\n",
      "test loss is 0.0007639370507617819\n",
      "Batch: 14700,train loss is: 0.0004721162487097099\n",
      "test loss is 0.0008032975238400743\n",
      "Batch: 14800,train loss is: 0.000883903030373285\n",
      "test loss is 0.0007717209952920065\n",
      "Batch: 14900,train loss is: 0.0004330185640820705\n",
      "test loss is 0.0007940764168477559\n",
      "Batch: 15000,train loss is: 0.000849291267732499\n",
      "test loss is 0.0007601556227601015\n",
      "Batch: 15100,train loss is: 0.000704725359305194\n",
      "test loss is 0.0007898984708816658\n",
      "Batch: 15200,train loss is: 0.001250441834002821\n",
      "test loss is 0.0007807277571894434\n",
      "Batch: 15300,train loss is: 0.000501059786729199\n",
      "test loss is 0.0007599953890710263\n",
      "Batch: 15400,train loss is: 0.0005140245851170335\n",
      "test loss is 0.0007891996708518227\n",
      "Batch: 15500,train loss is: 0.0008452275744770583\n",
      "test loss is 0.0007545637006917483\n",
      "Batch: 15600,train loss is: 0.001003487533269739\n",
      "test loss is 0.0008031174260407468\n",
      "Batch: 15700,train loss is: 0.0006092874392144787\n",
      "test loss is 0.0007656279834945392\n",
      "Batch: 15800,train loss is: 0.0006981932677350594\n",
      "test loss is 0.0007625212389703436\n",
      "Batch: 15900,train loss is: 0.0011336149434379144\n",
      "test loss is 0.0007830796506682391\n",
      "Batch: 16000,train loss is: 0.00042084941487429496\n",
      "test loss is 0.0007889251830468069\n",
      "Batch: 16100,train loss is: 0.0006002893691068776\n",
      "test loss is 0.0007735837598024849\n",
      "Batch: 16200,train loss is: 0.00043855354493921355\n",
      "test loss is 0.0007691622468629166\n",
      "Batch: 16300,train loss is: 0.0011766549528979921\n",
      "test loss is 0.0007823686164430658\n",
      "Batch: 16400,train loss is: 0.0004461421319103139\n",
      "test loss is 0.0007564122303919333\n",
      "Batch: 16500,train loss is: 0.0006079729998039753\n",
      "test loss is 0.0008010797880886941\n",
      "Batch: 16600,train loss is: 0.0003267549194855624\n",
      "test loss is 0.0007642521228546954\n",
      "Batch: 16700,train loss is: 0.0010529239852276834\n",
      "test loss is 0.00077945332319364\n",
      "Batch: 16800,train loss is: 0.0006021998390664835\n",
      "test loss is 0.0007882199661825457\n",
      "Batch: 16900,train loss is: 0.0007643180698434507\n",
      "test loss is 0.0007800310979173394\n",
      "Batch: 17000,train loss is: 0.0008127315485245828\n",
      "test loss is 0.000754059046253797\n",
      "Batch: 17100,train loss is: 0.0005976224404795731\n",
      "test loss is 0.0007659330243797217\n",
      "Batch: 17200,train loss is: 0.0005786927589557371\n",
      "test loss is 0.0007715403980739216\n",
      "Batch: 17300,train loss is: 0.0008810712364432492\n",
      "test loss is 0.0007815174420050561\n",
      "Batch: 17400,train loss is: 0.00046842894118102017\n",
      "test loss is 0.000768048290900254\n",
      "Batch: 17500,train loss is: 0.0005144410030924825\n",
      "test loss is 0.0007663046029624307\n",
      "Batch: 17600,train loss is: 0.0004296412951496038\n",
      "test loss is 0.0007876366417759997\n",
      "Batch: 17700,train loss is: 0.0003380268987885524\n",
      "test loss is 0.00079969363933248\n",
      "Batch: 17800,train loss is: 0.0011941755959735437\n",
      "test loss is 0.0007737957098342506\n",
      "Batch: 17900,train loss is: 0.00047821929760415784\n",
      "test loss is 0.0008056018325749359\n",
      "Batch: 18000,train loss is: 0.0003855161805668871\n",
      "test loss is 0.000755312296837155\n",
      "Batch: 18100,train loss is: 0.0004938783367748763\n",
      "test loss is 0.0007665521919538248\n",
      "Batch: 18200,train loss is: 0.0005866590556404873\n",
      "test loss is 0.000770472462925595\n",
      "Batch: 18300,train loss is: 0.0009834014348956222\n",
      "test loss is 0.0007659094534102258\n",
      "Batch: 18400,train loss is: 0.0005285202844202042\n",
      "test loss is 0.0007648258178412651\n",
      "Batch: 18500,train loss is: 0.0008475993545366746\n",
      "test loss is 0.0007802978843197171\n",
      "Batch: 18600,train loss is: 0.0005063963193867445\n",
      "test loss is 0.0007626948611775947\n",
      "Batch: 18700,train loss is: 0.0005658215398428826\n",
      "test loss is 0.0007717798933159723\n",
      "Batch: 18800,train loss is: 0.0007003736810004571\n",
      "test loss is 0.0007662982112273941\n",
      "Batch: 18900,train loss is: 0.0007577067686887596\n",
      "test loss is 0.0008068157744502474\n",
      "Batch: 19000,train loss is: 0.0006983730641466065\n",
      "test loss is 0.0007697198246035718\n",
      "Batch: 19100,train loss is: 0.00032177739748915924\n",
      "test loss is 0.0007879030502364751\n",
      "Batch: 19200,train loss is: 0.0006245908820597476\n",
      "test loss is 0.0007743468078321098\n",
      "Batch: 19300,train loss is: 0.0008833615161707108\n",
      "test loss is 0.000759626833923194\n",
      "Batch: 19400,train loss is: 0.0003220629719231304\n",
      "test loss is 0.0007551890936589822\n",
      "Batch: 19500,train loss is: 0.0005987241308770039\n",
      "test loss is 0.0008066528299534589\n",
      "Batch: 19600,train loss is: 0.0004909829244442507\n",
      "test loss is 0.0007797560534543748\n",
      "Batch: 19700,train loss is: 0.0008197751883067667\n",
      "test loss is 0.0008330247837278978\n",
      "Batch: 19800,train loss is: 0.0005651937624312457\n",
      "test loss is 0.0007735513043883075\n",
      "Batch: 19900,train loss is: 0.00026530452876662034\n",
      "test loss is 0.0008044101698247727\n",
      "Batch: 20000,train loss is: 0.0008243176341015811\n",
      "test loss is 0.0007715766161905184\n",
      "Batch: 20100,train loss is: 0.0005767008628181126\n",
      "test loss is 0.0007734975049531313\n",
      "Batch: 20200,train loss is: 0.0010020648598621227\n",
      "test loss is 0.0007869061928787004\n",
      "Batch: 20300,train loss is: 0.00039941867988818494\n",
      "test loss is 0.000787993907419654\n",
      "Batch: 20400,train loss is: 0.0005315441198512924\n",
      "test loss is 0.0008124419492836092\n",
      "Batch: 20500,train loss is: 0.0006283280052963109\n",
      "test loss is 0.0007741257058890338\n",
      "Batch: 20600,train loss is: 0.0006470541292577297\n",
      "test loss is 0.0008057016336268083\n",
      "Batch: 20700,train loss is: 0.0006244000228889775\n",
      "test loss is 0.0007800709777630362\n",
      "Batch: 20800,train loss is: 0.00037298167707305234\n",
      "test loss is 0.0007758926199735896\n",
      "Batch: 20900,train loss is: 0.0007146968413486546\n",
      "test loss is 0.0008124269024109836\n",
      "Batch: 21000,train loss is: 0.00033633950034824255\n",
      "test loss is 0.0007590813886986002\n",
      "Batch: 21100,train loss is: 0.0006692292733844186\n",
      "test loss is 0.0007916995509378817\n",
      "Batch: 21200,train loss is: 0.00033209335994035415\n",
      "test loss is 0.0007788412846834611\n",
      "Batch: 21300,train loss is: 0.00038982208687674496\n",
      "test loss is 0.000764862018960275\n",
      "Batch: 21400,train loss is: 0.0009086076206855103\n",
      "test loss is 0.0007769887651826975\n",
      "Batch: 21500,train loss is: 0.0002562438618950984\n",
      "test loss is 0.0007945786106426677\n",
      "Batch: 21600,train loss is: 0.0007892802728315214\n",
      "test loss is 0.0007827492637579361\n",
      "Batch: 21700,train loss is: 0.0003316438153946785\n",
      "test loss is 0.0007628690368378323\n",
      "Batch: 21800,train loss is: 0.000642674433425913\n",
      "test loss is 0.0008216004291370049\n",
      "Batch: 21900,train loss is: 0.0005699687028725505\n",
      "test loss is 0.0008512770329334288\n",
      "Batch: 22000,train loss is: 0.0007394845934977179\n",
      "test loss is 0.0007665244107899902\n",
      "Batch: 22100,train loss is: 0.0003494059195603476\n",
      "test loss is 0.0007697063782252134\n",
      "Batch: 22200,train loss is: 0.0005571865789942458\n",
      "test loss is 0.0007698849919975521\n",
      "Batch: 22300,train loss is: 0.000756401141732347\n",
      "test loss is 0.0007703217036139722\n",
      "Batch: 22400,train loss is: 0.0005590421058293997\n",
      "test loss is 0.000761969096905492\n",
      "Batch: 22500,train loss is: 0.00044554042832835994\n",
      "test loss is 0.0007594231764520835\n",
      "Batch: 22600,train loss is: 0.0011018809307657763\n",
      "test loss is 0.0008231133871368366\n",
      "Batch: 22700,train loss is: 0.00044403738746887886\n",
      "test loss is 0.0007673761102478929\n",
      "Batch: 22800,train loss is: 0.0005504813505881039\n",
      "test loss is 0.0007827199023123591\n",
      "Batch: 22900,train loss is: 0.00037255245043575055\n",
      "test loss is 0.0008309536677669288\n",
      "Batch: 23000,train loss is: 0.00042078289953122124\n",
      "test loss is 0.0007655957606696977\n",
      "Batch: 23100,train loss is: 0.0005821911837658976\n",
      "test loss is 0.0007516546980234018\n",
      "Batch: 23200,train loss is: 0.000370299512356256\n",
      "test loss is 0.0007469856945043515\n",
      "Batch: 23300,train loss is: 0.0005610133298867552\n",
      "test loss is 0.000757925300989231\n",
      "Batch: 23400,train loss is: 0.000729196315248435\n",
      "test loss is 0.0007828633360548184\n",
      "Batch: 23500,train loss is: 0.0026256948437451745\n",
      "test loss is 0.0007641499115859267\n",
      "Batch: 23600,train loss is: 0.00031402177519557233\n",
      "test loss is 0.000755093899179073\n",
      "Batch: 23700,train loss is: 0.0008853845493906133\n",
      "test loss is 0.0007847562318943051\n",
      "Batch: 23800,train loss is: 0.0009450904182395736\n",
      "test loss is 0.0007589153374478973\n",
      "Batch: 23900,train loss is: 0.0009192027028758803\n",
      "test loss is 0.0007681576189021825\n",
      "Batch: 24000,train loss is: 0.0007699863576098907\n",
      "test loss is 0.0008092408328752411\n",
      "Batch: 24100,train loss is: 0.0004575570983173254\n",
      "test loss is 0.0007528561673331816\n",
      "Batch: 24200,train loss is: 0.0017162202568353408\n",
      "test loss is 0.0007797465453433078\n",
      "Batch: 24300,train loss is: 0.00048682525080152546\n",
      "test loss is 0.0009042512939486644\n",
      "Batch: 24400,train loss is: 0.0013624926556285264\n",
      "test loss is 0.0007644756521775905\n",
      "Batch: 24500,train loss is: 0.0006571438058359136\n",
      "test loss is 0.000760155782674497\n",
      "Batch: 24600,train loss is: 0.0005651850423447262\n",
      "test loss is 0.0007966118659418189\n",
      "Batch: 24700,train loss is: 0.00034636712893764635\n",
      "test loss is 0.0007638102700192443\n",
      "Batch: 24800,train loss is: 0.0010045868075188027\n",
      "test loss is 0.0007637410384110153\n",
      "Batch: 24900,train loss is: 0.00017784898500148102\n",
      "test loss is 0.0007699454600865321\n",
      "Batch: 25000,train loss is: 0.0008179092411299669\n",
      "test loss is 0.0007605497410532875\n",
      "Batch: 25100,train loss is: 0.0006434133456211234\n",
      "test loss is 0.0007745106297385908\n",
      "Batch: 25200,train loss is: 0.0007859711212189086\n",
      "test loss is 0.0007791736793869641\n",
      "Batch: 25300,train loss is: 0.0008777358389034616\n",
      "test loss is 0.0007902447083557595\n",
      "Batch: 25400,train loss is: 0.00041560501484491955\n",
      "test loss is 0.0007557709915645794\n",
      "Batch: 25500,train loss is: 0.0005688804596269305\n",
      "test loss is 0.0007608586167982906\n",
      "Batch: 25600,train loss is: 0.0007211399490106115\n",
      "test loss is 0.0007682251543505018\n",
      "Batch: 25700,train loss is: 0.000621298196687052\n",
      "test loss is 0.0007838741220338167\n",
      "Batch: 25800,train loss is: 0.0003373566832426801\n",
      "test loss is 0.0007595966504419032\n",
      "Batch: 25900,train loss is: 0.000358296168118332\n",
      "test loss is 0.0008042591077236925\n",
      "Batch: 26000,train loss is: 0.0017626123912176624\n",
      "test loss is 0.00082390206011795\n",
      "Batch: 26100,train loss is: 0.0009462651776144747\n",
      "test loss is 0.0007596151537848934\n",
      "Batch: 26200,train loss is: 0.0007906568388785652\n",
      "test loss is 0.0007491015712040761\n",
      "Batch: 26300,train loss is: 0.0012199868970387245\n",
      "test loss is 0.0007822077705275159\n",
      "Batch: 26400,train loss is: 0.0006651967820299113\n",
      "test loss is 0.0007666008781802255\n",
      "Batch: 26500,train loss is: 0.0003653853645854925\n",
      "test loss is 0.0007699083116164514\n",
      "Batch: 26600,train loss is: 0.001269182098345057\n",
      "test loss is 0.000838932147696161\n",
      "Batch: 26700,train loss is: 0.00045233885379797383\n",
      "test loss is 0.0008800685207854155\n",
      "Batch: 26800,train loss is: 0.0012417062804529957\n",
      "test loss is 0.0007662690742091632\n",
      "Batch: 26900,train loss is: 0.0005300870560880675\n",
      "test loss is 0.0007623920196002332\n",
      "Batch: 27000,train loss is: 0.0006800510131075241\n",
      "test loss is 0.0008935308818865522\n",
      "Batch: 27100,train loss is: 0.000506725265540432\n",
      "test loss is 0.0008078426590804452\n",
      "Batch: 27200,train loss is: 0.0006900532696297985\n",
      "test loss is 0.0008053998346727224\n",
      "Batch: 27300,train loss is: 0.0006017301052392658\n",
      "test loss is 0.0007666835727008134\n",
      "Batch: 27400,train loss is: 0.0009177234510167914\n",
      "test loss is 0.0007555600488018967\n",
      "Batch: 27500,train loss is: 0.0005680319659874955\n",
      "test loss is 0.000847617078726267\n",
      "Batch: 27600,train loss is: 0.0006455522822816197\n",
      "test loss is 0.000815088423223801\n",
      "Batch: 27700,train loss is: 0.001805778984224883\n",
      "test loss is 0.000774106715699157\n",
      "Batch: 27800,train loss is: 0.0007330632079629642\n",
      "test loss is 0.0007924589212360527\n",
      "Batch: 27900,train loss is: 0.0013170760752704584\n",
      "test loss is 0.0007598253116670193\n",
      "Batch: 28000,train loss is: 0.00044648906369670687\n",
      "test loss is 0.0007681637654954081\n",
      "Batch: 28100,train loss is: 0.0008758373872346811\n",
      "test loss is 0.0007853016853796173\n",
      "Batch: 28200,train loss is: 0.0006959995216506827\n",
      "test loss is 0.0007449271351095935\n",
      "Batch: 28300,train loss is: 0.0021217908863056896\n",
      "test loss is 0.0007634054122015374\n",
      "Batch: 28400,train loss is: 0.0006085177892132786\n",
      "test loss is 0.0007567173651602224\n",
      "Batch: 28500,train loss is: 0.001446666029809703\n",
      "test loss is 0.000783983289920606\n",
      "Batch: 28600,train loss is: 0.00040946748941955897\n",
      "test loss is 0.000777382841727585\n",
      "Batch: 28700,train loss is: 0.0004836484477714582\n",
      "test loss is 0.0007592798761771437\n",
      "Batch: 28800,train loss is: 0.0012060825187398838\n",
      "test loss is 0.0007653794650248745\n",
      "Batch: 28900,train loss is: 0.000529006568906207\n",
      "test loss is 0.0007597812005435641\n",
      "Batch: 29000,train loss is: 0.0005845277913123942\n",
      "test loss is 0.0007514769017209096\n",
      "Batch: 29100,train loss is: 0.000699612802071122\n",
      "test loss is 0.000773252390092963\n",
      "Batch: 29200,train loss is: 0.00048020402601185226\n",
      "test loss is 0.0007765337351101362\n",
      "Batch: 29300,train loss is: 0.0010482285106411878\n",
      "test loss is 0.0007583811321490923\n",
      "Batch: 29400,train loss is: 0.0006837303160995591\n",
      "test loss is 0.0008217737337647771\n",
      "Batch: 29500,train loss is: 0.0004698058723778202\n",
      "test loss is 0.0007749567336773964\n",
      "Batch: 29600,train loss is: 0.0005617347273783855\n",
      "test loss is 0.0007629600914272138\n",
      "Batch: 29700,train loss is: 0.0003646370048918707\n",
      "test loss is 0.0007782514387316172\n",
      "Batch: 29800,train loss is: 0.0008791927968316368\n",
      "test loss is 0.0007540367985629938\n",
      "Batch: 29900,train loss is: 0.003361791831279986\n",
      "test loss is 0.0007643510629728201\n",
      "Batch: 30000,train loss is: 0.00033436666310144924\n",
      "test loss is 0.0008115983437095387\n",
      "Batch: 30100,train loss is: 0.0005037654140197523\n",
      "test loss is 0.000799353198087195\n",
      "Batch: 30200,train loss is: 0.0003744211684587269\n",
      "test loss is 0.000760392664212934\n",
      "Batch: 30300,train loss is: 0.00034384979070439104\n",
      "test loss is 0.0007520289918312159\n",
      "Batch: 30400,train loss is: 0.00041352143324568827\n",
      "test loss is 0.0007620404962121539\n",
      "Batch: 30500,train loss is: 0.0004263295080705614\n",
      "test loss is 0.0008116846854985786\n",
      "Batch: 30600,train loss is: 0.0002792280708683555\n",
      "test loss is 0.0007591350658293455\n",
      "Batch: 30700,train loss is: 0.0004880976146507856\n",
      "test loss is 0.0009020010849270288\n",
      "Batch: 30800,train loss is: 0.0002918852773967289\n",
      "test loss is 0.0008281951348672156\n",
      "Batch: 30900,train loss is: 0.00038753733863561323\n",
      "test loss is 0.0007617797967243552\n",
      "Batch: 31000,train loss is: 0.0005727577522181285\n",
      "test loss is 0.0007738066987600119\n",
      "Batch: 31100,train loss is: 0.0004298682416290352\n",
      "test loss is 0.0008375011078053583\n",
      "Batch: 31200,train loss is: 0.0006300601073940803\n",
      "test loss is 0.0007737769166562391\n",
      "Batch: 31300,train loss is: 0.0006582589489500495\n",
      "test loss is 0.0007792807802187573\n",
      "Batch: 31400,train loss is: 0.0008335114263427764\n",
      "test loss is 0.0007616158102338137\n",
      "Batch: 31500,train loss is: 0.0005933489555738052\n",
      "test loss is 0.0007729132940364926\n",
      "Batch: 31600,train loss is: 0.0005327644106378824\n",
      "test loss is 0.0007686602176768669\n",
      "Batch: 31700,train loss is: 0.0005691910769677635\n",
      "test loss is 0.0007985865485905376\n",
      "Batch: 31800,train loss is: 0.001219864219157411\n",
      "test loss is 0.0007635989063651757\n",
      "Batch: 31900,train loss is: 0.0009647584587143254\n",
      "test loss is 0.0007940053555836737\n",
      "Batch: 32000,train loss is: 0.004415423754389759\n",
      "test loss is 0.0007605585034054589\n",
      "Batch: 32100,train loss is: 0.0004652453965544863\n",
      "test loss is 0.0007575838553275805\n",
      "Batch: 32200,train loss is: 0.0004648754809428596\n",
      "test loss is 0.0007731043950156706\n",
      "Batch: 32300,train loss is: 0.00039687693999148515\n",
      "test loss is 0.0007635652703640681\n",
      "Batch: 32400,train loss is: 0.0004742708654920619\n",
      "test loss is 0.0007487397582749522\n",
      "Batch: 32500,train loss is: 0.0010897457729415543\n",
      "test loss is 0.0007925391862491183\n",
      "Batch: 32600,train loss is: 0.00046935911519859863\n",
      "test loss is 0.0007707000531039299\n",
      "Batch: 32700,train loss is: 0.00047105463047949136\n",
      "test loss is 0.0007665630572602883\n",
      "Batch: 32800,train loss is: 0.00041735069744597124\n",
      "test loss is 0.0007802919971145413\n",
      "Batch: 32900,train loss is: 0.0006682860139747663\n",
      "test loss is 0.0007559387014040824\n",
      "Batch: 33000,train loss is: 0.000560714033716519\n",
      "test loss is 0.0007484856835131912\n",
      "Batch: 33100,train loss is: 0.0004901131888747134\n",
      "test loss is 0.000788709394681516\n",
      "Batch: 33200,train loss is: 0.0005812689783006209\n",
      "test loss is 0.0008573328855232242\n",
      "Batch: 33300,train loss is: 0.0003853565605618623\n",
      "test loss is 0.000746608296956738\n",
      "Batch: 33400,train loss is: 0.0005503512725698272\n",
      "test loss is 0.0007731043130885532\n",
      "Batch: 33500,train loss is: 0.00048580378533973745\n",
      "test loss is 0.0007606100761119581\n",
      "Batch: 33600,train loss is: 0.0006108214004432245\n",
      "test loss is 0.0007613646783760761\n",
      "Batch: 33700,train loss is: 0.000673242685919666\n",
      "test loss is 0.0007732441654568945\n",
      "Batch: 33800,train loss is: 0.0008235003563907065\n",
      "test loss is 0.0007530385174727388\n",
      "Batch: 33900,train loss is: 0.0007094050628466882\n",
      "test loss is 0.0007609012641220961\n",
      "-----------------------Epoch: 6----------------------------------\n",
      "Batch: 0,train loss is: 0.00040309334014061997\n",
      "test loss is 0.0007669316609983986\n",
      "Batch: 100,train loss is: 0.0014967976605306467\n",
      "test loss is 0.0007636983738625583\n",
      "Batch: 200,train loss is: 0.0006837436367396506\n",
      "test loss is 0.0007663417951713046\n",
      "Batch: 300,train loss is: 0.0003404490561948824\n",
      "test loss is 0.0007606320384272973\n",
      "Batch: 400,train loss is: 0.0005465646591150678\n",
      "test loss is 0.0007567684842218528\n",
      "Batch: 500,train loss is: 0.000662123147125155\n",
      "test loss is 0.0007926655468791331\n",
      "Batch: 600,train loss is: 0.0002702171378100012\n",
      "test loss is 0.0007891745534158691\n",
      "Batch: 700,train loss is: 0.0003279944959181073\n",
      "test loss is 0.0007579650973137545\n",
      "Batch: 800,train loss is: 0.0004778268395245118\n",
      "test loss is 0.0007573568912265959\n",
      "Batch: 900,train loss is: 0.00036587079762939284\n",
      "test loss is 0.0007537387477637709\n",
      "Batch: 1000,train loss is: 0.0005562605017270664\n",
      "test loss is 0.0007917236848184598\n",
      "Batch: 1100,train loss is: 0.001867246325089846\n",
      "test loss is 0.0007522269789861546\n",
      "Batch: 1200,train loss is: 0.0003833295941025799\n",
      "test loss is 0.0007572147993260758\n",
      "Batch: 1300,train loss is: 0.0005509087078042115\n",
      "test loss is 0.0007558924839379617\n",
      "Batch: 1400,train loss is: 0.0005400929481004843\n",
      "test loss is 0.0007727132181221571\n",
      "Batch: 1500,train loss is: 0.0005257938552562559\n",
      "test loss is 0.0007822334860955868\n",
      "Batch: 1600,train loss is: 0.0005974734043758058\n",
      "test loss is 0.0008384565114077594\n",
      "Batch: 1700,train loss is: 0.001328968452250601\n",
      "test loss is 0.0007481525251371405\n",
      "Batch: 1800,train loss is: 0.0005068220291497629\n",
      "test loss is 0.000772483070243223\n",
      "Batch: 1900,train loss is: 0.00040923540782083074\n",
      "test loss is 0.0007998406813351663\n",
      "Batch: 2000,train loss is: 0.0006539012450319175\n",
      "test loss is 0.0008770742065458496\n",
      "Batch: 2100,train loss is: 0.0006799447694168681\n",
      "test loss is 0.0008870272564679743\n",
      "Batch: 2200,train loss is: 0.0009428801418190872\n",
      "test loss is 0.0007616599592406314\n",
      "Batch: 2300,train loss is: 0.00043347218970270646\n",
      "test loss is 0.0007988025076929777\n",
      "Batch: 2400,train loss is: 0.0006282099607923689\n",
      "test loss is 0.0007623993782322785\n",
      "Batch: 2500,train loss is: 0.0007504584973111982\n",
      "test loss is 0.0007546729727683374\n",
      "Batch: 2600,train loss is: 0.0003946643638122925\n",
      "test loss is 0.0007766279654029589\n",
      "Batch: 2700,train loss is: 0.0007034189544569778\n",
      "test loss is 0.0007464296589815276\n",
      "Batch: 2800,train loss is: 0.0005027272493620082\n",
      "test loss is 0.0007968988361715391\n",
      "Batch: 2900,train loss is: 0.0005368436012935413\n",
      "test loss is 0.0007468339726841386\n",
      "Batch: 3000,train loss is: 0.0005155272757580297\n",
      "test loss is 0.0007465271999056349\n",
      "Batch: 3100,train loss is: 0.0006700347499358488\n",
      "test loss is 0.0007997595778997932\n",
      "Batch: 3200,train loss is: 0.0009850489210620783\n",
      "test loss is 0.000768072524575145\n",
      "Batch: 3300,train loss is: 0.0004450511914120502\n",
      "test loss is 0.0007589793104265633\n",
      "Batch: 3400,train loss is: 0.0004885671448720094\n",
      "test loss is 0.0007604741353364258\n",
      "Batch: 3500,train loss is: 0.0002724263041227558\n",
      "test loss is 0.0007650222158184396\n",
      "Batch: 3600,train loss is: 0.0005517858235877294\n",
      "test loss is 0.0007947395942975774\n",
      "Batch: 3700,train loss is: 0.00045306749976854727\n",
      "test loss is 0.00074633035262194\n",
      "Batch: 3800,train loss is: 0.00044481803327373\n",
      "test loss is 0.0007868640986732987\n",
      "Batch: 3900,train loss is: 0.0006537469132895323\n",
      "test loss is 0.0007565952864541113\n",
      "Batch: 4000,train loss is: 0.0004335173879860278\n",
      "test loss is 0.0007619319365415598\n",
      "Batch: 4100,train loss is: 0.0007342495840142657\n",
      "test loss is 0.0007752754271904207\n",
      "Batch: 4200,train loss is: 0.0003967973096027328\n",
      "test loss is 0.0007923342726170291\n",
      "Batch: 4300,train loss is: 0.00043351100669720836\n",
      "test loss is 0.0007533276114639896\n",
      "Batch: 4400,train loss is: 0.0005102432621165848\n",
      "test loss is 0.0007548426001953937\n",
      "Batch: 4500,train loss is: 0.0006179827671657506\n",
      "test loss is 0.0007849149000100394\n",
      "Batch: 4600,train loss is: 0.000611998573438636\n",
      "test loss is 0.0007563083543718591\n",
      "Batch: 4700,train loss is: 0.0007408953629971191\n",
      "test loss is 0.0007910134883013899\n",
      "Batch: 4800,train loss is: 0.0012393214782705658\n",
      "test loss is 0.0007643534000239024\n",
      "Batch: 4900,train loss is: 0.0005881629664798399\n",
      "test loss is 0.0007604773842257826\n",
      "Batch: 5000,train loss is: 0.00060855566763044\n",
      "test loss is 0.0007676232050555188\n",
      "Batch: 5100,train loss is: 0.0006582602788358015\n",
      "test loss is 0.0008241175854398038\n",
      "Batch: 5200,train loss is: 0.000366465127459817\n",
      "test loss is 0.0007632890285653011\n",
      "Batch: 5300,train loss is: 0.0007128357008355901\n",
      "test loss is 0.0007895235522292099\n",
      "Batch: 5400,train loss is: 0.0008098408135180941\n",
      "test loss is 0.0007528325942032933\n",
      "Batch: 5500,train loss is: 0.0005115797743639696\n",
      "test loss is 0.0007763716608990736\n",
      "Batch: 5600,train loss is: 0.0006762253324904301\n",
      "test loss is 0.0007716762345465866\n",
      "Batch: 5700,train loss is: 0.0007851043085848135\n",
      "test loss is 0.0007567758413491259\n",
      "Batch: 5800,train loss is: 0.0005474854010108512\n",
      "test loss is 0.0007578791678851859\n",
      "Batch: 5900,train loss is: 0.003490747495807893\n",
      "test loss is 0.0007633063559061695\n",
      "Batch: 6000,train loss is: 0.0016781607938365188\n",
      "test loss is 0.0007743105566540549\n",
      "Batch: 6100,train loss is: 0.00048106997911980166\n",
      "test loss is 0.0007472634706586718\n",
      "Batch: 6200,train loss is: 0.000615373445991005\n",
      "test loss is 0.0007620002161243926\n",
      "Batch: 6300,train loss is: 0.0013472239259963042\n",
      "test loss is 0.0007769499603224448\n",
      "Batch: 6400,train loss is: 0.0004768035561392767\n",
      "test loss is 0.0008142663516395548\n",
      "Batch: 6500,train loss is: 0.00040203054783728606\n",
      "test loss is 0.0007843047436383418\n",
      "Batch: 6600,train loss is: 0.0022243175255997763\n",
      "test loss is 0.0007586458888821629\n",
      "Batch: 6700,train loss is: 0.00045692054573847436\n",
      "test loss is 0.0007799278034928906\n",
      "Batch: 6800,train loss is: 0.0009613527705178038\n",
      "test loss is 0.0007541996165628898\n",
      "Batch: 6900,train loss is: 0.0006281996197478011\n",
      "test loss is 0.0007595666706390632\n",
      "Batch: 7000,train loss is: 0.0006052531805616842\n",
      "test loss is 0.0007902879594294378\n",
      "Batch: 7100,train loss is: 0.0010321035955410873\n",
      "test loss is 0.0007665650365858751\n",
      "Batch: 7200,train loss is: 0.00042087656534379195\n",
      "test loss is 0.0007756240111173858\n",
      "Batch: 7300,train loss is: 0.0008530281823456821\n",
      "test loss is 0.0008167270342060014\n",
      "Batch: 7400,train loss is: 0.00041099889575352\n",
      "test loss is 0.0007551939014455538\n",
      "Batch: 7500,train loss is: 0.0007452481788644283\n",
      "test loss is 0.0007752921017600334\n",
      "Batch: 7600,train loss is: 0.0004848150519831183\n",
      "test loss is 0.0007504463884855397\n",
      "Batch: 7700,train loss is: 0.0007164910006199028\n",
      "test loss is 0.0007559202021856\n",
      "Batch: 7800,train loss is: 0.0003130657742274644\n",
      "test loss is 0.0007540317375268248\n",
      "Batch: 7900,train loss is: 0.0006328398575561483\n",
      "test loss is 0.0007634651480965861\n",
      "Batch: 8000,train loss is: 0.00048711314240237007\n",
      "test loss is 0.0007884917957681929\n",
      "Batch: 8100,train loss is: 0.000608916526971849\n",
      "test loss is 0.0007726806336136897\n",
      "Batch: 8200,train loss is: 0.0013080904279730765\n",
      "test loss is 0.0007761661292219486\n",
      "Batch: 8300,train loss is: 0.0006086432748027056\n",
      "test loss is 0.0007780049235324197\n",
      "Batch: 8400,train loss is: 0.0005745071694825273\n",
      "test loss is 0.0007593044566093547\n",
      "Batch: 8500,train loss is: 0.0009744290945264899\n",
      "test loss is 0.000777911893767545\n",
      "Batch: 8600,train loss is: 0.0007141338875161482\n",
      "test loss is 0.0007777452394932475\n",
      "Batch: 8700,train loss is: 0.0005099432307302538\n",
      "test loss is 0.0007890933776967427\n",
      "Batch: 8800,train loss is: 0.0003374417424612431\n",
      "test loss is 0.0007509585915127238\n",
      "Batch: 8900,train loss is: 0.0011036289451874307\n",
      "test loss is 0.0007516874816804304\n",
      "Batch: 9000,train loss is: 0.00029161773383412095\n",
      "test loss is 0.0007636440241844302\n",
      "Batch: 9100,train loss is: 0.00041827167336814825\n",
      "test loss is 0.0007527811451857029\n",
      "Batch: 9200,train loss is: 0.0009225600894905182\n",
      "test loss is 0.0008193813311600167\n",
      "Batch: 9300,train loss is: 0.0006094710176496386\n",
      "test loss is 0.0007644819735882693\n",
      "Batch: 9400,train loss is: 0.0004801906569707187\n",
      "test loss is 0.0007936088741873185\n",
      "Batch: 9500,train loss is: 0.00044592158982038936\n",
      "test loss is 0.0007779223596494913\n",
      "Batch: 9600,train loss is: 0.0004793119425060241\n",
      "test loss is 0.0007529377100940699\n",
      "Batch: 9700,train loss is: 0.00040358608678220815\n",
      "test loss is 0.0007764747301229333\n",
      "Batch: 9800,train loss is: 0.0004979376184146359\n",
      "test loss is 0.0007581114158950196\n",
      "Batch: 9900,train loss is: 0.000546184393182727\n",
      "test loss is 0.0007972899839878676\n",
      "Batch: 10000,train loss is: 0.0005359312381828053\n",
      "test loss is 0.0007560145743307302\n",
      "Batch: 10100,train loss is: 0.0010938546462514523\n",
      "test loss is 0.0007563451152007091\n",
      "Batch: 10200,train loss is: 0.0017308865568056982\n",
      "test loss is 0.0008605640388457674\n",
      "Batch: 10300,train loss is: 0.0005157314392232594\n",
      "test loss is 0.0007634455930396573\n",
      "Batch: 10400,train loss is: 0.00040010219187053546\n",
      "test loss is 0.0007564909255993013\n",
      "Batch: 10500,train loss is: 0.0007450031511496352\n",
      "test loss is 0.0007563695398246412\n",
      "Batch: 10600,train loss is: 0.0003801803399165615\n",
      "test loss is 0.0008052031905034051\n",
      "Batch: 10700,train loss is: 0.000640398996802722\n",
      "test loss is 0.0007575493782622951\n",
      "Batch: 10800,train loss is: 0.00053589796301315\n",
      "test loss is 0.000767261873285213\n",
      "Batch: 10900,train loss is: 0.0007026284977361746\n",
      "test loss is 0.0007611200457469982\n",
      "Batch: 11000,train loss is: 0.0007628495440046464\n",
      "test loss is 0.0007893395627471892\n",
      "Batch: 11100,train loss is: 0.0010539918478718227\n",
      "test loss is 0.0007528230015568865\n",
      "Batch: 11200,train loss is: 0.0006836271317236925\n",
      "test loss is 0.0007900325018051045\n",
      "Batch: 11300,train loss is: 0.00034382567222488553\n",
      "test loss is 0.0007899698459374637\n",
      "Batch: 11400,train loss is: 0.0005881203717799695\n",
      "test loss is 0.0007706920952882349\n",
      "Batch: 11500,train loss is: 0.0005699601869490363\n",
      "test loss is 0.0007639465544874959\n",
      "Batch: 11600,train loss is: 0.0019824868908593887\n",
      "test loss is 0.0007711576975114615\n",
      "Batch: 11700,train loss is: 0.0003210557712677651\n",
      "test loss is 0.0007417194267933245\n",
      "Batch: 11800,train loss is: 0.0006216381130843512\n",
      "test loss is 0.000757795994836232\n",
      "Batch: 11900,train loss is: 0.0006844070612672351\n",
      "test loss is 0.0007722426365238086\n",
      "Batch: 12000,train loss is: 0.0004635434852817206\n",
      "test loss is 0.000762924924065289\n",
      "Batch: 12100,train loss is: 0.001715462382560317\n",
      "test loss is 0.0007625754857980557\n",
      "Batch: 12200,train loss is: 0.0007547341024105834\n",
      "test loss is 0.0007849191289697391\n",
      "Batch: 12300,train loss is: 0.000479398023609566\n",
      "test loss is 0.0007802125902350706\n",
      "Batch: 12400,train loss is: 0.0006226097745593818\n",
      "test loss is 0.0007550284570915409\n",
      "Batch: 12500,train loss is: 0.00037422292514736465\n",
      "test loss is 0.000782167898133341\n",
      "Batch: 12600,train loss is: 0.0004285465999096107\n",
      "test loss is 0.0007720977596830472\n",
      "Batch: 12700,train loss is: 0.0005077365662485437\n",
      "test loss is 0.00074865588609857\n",
      "Batch: 12800,train loss is: 0.0004332854914098309\n",
      "test loss is 0.0007560584905401928\n",
      "Batch: 12900,train loss is: 0.0007052889421667494\n",
      "test loss is 0.0007735323478629222\n",
      "Batch: 13000,train loss is: 0.00025987822114818446\n",
      "test loss is 0.0007427731159804157\n",
      "Batch: 13100,train loss is: 0.0023188640620727197\n",
      "test loss is 0.0007491248663094554\n",
      "Batch: 13200,train loss is: 0.0007780832780217902\n",
      "test loss is 0.0008532226769709945\n",
      "Batch: 13300,train loss is: 0.0005822967893440819\n",
      "test loss is 0.0007945191729266294\n",
      "Batch: 13400,train loss is: 0.0003945448047174494\n",
      "test loss is 0.0007813016857975399\n",
      "Batch: 13500,train loss is: 0.0006482238342175005\n",
      "test loss is 0.0007890816439954693\n",
      "Batch: 13600,train loss is: 0.0005438623472140202\n",
      "test loss is 0.0007647094417181104\n",
      "Batch: 13700,train loss is: 0.0007177676365767036\n",
      "test loss is 0.0007554365771493404\n",
      "Batch: 13800,train loss is: 0.00060524011932848\n",
      "test loss is 0.000760416538071558\n",
      "Batch: 13900,train loss is: 0.0006228453496995473\n",
      "test loss is 0.0007947469673988621\n",
      "Batch: 14000,train loss is: 0.00044017771682564836\n",
      "test loss is 0.0007665212068412317\n",
      "Batch: 14100,train loss is: 0.0008234685374169249\n",
      "test loss is 0.0008068611802496479\n",
      "Batch: 14200,train loss is: 0.0005585103066393515\n",
      "test loss is 0.0007405401128893236\n",
      "Batch: 14300,train loss is: 0.00040034664824379614\n",
      "test loss is 0.0007491150006441478\n",
      "Batch: 14400,train loss is: 0.0013880423259423603\n",
      "test loss is 0.0007524819956739688\n",
      "Batch: 14500,train loss is: 0.0004375234630909441\n",
      "test loss is 0.0007558787443997209\n",
      "Batch: 14600,train loss is: 0.0016150252300223809\n",
      "test loss is 0.0007518675972914383\n",
      "Batch: 14700,train loss is: 0.00048189172864704626\n",
      "test loss is 0.0007957305120487045\n",
      "Batch: 14800,train loss is: 0.0008720534822029591\n",
      "test loss is 0.0007605319745924635\n",
      "Batch: 14900,train loss is: 0.00042290512916639335\n",
      "test loss is 0.0007850957852497147\n",
      "Batch: 15000,train loss is: 0.0008415436908151726\n",
      "test loss is 0.0007474406792386542\n",
      "Batch: 15100,train loss is: 0.0007001947623799246\n",
      "test loss is 0.0007792029386391798\n",
      "Batch: 15200,train loss is: 0.0011897515415797318\n",
      "test loss is 0.0007662215786993316\n",
      "Batch: 15300,train loss is: 0.0004903025837075001\n",
      "test loss is 0.0007480955211462854\n",
      "Batch: 15400,train loss is: 0.0005046054429824006\n",
      "test loss is 0.0007748549011257301\n",
      "Batch: 15500,train loss is: 0.00082722276277048\n",
      "test loss is 0.0007423815894775491\n",
      "Batch: 15600,train loss is: 0.0009876732272494618\n",
      "test loss is 0.0007898836419252885\n",
      "Batch: 15700,train loss is: 0.0005861304933921392\n",
      "test loss is 0.0007537616409328039\n",
      "Batch: 15800,train loss is: 0.0007081756835829852\n",
      "test loss is 0.0007492397363995457\n",
      "Batch: 15900,train loss is: 0.0011000506021558688\n",
      "test loss is 0.0007698560349481126\n",
      "Batch: 16000,train loss is: 0.00041096084478423923\n",
      "test loss is 0.0007757967728493632\n",
      "Batch: 16100,train loss is: 0.0005877405146034961\n",
      "test loss is 0.0007593897157532215\n",
      "Batch: 16200,train loss is: 0.00042589545531833623\n",
      "test loss is 0.0007573908190015092\n",
      "Batch: 16300,train loss is: 0.0011549773460997799\n",
      "test loss is 0.0007746984439824809\n",
      "Batch: 16400,train loss is: 0.0004441109793938185\n",
      "test loss is 0.0007451081735712336\n",
      "Batch: 16500,train loss is: 0.0006082539391417369\n",
      "test loss is 0.0007880909657018444\n",
      "Batch: 16600,train loss is: 0.000322493058568474\n",
      "test loss is 0.0007510031346753226\n",
      "Batch: 16700,train loss is: 0.0010233601986000232\n",
      "test loss is 0.0007672893985131331\n",
      "Batch: 16800,train loss is: 0.0006023620025303762\n",
      "test loss is 0.0007781315589392069\n",
      "Batch: 16900,train loss is: 0.0007429240497819642\n",
      "test loss is 0.00076654673747001\n",
      "Batch: 17000,train loss is: 0.0008001159208850444\n",
      "test loss is 0.0007414320783556864\n",
      "Batch: 17100,train loss is: 0.0005970233195831787\n",
      "test loss is 0.0007539646884759325\n",
      "Batch: 17200,train loss is: 0.0005661685675848873\n",
      "test loss is 0.0007599203660644668\n",
      "Batch: 17300,train loss is: 0.00089458166606271\n",
      "test loss is 0.000771198703529519\n",
      "Batch: 17400,train loss is: 0.0004558263031430899\n",
      "test loss is 0.000756498774197477\n",
      "Batch: 17500,train loss is: 0.0005064086273218973\n",
      "test loss is 0.000753297451451279\n",
      "Batch: 17600,train loss is: 0.00042565692499099255\n",
      "test loss is 0.0007759118691505391\n",
      "Batch: 17700,train loss is: 0.0003451076875201742\n",
      "test loss is 0.0007876819665992212\n",
      "Batch: 17800,train loss is: 0.0012081880622864304\n",
      "test loss is 0.0007611203928044109\n",
      "Batch: 17900,train loss is: 0.00047027040160753803\n",
      "test loss is 0.0007923714760920067\n",
      "Batch: 18000,train loss is: 0.00038012401936257477\n",
      "test loss is 0.0007425879624795018\n",
      "Batch: 18100,train loss is: 0.00048353958515112385\n",
      "test loss is 0.000754480042125979\n",
      "Batch: 18200,train loss is: 0.0005885591623882934\n",
      "test loss is 0.0007591822531113512\n",
      "Batch: 18300,train loss is: 0.0009798670053429187\n",
      "test loss is 0.000755084222546795\n",
      "Batch: 18400,train loss is: 0.000504384506304769\n",
      "test loss is 0.0007525021607142017\n",
      "Batch: 18500,train loss is: 0.0008653032075580283\n",
      "test loss is 0.0007678447984169921\n",
      "Batch: 18600,train loss is: 0.0005161602109520329\n",
      "test loss is 0.0007516971038481224\n",
      "Batch: 18700,train loss is: 0.0005626714102615354\n",
      "test loss is 0.0007610091016246849\n",
      "Batch: 18800,train loss is: 0.0006613604824886515\n",
      "test loss is 0.0007533789934752412\n",
      "Batch: 18900,train loss is: 0.0007356328793861478\n",
      "test loss is 0.0007930167983969387\n",
      "Batch: 19000,train loss is: 0.000685326425118389\n",
      "test loss is 0.0007578450229597757\n",
      "Batch: 19100,train loss is: 0.00030756160993155647\n",
      "test loss is 0.000777589227692571\n",
      "Batch: 19200,train loss is: 0.0006038076829996207\n",
      "test loss is 0.0007615497043277211\n",
      "Batch: 19300,train loss is: 0.000879151936555431\n",
      "test loss is 0.0007467891898580761\n",
      "Batch: 19400,train loss is: 0.0003180925623094992\n",
      "test loss is 0.0007434416697556372\n",
      "Batch: 19500,train loss is: 0.000589392924210192\n",
      "test loss is 0.0007929626809224568\n",
      "Batch: 19600,train loss is: 0.00048761729302113674\n",
      "test loss is 0.0007689818945020212\n",
      "Batch: 19700,train loss is: 0.0008204018139365315\n",
      "test loss is 0.00082587660265408\n",
      "Batch: 19800,train loss is: 0.0005717451827066238\n",
      "test loss is 0.00076067292157641\n",
      "Batch: 19900,train loss is: 0.0002581334035138933\n",
      "test loss is 0.0007960516098951864\n",
      "Batch: 20000,train loss is: 0.0008061749539912784\n",
      "test loss is 0.0007596682066099388\n",
      "Batch: 20100,train loss is: 0.000549441479483868\n",
      "test loss is 0.0007623823872211125\n",
      "Batch: 20200,train loss is: 0.0009839568371780676\n",
      "test loss is 0.0007743058002465382\n",
      "Batch: 20300,train loss is: 0.0003943086341293824\n",
      "test loss is 0.0007773515114066341\n",
      "Batch: 20400,train loss is: 0.0005114308453508391\n",
      "test loss is 0.0007994642094964577\n",
      "Batch: 20500,train loss is: 0.0006278536805456214\n",
      "test loss is 0.0007611385216975776\n",
      "Batch: 20600,train loss is: 0.0006096908542959005\n",
      "test loss is 0.0007918985162237216\n",
      "Batch: 20700,train loss is: 0.0006362112715688574\n",
      "test loss is 0.0007669654777067482\n",
      "Batch: 20800,train loss is: 0.0003680573694406991\n",
      "test loss is 0.0007648725549171041\n",
      "Batch: 20900,train loss is: 0.0006982280588435811\n",
      "test loss is 0.0008016963796789616\n",
      "Batch: 21000,train loss is: 0.00031787004575666136\n",
      "test loss is 0.0007471067230976468\n",
      "Batch: 21100,train loss is: 0.000681452378000803\n",
      "test loss is 0.0007794620970256186\n",
      "Batch: 21200,train loss is: 0.0003319386378008419\n",
      "test loss is 0.0007702152117199987\n",
      "Batch: 21300,train loss is: 0.000383979705256565\n",
      "test loss is 0.000752631104937883\n",
      "Batch: 21400,train loss is: 0.0008837041762548017\n",
      "test loss is 0.0007654881397999653\n",
      "Batch: 21500,train loss is: 0.00024748098801948037\n",
      "test loss is 0.000781571951368006\n",
      "Batch: 21600,train loss is: 0.0007767507690200979\n",
      "test loss is 0.0007697793354849364\n",
      "Batch: 21700,train loss is: 0.00032849238586183613\n",
      "test loss is 0.000750778834332515\n",
      "Batch: 21800,train loss is: 0.0006378010381726753\n",
      "test loss is 0.0008086792119226683\n",
      "Batch: 21900,train loss is: 0.0005579773073929056\n",
      "test loss is 0.0008388432625954402\n",
      "Batch: 22000,train loss is: 0.0007344704886381645\n",
      "test loss is 0.0007546991212949569\n",
      "Batch: 22100,train loss is: 0.0003495590813569752\n",
      "test loss is 0.000758126364166924\n",
      "Batch: 22200,train loss is: 0.000535461719970566\n",
      "test loss is 0.0007570475523803685\n",
      "Batch: 22300,train loss is: 0.0007307651821455006\n",
      "test loss is 0.0007586769609659557\n",
      "Batch: 22400,train loss is: 0.000551876757026299\n",
      "test loss is 0.0007520150776664954\n",
      "Batch: 22500,train loss is: 0.00045025332694639776\n",
      "test loss is 0.00074744511009452\n",
      "Batch: 22600,train loss is: 0.001085177312321685\n",
      "test loss is 0.0008076350777777136\n",
      "Batch: 22700,train loss is: 0.00043924392263043795\n",
      "test loss is 0.0007547267630824431\n",
      "Batch: 22800,train loss is: 0.0005483949953316578\n",
      "test loss is 0.0007727182740751907\n",
      "Batch: 22900,train loss is: 0.0003685882564756949\n",
      "test loss is 0.0008194861837657321\n",
      "Batch: 23000,train loss is: 0.0004223097711748662\n",
      "test loss is 0.0007536448384124564\n",
      "Batch: 23100,train loss is: 0.0005652497483843939\n",
      "test loss is 0.000740054545876054\n",
      "Batch: 23200,train loss is: 0.0003618568365464842\n",
      "test loss is 0.0007346500577958965\n",
      "Batch: 23300,train loss is: 0.0005517227718001318\n",
      "test loss is 0.0007461471054015112\n",
      "Batch: 23400,train loss is: 0.0007128923770729217\n",
      "test loss is 0.0007744548399462159\n",
      "Batch: 23500,train loss is: 0.0024885748158389527\n",
      "test loss is 0.0007517685791120661\n",
      "Batch: 23600,train loss is: 0.00030467173129252844\n",
      "test loss is 0.0007434470265612306\n",
      "Batch: 23700,train loss is: 0.0008904000085987996\n",
      "test loss is 0.0007696582100249494\n",
      "Batch: 23800,train loss is: 0.0009118827166944357\n",
      "test loss is 0.0007475559974034372\n",
      "Batch: 23900,train loss is: 0.0009184080897760259\n",
      "test loss is 0.0007557923171920351\n",
      "Batch: 24000,train loss is: 0.0008064232479672537\n",
      "test loss is 0.0007982753790238576\n",
      "Batch: 24100,train loss is: 0.00043062420816958856\n",
      "test loss is 0.0007409064777283374\n",
      "Batch: 24200,train loss is: 0.0016771077227134639\n",
      "test loss is 0.0007686149253367384\n",
      "Batch: 24300,train loss is: 0.0004853600284285391\n",
      "test loss is 0.0008927752489907414\n",
      "Batch: 24400,train loss is: 0.0013855612408050588\n",
      "test loss is 0.0007517100528254911\n",
      "Batch: 24500,train loss is: 0.0006587136810181459\n",
      "test loss is 0.0007472411856888468\n",
      "Batch: 24600,train loss is: 0.0005491267858434781\n",
      "test loss is 0.0007844524784214676\n",
      "Batch: 24700,train loss is: 0.0003335829145415109\n",
      "test loss is 0.0007528103314400875\n",
      "Batch: 24800,train loss is: 0.0009877317929619044\n",
      "test loss is 0.0007520242002894481\n",
      "Batch: 24900,train loss is: 0.00018056921448833885\n",
      "test loss is 0.0007563955715832257\n",
      "Batch: 25000,train loss is: 0.0008051803824257088\n",
      "test loss is 0.0007486295656866244\n",
      "Batch: 25100,train loss is: 0.0006350166461453401\n",
      "test loss is 0.000762952126483682\n",
      "Batch: 25200,train loss is: 0.0007753691399666852\n",
      "test loss is 0.0007671218061454537\n",
      "Batch: 25300,train loss is: 0.0008634748000882509\n",
      "test loss is 0.0007802042482538248\n",
      "Batch: 25400,train loss is: 0.0004177028959384437\n",
      "test loss is 0.0007444549786109225\n",
      "Batch: 25500,train loss is: 0.0005744248881508376\n",
      "test loss is 0.0007508776123652899\n",
      "Batch: 25600,train loss is: 0.0007022101403892658\n",
      "test loss is 0.0007551644926893508\n",
      "Batch: 25700,train loss is: 0.0006136453541960094\n",
      "test loss is 0.0007724029777240964\n",
      "Batch: 25800,train loss is: 0.0003455999482165157\n",
      "test loss is 0.0007470404955961438\n",
      "Batch: 25900,train loss is: 0.0003450879291235891\n",
      "test loss is 0.0007933009896932211\n",
      "Batch: 26000,train loss is: 0.0017456607583585615\n",
      "test loss is 0.0008127941344316655\n",
      "Batch: 26100,train loss is: 0.0009365799182993753\n",
      "test loss is 0.0007456627370547134\n",
      "Batch: 26200,train loss is: 0.0007526605953097321\n",
      "test loss is 0.0007385077467685646\n",
      "Batch: 26300,train loss is: 0.0012113435682327921\n",
      "test loss is 0.0007699027605782379\n",
      "Batch: 26400,train loss is: 0.0006495557739715949\n",
      "test loss is 0.0007537962844902389\n",
      "Batch: 26500,train loss is: 0.00035756716871384334\n",
      "test loss is 0.0007571799608181302\n",
      "Batch: 26600,train loss is: 0.0013153538051308242\n",
      "test loss is 0.0008306758529008401\n",
      "Batch: 26700,train loss is: 0.00044262927816295994\n",
      "test loss is 0.0008653590456974727\n",
      "Batch: 26800,train loss is: 0.0012243497177083598\n",
      "test loss is 0.0007547856829877706\n",
      "Batch: 26900,train loss is: 0.0005162906166300738\n",
      "test loss is 0.000752521342716972\n",
      "Batch: 27000,train loss is: 0.000674071307390096\n",
      "test loss is 0.0008840702564223915\n",
      "Batch: 27100,train loss is: 0.0005353376216255264\n",
      "test loss is 0.0008002778708023441\n",
      "Batch: 27200,train loss is: 0.0006928868402718132\n",
      "test loss is 0.0007935698308139443\n",
      "Batch: 27300,train loss is: 0.0005924245585292374\n",
      "test loss is 0.0007556865018551112\n",
      "Batch: 27400,train loss is: 0.0009286307562744875\n",
      "test loss is 0.0007460158944410292\n",
      "Batch: 27500,train loss is: 0.0005595229279194486\n",
      "test loss is 0.0008324781652391358\n",
      "Batch: 27600,train loss is: 0.0006349991942382268\n",
      "test loss is 0.0008036546720426248\n",
      "Batch: 27700,train loss is: 0.0017688428780625806\n",
      "test loss is 0.000762573033466353\n",
      "Batch: 27800,train loss is: 0.0007558490126437125\n",
      "test loss is 0.0007814477589389127\n",
      "Batch: 27900,train loss is: 0.0013342781474502442\n",
      "test loss is 0.000749686494189217\n",
      "Batch: 28000,train loss is: 0.000441235667660236\n",
      "test loss is 0.0007551735719886194\n",
      "Batch: 28100,train loss is: 0.000887803420331053\n",
      "test loss is 0.0007742920319372388\n",
      "Batch: 28200,train loss is: 0.0006712307485367436\n",
      "test loss is 0.0007334371872354568\n",
      "Batch: 28300,train loss is: 0.0020796494962753726\n",
      "test loss is 0.0007530992761766211\n",
      "Batch: 28400,train loss is: 0.0005971625354296661\n",
      "test loss is 0.0007443351248933876\n",
      "Batch: 28500,train loss is: 0.0014477356709009257\n",
      "test loss is 0.0007737808898012773\n",
      "Batch: 28600,train loss is: 0.00039893320467565\n",
      "test loss is 0.0007664053065232777\n",
      "Batch: 28700,train loss is: 0.00048618060353712025\n",
      "test loss is 0.0007483742714182707\n",
      "Batch: 28800,train loss is: 0.0011677286108727912\n",
      "test loss is 0.0007548297161596972\n",
      "Batch: 28900,train loss is: 0.0005098289656708868\n",
      "test loss is 0.0007471838727781645\n",
      "Batch: 29000,train loss is: 0.0005685407121778021\n",
      "test loss is 0.0007409698717562463\n",
      "Batch: 29100,train loss is: 0.0006902995651139612\n",
      "test loss is 0.0007609060733493909\n",
      "Batch: 29200,train loss is: 0.0004688046916104494\n",
      "test loss is 0.0007652492322125177\n",
      "Batch: 29300,train loss is: 0.0010564601822196365\n",
      "test loss is 0.0007455703547558559\n",
      "Batch: 29400,train loss is: 0.0006841510090900022\n",
      "test loss is 0.0008134522103918423\n",
      "Batch: 29500,train loss is: 0.0004692918725619031\n",
      "test loss is 0.000764849783127302\n",
      "Batch: 29600,train loss is: 0.0005481727830341756\n",
      "test loss is 0.0007505220342870939\n",
      "Batch: 29700,train loss is: 0.00036306997541331955\n",
      "test loss is 0.0007665580004168481\n",
      "Batch: 29800,train loss is: 0.0008641433518446411\n",
      "test loss is 0.0007413288203508141\n",
      "Batch: 29900,train loss is: 0.0033224145623158585\n",
      "test loss is 0.0007514912372206221\n",
      "Batch: 30000,train loss is: 0.00033174568348451366\n",
      "test loss is 0.0008020959612315856\n",
      "Batch: 30100,train loss is: 0.0005048612904446363\n",
      "test loss is 0.0007869632846170305\n",
      "Batch: 30200,train loss is: 0.0003719556697903681\n",
      "test loss is 0.0007494567581674578\n",
      "Batch: 30300,train loss is: 0.0003254657419923365\n",
      "test loss is 0.0007403894653770459\n",
      "Batch: 30400,train loss is: 0.00040263567518606345\n",
      "test loss is 0.0007517290551260258\n",
      "Batch: 30500,train loss is: 0.00041142624714229236\n",
      "test loss is 0.0008035877805159537\n",
      "Batch: 30600,train loss is: 0.0002717291046169343\n",
      "test loss is 0.0007477882482095355\n",
      "Batch: 30700,train loss is: 0.0004578357888262857\n",
      "test loss is 0.0008834378730684483\n",
      "Batch: 30800,train loss is: 0.0002890412725593739\n",
      "test loss is 0.0008169465912806331\n",
      "Batch: 30900,train loss is: 0.00038147632716124705\n",
      "test loss is 0.0007500383702825033\n",
      "Batch: 31000,train loss is: 0.0005626076842941188\n",
      "test loss is 0.0007615700809167988\n",
      "Batch: 31100,train loss is: 0.00041750976620074526\n",
      "test loss is 0.0008280505934913449\n",
      "Batch: 31200,train loss is: 0.0006196530004932868\n",
      "test loss is 0.0007614516992827412\n",
      "Batch: 31300,train loss is: 0.0006408053661851439\n",
      "test loss is 0.0007655060551085646\n",
      "Batch: 31400,train loss is: 0.0008218948893580733\n",
      "test loss is 0.0007509251506759547\n",
      "Batch: 31500,train loss is: 0.0005655476449780955\n",
      "test loss is 0.0007605630244001854\n",
      "Batch: 31600,train loss is: 0.00051976506647053\n",
      "test loss is 0.0007572421708199803\n",
      "Batch: 31700,train loss is: 0.0005764753812137338\n",
      "test loss is 0.0007848909991591889\n",
      "Batch: 31800,train loss is: 0.0012492875787070542\n",
      "test loss is 0.0007518619747348739\n",
      "Batch: 31900,train loss is: 0.0009281510606016322\n",
      "test loss is 0.0007806621912222392\n",
      "Batch: 32000,train loss is: 0.004339225178714139\n",
      "test loss is 0.0007498349309267344\n",
      "Batch: 32100,train loss is: 0.000474173919798769\n",
      "test loss is 0.0007478061013287724\n",
      "Batch: 32200,train loss is: 0.000443037055544921\n",
      "test loss is 0.0007613787941532453\n",
      "Batch: 32300,train loss is: 0.0003845539129784941\n",
      "test loss is 0.0007512850558494866\n",
      "Batch: 32400,train loss is: 0.0004580161635455779\n",
      "test loss is 0.0007365980211414833\n",
      "Batch: 32500,train loss is: 0.0010631470403861649\n",
      "test loss is 0.0007801225357831901\n",
      "Batch: 32600,train loss is: 0.00046929975951140934\n",
      "test loss is 0.0007595155249201042\n",
      "Batch: 32700,train loss is: 0.0004693461000384744\n",
      "test loss is 0.0007576459287889421\n",
      "Batch: 32800,train loss is: 0.0004097711638470829\n",
      "test loss is 0.0007710045521676604\n",
      "Batch: 32900,train loss is: 0.0006608060624679185\n",
      "test loss is 0.0007425269286041427\n",
      "Batch: 33000,train loss is: 0.0005464712901776071\n",
      "test loss is 0.0007377014103565056\n",
      "Batch: 33100,train loss is: 0.0004772273060609434\n",
      "test loss is 0.0007775167088463842\n",
      "Batch: 33200,train loss is: 0.000563464209163385\n",
      "test loss is 0.0008456243088524743\n",
      "Batch: 33300,train loss is: 0.0003922286278442013\n",
      "test loss is 0.00073597832168554\n",
      "Batch: 33400,train loss is: 0.0005426870688134963\n",
      "test loss is 0.0007607421521838707\n",
      "Batch: 33500,train loss is: 0.0004565441027841624\n",
      "test loss is 0.0007497174925114063\n",
      "Batch: 33600,train loss is: 0.0006065008645632746\n",
      "test loss is 0.0007491604221190962\n",
      "Batch: 33700,train loss is: 0.0006647344243957918\n",
      "test loss is 0.0007626306356306482\n",
      "Batch: 33800,train loss is: 0.0008251024884531103\n",
      "test loss is 0.0007414468797397756\n",
      "Batch: 33900,train loss is: 0.0007131417553913703\n",
      "test loss is 0.0007492843035173009\n",
      "-----------------------Epoch: 7----------------------------------\n",
      "Batch: 0,train loss is: 0.00039137844231633206\n",
      "test loss is 0.0007570205536695219\n",
      "Batch: 100,train loss is: 0.001478908340344151\n",
      "test loss is 0.0007520819498806155\n",
      "Batch: 200,train loss is: 0.0006610132125553219\n",
      "test loss is 0.0007562570691055375\n",
      "Batch: 300,train loss is: 0.0003327643772852051\n",
      "test loss is 0.0007496316314078928\n",
      "Batch: 400,train loss is: 0.0005313013383894754\n",
      "test loss is 0.0007448413861823523\n",
      "Batch: 500,train loss is: 0.0006458740172618434\n",
      "test loss is 0.0007807887494059576\n",
      "Batch: 600,train loss is: 0.00025840090602263755\n",
      "test loss is 0.0007791753465980526\n",
      "Batch: 700,train loss is: 0.0003240581467009213\n",
      "test loss is 0.0007453852767926129\n",
      "Batch: 800,train loss is: 0.000473005373146384\n",
      "test loss is 0.0007464954451858525\n",
      "Batch: 900,train loss is: 0.0003489931891433437\n",
      "test loss is 0.0007423768711161505\n",
      "Batch: 1000,train loss is: 0.0005310011447976141\n",
      "test loss is 0.0007811293437884438\n",
      "Batch: 1100,train loss is: 0.001829380481496627\n",
      "test loss is 0.0007404759361248306\n",
      "Batch: 1200,train loss is: 0.00037660620531513347\n",
      "test loss is 0.0007487241824272931\n",
      "Batch: 1300,train loss is: 0.0005393929959652467\n",
      "test loss is 0.0007445292273286845\n",
      "Batch: 1400,train loss is: 0.0005296228688994631\n",
      "test loss is 0.0007621489765636608\n",
      "Batch: 1500,train loss is: 0.000519088204274883\n",
      "test loss is 0.0007716226928676072\n",
      "Batch: 1600,train loss is: 0.0006192239264305003\n",
      "test loss is 0.0008276092776887277\n",
      "Batch: 1700,train loss is: 0.0013174689092644533\n",
      "test loss is 0.0007372278206192681\n",
      "Batch: 1800,train loss is: 0.0004961407114793315\n",
      "test loss is 0.0007606298114983048\n",
      "Batch: 1900,train loss is: 0.00039828902361040917\n",
      "test loss is 0.0007851722035799799\n",
      "Batch: 2000,train loss is: 0.0006433918215676438\n",
      "test loss is 0.000870915828982266\n",
      "Batch: 2100,train loss is: 0.0006684270157308645\n",
      "test loss is 0.0008815774681312508\n",
      "Batch: 2200,train loss is: 0.00091535986839062\n",
      "test loss is 0.0007500388662513332\n",
      "Batch: 2300,train loss is: 0.00042726831338096936\n",
      "test loss is 0.0007876731027156856\n",
      "Batch: 2400,train loss is: 0.0006013412847527811\n",
      "test loss is 0.0007512746273130149\n",
      "Batch: 2500,train loss is: 0.0007382408695511287\n",
      "test loss is 0.0007438146103135438\n",
      "Batch: 2600,train loss is: 0.0003927120335662426\n",
      "test loss is 0.0007653539650210288\n",
      "Batch: 2700,train loss is: 0.0006930549561775086\n",
      "test loss is 0.0007349600751518768\n",
      "Batch: 2800,train loss is: 0.0004982222536414673\n",
      "test loss is 0.0007837310543355698\n",
      "Batch: 2900,train loss is: 0.0005258046049343747\n",
      "test loss is 0.0007357404011008638\n",
      "Batch: 3000,train loss is: 0.0005149564767922336\n",
      "test loss is 0.0007351810423230625\n",
      "Batch: 3100,train loss is: 0.0006757934830419045\n",
      "test loss is 0.0007915319886977541\n",
      "Batch: 3200,train loss is: 0.0009896866541062234\n",
      "test loss is 0.0007581769418068596\n",
      "Batch: 3300,train loss is: 0.0004584348455244278\n",
      "test loss is 0.0007485485429099879\n",
      "Batch: 3400,train loss is: 0.0004869643519858822\n",
      "test loss is 0.0007485592877512925\n",
      "Batch: 3500,train loss is: 0.0002586044393376874\n",
      "test loss is 0.0007516372889745335\n",
      "Batch: 3600,train loss is: 0.0005373172305371977\n",
      "test loss is 0.0007838392112255277\n",
      "Batch: 3700,train loss is: 0.0004411251074437395\n",
      "test loss is 0.0007359893024005899\n",
      "Batch: 3800,train loss is: 0.0004529573572682847\n",
      "test loss is 0.0007766717641836363\n",
      "Batch: 3900,train loss is: 0.0006592658027531747\n",
      "test loss is 0.0007463111684807344\n",
      "Batch: 4000,train loss is: 0.00043634063820873186\n",
      "test loss is 0.0007514918092037132\n",
      "Batch: 4100,train loss is: 0.0007334387950986804\n",
      "test loss is 0.0007654156363023111\n",
      "Batch: 4200,train loss is: 0.0003755333691126475\n",
      "test loss is 0.0007805346929668985\n",
      "Batch: 4300,train loss is: 0.0004288692436197205\n",
      "test loss is 0.0007416347280965052\n",
      "Batch: 4400,train loss is: 0.00047853589486468044\n",
      "test loss is 0.0007451219335991691\n",
      "Batch: 4500,train loss is: 0.0006327750993552708\n",
      "test loss is 0.0007742424400025969\n",
      "Batch: 4600,train loss is: 0.0006057785967311081\n",
      "test loss is 0.0007458712839437502\n",
      "Batch: 4700,train loss is: 0.0007073559017218396\n",
      "test loss is 0.0007796820389831033\n",
      "Batch: 4800,train loss is: 0.0012282612616065058\n",
      "test loss is 0.0007512588591534125\n",
      "Batch: 4900,train loss is: 0.0005770723649240593\n",
      "test loss is 0.0007500356499813438\n",
      "Batch: 5000,train loss is: 0.0006078144032488572\n",
      "test loss is 0.0007573198911410844\n",
      "Batch: 5100,train loss is: 0.0006439974085897592\n",
      "test loss is 0.0008149328828965879\n",
      "Batch: 5200,train loss is: 0.0003584509159765518\n",
      "test loss is 0.0007529168055908685\n",
      "Batch: 5300,train loss is: 0.0006949083752677209\n",
      "test loss is 0.0007768199175109017\n",
      "Batch: 5400,train loss is: 0.0007920190228528373\n",
      "test loss is 0.0007409675392089422\n",
      "Batch: 5500,train loss is: 0.0004937330281418334\n",
      "test loss is 0.0007638662976173876\n",
      "Batch: 5600,train loss is: 0.0006626479104781878\n",
      "test loss is 0.0007609103812483319\n",
      "Batch: 5700,train loss is: 0.0007534556171888545\n",
      "test loss is 0.0007464595793176369\n",
      "Batch: 5800,train loss is: 0.0005339129940095388\n",
      "test loss is 0.0007467055205077388\n",
      "Batch: 5900,train loss is: 0.0034564063591072973\n",
      "test loss is 0.0007526648380827375\n",
      "Batch: 6000,train loss is: 0.0016722883765910126\n",
      "test loss is 0.0007624776164154917\n",
      "Batch: 6100,train loss is: 0.00046649625969727155\n",
      "test loss is 0.0007363151979522984\n",
      "Batch: 6200,train loss is: 0.0006066493432988378\n",
      "test loss is 0.0007511425018738351\n",
      "Batch: 6300,train loss is: 0.001300178180372016\n",
      "test loss is 0.0007651298559114499\n",
      "Batch: 6400,train loss is: 0.0004769926749317662\n",
      "test loss is 0.0008010611133519619\n",
      "Batch: 6500,train loss is: 0.0003904958350273055\n",
      "test loss is 0.000772352753937405\n",
      "Batch: 6600,train loss is: 0.0022041739772441934\n",
      "test loss is 0.0007475511885387411\n",
      "Batch: 6700,train loss is: 0.0004330504113020901\n",
      "test loss is 0.0007683932580324907\n",
      "Batch: 6800,train loss is: 0.0009134917809568681\n",
      "test loss is 0.0007437676406555788\n",
      "Batch: 6900,train loss is: 0.0006362025441855242\n",
      "test loss is 0.0007489358558162675\n",
      "Batch: 7000,train loss is: 0.0005986493036924232\n",
      "test loss is 0.0007769097269285046\n",
      "Batch: 7100,train loss is: 0.00105590793559496\n",
      "test loss is 0.0007571482268040513\n",
      "Batch: 7200,train loss is: 0.000413702810315463\n",
      "test loss is 0.0007632429583494999\n",
      "Batch: 7300,train loss is: 0.0008512463279102537\n",
      "test loss is 0.0008059406030878566\n",
      "Batch: 7400,train loss is: 0.0003945326982606044\n",
      "test loss is 0.0007436084350475668\n",
      "Batch: 7500,train loss is: 0.0007136747865230784\n",
      "test loss is 0.0007637445350560496\n",
      "Batch: 7600,train loss is: 0.0004593512195123931\n",
      "test loss is 0.0007377908680223465\n",
      "Batch: 7700,train loss is: 0.0007323542059358228\n",
      "test loss is 0.0007459918262379481\n",
      "Batch: 7800,train loss is: 0.0003090924859225659\n",
      "test loss is 0.0007420250167308457\n",
      "Batch: 7900,train loss is: 0.000630692010268219\n",
      "test loss is 0.0007537904486060157\n",
      "Batch: 8000,train loss is: 0.00048282584724449617\n",
      "test loss is 0.0007785826589739688\n",
      "Batch: 8100,train loss is: 0.0005971208633362403\n",
      "test loss is 0.000762454793308572\n",
      "Batch: 8200,train loss is: 0.0012953281574379185\n",
      "test loss is 0.0007649307630075094\n",
      "Batch: 8300,train loss is: 0.0005794483637353677\n",
      "test loss is 0.0007668551160125501\n",
      "Batch: 8400,train loss is: 0.0005749573155851576\n",
      "test loss is 0.0007479343676844124\n",
      "Batch: 8500,train loss is: 0.0009232313950511002\n",
      "test loss is 0.0007665105350424058\n",
      "Batch: 8600,train loss is: 0.0007105953157291698\n",
      "test loss is 0.0007681924773371406\n",
      "Batch: 8700,train loss is: 0.0005141881084117001\n",
      "test loss is 0.0007804753969094377\n",
      "Batch: 8800,train loss is: 0.00032849156270875707\n",
      "test loss is 0.000739632927080756\n",
      "Batch: 8900,train loss is: 0.001100098189080506\n",
      "test loss is 0.0007410665206005978\n",
      "Batch: 9000,train loss is: 0.00029397171179874784\n",
      "test loss is 0.0007531780705927105\n",
      "Batch: 9100,train loss is: 0.00040573197304201393\n",
      "test loss is 0.0007413707428949675\n",
      "Batch: 9200,train loss is: 0.0008851382672185887\n",
      "test loss is 0.0008108705052413486\n",
      "Batch: 9300,train loss is: 0.0006048045945355316\n",
      "test loss is 0.0007569723806380918\n",
      "Batch: 9400,train loss is: 0.0004720619742810699\n",
      "test loss is 0.0007804730196731\n",
      "Batch: 9500,train loss is: 0.0004376334014763898\n",
      "test loss is 0.0007678457733611023\n",
      "Batch: 9600,train loss is: 0.00047063719560053836\n",
      "test loss is 0.0007415285878564538\n",
      "Batch: 9700,train loss is: 0.00038415894251918683\n",
      "test loss is 0.0007662746875669403\n",
      "Batch: 9800,train loss is: 0.0004665568432387478\n",
      "test loss is 0.0007463041723264674\n",
      "Batch: 9900,train loss is: 0.0005409912325280149\n",
      "test loss is 0.0007845727416230239\n",
      "Batch: 10000,train loss is: 0.0005135749750962439\n",
      "test loss is 0.0007444623713650789\n",
      "Batch: 10100,train loss is: 0.0010934400932989853\n",
      "test loss is 0.0007470714147936878\n",
      "Batch: 10200,train loss is: 0.001731051765037804\n",
      "test loss is 0.0008450292802721603\n",
      "Batch: 10300,train loss is: 0.0005083661840520877\n",
      "test loss is 0.0007524586017035668\n",
      "Batch: 10400,train loss is: 0.00038224943986640223\n",
      "test loss is 0.0007455236286818223\n",
      "Batch: 10500,train loss is: 0.000749911653939532\n",
      "test loss is 0.0007454014215904043\n",
      "Batch: 10600,train loss is: 0.0003780824528133038\n",
      "test loss is 0.0007973503762989908\n",
      "Batch: 10700,train loss is: 0.0006462879789090332\n",
      "test loss is 0.0007463378256334922\n",
      "Batch: 10800,train loss is: 0.0005369892024515288\n",
      "test loss is 0.0007560332937250432\n",
      "Batch: 10900,train loss is: 0.0007111994791857203\n",
      "test loss is 0.0007509297477850128\n",
      "Batch: 11000,train loss is: 0.0007829341363619418\n",
      "test loss is 0.000777847007356141\n",
      "Batch: 11100,train loss is: 0.0010659795003941097\n",
      "test loss is 0.0007418860358569415\n",
      "Batch: 11200,train loss is: 0.0006798207890915512\n",
      "test loss is 0.0007807821874593548\n",
      "Batch: 11300,train loss is: 0.00033940639208603426\n",
      "test loss is 0.0007744344472059772\n",
      "Batch: 11400,train loss is: 0.0005838562701966125\n",
      "test loss is 0.0007587437455034215\n",
      "Batch: 11500,train loss is: 0.0005598005216866889\n",
      "test loss is 0.0007530031289612087\n",
      "Batch: 11600,train loss is: 0.0019676216092670866\n",
      "test loss is 0.0007586773158407929\n",
      "Batch: 11700,train loss is: 0.0003109514554130876\n",
      "test loss is 0.0007310479732536971\n",
      "Batch: 11800,train loss is: 0.0006238401500688161\n",
      "test loss is 0.0007464504405861652\n",
      "Batch: 11900,train loss is: 0.0006687639060651334\n",
      "test loss is 0.0007612495609797842\n",
      "Batch: 12000,train loss is: 0.000446668329579668\n",
      "test loss is 0.0007503535209484491\n",
      "Batch: 12100,train loss is: 0.00167605505121354\n",
      "test loss is 0.0007516977990432662\n",
      "Batch: 12200,train loss is: 0.0007423430759401745\n",
      "test loss is 0.0007736809557023978\n",
      "Batch: 12300,train loss is: 0.0004762003894621565\n",
      "test loss is 0.0007693012894823591\n",
      "Batch: 12400,train loss is: 0.000590449862626268\n",
      "test loss is 0.00074548389729614\n",
      "Batch: 12500,train loss is: 0.00038642605908020356\n",
      "test loss is 0.0007711664905928796\n",
      "Batch: 12600,train loss is: 0.00043548512312390045\n",
      "test loss is 0.0007621234104578689\n",
      "Batch: 12700,train loss is: 0.0004993332264796206\n",
      "test loss is 0.0007377071251151958\n",
      "Batch: 12800,train loss is: 0.0004234402001783269\n",
      "test loss is 0.0007447469513950911\n",
      "Batch: 12900,train loss is: 0.0006980706499429622\n",
      "test loss is 0.0007619267415638336\n",
      "Batch: 13000,train loss is: 0.00025372419856158843\n",
      "test loss is 0.0007321002980716539\n",
      "Batch: 13100,train loss is: 0.002290017432392342\n",
      "test loss is 0.0007378664711693991\n",
      "Batch: 13200,train loss is: 0.0007838282606935059\n",
      "test loss is 0.0008479121889775161\n",
      "Batch: 13300,train loss is: 0.0005831087251136718\n",
      "test loss is 0.0007795763784898359\n",
      "Batch: 13400,train loss is: 0.00040151245963109867\n",
      "test loss is 0.0007720044965791716\n",
      "Batch: 13500,train loss is: 0.0006431800275148894\n",
      "test loss is 0.0007784166625651501\n",
      "Batch: 13600,train loss is: 0.0005399678773368085\n",
      "test loss is 0.0007552163955251325\n",
      "Batch: 13700,train loss is: 0.0007132880990548648\n",
      "test loss is 0.0007432768981942052\n",
      "Batch: 13800,train loss is: 0.0005655836018077754\n",
      "test loss is 0.0007474455960044097\n",
      "Batch: 13900,train loss is: 0.0005861714477464772\n",
      "test loss is 0.0007854089636450496\n",
      "Batch: 14000,train loss is: 0.0004400575233534608\n",
      "test loss is 0.0007565126174710351\n",
      "Batch: 14100,train loss is: 0.0008190114956601295\n",
      "test loss is 0.0007967346694197941\n",
      "Batch: 14200,train loss is: 0.0005669002774929745\n",
      "test loss is 0.0007296576395143058\n",
      "Batch: 14300,train loss is: 0.00037212538218800813\n",
      "test loss is 0.0007383087855557434\n",
      "Batch: 14400,train loss is: 0.001370871580089762\n",
      "test loss is 0.0007420052550005567\n",
      "Batch: 14500,train loss is: 0.0004320076670655025\n",
      "test loss is 0.0007452043469435698\n",
      "Batch: 14600,train loss is: 0.0015408720560350375\n",
      "test loss is 0.0007413318609525201\n",
      "Batch: 14700,train loss is: 0.0004827844927602073\n",
      "test loss is 0.0007864263702241364\n",
      "Batch: 14800,train loss is: 0.0008722094503695963\n",
      "test loss is 0.0007508062249515506\n",
      "Batch: 14900,train loss is: 0.00041027115614463534\n",
      "test loss is 0.000777383166975646\n",
      "Batch: 15000,train loss is: 0.0008246273278784552\n",
      "test loss is 0.0007358732082908421\n",
      "Batch: 15100,train loss is: 0.0006892893902419441\n",
      "test loss is 0.000768394755881036\n",
      "Batch: 15200,train loss is: 0.0011402595654859578\n",
      "test loss is 0.0007546321606453801\n",
      "Batch: 15300,train loss is: 0.0004745862706073446\n",
      "test loss is 0.0007376040297753869\n",
      "Batch: 15400,train loss is: 0.0004934669283869204\n",
      "test loss is 0.0007618108196885525\n",
      "Batch: 15500,train loss is: 0.000807867629509075\n",
      "test loss is 0.0007319451681128193\n",
      "Batch: 15600,train loss is: 0.000970758615915659\n",
      "test loss is 0.0007782929190176927\n",
      "Batch: 15700,train loss is: 0.000562519032627552\n",
      "test loss is 0.00074263537666015\n",
      "Batch: 15800,train loss is: 0.0007120340662563694\n",
      "test loss is 0.0007375031814324083\n",
      "Batch: 15900,train loss is: 0.0010594416962379652\n",
      "test loss is 0.0007580581196958478\n",
      "Batch: 16000,train loss is: 0.0004035303290334378\n",
      "test loss is 0.000764932703139198\n",
      "Batch: 16100,train loss is: 0.0005808214379336377\n",
      "test loss is 0.0007465979464723809\n",
      "Batch: 16200,train loss is: 0.00041714484904224384\n",
      "test loss is 0.0007465650141991248\n",
      "Batch: 16300,train loss is: 0.0011398848826432728\n",
      "test loss is 0.0007668658838507848\n",
      "Batch: 16400,train loss is: 0.00043242861015697405\n",
      "test loss is 0.0007344962652933294\n",
      "Batch: 16500,train loss is: 0.0006142851811729426\n",
      "test loss is 0.0007761878926790815\n",
      "Batch: 16600,train loss is: 0.0003219434422138292\n",
      "test loss is 0.0007386312916071498\n",
      "Batch: 16700,train loss is: 0.0010047115573003766\n",
      "test loss is 0.0007562881693624184\n",
      "Batch: 16800,train loss is: 0.0006055287581108797\n",
      "test loss is 0.0007677404303603741\n",
      "Batch: 16900,train loss is: 0.0007265392356280612\n",
      "test loss is 0.0007536165831736797\n",
      "Batch: 17000,train loss is: 0.0007857574698514897\n",
      "test loss is 0.0007300140948460195\n",
      "Batch: 17100,train loss is: 0.0005915575082572976\n",
      "test loss is 0.0007432282683185494\n",
      "Batch: 17200,train loss is: 0.0005544003339374174\n",
      "test loss is 0.0007487755573389441\n",
      "Batch: 17300,train loss is: 0.0009162439359080585\n",
      "test loss is 0.0007633332290243564\n",
      "Batch: 17400,train loss is: 0.0004458324564521646\n",
      "test loss is 0.0007461466797979694\n",
      "Batch: 17500,train loss is: 0.0005026640991245886\n",
      "test loss is 0.0007417427497937174\n",
      "Batch: 17600,train loss is: 0.0004227739492272193\n",
      "test loss is 0.0007637538691702093\n",
      "Batch: 17700,train loss is: 0.0003458081216264117\n",
      "test loss is 0.0007762693586834577\n",
      "Batch: 17800,train loss is: 0.0012307497694285644\n",
      "test loss is 0.0007496989563485656\n",
      "Batch: 17900,train loss is: 0.00047170109378615873\n",
      "test loss is 0.0007811396496158481\n",
      "Batch: 18000,train loss is: 0.0003748754935471022\n",
      "test loss is 0.0007311178368944852\n",
      "Batch: 18100,train loss is: 0.0004750647297016909\n",
      "test loss is 0.0007436950612849929\n",
      "Batch: 18200,train loss is: 0.0005860573479054275\n",
      "test loss is 0.0007487441532373219\n",
      "Batch: 18300,train loss is: 0.0009973209297625602\n",
      "test loss is 0.0007451018324523905\n",
      "Batch: 18400,train loss is: 0.0004897168246384619\n",
      "test loss is 0.0007410521202452868\n",
      "Batch: 18500,train loss is: 0.000888268105173175\n",
      "test loss is 0.000756341312734964\n",
      "Batch: 18600,train loss is: 0.00052685410727032\n",
      "test loss is 0.0007418299933853598\n",
      "Batch: 18700,train loss is: 0.0005588757569036494\n",
      "test loss is 0.0007515402707227104\n",
      "Batch: 18800,train loss is: 0.0006313313390244572\n",
      "test loss is 0.0007420333077537594\n",
      "Batch: 18900,train loss is: 0.000708310496097536\n",
      "test loss is 0.0007795375624689275\n",
      "Batch: 19000,train loss is: 0.0006766014622522431\n",
      "test loss is 0.0007460188793646708\n",
      "Batch: 19100,train loss is: 0.00030040292158939157\n",
      "test loss is 0.0007678990258911623\n",
      "Batch: 19200,train loss is: 0.000583189863888972\n",
      "test loss is 0.000749910705864888\n",
      "Batch: 19300,train loss is: 0.0008591317585578176\n",
      "test loss is 0.0007352181719735551\n",
      "Batch: 19400,train loss is: 0.0003130845952990162\n",
      "test loss is 0.0007323015549974279\n",
      "Batch: 19500,train loss is: 0.000575882319560832\n",
      "test loss is 0.0007816577679413705\n",
      "Batch: 19600,train loss is: 0.0004761163335919835\n",
      "test loss is 0.0007569577632834366\n",
      "Batch: 19700,train loss is: 0.0008103609523478592\n",
      "test loss is 0.0008177306437325417\n",
      "Batch: 19800,train loss is: 0.0005865757596744329\n",
      "test loss is 0.000749954049497576\n",
      "Batch: 19900,train loss is: 0.0002547147355487717\n",
      "test loss is 0.0007849311954923352\n",
      "Batch: 20000,train loss is: 0.0007873783216683559\n",
      "test loss is 0.000748286548766676\n",
      "Batch: 20100,train loss is: 0.0005228890279255332\n",
      "test loss is 0.0007512559987255395\n",
      "Batch: 20200,train loss is: 0.0009617558495507965\n",
      "test loss is 0.000762536990726818\n",
      "Batch: 20300,train loss is: 0.00038947777809162016\n",
      "test loss is 0.0007668504095909591\n",
      "Batch: 20400,train loss is: 0.0004930982401751002\n",
      "test loss is 0.0007868975071797451\n",
      "Batch: 20500,train loss is: 0.000626245343145702\n",
      "test loss is 0.0007482156546229015\n",
      "Batch: 20600,train loss is: 0.0005828082568406135\n",
      "test loss is 0.000779211210115709\n",
      "Batch: 20700,train loss is: 0.0006520234597946868\n",
      "test loss is 0.0007551406772138446\n",
      "Batch: 20800,train loss is: 0.0003663734793330859\n",
      "test loss is 0.0007557237731209883\n",
      "Batch: 20900,train loss is: 0.000683286143369332\n",
      "test loss is 0.0007914205188965018\n",
      "Batch: 21000,train loss is: 0.0002992497897478637\n",
      "test loss is 0.0007358750814440363\n",
      "Batch: 21100,train loss is: 0.0006919455559978371\n",
      "test loss is 0.0007673478642647876\n",
      "Batch: 21200,train loss is: 0.00033403655772800263\n",
      "test loss is 0.0007612901974246528\n",
      "Batch: 21300,train loss is: 0.0003754363487125407\n",
      "test loss is 0.0007415412748924168\n",
      "Batch: 21400,train loss is: 0.0008632925562883152\n",
      "test loss is 0.0007545558336105094\n",
      "Batch: 21500,train loss is: 0.00023739663853756946\n",
      "test loss is 0.0007687831815705175\n",
      "Batch: 21600,train loss is: 0.0007622064696709276\n",
      "test loss is 0.0007580572344686144\n",
      "Batch: 21700,train loss is: 0.00032844885558294554\n",
      "test loss is 0.0007395257904665748\n",
      "Batch: 21800,train loss is: 0.0006347184144165089\n",
      "test loss is 0.0007977541445771399\n",
      "Batch: 21900,train loss is: 0.0005499869693047766\n",
      "test loss is 0.0008267990649443225\n",
      "Batch: 22000,train loss is: 0.0007298049918992723\n",
      "test loss is 0.0007429825908531926\n",
      "Batch: 22100,train loss is: 0.0003481931418428665\n",
      "test loss is 0.0007480348987842741\n",
      "Batch: 22200,train loss is: 0.0005246521806159622\n",
      "test loss is 0.0007463925543647697\n",
      "Batch: 22300,train loss is: 0.0007075136736731906\n",
      "test loss is 0.0007484994410520036\n",
      "Batch: 22400,train loss is: 0.0005380929383688166\n",
      "test loss is 0.0007428884937526379\n",
      "Batch: 22500,train loss is: 0.0004534211742565094\n",
      "test loss is 0.000736394303830947\n",
      "Batch: 22600,train loss is: 0.0010774073003538733\n",
      "test loss is 0.0007929407668746133\n",
      "Batch: 22700,train loss is: 0.00042813952912123583\n",
      "test loss is 0.0007408208649122521\n",
      "Batch: 22800,train loss is: 0.0005505902117016428\n",
      "test loss is 0.0007620587007963025\n",
      "Batch: 22900,train loss is: 0.00037016872705184704\n",
      "test loss is 0.0008100167733008118\n",
      "Batch: 23000,train loss is: 0.00042108311864503303\n",
      "test loss is 0.0007433442580050539\n",
      "Batch: 23100,train loss is: 0.0005509026916684681\n",
      "test loss is 0.0007295758329621633\n",
      "Batch: 23200,train loss is: 0.00034824099429081823\n",
      "test loss is 0.0007234708392487637\n",
      "Batch: 23300,train loss is: 0.0005437198397618431\n",
      "test loss is 0.0007353537228513177\n",
      "Batch: 23400,train loss is: 0.0006997712043136411\n",
      "test loss is 0.0007641773101791693\n",
      "Batch: 23500,train loss is: 0.0023706591765758887\n",
      "test loss is 0.0007402555846165954\n",
      "Batch: 23600,train loss is: 0.00030007795565090606\n",
      "test loss is 0.0007334966474340407\n",
      "Batch: 23700,train loss is: 0.0008990060204494169\n",
      "test loss is 0.0007559899176405256\n",
      "Batch: 23800,train loss is: 0.0008926059226388148\n",
      "test loss is 0.0007379267939105129\n",
      "Batch: 23900,train loss is: 0.0009034010134879615\n",
      "test loss is 0.0007461667021813428\n",
      "Batch: 24000,train loss is: 0.0008361705366381069\n",
      "test loss is 0.0007892920824540479\n",
      "Batch: 24100,train loss is: 0.0004021916735312221\n",
      "test loss is 0.000729498799641856\n",
      "Batch: 24200,train loss is: 0.0016222406925167608\n",
      "test loss is 0.0007578583305170738\n",
      "Batch: 24300,train loss is: 0.00048691476022943487\n",
      "test loss is 0.0008838825189304906\n",
      "Batch: 24400,train loss is: 0.001379489723531805\n",
      "test loss is 0.000739856159581262\n",
      "Batch: 24500,train loss is: 0.0006607965846454202\n",
      "test loss is 0.0007356321081663566\n",
      "Batch: 24600,train loss is: 0.0005349575166409506\n",
      "test loss is 0.0007733711635628566\n",
      "Batch: 24700,train loss is: 0.000320152218056005\n",
      "test loss is 0.0007421199207888582\n",
      "Batch: 24800,train loss is: 0.0009654562163005491\n",
      "test loss is 0.0007427885014929025\n",
      "Batch: 24900,train loss is: 0.0001809640290801798\n",
      "test loss is 0.0007438970935983013\n",
      "Batch: 25000,train loss is: 0.0007879568671514139\n",
      "test loss is 0.0007382976411729696\n",
      "Batch: 25100,train loss is: 0.0006238905260362712\n",
      "test loss is 0.0007527658796204089\n",
      "Batch: 25200,train loss is: 0.0007706664757047163\n",
      "test loss is 0.000756274516328416\n",
      "Batch: 25300,train loss is: 0.0008512766872015882\n",
      "test loss is 0.0007709888417946203\n",
      "Batch: 25400,train loss is: 0.00041697630278249504\n",
      "test loss is 0.0007343145491239047\n",
      "Batch: 25500,train loss is: 0.000579720183986477\n",
      "test loss is 0.0007412963745604779\n",
      "Batch: 25600,train loss is: 0.0006901174885797118\n",
      "test loss is 0.0007433471941411227\n",
      "Batch: 25700,train loss is: 0.0005992685247497975\n",
      "test loss is 0.0007616190658465398\n",
      "Batch: 25800,train loss is: 0.00035101798211872794\n",
      "test loss is 0.0007355122560740212\n",
      "Batch: 25900,train loss is: 0.00033128776917610826\n",
      "test loss is 0.000783327755679461\n",
      "Batch: 26000,train loss is: 0.0017191738109955793\n",
      "test loss is 0.0008026574022004337\n",
      "Batch: 26100,train loss is: 0.0009273910093148998\n",
      "test loss is 0.0007329522543926959\n",
      "Batch: 26200,train loss is: 0.0007248300751540401\n",
      "test loss is 0.0007289059458463995\n",
      "Batch: 26300,train loss is: 0.0012004857800431824\n",
      "test loss is 0.0007581027831619234\n",
      "Batch: 26400,train loss is: 0.0006380385360048424\n",
      "test loss is 0.0007422765865050963\n",
      "Batch: 26500,train loss is: 0.00035410759359738277\n",
      "test loss is 0.0007448905849561473\n",
      "Batch: 26600,train loss is: 0.0013493043211426404\n",
      "test loss is 0.0008204165817272015\n",
      "Batch: 26700,train loss is: 0.00043600640616114867\n",
      "test loss is 0.0008512949524200524\n",
      "Batch: 26800,train loss is: 0.0012183704744169464\n",
      "test loss is 0.0007446427650348729\n",
      "Batch: 26900,train loss is: 0.0005066158130375202\n",
      "test loss is 0.0007431944938966629\n",
      "Batch: 27000,train loss is: 0.0006672042474005734\n",
      "test loss is 0.0008748303233163913\n",
      "Batch: 27100,train loss is: 0.0005636945306213995\n",
      "test loss is 0.0007925757784539368\n",
      "Batch: 27200,train loss is: 0.0006981474087666566\n",
      "test loss is 0.0007816797771886968\n",
      "Batch: 27300,train loss is: 0.0005776123607975817\n",
      "test loss is 0.0007461492615584756\n",
      "Batch: 27400,train loss is: 0.0009439814823442978\n",
      "test loss is 0.0007381218480529164\n",
      "Batch: 27500,train loss is: 0.0005528046776570202\n",
      "test loss is 0.0008208995394204552\n",
      "Batch: 27600,train loss is: 0.0006192689443438747\n",
      "test loss is 0.0007902120645486736\n",
      "Batch: 27700,train loss is: 0.0017383617062727056\n",
      "test loss is 0.0007526482586819954\n",
      "Batch: 27800,train loss is: 0.0007757666131960965\n",
      "test loss is 0.000771837080505466\n",
      "Batch: 27900,train loss is: 0.0013460028023152015\n",
      "test loss is 0.0007395425982443185\n",
      "Batch: 28000,train loss is: 0.0004354097558291697\n",
      "test loss is 0.0007434744029696826\n",
      "Batch: 28100,train loss is: 0.0008859688436392392\n",
      "test loss is 0.0007635029280003492\n",
      "Batch: 28200,train loss is: 0.0006479705377344726\n",
      "test loss is 0.0007226501685800927\n",
      "Batch: 28300,train loss is: 0.0020274265564046313\n",
      "test loss is 0.0007441011069960514\n",
      "Batch: 28400,train loss is: 0.0005792040892779938\n",
      "test loss is 0.0007334588576181525\n",
      "Batch: 28500,train loss is: 0.0014682054130605387\n",
      "test loss is 0.0007635363067527817\n",
      "Batch: 28600,train loss is: 0.00039176097180070117\n",
      "test loss is 0.000755237957638762\n",
      "Batch: 28700,train loss is: 0.000496384425406433\n",
      "test loss is 0.0007384411729328778\n",
      "Batch: 28800,train loss is: 0.0011428610580126119\n",
      "test loss is 0.0007446067519761468\n",
      "Batch: 28900,train loss is: 0.0005003335273013357\n",
      "test loss is 0.0007354714588464292\n",
      "Batch: 29000,train loss is: 0.000557943566980645\n",
      "test loss is 0.0007316354404531413\n",
      "Batch: 29100,train loss is: 0.0006829929322091613\n",
      "test loss is 0.0007498316299996315\n",
      "Batch: 29200,train loss is: 0.00046093273438351163\n",
      "test loss is 0.000754273012875654\n",
      "Batch: 29300,train loss is: 0.0010510523760498074\n",
      "test loss is 0.0007338408259998435\n",
      "Batch: 29400,train loss is: 0.0006893432502664215\n",
      "test loss is 0.0008057651724144541\n",
      "Batch: 29500,train loss is: 0.00047164943994783476\n",
      "test loss is 0.0007557094050411081\n",
      "Batch: 29600,train loss is: 0.000535073829979436\n",
      "test loss is 0.0007367989111536378\n",
      "Batch: 29700,train loss is: 0.0003590224136102176\n",
      "test loss is 0.0007549810635880286\n",
      "Batch: 29800,train loss is: 0.0008579317948468937\n",
      "test loss is 0.0007299707386432913\n",
      "Batch: 29900,train loss is: 0.003303867719033605\n",
      "test loss is 0.0007393104303218849\n",
      "Batch: 30000,train loss is: 0.0003343903346115564\n",
      "test loss is 0.0007912851497850328\n",
      "Batch: 30100,train loss is: 0.0005052581924234184\n",
      "test loss is 0.0007765241397526935\n",
      "Batch: 30200,train loss is: 0.0003710612151785636\n",
      "test loss is 0.0007395670157196658\n",
      "Batch: 30300,train loss is: 0.00031122359812850355\n",
      "test loss is 0.0007298047081730488\n",
      "Batch: 30400,train loss is: 0.00039330541229941546\n",
      "test loss is 0.0007416961919340453\n",
      "Batch: 30500,train loss is: 0.00040282146359967887\n",
      "test loss is 0.0007923626094108415\n",
      "Batch: 30600,train loss is: 0.00026065536779100527\n",
      "test loss is 0.0007371452414599004\n",
      "Batch: 30700,train loss is: 0.00043422274339024677\n",
      "test loss is 0.0008672893708891304\n",
      "Batch: 30800,train loss is: 0.00028735496574024296\n",
      "test loss is 0.0008074560507948862\n",
      "Batch: 30900,train loss is: 0.0003725377136181705\n",
      "test loss is 0.0007387122445050453\n",
      "Batch: 31000,train loss is: 0.0005521141694728247\n",
      "test loss is 0.0007509371137377905\n",
      "Batch: 31100,train loss is: 0.00040669742276458096\n",
      "test loss is 0.0008179991517204475\n",
      "Batch: 31200,train loss is: 0.0006092282721234445\n",
      "test loss is 0.0007496142371344679\n",
      "Batch: 31300,train loss is: 0.000614682043706495\n",
      "test loss is 0.0007530693087158109\n",
      "Batch: 31400,train loss is: 0.0008126460910258897\n",
      "test loss is 0.0007409292342195968\n",
      "Batch: 31500,train loss is: 0.0005495151753540864\n",
      "test loss is 0.0007486770999894978\n",
      "Batch: 31600,train loss is: 0.0005033659634770595\n",
      "test loss is 0.0007465571259283357\n",
      "Batch: 31700,train loss is: 0.0005828403255461151\n",
      "test loss is 0.0007724996010020235\n",
      "Batch: 31800,train loss is: 0.001269647766054783\n",
      "test loss is 0.0007414209331848775\n",
      "Batch: 31900,train loss is: 0.000883482088842934\n",
      "test loss is 0.0007683862489557559\n",
      "Batch: 32000,train loss is: 0.004261463939078141\n",
      "test loss is 0.0007401070763092635\n",
      "Batch: 32100,train loss is: 0.0004774888273953404\n",
      "test loss is 0.0007393092763041269\n",
      "Batch: 32200,train loss is: 0.0004227789778215713\n",
      "test loss is 0.0007509950603837992\n",
      "Batch: 32300,train loss is: 0.00037374999608386544\n",
      "test loss is 0.0007400380060460246\n",
      "Batch: 32400,train loss is: 0.00044610311500581866\n",
      "test loss is 0.0007261656663931445\n",
      "Batch: 32500,train loss is: 0.001031070326788107\n",
      "test loss is 0.0007685015702489644\n",
      "Batch: 32600,train loss is: 0.00046984484638966313\n",
      "test loss is 0.0007493354939841104\n",
      "Batch: 32700,train loss is: 0.00047419479510765343\n",
      "test loss is 0.0007502428939938753\n",
      "Batch: 32800,train loss is: 0.0004113371368606147\n",
      "test loss is 0.0007628829718385755\n",
      "Batch: 32900,train loss is: 0.000652749308881887\n",
      "test loss is 0.0007309044097010656\n",
      "Batch: 33000,train loss is: 0.0005391738521305959\n",
      "test loss is 0.0007277409289584346\n",
      "Batch: 33100,train loss is: 0.00046319889105552614\n",
      "test loss is 0.000767442660893636\n",
      "Batch: 33200,train loss is: 0.0005444947402999183\n",
      "test loss is 0.0008322902648616382\n",
      "Batch: 33300,train loss is: 0.0004045866499675601\n",
      "test loss is 0.0007259936106972127\n",
      "Batch: 33400,train loss is: 0.0005308503600873573\n",
      "test loss is 0.0007491407739958596\n",
      "Batch: 33500,train loss is: 0.0004317915424143032\n",
      "test loss is 0.0007399985135701288\n",
      "Batch: 33600,train loss is: 0.000598695277786324\n",
      "test loss is 0.000737512348108881\n",
      "Batch: 33700,train loss is: 0.0006509479819613876\n",
      "test loss is 0.0007528870704783103\n",
      "Batch: 33800,train loss is: 0.0008139661484459458\n",
      "test loss is 0.0007308982994521615\n",
      "Batch: 33900,train loss is: 0.0007085209542230963\n",
      "test loss is 0.0007386062304844949\n",
      "-----------------------Epoch: 8----------------------------------\n",
      "Batch: 0,train loss is: 0.00038819036849927136\n",
      "test loss is 0.0007480241049016557\n",
      "Batch: 100,train loss is: 0.001469171129612902\n",
      "test loss is 0.0007419985972209271\n",
      "Batch: 200,train loss is: 0.0006283511970436147\n",
      "test loss is 0.0007469439768631389\n",
      "Batch: 300,train loss is: 0.0003197063308855862\n",
      "test loss is 0.0007392414144547547\n",
      "Batch: 400,train loss is: 0.0005206402447820557\n",
      "test loss is 0.0007336980567200928\n",
      "Batch: 500,train loss is: 0.0006165286747425033\n",
      "test loss is 0.0007686304123666886\n",
      "Batch: 600,train loss is: 0.0002520518067048254\n",
      "test loss is 0.0007686983769263036\n",
      "Batch: 700,train loss is: 0.00032248109996284167\n",
      "test loss is 0.000734079878906245\n",
      "Batch: 800,train loss is: 0.00046209934309131523\n",
      "test loss is 0.0007371168792721141\n",
      "Batch: 900,train loss is: 0.00033725443014105504\n",
      "test loss is 0.0007325282718898958\n",
      "Batch: 1000,train loss is: 0.0005070117661172493\n",
      "test loss is 0.0007717040093443694\n",
      "Batch: 1100,train loss is: 0.0018003546507151906\n",
      "test loss is 0.0007293902619485499\n",
      "Batch: 1200,train loss is: 0.00036099260051366183\n",
      "test loss is 0.0007408057591151611\n",
      "Batch: 1300,train loss is: 0.0005333588492836814\n",
      "test loss is 0.0007342812212002164\n",
      "Batch: 1400,train loss is: 0.0005026058866319765\n",
      "test loss is 0.0007534673949912314\n",
      "Batch: 1500,train loss is: 0.0005150294740126247\n",
      "test loss is 0.0007613427227624697\n",
      "Batch: 1600,train loss is: 0.00064031736112546\n",
      "test loss is 0.0008169229462200698\n",
      "Batch: 1700,train loss is: 0.0013068379384892268\n",
      "test loss is 0.0007266510823383563\n",
      "Batch: 1800,train loss is: 0.0004755389525903423\n",
      "test loss is 0.0007485058270459762\n",
      "Batch: 1900,train loss is: 0.0003924341015321237\n",
      "test loss is 0.0007734500553275034\n",
      "Batch: 2000,train loss is: 0.000631666432300206\n",
      "test loss is 0.0008630617637567086\n",
      "Batch: 2100,train loss is: 0.0006630264374802629\n",
      "test loss is 0.0008765426870039487\n",
      "Batch: 2200,train loss is: 0.0008739872408476151\n",
      "test loss is 0.0007392749897084793\n",
      "Batch: 2300,train loss is: 0.0004237612066138076\n",
      "test loss is 0.0007760864370752301\n",
      "Batch: 2400,train loss is: 0.0005743061939869059\n",
      "test loss is 0.0007409117434563231\n",
      "Batch: 2500,train loss is: 0.0007217260934737065\n",
      "test loss is 0.000733424999178447\n",
      "Batch: 2600,train loss is: 0.00038987522880795883\n",
      "test loss is 0.0007543637364980984\n",
      "Batch: 2700,train loss is: 0.0006871335357365038\n",
      "test loss is 0.0007244557001919948\n",
      "Batch: 2800,train loss is: 0.0004956073786454375\n",
      "test loss is 0.0007705959055227751\n",
      "Batch: 2900,train loss is: 0.0005103127320500909\n",
      "test loss is 0.0007256358035252636\n",
      "Batch: 3000,train loss is: 0.0005195915630815884\n",
      "test loss is 0.0007246743779238644\n",
      "Batch: 3100,train loss is: 0.000681789157575938\n",
      "test loss is 0.0007828643678162642\n",
      "Batch: 3200,train loss is: 0.0009880403184772718\n",
      "test loss is 0.0007491111299854952\n",
      "Batch: 3300,train loss is: 0.0004677428760660223\n",
      "test loss is 0.000739212556024141\n",
      "Batch: 3400,train loss is: 0.00048700944916296935\n",
      "test loss is 0.0007374684993741228\n",
      "Batch: 3500,train loss is: 0.0002464593383584251\n",
      "test loss is 0.0007397513471093885\n",
      "Batch: 3600,train loss is: 0.0005336922487394465\n",
      "test loss is 0.0007739493999057519\n",
      "Batch: 3700,train loss is: 0.00042518071204183293\n",
      "test loss is 0.0007270262499719842\n",
      "Batch: 3800,train loss is: 0.00045944777702780715\n",
      "test loss is 0.0007669489840432621\n",
      "Batch: 3900,train loss is: 0.0006607804515540449\n",
      "test loss is 0.0007370340309930038\n",
      "Batch: 4000,train loss is: 0.0004360920286881765\n",
      "test loss is 0.0007416929882021945\n",
      "Batch: 4100,train loss is: 0.0007345233731231338\n",
      "test loss is 0.0007560410845936683\n",
      "Batch: 4200,train loss is: 0.00035721720621475426\n",
      "test loss is 0.0007688160086560637\n",
      "Batch: 4300,train loss is: 0.00042867149012981053\n",
      "test loss is 0.0007307995816531966\n",
      "Batch: 4400,train loss is: 0.00044436948891768917\n",
      "test loss is 0.0007364113591977875\n",
      "Batch: 4500,train loss is: 0.0006579670351943842\n",
      "test loss is 0.000763463149182737\n",
      "Batch: 4600,train loss is: 0.0005945794733718449\n",
      "test loss is 0.0007358029410591962\n",
      "Batch: 4700,train loss is: 0.0006761908596946013\n",
      "test loss is 0.0007691841701799053\n",
      "Batch: 4800,train loss is: 0.0012292452006579782\n",
      "test loss is 0.0007386780437290769\n",
      "Batch: 4900,train loss is: 0.0005657148134485672\n",
      "test loss is 0.0007403281949368987\n",
      "Batch: 5000,train loss is: 0.0006068562931873379\n",
      "test loss is 0.0007477152983923244\n",
      "Batch: 5100,train loss is: 0.0006308218503825197\n",
      "test loss is 0.0008055742850732303\n",
      "Batch: 5200,train loss is: 0.00035027136620931914\n",
      "test loss is 0.0007433288954975536\n",
      "Batch: 5300,train loss is: 0.000678009299062223\n",
      "test loss is 0.0007658532094274762\n",
      "Batch: 5400,train loss is: 0.0007698588695045201\n",
      "test loss is 0.0007299891021234221\n",
      "Batch: 5500,train loss is: 0.00047803146710177015\n",
      "test loss is 0.000753570923790744\n",
      "Batch: 5600,train loss is: 0.0006507355579114314\n",
      "test loss is 0.000750533180697049\n",
      "Batch: 5700,train loss is: 0.0007298443389743748\n",
      "test loss is 0.0007376815484967738\n",
      "Batch: 5800,train loss is: 0.0005244407176279982\n",
      "test loss is 0.0007365548142506501\n",
      "Batch: 5900,train loss is: 0.003444434625517198\n",
      "test loss is 0.0007426208412827396\n",
      "Batch: 6000,train loss is: 0.001680875036649372\n",
      "test loss is 0.0007523241455238393\n",
      "Batch: 6100,train loss is: 0.0004541346193625469\n",
      "test loss is 0.0007266617677393459\n",
      "Batch: 6200,train loss is: 0.0006027777979335312\n",
      "test loss is 0.0007417004622797066\n",
      "Batch: 6300,train loss is: 0.0012783897459818316\n",
      "test loss is 0.000754959683720793\n",
      "Batch: 6400,train loss is: 0.00048122260375812656\n",
      "test loss is 0.000789759807766672\n",
      "Batch: 6500,train loss is: 0.00037296888401400994\n",
      "test loss is 0.0007606424620527065\n",
      "Batch: 6600,train loss is: 0.002189470832318026\n",
      "test loss is 0.0007364122073899308\n",
      "Batch: 6700,train loss is: 0.00040759992788383376\n",
      "test loss is 0.0007562180285190232\n",
      "Batch: 6800,train loss is: 0.0008719352377577853\n",
      "test loss is 0.000733485777061844\n",
      "Batch: 6900,train loss is: 0.000658835680439871\n",
      "test loss is 0.0007385545824130259\n",
      "Batch: 7000,train loss is: 0.0005980762370161255\n",
      "test loss is 0.0007652880724696488\n",
      "Batch: 7100,train loss is: 0.0010809481858915068\n",
      "test loss is 0.0007488282649886756\n",
      "Batch: 7200,train loss is: 0.00041192298613670626\n",
      "test loss is 0.0007528718203010392\n",
      "Batch: 7300,train loss is: 0.000842456717201181\n",
      "test loss is 0.0007941256236784499\n",
      "Batch: 7400,train loss is: 0.00038416298157605157\n",
      "test loss is 0.0007339496441003488\n",
      "Batch: 7500,train loss is: 0.000695576122078037\n",
      "test loss is 0.0007533352462088782\n",
      "Batch: 7600,train loss is: 0.0004427141071670106\n",
      "test loss is 0.0007264176736211032\n",
      "Batch: 7700,train loss is: 0.0007437032259253331\n",
      "test loss is 0.0007366427685237493\n",
      "Batch: 7800,train loss is: 0.00030387101031900465\n",
      "test loss is 0.0007310825441632668\n",
      "Batch: 7900,train loss is: 0.0006282016574302416\n",
      "test loss is 0.0007451496065523451\n",
      "Batch: 8000,train loss is: 0.00048146128893380985\n",
      "test loss is 0.0007706369593423894\n",
      "Batch: 8100,train loss is: 0.0005906266706824638\n",
      "test loss is 0.0007529799980637521\n",
      "Batch: 8200,train loss is: 0.0012774502906372134\n",
      "test loss is 0.0007539591635089571\n",
      "Batch: 8300,train loss is: 0.0005429555246092383\n",
      "test loss is 0.0007571412279262698\n",
      "Batch: 8400,train loss is: 0.0005779219334667439\n",
      "test loss is 0.0007374346972914399\n",
      "Batch: 8500,train loss is: 0.0008801581422049262\n",
      "test loss is 0.0007558049439649337\n",
      "Batch: 8600,train loss is: 0.0007031756076958519\n",
      "test loss is 0.0007593520509458889\n",
      "Batch: 8700,train loss is: 0.000516312535363191\n",
      "test loss is 0.0007722068862779288\n",
      "Batch: 8800,train loss is: 0.0003266049946424858\n",
      "test loss is 0.0007287638754804924\n",
      "Batch: 8900,train loss is: 0.0010978266132561497\n",
      "test loss is 0.0007310174502392874\n",
      "Batch: 9000,train loss is: 0.00029070657904198396\n",
      "test loss is 0.000743254028049242\n",
      "Batch: 9100,train loss is: 0.00039640205928312774\n",
      "test loss is 0.0007309059152437937\n",
      "Batch: 9200,train loss is: 0.0008707091624339233\n",
      "test loss is 0.0008020868675221604\n",
      "Batch: 9300,train loss is: 0.0005997774615467572\n",
      "test loss is 0.0007509632647174388\n",
      "Batch: 9400,train loss is: 0.00046121532401147277\n",
      "test loss is 0.0007681104890537805\n",
      "Batch: 9500,train loss is: 0.0004273828600974015\n",
      "test loss is 0.0007571588335976767\n",
      "Batch: 9600,train loss is: 0.00045763733360452\n",
      "test loss is 0.0007292312118535721\n",
      "Batch: 9700,train loss is: 0.0003741318153016727\n",
      "test loss is 0.0007548690943224524\n",
      "Batch: 9800,train loss is: 0.00042768288399694534\n",
      "test loss is 0.0007351594348534246\n",
      "Batch: 9900,train loss is: 0.0005380502494199476\n",
      "test loss is 0.0007723616946528298\n",
      "Batch: 10000,train loss is: 0.0004945862239044157\n",
      "test loss is 0.0007326367692125538\n",
      "Batch: 10100,train loss is: 0.0011096172268613186\n",
      "test loss is 0.0007392703646024081\n",
      "Batch: 10200,train loss is: 0.0017476683892666772\n",
      "test loss is 0.0008296503757969429\n",
      "Batch: 10300,train loss is: 0.0005050617679948115\n",
      "test loss is 0.0007404789618488071\n",
      "Batch: 10400,train loss is: 0.0003630600388635555\n",
      "test loss is 0.0007350566089461149\n",
      "Batch: 10500,train loss is: 0.0007487804322133157\n",
      "test loss is 0.0007352328583802694\n",
      "Batch: 10600,train loss is: 0.0003615304432887484\n",
      "test loss is 0.0007860601764722559\n",
      "Batch: 10700,train loss is: 0.0006479463326038097\n",
      "test loss is 0.0007359425971570547\n",
      "Batch: 10800,train loss is: 0.0005444587680879142\n",
      "test loss is 0.0007460376361501097\n",
      "Batch: 10900,train loss is: 0.0007126935776577623\n",
      "test loss is 0.000740185789204344\n",
      "Batch: 11000,train loss is: 0.0007988384621448397\n",
      "test loss is 0.000766029695685137\n",
      "Batch: 11100,train loss is: 0.0010723338933609178\n",
      "test loss is 0.0007318347323048626\n",
      "Batch: 11200,train loss is: 0.0006848185421864283\n",
      "test loss is 0.0007731959681465902\n",
      "Batch: 11300,train loss is: 0.00033040615330599246\n",
      "test loss is 0.0007596856781141344\n",
      "Batch: 11400,train loss is: 0.0005860025784287587\n",
      "test loss is 0.0007468163384511632\n",
      "Batch: 11500,train loss is: 0.0005570118716311162\n",
      "test loss is 0.0007419287749218808\n",
      "Batch: 11600,train loss is: 0.0019520060401398876\n",
      "test loss is 0.0007472253668672296\n",
      "Batch: 11700,train loss is: 0.0003037284783012065\n",
      "test loss is 0.0007215114840818711\n",
      "Batch: 11800,train loss is: 0.0006235086474127834\n",
      "test loss is 0.0007347854157864949\n",
      "Batch: 11900,train loss is: 0.0006481178534659491\n",
      "test loss is 0.0007504516724311832\n",
      "Batch: 12000,train loss is: 0.0004186877697175786\n",
      "test loss is 0.0007391092212832844\n",
      "Batch: 12100,train loss is: 0.001662756144446347\n",
      "test loss is 0.0007420890643722954\n",
      "Batch: 12200,train loss is: 0.0007305878953309287\n",
      "test loss is 0.0007638941009524343\n",
      "Batch: 12300,train loss is: 0.0004667679944654433\n",
      "test loss is 0.0007549815122005065\n",
      "Batch: 12400,train loss is: 0.000558988248411803\n",
      "test loss is 0.0007367270439816685\n",
      "Batch: 12500,train loss is: 0.00039286509958813467\n",
      "test loss is 0.0007608235497850327\n",
      "Batch: 12600,train loss is: 0.00043992189796169585\n",
      "test loss is 0.0007528588609166026\n",
      "Batch: 12700,train loss is: 0.0004802485385676888\n",
      "test loss is 0.000727375225251658\n",
      "Batch: 12800,train loss is: 0.0004077281222082729\n",
      "test loss is 0.0007343785421303825\n",
      "Batch: 12900,train loss is: 0.0006928722108821416\n",
      "test loss is 0.0007518480997602134\n",
      "Batch: 13000,train loss is: 0.0002449624295524006\n",
      "test loss is 0.0007220935434912035\n",
      "Batch: 13100,train loss is: 0.0022624658523062707\n",
      "test loss is 0.0007271941900612732\n",
      "Batch: 13200,train loss is: 0.0007863584365213459\n",
      "test loss is 0.0008408665569130146\n",
      "Batch: 13300,train loss is: 0.0005915888322090639\n",
      "test loss is 0.0007660073503075152\n",
      "Batch: 13400,train loss is: 0.0004000535002357005\n",
      "test loss is 0.0007626139758647247\n",
      "Batch: 13500,train loss is: 0.0006350195819552488\n",
      "test loss is 0.0007675166128928648\n",
      "Batch: 13600,train loss is: 0.0005353946047950343\n",
      "test loss is 0.000745998039055875\n",
      "Batch: 13700,train loss is: 0.0007089380901126654\n",
      "test loss is 0.0007316522924749236\n",
      "Batch: 13800,train loss is: 0.0005347763172565693\n",
      "test loss is 0.0007355604721714222\n",
      "Batch: 13900,train loss is: 0.0005496317766397893\n",
      "test loss is 0.0007776923419140002\n",
      "Batch: 14000,train loss is: 0.0004362674122291417\n",
      "test loss is 0.0007477972449236396\n",
      "Batch: 14100,train loss is: 0.0008167453960138758\n",
      "test loss is 0.0007879803712019377\n",
      "Batch: 14200,train loss is: 0.00057915878331811\n",
      "test loss is 0.0007193620865308468\n",
      "Batch: 14300,train loss is: 0.0003438439707616768\n",
      "test loss is 0.0007282150661934529\n",
      "Batch: 14400,train loss is: 0.0013553236929388236\n",
      "test loss is 0.0007319952741976195\n",
      "Batch: 14500,train loss is: 0.00042980975293281014\n",
      "test loss is 0.0007352278793436432\n",
      "Batch: 14600,train loss is: 0.0014682317796257582\n",
      "test loss is 0.0007321162574036278\n",
      "Batch: 14700,train loss is: 0.0004781376361705149\n",
      "test loss is 0.0007786879735618777\n",
      "Batch: 14800,train loss is: 0.0008645950030610707\n",
      "test loss is 0.0007418518221282426\n",
      "Batch: 14900,train loss is: 0.00040162799414021\n",
      "test loss is 0.0007703629815601224\n",
      "Batch: 15000,train loss is: 0.0008082672620785554\n",
      "test loss is 0.0007257102765047587\n",
      "Batch: 15100,train loss is: 0.0006742871995584797\n",
      "test loss is 0.0007581839676778431\n",
      "Batch: 15200,train loss is: 0.0010825611927611043\n",
      "test loss is 0.0007429829003159664\n",
      "Batch: 15300,train loss is: 0.00046052915542345204\n",
      "test loss is 0.0007278136871170418\n",
      "Batch: 15400,train loss is: 0.0004850710166805147\n",
      "test loss is 0.0007494142612450642\n",
      "Batch: 15500,train loss is: 0.0007920959397618166\n",
      "test loss is 0.0007220512678862401\n",
      "Batch: 15600,train loss is: 0.0009641588774393771\n",
      "test loss is 0.0007680791615272387\n",
      "Batch: 15700,train loss is: 0.0005370681291335784\n",
      "test loss is 0.0007319254714293353\n",
      "Batch: 15800,train loss is: 0.0007115365341970887\n",
      "test loss is 0.0007268412729750099\n",
      "Batch: 15900,train loss is: 0.0010059543080863703\n",
      "test loss is 0.0007479231528706694\n",
      "Batch: 16000,train loss is: 0.00039944292570980066\n",
      "test loss is 0.0007549031284381087\n",
      "Batch: 16100,train loss is: 0.0005801139611687144\n",
      "test loss is 0.0007352602892013843\n",
      "Batch: 16200,train loss is: 0.0004080296190945755\n",
      "test loss is 0.0007352979919397374\n",
      "Batch: 16300,train loss is: 0.0011275302036888577\n",
      "test loss is 0.0007596027714795673\n",
      "Batch: 16400,train loss is: 0.0004183592515262299\n",
      "test loss is 0.0007246543774232122\n",
      "Batch: 16500,train loss is: 0.0006161256785461802\n",
      "test loss is 0.0007644816583640136\n",
      "Batch: 16600,train loss is: 0.0003227455273663473\n",
      "test loss is 0.0007261867945505705\n",
      "Batch: 16700,train loss is: 0.0009862466801931924\n",
      "test loss is 0.0007458493307704354\n",
      "Batch: 16800,train loss is: 0.0006016607666426999\n",
      "test loss is 0.0007588777955544009\n",
      "Batch: 16900,train loss is: 0.0007192396046338482\n",
      "test loss is 0.0007417191644289788\n",
      "Batch: 17000,train loss is: 0.0007674490272015276\n",
      "test loss is 0.0007195643266814798\n",
      "Batch: 17100,train loss is: 0.0005846686610746105\n",
      "test loss is 0.0007340985019720827\n",
      "Batch: 17200,train loss is: 0.0005434297759630217\n",
      "test loss is 0.0007379681629050736\n",
      "Batch: 17300,train loss is: 0.0009277803210819826\n",
      "test loss is 0.000754040803428833\n",
      "Batch: 17400,train loss is: 0.00043732365171724526\n",
      "test loss is 0.000736279725899047\n",
      "Batch: 17500,train loss is: 0.0005017577195740046\n",
      "test loss is 0.0007314423779072038\n",
      "Batch: 17600,train loss is: 0.0004218525560829619\n",
      "test loss is 0.0007520562535020345\n",
      "Batch: 17700,train loss is: 0.0003473667759696658\n",
      "test loss is 0.0007670142574099896\n",
      "Batch: 17800,train loss is: 0.001277598361958515\n",
      "test loss is 0.0007389173095647089\n",
      "Batch: 17900,train loss is: 0.0004774888218319403\n",
      "test loss is 0.0007707498551059156\n",
      "Batch: 18000,train loss is: 0.0003669783067653335\n",
      "test loss is 0.0007205186868421805\n",
      "Batch: 18100,train loss is: 0.0004681957387366767\n",
      "test loss is 0.0007330587538273192\n",
      "Batch: 18200,train loss is: 0.0005829228888972523\n",
      "test loss is 0.0007393416252933586\n",
      "Batch: 18300,train loss is: 0.0010099173189544771\n",
      "test loss is 0.000735116631045563\n",
      "Batch: 18400,train loss is: 0.00046964511590389833\n",
      "test loss is 0.0007306129854241486\n",
      "Batch: 18500,train loss is: 0.0009134094485758395\n",
      "test loss is 0.0007462629454137417\n",
      "Batch: 18600,train loss is: 0.000523518717156658\n",
      "test loss is 0.0007323023337470845\n",
      "Batch: 18700,train loss is: 0.0005583086423820002\n",
      "test loss is 0.0007423677906624644\n",
      "Batch: 18800,train loss is: 0.0006052586554041638\n",
      "test loss is 0.0007321231693905285\n",
      "Batch: 18900,train loss is: 0.0006858984200890113\n",
      "test loss is 0.0007674438009842231\n",
      "Batch: 19000,train loss is: 0.0006703224194127274\n",
      "test loss is 0.0007351461821443085\n",
      "Batch: 19100,train loss is: 0.0003013548362690705\n",
      "test loss is 0.0007581433200550339\n",
      "Batch: 19200,train loss is: 0.0005585998820142363\n",
      "test loss is 0.0007404182866953908\n",
      "Batch: 19300,train loss is: 0.0008340616423193075\n",
      "test loss is 0.0007244394882735379\n",
      "Batch: 19400,train loss is: 0.0003104954892414409\n",
      "test loss is 0.0007225275331136875\n",
      "Batch: 19500,train loss is: 0.0005703979646084231\n",
      "test loss is 0.0007696238185486299\n",
      "Batch: 19600,train loss is: 0.0004634816356204341\n",
      "test loss is 0.0007464223183163587\n",
      "Batch: 19700,train loss is: 0.0007978700802919017\n",
      "test loss is 0.0008089643901768415\n",
      "Batch: 19800,train loss is: 0.0005905746913099813\n",
      "test loss is 0.0007404171074189783\n",
      "Batch: 19900,train loss is: 0.00025316526066426423\n",
      "test loss is 0.0007742728513990589\n",
      "Batch: 20000,train loss is: 0.000772031453337332\n",
      "test loss is 0.0007378330479169985\n",
      "Batch: 20100,train loss is: 0.0004993094001454637\n",
      "test loss is 0.0007401919600329309\n",
      "Batch: 20200,train loss is: 0.0009226565331851357\n",
      "test loss is 0.0007518086611650047\n",
      "Batch: 20300,train loss is: 0.0003754486839513762\n",
      "test loss is 0.0007570555405503046\n",
      "Batch: 20400,train loss is: 0.00048804465262224583\n",
      "test loss is 0.0007767532969161367\n",
      "Batch: 20500,train loss is: 0.0006222614433662422\n",
      "test loss is 0.0007363536576407909\n",
      "Batch: 20600,train loss is: 0.0005572739358852036\n",
      "test loss is 0.0007681029271313122\n",
      "Batch: 20700,train loss is: 0.0006697708771895042\n",
      "test loss is 0.0007443842908352339\n",
      "Batch: 20800,train loss is: 0.0003661443641146454\n",
      "test loss is 0.000748329682977739\n",
      "Batch: 20900,train loss is: 0.0006643129905190587\n",
      "test loss is 0.000783327138274053\n",
      "Batch: 21000,train loss is: 0.0002865157806395462\n",
      "test loss is 0.0007257126599617917\n",
      "Batch: 21100,train loss is: 0.0006971469120896959\n",
      "test loss is 0.0007573806821298836\n",
      "Batch: 21200,train loss is: 0.00033742476841335656\n",
      "test loss is 0.0007549758596706418\n",
      "Batch: 21300,train loss is: 0.00036490489712823773\n",
      "test loss is 0.0007317045292633899\n",
      "Batch: 21400,train loss is: 0.0008521213083304154\n",
      "test loss is 0.0007436191431600028\n",
      "Batch: 21500,train loss is: 0.00022458172761642892\n",
      "test loss is 0.0007554321765677255\n",
      "Batch: 21600,train loss is: 0.0007511498214394484\n",
      "test loss is 0.0007465623595460294\n",
      "Batch: 21700,train loss is: 0.0003264936171181663\n",
      "test loss is 0.0007292371558941657\n",
      "Batch: 21800,train loss is: 0.0006417576384096823\n",
      "test loss is 0.0007883232653909451\n",
      "Batch: 21900,train loss is: 0.0005459277698632719\n",
      "test loss is 0.0008149364207695692\n",
      "Batch: 22000,train loss is: 0.0007172900138975358\n",
      "test loss is 0.0007323641465149192\n",
      "Batch: 22100,train loss is: 0.0003463587853534072\n",
      "test loss is 0.0007378299995260953\n",
      "Batch: 22200,train loss is: 0.0005192035495368959\n",
      "test loss is 0.0007362477237165191\n",
      "Batch: 22300,train loss is: 0.0006845311170078127\n",
      "test loss is 0.0007385593554388672\n",
      "Batch: 22400,train loss is: 0.0005209391375878054\n",
      "test loss is 0.0007343347551740081\n",
      "Batch: 22500,train loss is: 0.00046061398889449284\n",
      "test loss is 0.0007262780781332367\n",
      "Batch: 22600,train loss is: 0.0010695055884181353\n",
      "test loss is 0.0007793002751172169\n",
      "Batch: 22700,train loss is: 0.0004203053184996506\n",
      "test loss is 0.0007290134412416217\n",
      "Batch: 22800,train loss is: 0.0005544428969320173\n",
      "test loss is 0.0007524347541087825\n",
      "Batch: 22900,train loss is: 0.0003616146690420457\n",
      "test loss is 0.0008018984020745363\n",
      "Batch: 23000,train loss is: 0.0004241298159942374\n",
      "test loss is 0.0007341519355835329\n",
      "Batch: 23100,train loss is: 0.000532278262094386\n",
      "test loss is 0.0007189891168794078\n",
      "Batch: 23200,train loss is: 0.00033625631281308175\n",
      "test loss is 0.0007135180957914477\n",
      "Batch: 23300,train loss is: 0.0005315396064489317\n",
      "test loss is 0.0007255664286798486\n",
      "Batch: 23400,train loss is: 0.0006831161499647192\n",
      "test loss is 0.0007565188906759081\n",
      "Batch: 23500,train loss is: 0.002245020877006692\n",
      "test loss is 0.0007291132328158623\n",
      "Batch: 23600,train loss is: 0.0002965891762344325\n",
      "test loss is 0.0007235318530392846\n",
      "Batch: 23700,train loss is: 0.0009003584052947293\n",
      "test loss is 0.0007431010161693558\n",
      "Batch: 23800,train loss is: 0.000869982014464419\n",
      "test loss is 0.000729174427277596\n",
      "Batch: 23900,train loss is: 0.0009023034324735057\n",
      "test loss is 0.000737891132803106\n",
      "Batch: 24000,train loss is: 0.0008303329235552396\n",
      "test loss is 0.0007828337431296461\n",
      "Batch: 24100,train loss is: 0.00038535476596028383\n",
      "test loss is 0.0007186736960541312\n",
      "Batch: 24200,train loss is: 0.0015828212821694392\n",
      "test loss is 0.0007478526913393927\n",
      "Batch: 24300,train loss is: 0.0004917658456499368\n",
      "test loss is 0.000875027358030619\n",
      "Batch: 24400,train loss is: 0.0013561608502544286\n",
      "test loss is 0.0007301460208793884\n",
      "Batch: 24500,train loss is: 0.0006555699381382481\n",
      "test loss is 0.0007254185369077394\n",
      "Batch: 24600,train loss is: 0.0005154039114585034\n",
      "test loss is 0.0007617263696391433\n",
      "Batch: 24700,train loss is: 0.00030972245924053754\n",
      "test loss is 0.0007316662987961889\n",
      "Batch: 24800,train loss is: 0.0009452080680690275\n",
      "test loss is 0.0007333830553922647\n",
      "Batch: 24900,train loss is: 0.0001814196730472198\n",
      "test loss is 0.0007330007448829424\n",
      "Batch: 25000,train loss is: 0.0007629879282178054\n",
      "test loss is 0.0007283613554898962\n",
      "Batch: 25100,train loss is: 0.0006138913653563779\n",
      "test loss is 0.0007424994029590796\n",
      "Batch: 25200,train loss is: 0.0007689328254705835\n",
      "test loss is 0.0007458439820503815\n",
      "Batch: 25300,train loss is: 0.0008366496312300893\n",
      "test loss is 0.0007627691292205078\n",
      "Batch: 25400,train loss is: 0.0004158565603771185\n",
      "test loss is 0.0007247586826945106\n",
      "Batch: 25500,train loss is: 0.0005801177670900703\n",
      "test loss is 0.0007317636804103342\n",
      "Batch: 25600,train loss is: 0.000676484537655107\n",
      "test loss is 0.0007335380353711585\n",
      "Batch: 25700,train loss is: 0.0005876953538336349\n",
      "test loss is 0.000751772739595587\n",
      "Batch: 25800,train loss is: 0.0003489297918882446\n",
      "test loss is 0.0007240174771881092\n",
      "Batch: 25900,train loss is: 0.00031777716478971154\n",
      "test loss is 0.000774267727692937\n",
      "Batch: 26000,train loss is: 0.0016871677238367\n",
      "test loss is 0.0007924833906593267\n",
      "Batch: 26100,train loss is: 0.0009092911118654682\n",
      "test loss is 0.0007218821638244201\n",
      "Batch: 26200,train loss is: 0.0006970705854976337\n",
      "test loss is 0.0007183983901351718\n",
      "Batch: 26300,train loss is: 0.0011854989584527986\n",
      "test loss is 0.0007465985366636252\n",
      "Batch: 26400,train loss is: 0.0006247943033482129\n",
      "test loss is 0.000730294610717588\n",
      "Batch: 26500,train loss is: 0.0003590203569850693\n",
      "test loss is 0.0007341913847591045\n",
      "Batch: 26600,train loss is: 0.001371582028929955\n",
      "test loss is 0.0008104836788449977\n",
      "Batch: 26700,train loss is: 0.0004352585661475483\n",
      "test loss is 0.0008374916784566136\n",
      "Batch: 26800,train loss is: 0.0012147589541710224\n",
      "test loss is 0.0007349709443045226\n",
      "Batch: 26900,train loss is: 0.0004986066507356047\n",
      "test loss is 0.0007340685285418106\n",
      "Batch: 27000,train loss is: 0.0006762446600302609\n",
      "test loss is 0.0008667427312957225\n",
      "Batch: 27100,train loss is: 0.0005799616552079885\n",
      "test loss is 0.000783692581293294\n",
      "Batch: 27200,train loss is: 0.0006990497071058913\n",
      "test loss is 0.0007724337648644011\n",
      "Batch: 27300,train loss is: 0.0005628673398164646\n",
      "test loss is 0.0007375201789070548\n",
      "Batch: 27400,train loss is: 0.000958012169851163\n",
      "test loss is 0.0007303612447028118\n",
      "Batch: 27500,train loss is: 0.0005468342416693\n",
      "test loss is 0.0008109081850427507\n",
      "Batch: 27600,train loss is: 0.0006090896183730584\n",
      "test loss is 0.0007785531587621437\n",
      "Batch: 27700,train loss is: 0.001716823936782922\n",
      "test loss is 0.0007433223987069237\n",
      "Batch: 27800,train loss is: 0.0007739499603379853\n",
      "test loss is 0.0007611255950046257\n",
      "Batch: 27900,train loss is: 0.0013586139537142793\n",
      "test loss is 0.0007310595528664228\n",
      "Batch: 28000,train loss is: 0.00043235085272254114\n",
      "test loss is 0.0007320651406255145\n",
      "Batch: 28100,train loss is: 0.000891919813671025\n",
      "test loss is 0.0007530473272652675\n",
      "Batch: 28200,train loss is: 0.000629439970876783\n",
      "test loss is 0.0007125273792408196\n",
      "Batch: 28300,train loss is: 0.001995524434269542\n",
      "test loss is 0.0007365491826344221\n",
      "Batch: 28400,train loss is: 0.0005626357989827226\n",
      "test loss is 0.0007226443991588056\n",
      "Batch: 28500,train loss is: 0.001473140653980713\n",
      "test loss is 0.0007550116801324917\n",
      "Batch: 28600,train loss is: 0.00038377763112966217\n",
      "test loss is 0.0007458574575630115\n",
      "Batch: 28700,train loss is: 0.0005084360830980108\n",
      "test loss is 0.0007292912261000593\n",
      "Batch: 28800,train loss is: 0.0011281797688648678\n",
      "test loss is 0.0007356998016587693\n",
      "Batch: 28900,train loss is: 0.0004857692840449019\n",
      "test loss is 0.0007245899872354515\n",
      "Batch: 29000,train loss is: 0.0005478845593728913\n",
      "test loss is 0.0007229678818825217\n",
      "Batch: 29100,train loss is: 0.0006631984968935137\n",
      "test loss is 0.0007386018812519121\n",
      "Batch: 29200,train loss is: 0.00045079308848518007\n",
      "test loss is 0.0007442081824549973\n",
      "Batch: 29300,train loss is: 0.0010479467102349502\n",
      "test loss is 0.0007234613957963674\n",
      "Batch: 29400,train loss is: 0.0006975321881835539\n",
      "test loss is 0.0007982163286453911\n",
      "Batch: 29500,train loss is: 0.00047498994391242315\n",
      "test loss is 0.0007477326428503338\n",
      "Batch: 29600,train loss is: 0.000524854467620941\n",
      "test loss is 0.0007253590377174259\n",
      "Batch: 29700,train loss is: 0.00035595758927290245\n",
      "test loss is 0.0007436109719433725\n",
      "Batch: 29800,train loss is: 0.0008517513367908145\n",
      "test loss is 0.000719101571488559\n",
      "Batch: 29900,train loss is: 0.0032755200460972884\n",
      "test loss is 0.0007279413232055914\n",
      "Batch: 30000,train loss is: 0.0003397249801201436\n",
      "test loss is 0.0007791689447865655\n",
      "Batch: 30100,train loss is: 0.0005093097255576506\n",
      "test loss is 0.0007667034883916484\n",
      "Batch: 30200,train loss is: 0.0003681175273578377\n",
      "test loss is 0.0007299194420273852\n",
      "Batch: 30300,train loss is: 0.000300035441431296\n",
      "test loss is 0.0007197119688795702\n",
      "Batch: 30400,train loss is: 0.0003790601713956081\n",
      "test loss is 0.0007313911961235443\n",
      "Batch: 30500,train loss is: 0.00039183839066987653\n",
      "test loss is 0.0007816819682340075\n",
      "Batch: 30600,train loss is: 0.0002537161031454883\n",
      "test loss is 0.0007264048779273558\n",
      "Batch: 30700,train loss is: 0.0004163724617301922\n",
      "test loss is 0.0008517836736085364\n",
      "Batch: 30800,train loss is: 0.00028783526618555543\n",
      "test loss is 0.0007980286235642243\n",
      "Batch: 30900,train loss is: 0.0003633442051718984\n",
      "test loss is 0.0007288953754995088\n",
      "Batch: 31000,train loss is: 0.0005400934986404494\n",
      "test loss is 0.0007405874090967052\n",
      "Batch: 31100,train loss is: 0.0003935684691960276\n",
      "test loss is 0.0008089473309938791\n",
      "Batch: 31200,train loss is: 0.0006016754702571072\n",
      "test loss is 0.000738493877765603\n",
      "Batch: 31300,train loss is: 0.0005865038549315347\n",
      "test loss is 0.000741088907024672\n",
      "Batch: 31400,train loss is: 0.0008072992087598877\n",
      "test loss is 0.000730844798192581\n",
      "Batch: 31500,train loss is: 0.0005295048012807922\n",
      "test loss is 0.0007367738941478185\n",
      "Batch: 31600,train loss is: 0.0004850309120556953\n",
      "test loss is 0.0007366368296052674\n",
      "Batch: 31700,train loss is: 0.000589555784868932\n",
      "test loss is 0.0007619267056461611\n",
      "Batch: 31800,train loss is: 0.0012838119451798436\n",
      "test loss is 0.0007307071977029129\n",
      "Batch: 31900,train loss is: 0.0008542861259416945\n",
      "test loss is 0.000754618659629458\n",
      "Batch: 32000,train loss is: 0.00422928071802357\n",
      "test loss is 0.000731119220439479\n",
      "Batch: 32100,train loss is: 0.0004813463809525024\n",
      "test loss is 0.0007304612908292841\n",
      "Batch: 32200,train loss is: 0.0003970147491094413\n",
      "test loss is 0.0007411814407339735\n",
      "Batch: 32300,train loss is: 0.00037152532595944134\n",
      "test loss is 0.0007292560864129\n",
      "Batch: 32400,train loss is: 0.00043434675104646184\n",
      "test loss is 0.0007162639482673975\n",
      "Batch: 32500,train loss is: 0.0010078634851738168\n",
      "test loss is 0.0007590543990619766\n",
      "Batch: 32600,train loss is: 0.0004712436108198128\n",
      "test loss is 0.0007397060465532743\n",
      "Batch: 32700,train loss is: 0.00047606208580819125\n",
      "test loss is 0.0007427675561156107\n",
      "Batch: 32800,train loss is: 0.00041587557277397046\n",
      "test loss is 0.0007549267406846321\n",
      "Batch: 32900,train loss is: 0.0006489574594695294\n",
      "test loss is 0.0007205382328463844\n",
      "Batch: 33000,train loss is: 0.0005323251359956589\n",
      "test loss is 0.0007190065065013142\n",
      "Batch: 33100,train loss is: 0.00045544455145418396\n",
      "test loss is 0.0007571960301698353\n",
      "Batch: 33200,train loss is: 0.0005292045202651582\n",
      "test loss is 0.0008194162039135126\n",
      "Batch: 33300,train loss is: 0.0004183967259304987\n",
      "test loss is 0.000716426134114288\n",
      "Batch: 33400,train loss is: 0.000515310501007116\n",
      "test loss is 0.0007384324700864574\n",
      "Batch: 33500,train loss is: 0.00041452067568640476\n",
      "test loss is 0.0007313479742478364\n",
      "Batch: 33600,train loss is: 0.0005940644117874997\n",
      "test loss is 0.0007272834917829873\n",
      "Batch: 33700,train loss is: 0.0006342795638182756\n",
      "test loss is 0.0007421743875452334\n",
      "Batch: 33800,train loss is: 0.0008102371519890517\n",
      "test loss is 0.000721256714865604\n",
      "Batch: 33900,train loss is: 0.0007009160746054306\n",
      "test loss is 0.0007285983876135192\n",
      "-----------------------Epoch: 9----------------------------------\n",
      "Batch: 0,train loss is: 0.0003743319923583937\n",
      "test loss is 0.0007383225309681754\n",
      "Batch: 100,train loss is: 0.0014602737383529797\n",
      "test loss is 0.0007323811747777743\n",
      "Batch: 200,train loss is: 0.0005926710965332208\n",
      "test loss is 0.0007381589054253882\n",
      "Batch: 300,train loss is: 0.0003097704987392251\n",
      "test loss is 0.000729460371709269\n",
      "Batch: 400,train loss is: 0.0005066067504779745\n",
      "test loss is 0.0007235231431293127\n",
      "Batch: 500,train loss is: 0.0005924796698815806\n",
      "test loss is 0.0007571729452427304\n",
      "Batch: 600,train loss is: 0.00024913008199926726\n",
      "test loss is 0.0007598598183753819\n",
      "Batch: 700,train loss is: 0.00032304548516364054\n",
      "test loss is 0.0007235711128550466\n",
      "Batch: 800,train loss is: 0.0004588793025613422\n",
      "test loss is 0.0007274518107387652\n",
      "Batch: 900,train loss is: 0.0003259779794835756\n",
      "test loss is 0.0007234315503530055\n",
      "Batch: 1000,train loss is: 0.00047996001540006933\n",
      "test loss is 0.0007611810365272001\n",
      "Batch: 1100,train loss is: 0.0017869760169638093\n",
      "test loss is 0.0007194408787258807\n",
      "Batch: 1200,train loss is: 0.00034976064862341866\n",
      "test loss is 0.0007327349567160852\n",
      "Batch: 1300,train loss is: 0.000525989423949421\n",
      "test loss is 0.0007253072078874863\n",
      "Batch: 1400,train loss is: 0.0004841324442157174\n",
      "test loss is 0.0007438983972636342\n",
      "Batch: 1500,train loss is: 0.0005112123437322259\n",
      "test loss is 0.0007513855183062126\n",
      "Batch: 1600,train loss is: 0.0006545553223948117\n",
      "test loss is 0.0008043970195141682\n",
      "Batch: 1700,train loss is: 0.0012992953545643805\n",
      "test loss is 0.0007166582399682131\n",
      "Batch: 1800,train loss is: 0.00044963365139697955\n",
      "test loss is 0.0007374648780191335\n",
      "Batch: 1900,train loss is: 0.0003872197824877962\n",
      "test loss is 0.0007631520372846704\n",
      "Batch: 2000,train loss is: 0.0006382770497418719\n",
      "test loss is 0.0008554926843904586\n",
      "Batch: 2100,train loss is: 0.0006550920603746969\n",
      "test loss is 0.0008674355275666969\n",
      "Batch: 2200,train loss is: 0.0008520516024755013\n",
      "test loss is 0.0007296895278019356\n",
      "Batch: 2300,train loss is: 0.00042523553200964184\n",
      "test loss is 0.0007649831202359407\n",
      "Batch: 2400,train loss is: 0.0005584727965272676\n",
      "test loss is 0.0007313109212754524\n",
      "Batch: 2500,train loss is: 0.000694527279118587\n",
      "test loss is 0.0007241913611328759\n",
      "Batch: 2600,train loss is: 0.0003885070972019795\n",
      "test loss is 0.0007437992211292654\n",
      "Batch: 2700,train loss is: 0.0006776779290474904\n",
      "test loss is 0.000714298098241947\n",
      "Batch: 2800,train loss is: 0.0004881573251678568\n",
      "test loss is 0.0007583503420853409\n",
      "Batch: 2900,train loss is: 0.000490686858481569\n",
      "test loss is 0.000716167755944222\n",
      "Batch: 3000,train loss is: 0.0005206644767129121\n",
      "test loss is 0.0007155437633274236\n",
      "Batch: 3100,train loss is: 0.0006818216621624645\n",
      "test loss is 0.0007727244307713629\n",
      "Batch: 3200,train loss is: 0.0009818730514472133\n",
      "test loss is 0.0007406390220586864\n",
      "Batch: 3300,train loss is: 0.00047405612020955514\n",
      "test loss is 0.0007298764543347526\n",
      "Batch: 3400,train loss is: 0.0004836269330492556\n",
      "test loss is 0.0007269726347153857\n",
      "Batch: 3500,train loss is: 0.0002356617004940915\n",
      "test loss is 0.0007276010540613432\n",
      "Batch: 3600,train loss is: 0.0005334802767797567\n",
      "test loss is 0.000761964380904938\n",
      "Batch: 3700,train loss is: 0.0004059401899970061\n",
      "test loss is 0.0007183676005102451\n",
      "Batch: 3800,train loss is: 0.0004627455211702248\n",
      "test loss is 0.0007565946765156192\n",
      "Batch: 3900,train loss is: 0.0006569020133963766\n",
      "test loss is 0.0007278636670377843\n",
      "Batch: 4000,train loss is: 0.00043867151642943146\n",
      "test loss is 0.0007314886033302923\n",
      "Batch: 4100,train loss is: 0.0007387945185287007\n",
      "test loss is 0.0007477766007630662\n",
      "Batch: 4200,train loss is: 0.0003380070167001318\n",
      "test loss is 0.0007589423336753748\n",
      "Batch: 4300,train loss is: 0.000429961510659436\n",
      "test loss is 0.0007205557362062481\n",
      "Batch: 4400,train loss is: 0.00042052445794358445\n",
      "test loss is 0.000728097472472486\n",
      "Batch: 4500,train loss is: 0.0006769368446236915\n",
      "test loss is 0.0007530905128214185\n",
      "Batch: 4600,train loss is: 0.0005904761393614966\n",
      "test loss is 0.0007266657767928536\n",
      "Batch: 4700,train loss is: 0.000645774724539716\n",
      "test loss is 0.0007579105385007847\n",
      "Batch: 4800,train loss is: 0.001237443057418687\n",
      "test loss is 0.0007271327325482969\n",
      "Batch: 4900,train loss is: 0.0005623084616605377\n",
      "test loss is 0.0007314058957304932\n",
      "Batch: 5000,train loss is: 0.0006070779561339224\n",
      "test loss is 0.0007379921838938338\n",
      "Batch: 5100,train loss is: 0.0006073468857675257\n",
      "test loss is 0.0007940915033183699\n",
      "Batch: 5200,train loss is: 0.00033510408507256837\n",
      "test loss is 0.0007341012714937573\n",
      "Batch: 5300,train loss is: 0.0006613182583632149\n",
      "test loss is 0.0007545829198137116\n",
      "Batch: 5400,train loss is: 0.0007595003282623021\n",
      "test loss is 0.0007194914757266973\n",
      "Batch: 5500,train loss is: 0.0004674198298448228\n",
      "test loss is 0.0007446993947635311\n",
      "Batch: 5600,train loss is: 0.0006246076803148796\n",
      "test loss is 0.0007408246128010899\n",
      "Batch: 5700,train loss is: 0.0006945058685218892\n",
      "test loss is 0.0007291869741234209\n",
      "Batch: 5800,train loss is: 0.0005149308276873013\n",
      "test loss is 0.0007272790819552993\n",
      "Batch: 5900,train loss is: 0.0034405980107058616\n",
      "test loss is 0.000732112043003401\n",
      "Batch: 6000,train loss is: 0.0016442884964364111\n",
      "test loss is 0.0007385144270662928\n",
      "Batch: 6100,train loss is: 0.000443586796507904\n",
      "test loss is 0.0007170742724332519\n",
      "Batch: 6200,train loss is: 0.000597476279527383\n",
      "test loss is 0.0007326290512986824\n",
      "Batch: 6300,train loss is: 0.0012448044213341626\n",
      "test loss is 0.0007456692525878921\n",
      "Batch: 6400,train loss is: 0.0004911306515163618\n",
      "test loss is 0.0007793247375670727\n",
      "Batch: 6500,train loss is: 0.00035499414834896226\n",
      "test loss is 0.0007481332947756587\n",
      "Batch: 6600,train loss is: 0.0021858577705703213\n",
      "test loss is 0.0007263419970280067\n",
      "Batch: 6700,train loss is: 0.00039251923797797194\n",
      "test loss is 0.0007448749396557022\n",
      "Batch: 6800,train loss is: 0.0008473798480129774\n",
      "test loss is 0.00072356959185603\n",
      "Batch: 6900,train loss is: 0.0006727557726884097\n",
      "test loss is 0.0007288038808218023\n",
      "Batch: 7000,train loss is: 0.0005989113984194918\n",
      "test loss is 0.0007540885210817346\n",
      "Batch: 7100,train loss is: 0.0010994834332379742\n",
      "test loss is 0.0007401672375590463\n",
      "Batch: 7200,train loss is: 0.0004136013125204896\n",
      "test loss is 0.0007426943414250102\n",
      "Batch: 7300,train loss is: 0.0008381021147566524\n",
      "test loss is 0.0007822745982035002\n",
      "Batch: 7400,train loss is: 0.00037851556100221506\n",
      "test loss is 0.0007241568664597566\n",
      "Batch: 7500,train loss is: 0.0006826408046386589\n",
      "test loss is 0.0007438646796968665\n",
      "Batch: 7600,train loss is: 0.0004247553873740088\n",
      "test loss is 0.0007152449904013899\n",
      "Batch: 7700,train loss is: 0.0007595836848476345\n",
      "test loss is 0.0007273432065970127\n",
      "Batch: 7800,train loss is: 0.00029738535539619907\n",
      "test loss is 0.0007203429034388908\n",
      "Batch: 7900,train loss is: 0.0006191209820566422\n",
      "test loss is 0.0007361252670334302\n",
      "Batch: 8000,train loss is: 0.00047818128218886054\n",
      "test loss is 0.0007611704331871807\n",
      "Batch: 8100,train loss is: 0.0005867789687896181\n",
      "test loss is 0.00074479233356877\n",
      "Batch: 8200,train loss is: 0.0012421281667366886\n",
      "test loss is 0.0007436886123383868\n",
      "Batch: 8300,train loss is: 0.0005103456842514915\n",
      "test loss is 0.0007473326692073738\n",
      "Batch: 8400,train loss is: 0.000578399793446202\n",
      "test loss is 0.0007270930657048355\n",
      "Batch: 8500,train loss is: 0.000843298026342273\n",
      "test loss is 0.0007453706556652762\n",
      "Batch: 8600,train loss is: 0.0006980902476956984\n",
      "test loss is 0.0007491161124913356\n",
      "Batch: 8700,train loss is: 0.0005125790912554111\n",
      "test loss is 0.0007651001954542993\n",
      "Batch: 8800,train loss is: 0.000320040528767444\n",
      "test loss is 0.0007185445579878458\n",
      "Batch: 8900,train loss is: 0.001102687921715818\n",
      "test loss is 0.0007211627776750595\n",
      "Batch: 9000,train loss is: 0.00028477072505540834\n",
      "test loss is 0.0007345641238339654\n",
      "Batch: 9100,train loss is: 0.000382043902840309\n",
      "test loss is 0.0007205342170418334\n",
      "Batch: 9200,train loss is: 0.0008536773600459522\n",
      "test loss is 0.0007935336055308899\n",
      "Batch: 9300,train loss is: 0.0005957353507642192\n",
      "test loss is 0.0007454855707947593\n",
      "Batch: 9400,train loss is: 0.0004543186670781985\n",
      "test loss is 0.0007565408798290306\n",
      "Batch: 9500,train loss is: 0.0004126459976988194\n",
      "test loss is 0.0007478433194608302\n",
      "Batch: 9600,train loss is: 0.00045087057643675615\n",
      "test loss is 0.0007184844491431838\n",
      "Batch: 9700,train loss is: 0.0003601210491158041\n",
      "test loss is 0.0007443645742374574\n",
      "Batch: 9800,train loss is: 0.0003906262957148734\n",
      "test loss is 0.0007247876064807364\n",
      "Batch: 9900,train loss is: 0.0005324694686150013\n",
      "test loss is 0.0007598265728004416\n",
      "Batch: 10000,train loss is: 0.0004803108564101754\n",
      "test loss is 0.0007215732949413594\n",
      "Batch: 10100,train loss is: 0.0011039891812926188\n",
      "test loss is 0.0007313270215302775\n",
      "Batch: 10200,train loss is: 0.0017356429802464176\n",
      "test loss is 0.0008144152994716757\n",
      "Batch: 10300,train loss is: 0.000501099376202977\n",
      "test loss is 0.0007305193708881984\n",
      "Batch: 10400,train loss is: 0.00034302571617654065\n",
      "test loss is 0.0007249596520313346\n",
      "Batch: 10500,train loss is: 0.0007497893693809324\n",
      "test loss is 0.000726522102888319\n",
      "Batch: 10600,train loss is: 0.00034697521658307257\n",
      "test loss is 0.0007767208454778217\n",
      "Batch: 10700,train loss is: 0.0006481492689822735\n",
      "test loss is 0.0007252112729974681\n",
      "Batch: 10800,train loss is: 0.0005503282378641804\n",
      "test loss is 0.0007357485021375553\n",
      "Batch: 10900,train loss is: 0.0007083598698253118\n",
      "test loss is 0.0007303032080042244\n",
      "Batch: 11000,train loss is: 0.0008074970603804496\n",
      "test loss is 0.0007552493710389847\n",
      "Batch: 11100,train loss is: 0.001071195185019118\n",
      "test loss is 0.0007213918161839411\n",
      "Batch: 11200,train loss is: 0.0006883027037007064\n",
      "test loss is 0.0007643924570769438\n",
      "Batch: 11300,train loss is: 0.00032553474090451357\n",
      "test loss is 0.0007469606147313125\n",
      "Batch: 11400,train loss is: 0.0005930892346244914\n",
      "test loss is 0.0007358552697019427\n",
      "Batch: 11500,train loss is: 0.0005562441233232571\n",
      "test loss is 0.000732147391771966\n",
      "Batch: 11600,train loss is: 0.0019293462697393658\n",
      "test loss is 0.0007362430398488669\n",
      "Batch: 11700,train loss is: 0.0003030026396398384\n",
      "test loss is 0.0007123020360767305\n",
      "Batch: 11800,train loss is: 0.0006227284332242221\n",
      "test loss is 0.0007239538128760119\n",
      "Batch: 11900,train loss is: 0.0006260578894162472\n",
      "test loss is 0.0007407629407458717\n",
      "Batch: 12000,train loss is: 0.00039585779452349886\n",
      "test loss is 0.0007284701741762695\n",
      "Batch: 12100,train loss is: 0.0016450870289779207\n",
      "test loss is 0.0007326732879176719\n",
      "Batch: 12200,train loss is: 0.0007192457539253411\n",
      "test loss is 0.000753037119037749\n",
      "Batch: 12300,train loss is: 0.000462510577069236\n",
      "test loss is 0.0007440205033177791\n",
      "Batch: 12400,train loss is: 0.0005316854925361928\n",
      "test loss is 0.0007275897253298457\n",
      "Batch: 12500,train loss is: 0.00040024559050889447\n",
      "test loss is 0.0007513104095383051\n",
      "Batch: 12600,train loss is: 0.0004411095482310204\n",
      "test loss is 0.0007430132752976036\n",
      "Batch: 12700,train loss is: 0.0004566134347368684\n",
      "test loss is 0.0007175217187767537\n",
      "Batch: 12800,train loss is: 0.000393936437792179\n",
      "test loss is 0.0007235278754123974\n",
      "Batch: 12900,train loss is: 0.0006859717018650027\n",
      "test loss is 0.0007411145170557596\n",
      "Batch: 13000,train loss is: 0.00023579595717286266\n",
      "test loss is 0.000712711276036902\n",
      "Batch: 13100,train loss is: 0.0022627772885504024\n",
      "test loss is 0.0007177760205570306\n",
      "Batch: 13200,train loss is: 0.0007871602764996598\n",
      "test loss is 0.0008354178689791142\n",
      "Batch: 13300,train loss is: 0.0006042018421908381\n",
      "test loss is 0.0007527805310903963\n",
      "Batch: 13400,train loss is: 0.00039725670331864374\n",
      "test loss is 0.0007543176965502313\n",
      "Batch: 13500,train loss is: 0.000627097285573043\n",
      "test loss is 0.0007581067748096096\n",
      "Batch: 13600,train loss is: 0.0005348718989965586\n",
      "test loss is 0.0007377673608117107\n",
      "Batch: 13700,train loss is: 0.0007148037410290446\n",
      "test loss is 0.0007212204281594873\n",
      "Batch: 13800,train loss is: 0.0005049606072269429\n",
      "test loss is 0.0007246179196378188\n",
      "Batch: 13900,train loss is: 0.0005151606739416631\n",
      "test loss is 0.0007700141377595774\n",
      "Batch: 14000,train loss is: 0.00043242401339664473\n",
      "test loss is 0.0007400016277662318\n",
      "Batch: 14100,train loss is: 0.0008072976780759127\n",
      "test loss is 0.0007792149298811342\n",
      "Batch: 14200,train loss is: 0.0005893338545390817\n",
      "test loss is 0.0007100414507646259\n",
      "Batch: 14300,train loss is: 0.0003164134344961577\n",
      "test loss is 0.0007200859407408499\n",
      "Batch: 14400,train loss is: 0.0013377341355996525\n",
      "test loss is 0.0007228463511473412\n",
      "Batch: 14500,train loss is: 0.00043559876756657577\n",
      "test loss is 0.000725724943966151\n",
      "Batch: 14600,train loss is: 0.0013976387504149868\n",
      "test loss is 0.0007232622780128595\n",
      "Batch: 14700,train loss is: 0.000477587383849069\n",
      "test loss is 0.0007708277772927371\n",
      "Batch: 14800,train loss is: 0.0008644271764341093\n",
      "test loss is 0.0007326424536737927\n",
      "Batch: 14900,train loss is: 0.00040981230529888476\n",
      "test loss is 0.0007637692712990224\n",
      "Batch: 15000,train loss is: 0.0007905481873944773\n",
      "test loss is 0.0007165447861702399\n",
      "Batch: 15100,train loss is: 0.0006690542033527428\n",
      "test loss is 0.0007492361245463549\n",
      "Batch: 15200,train loss is: 0.0010340064558452778\n",
      "test loss is 0.0007333732858851002\n",
      "Batch: 15300,train loss is: 0.00044402718732965157\n",
      "test loss is 0.0007180305964521634\n",
      "Batch: 15400,train loss is: 0.00047914442456845475\n",
      "test loss is 0.0007383555720736732\n",
      "Batch: 15500,train loss is: 0.0007760739014467368\n",
      "test loss is 0.0007126970814440159\n",
      "Batch: 15600,train loss is: 0.0009522455825126293\n",
      "test loss is 0.0007573825340713677\n",
      "Batch: 15700,train loss is: 0.0005133669653673102\n",
      "test loss is 0.0007222627713846319\n",
      "Batch: 15800,train loss is: 0.0007105464658969211\n",
      "test loss is 0.0007164849335736769\n",
      "Batch: 15900,train loss is: 0.0009573288457544012\n",
      "test loss is 0.0007371670913166987\n",
      "Batch: 16000,train loss is: 0.0003902069437273831\n",
      "test loss is 0.0007439840413635007\n",
      "Batch: 16100,train loss is: 0.0005657932047759105\n",
      "test loss is 0.0007248471793628974\n",
      "Batch: 16200,train loss is: 0.0004029380794374149\n",
      "test loss is 0.0007241836868731111\n",
      "Batch: 16300,train loss is: 0.0010986221949535462\n",
      "test loss is 0.0007527070152889854\n",
      "Batch: 16400,train loss is: 0.0004063579186760403\n",
      "test loss is 0.0007155820301634104\n",
      "Batch: 16500,train loss is: 0.0006147851965422751\n",
      "test loss is 0.0007519653349861514\n",
      "Batch: 16600,train loss is: 0.00031743815934854414\n",
      "test loss is 0.0007158561990689269\n",
      "Batch: 16700,train loss is: 0.0009592543157417298\n",
      "test loss is 0.0007356443773566503\n",
      "Batch: 16800,train loss is: 0.0005946846643950304\n",
      "test loss is 0.0007499192365827208\n",
      "Batch: 16900,train loss is: 0.000702667117665788\n",
      "test loss is 0.0007317480779994067\n",
      "Batch: 17000,train loss is: 0.0007298488344673302\n",
      "test loss is 0.0007098475022975783\n",
      "Batch: 17100,train loss is: 0.0005703489792269463\n",
      "test loss is 0.0007242971973446939\n",
      "Batch: 17200,train loss is: 0.0005344914644531442\n",
      "test loss is 0.0007288367151216341\n",
      "Batch: 17300,train loss is: 0.000924647260828986\n",
      "test loss is 0.0007446160321368913\n",
      "Batch: 17400,train loss is: 0.000429574223717529\n",
      "test loss is 0.0007265597503459549\n",
      "Batch: 17500,train loss is: 0.0005022447444367536\n",
      "test loss is 0.000721736061100776\n",
      "Batch: 17600,train loss is: 0.000423895976778799\n",
      "test loss is 0.0007410904451123364\n",
      "Batch: 17700,train loss is: 0.0003510688681770404\n",
      "test loss is 0.0007569061934069839\n",
      "Batch: 17800,train loss is: 0.0013211950755756391\n",
      "test loss is 0.0007285546574644148\n",
      "Batch: 17900,train loss is: 0.00048475775548945766\n",
      "test loss is 0.0007602473603239033\n",
      "Batch: 18000,train loss is: 0.0003571461955306573\n",
      "test loss is 0.0007104645243116959\n",
      "Batch: 18100,train loss is: 0.000456178462586345\n",
      "test loss is 0.0007225971490097898\n",
      "Batch: 18200,train loss is: 0.0005776237641998794\n",
      "test loss is 0.000729975229095698\n",
      "Batch: 18300,train loss is: 0.0010342063320159362\n",
      "test loss is 0.0007258220246244874\n",
      "Batch: 18400,train loss is: 0.0004460012791343068\n",
      "test loss is 0.0007207462500915021\n",
      "Batch: 18500,train loss is: 0.0009073290862627068\n",
      "test loss is 0.0007349483359530816\n",
      "Batch: 18600,train loss is: 0.000513396696475538\n",
      "test loss is 0.0007222690387493398\n",
      "Batch: 18700,train loss is: 0.0005575944401971198\n",
      "test loss is 0.0007332826631918869\n",
      "Batch: 18800,train loss is: 0.0005840907877029848\n",
      "test loss is 0.0007229755581890301\n",
      "Batch: 18900,train loss is: 0.0006616461380199049\n",
      "test loss is 0.000756189441310686\n",
      "Batch: 19000,train loss is: 0.0006644542607667386\n",
      "test loss is 0.000725046662511343\n",
      "Batch: 19100,train loss is: 0.00030464709330717027\n",
      "test loss is 0.0007487644593573551\n",
      "Batch: 19200,train loss is: 0.0005510861099703442\n",
      "test loss is 0.000730829953751061\n",
      "Batch: 19300,train loss is: 0.0007988357917751073\n",
      "test loss is 0.0007141809387438688\n",
      "Batch: 19400,train loss is: 0.00030405322902524336\n",
      "test loss is 0.0007130983497922658\n",
      "Batch: 19500,train loss is: 0.000564169410866247\n",
      "test loss is 0.0007589624821015815\n",
      "Batch: 19600,train loss is: 0.0004523041907657179\n",
      "test loss is 0.0007354097172489317\n",
      "Batch: 19700,train loss is: 0.0007821837741717833\n",
      "test loss is 0.0008025435881710683\n",
      "Batch: 19800,train loss is: 0.0005989088991776459\n",
      "test loss is 0.0007311130194681894\n",
      "Batch: 19900,train loss is: 0.00025574366357765084\n",
      "test loss is 0.0007623446953740619\n",
      "Batch: 20000,train loss is: 0.0007604325486190684\n",
      "test loss is 0.0007272332045270648\n",
      "Batch: 20100,train loss is: 0.0004828047067110408\n",
      "test loss is 0.0007307970743477917\n",
      "Batch: 20200,train loss is: 0.0008963369022946737\n",
      "test loss is 0.0007420233998890123\n",
      "Batch: 20300,train loss is: 0.000363168273957299\n",
      "test loss is 0.0007474797355698158\n",
      "Batch: 20400,train loss is: 0.00048519528036708844\n",
      "test loss is 0.000767788500983772\n",
      "Batch: 20500,train loss is: 0.0006166799391509203\n",
      "test loss is 0.0007255472648198302\n",
      "Batch: 20600,train loss is: 0.0005369022057152916\n",
      "test loss is 0.0007565000425915516\n",
      "Batch: 20700,train loss is: 0.000679769569518089\n",
      "test loss is 0.0007332643942947367\n",
      "Batch: 20800,train loss is: 0.00036055315602133897\n",
      "test loss is 0.0007408303617110833\n",
      "Batch: 20900,train loss is: 0.0006463302033648154\n",
      "test loss is 0.0007746673978182243\n",
      "Batch: 21000,train loss is: 0.00027515267808696074\n",
      "test loss is 0.0007158899438287297\n",
      "Batch: 21100,train loss is: 0.0006997167968145216\n",
      "test loss is 0.0007472963719195732\n",
      "Batch: 21200,train loss is: 0.0003428369163984182\n",
      "test loss is 0.0007477419149981136\n",
      "Batch: 21300,train loss is: 0.00035386860021544616\n",
      "test loss is 0.0007218430762711171\n",
      "Batch: 21400,train loss is: 0.0008401577700902406\n",
      "test loss is 0.0007329641380675677\n",
      "Batch: 21500,train loss is: 0.0002236516030273493\n",
      "test loss is 0.0007428710077410824\n",
      "Batch: 21600,train loss is: 0.0007347640902860711\n",
      "test loss is 0.0007375083015044429\n",
      "Batch: 21700,train loss is: 0.0003306977258447492\n",
      "test loss is 0.0007184410037238486\n",
      "Batch: 21800,train loss is: 0.0006558162771474629\n",
      "test loss is 0.0007780792498734008\n",
      "Batch: 21900,train loss is: 0.0005345306382190571\n",
      "test loss is 0.0008051919439038579\n",
      "Batch: 22000,train loss is: 0.0007034042155617317\n",
      "test loss is 0.0007218812269737463\n",
      "Batch: 22100,train loss is: 0.0003476779088123293\n",
      "test loss is 0.0007271795554587912\n",
      "Batch: 22200,train loss is: 0.0005168776569653445\n",
      "test loss is 0.0007261893860272167\n",
      "Batch: 22300,train loss is: 0.0006662481637874066\n",
      "test loss is 0.0007296450490415107\n",
      "Batch: 22400,train loss is: 0.0005114329307837667\n",
      "test loss is 0.0007267189945428202\n",
      "Batch: 22500,train loss is: 0.0004669165830012141\n",
      "test loss is 0.0007162951640674912\n",
      "Batch: 22600,train loss is: 0.0010526135655958583\n",
      "test loss is 0.0007654996595750081\n",
      "Batch: 22700,train loss is: 0.0004101776691664756\n",
      "test loss is 0.0007175139186963521\n",
      "Batch: 22800,train loss is: 0.0005523651979223901\n",
      "test loss is 0.0007434401877465022\n",
      "Batch: 22900,train loss is: 0.0003531933529921378\n",
      "test loss is 0.0007927926876012687\n",
      "Batch: 23000,train loss is: 0.00042238852160376923\n",
      "test loss is 0.0007250904765128844\n",
      "Batch: 23100,train loss is: 0.0005214321008039486\n",
      "test loss is 0.0007091790328498059\n",
      "Batch: 23200,train loss is: 0.00032407481899873904\n",
      "test loss is 0.0007044052181019268\n",
      "Batch: 23300,train loss is: 0.0005221215780859113\n",
      "test loss is 0.000715923250714944\n",
      "Batch: 23400,train loss is: 0.0006698598916918078\n",
      "test loss is 0.0007487347727641972\n",
      "Batch: 23500,train loss is: 0.002153549398322939\n",
      "test loss is 0.0007193148060525316\n",
      "Batch: 23600,train loss is: 0.0002931734430570793\n",
      "test loss is 0.0007148642691410054\n",
      "Batch: 23700,train loss is: 0.000914880007693804\n",
      "test loss is 0.0007316905297751914\n",
      "Batch: 23800,train loss is: 0.0008572996375531455\n",
      "test loss is 0.0007209038612922645\n",
      "Batch: 23900,train loss is: 0.0009002848376256623\n",
      "test loss is 0.0007296819261230313\n",
      "Batch: 24000,train loss is: 0.0008122930390039885\n",
      "test loss is 0.0007746604424227952\n",
      "Batch: 24100,train loss is: 0.00036800801207217206\n",
      "test loss is 0.0007084082012131596\n",
      "Batch: 24200,train loss is: 0.001546267546522134\n",
      "test loss is 0.0007379931017786339\n",
      "Batch: 24300,train loss is: 0.0005020271371175372\n",
      "test loss is 0.000865674261957028\n",
      "Batch: 24400,train loss is: 0.001334251265631201\n",
      "test loss is 0.0007199904006104102\n",
      "Batch: 24500,train loss is: 0.0006584981222295446\n",
      "test loss is 0.0007148070112686789\n",
      "Batch: 24600,train loss is: 0.0004979547840047272\n",
      "test loss is 0.0007522060392841567\n",
      "Batch: 24700,train loss is: 0.0002992861751892967\n",
      "test loss is 0.0007231531355092378\n",
      "Batch: 24800,train loss is: 0.0009140795638431266\n",
      "test loss is 0.0007253676245344002\n",
      "Batch: 24900,train loss is: 0.00018107738721857372\n",
      "test loss is 0.0007226187346017905\n",
      "Batch: 25000,train loss is: 0.0007391768997176022\n",
      "test loss is 0.0007195014955879855\n",
      "Batch: 25100,train loss is: 0.0006005782468464089\n",
      "test loss is 0.0007326504664237133\n",
      "Batch: 25200,train loss is: 0.0007600351028625572\n",
      "test loss is 0.0007353085506721374\n",
      "Batch: 25300,train loss is: 0.0008183187840945841\n",
      "test loss is 0.0007560756460844758\n",
      "Batch: 25400,train loss is: 0.00041142572483251393\n",
      "test loss is 0.000715936442506662\n",
      "Batch: 25500,train loss is: 0.0005869676527877507\n",
      "test loss is 0.0007234954972085197\n",
      "Batch: 25600,train loss is: 0.0006616689572839835\n",
      "test loss is 0.0007244957076551114\n",
      "Batch: 25700,train loss is: 0.0005820267088740108\n",
      "test loss is 0.0007433584905391137\n",
      "Batch: 25800,train loss is: 0.0003548242619965324\n",
      "test loss is 0.0007131148799231275\n",
      "Batch: 25900,train loss is: 0.00031032614142722967\n",
      "test loss is 0.0007658212358430398\n",
      "Batch: 26000,train loss is: 0.001661615724423574\n",
      "test loss is 0.0007820872454820613\n",
      "Batch: 26100,train loss is: 0.0008943459782450348\n",
      "test loss is 0.0007103950691219354\n",
      "Batch: 26200,train loss is: 0.0006728973001533335\n",
      "test loss is 0.0007090606226322438\n",
      "Batch: 26300,train loss is: 0.0011713083816779059\n",
      "test loss is 0.000736684447766944\n",
      "Batch: 26400,train loss is: 0.0006087858869756568\n",
      "test loss is 0.0007192723983653189\n",
      "Batch: 26500,train loss is: 0.00037441099915113546\n",
      "test loss is 0.0007252397260167085\n",
      "Batch: 26600,train loss is: 0.001386945259299122\n",
      "test loss is 0.0008006061796284566\n",
      "Batch: 26700,train loss is: 0.0004379525176451846\n",
      "test loss is 0.0008250881269383886\n",
      "Batch: 26800,train loss is: 0.0012103261843937102\n",
      "test loss is 0.0007260553526660346\n",
      "Batch: 26900,train loss is: 0.0004937771238909064\n",
      "test loss is 0.0007257727204503962\n",
      "Batch: 27000,train loss is: 0.000682559670979161\n",
      "test loss is 0.0008552431322214935\n",
      "Batch: 27100,train loss is: 0.0005954043100656302\n",
      "test loss is 0.0007745243240206213\n",
      "Batch: 27200,train loss is: 0.0006955642751563584\n",
      "test loss is 0.000764007571184094\n",
      "Batch: 27300,train loss is: 0.0005481936499298563\n",
      "test loss is 0.0007293955702299653\n",
      "Batch: 27400,train loss is: 0.0009414419134220573\n",
      "test loss is 0.0007231447162996114\n",
      "Batch: 27500,train loss is: 0.0005286168055028871\n",
      "test loss is 0.0007998276586567881\n",
      "Batch: 27600,train loss is: 0.0006006379421525141\n",
      "test loss is 0.0007667334283293906\n",
      "Batch: 27700,train loss is: 0.0016818178046719746\n",
      "test loss is 0.0007342493071808488\n",
      "Batch: 27800,train loss is: 0.0007728089410781573\n",
      "test loss is 0.0007529548649585858\n",
      "Batch: 27900,train loss is: 0.001367983107917949\n",
      "test loss is 0.0007225315074730449\n",
      "Batch: 28000,train loss is: 0.00042912644911369203\n",
      "test loss is 0.0007219386464660685\n",
      "Batch: 28100,train loss is: 0.0008979411566592575\n",
      "test loss is 0.0007435474266683183\n",
      "Batch: 28200,train loss is: 0.0006189452482770005\n",
      "test loss is 0.0007033287371674477\n",
      "Batch: 28300,train loss is: 0.0019223667203807054\n",
      "test loss is 0.0007294405196141043\n",
      "Batch: 28400,train loss is: 0.0005495249635548625\n",
      "test loss is 0.0007133447546889215\n",
      "Batch: 28500,train loss is: 0.001483165135759781\n",
      "test loss is 0.0007471596842899043\n",
      "Batch: 28600,train loss is: 0.0003794649147936207\n",
      "test loss is 0.0007368008853904041\n",
      "Batch: 28700,train loss is: 0.0005224225319181939\n",
      "test loss is 0.0007205758273395156\n",
      "Batch: 28800,train loss is: 0.0011149410751536732\n",
      "test loss is 0.0007271326137856234\n",
      "Batch: 28900,train loss is: 0.0004770119551497638\n",
      "test loss is 0.0007143164486895217\n",
      "Batch: 29000,train loss is: 0.0005326499178075566\n",
      "test loss is 0.0007148636766349909\n",
      "Batch: 29100,train loss is: 0.0006337629965654618\n",
      "test loss is 0.0007278658909826189\n",
      "Batch: 29200,train loss is: 0.00044043299178513485\n",
      "test loss is 0.0007335407245439866\n",
      "Batch: 29300,train loss is: 0.0010622087196634378\n",
      "test loss is 0.0007139065073035299\n",
      "Batch: 29400,train loss is: 0.0007160552224178454\n",
      "test loss is 0.0007912308080282559\n",
      "Batch: 29500,train loss is: 0.00047429279122897965\n",
      "test loss is 0.0007397414636632399\n",
      "Batch: 29600,train loss is: 0.0005148882271821802\n",
      "test loss is 0.0007143830512234611\n",
      "Batch: 29700,train loss is: 0.00035303433845217927\n",
      "test loss is 0.0007326935963369938\n",
      "Batch: 29800,train loss is: 0.0008437911873095394\n",
      "test loss is 0.0007096474776196409\n",
      "Batch: 29900,train loss is: 0.003218040903902958\n",
      "test loss is 0.0007178096363327908\n",
      "Batch: 30000,train loss is: 0.0003468096379034873\n",
      "test loss is 0.0007688489046070453\n",
      "Batch: 30100,train loss is: 0.0005113814110786134\n",
      "test loss is 0.0007570656147404448\n",
      "Batch: 30200,train loss is: 0.00037059600258341807\n",
      "test loss is 0.0007215250040831053\n",
      "Batch: 30300,train loss is: 0.00029028888000492346\n",
      "test loss is 0.000710230062075245\n",
      "Batch: 30400,train loss is: 0.0003710745336545588\n",
      "test loss is 0.0007228928292892986\n",
      "Batch: 30500,train loss is: 0.0003792612917362974\n",
      "test loss is 0.0007717965141972758\n",
      "Batch: 30600,train loss is: 0.0002504009957164404\n",
      "test loss is 0.0007166493396469474\n",
      "Batch: 30700,train loss is: 0.00040109022237419664\n",
      "test loss is 0.0008351758383267231\n",
      "Batch: 30800,train loss is: 0.000286557000898148\n",
      "test loss is 0.000787611873882673\n",
      "Batch: 30900,train loss is: 0.00036023018476339974\n",
      "test loss is 0.0007196734577803321\n",
      "Batch: 31000,train loss is: 0.0005281509693906536\n",
      "test loss is 0.0007314054533028238\n",
      "Batch: 31100,train loss is: 0.0003855432637212092\n",
      "test loss is 0.0007996105336965029\n",
      "Batch: 31200,train loss is: 0.000589557179105452\n",
      "test loss is 0.0007274672573093703\n",
      "Batch: 31300,train loss is: 0.0005667390441755806\n",
      "test loss is 0.0007298983206736395\n",
      "Batch: 31400,train loss is: 0.0008088949026058591\n",
      "test loss is 0.0007216044619249013\n",
      "Batch: 31500,train loss is: 0.0005097026588333706\n",
      "test loss is 0.0007258725168590274\n",
      "Batch: 31600,train loss is: 0.00047052518698701193\n",
      "test loss is 0.0007270005168220732\n",
      "Batch: 31700,train loss is: 0.0005853465741534224\n",
      "test loss is 0.0007510088324163034\n",
      "Batch: 31800,train loss is: 0.0013020092092781593\n",
      "test loss is 0.0007216387688278246\n",
      "Batch: 31900,train loss is: 0.0008266396450752121\n",
      "test loss is 0.0007431139640266052\n",
      "Batch: 32000,train loss is: 0.004216096102773768\n",
      "test loss is 0.0007229844974114891\n",
      "Batch: 32100,train loss is: 0.0004775527058759405\n",
      "test loss is 0.0007241847107916125\n",
      "Batch: 32200,train loss is: 0.0003843663545971841\n",
      "test loss is 0.0007326882165337183\n",
      "Batch: 32300,train loss is: 0.0003655097868873499\n",
      "test loss is 0.0007190578905970362\n",
      "Batch: 32400,train loss is: 0.0004224404101452325\n",
      "test loss is 0.0007068302858634968\n",
      "Batch: 32500,train loss is: 0.0009950414067144522\n",
      "test loss is 0.0007508228750631937\n",
      "Batch: 32600,train loss is: 0.00047616355860874036\n",
      "test loss is 0.0007319415076660277\n",
      "Batch: 32700,train loss is: 0.00047874443821797853\n",
      "test loss is 0.000736225151147563\n",
      "Batch: 32800,train loss is: 0.0004158226761461183\n",
      "test loss is 0.0007480878649187399\n",
      "Batch: 32900,train loss is: 0.0006482336593763056\n",
      "test loss is 0.0007108215812004677\n",
      "Batch: 33000,train loss is: 0.0005221957889251055\n",
      "test loss is 0.000710995501030778\n",
      "Batch: 33100,train loss is: 0.0004451590064135901\n",
      "test loss is 0.0007474787037606473\n",
      "Batch: 33200,train loss is: 0.0005153571749708154\n",
      "test loss is 0.000806196511956888\n",
      "Batch: 33300,train loss is: 0.00043431881320074025\n",
      "test loss is 0.0007078986940094284\n",
      "Batch: 33400,train loss is: 0.000503463004726226\n",
      "test loss is 0.0007289267010349098\n",
      "Batch: 33500,train loss is: 0.00040056100186174683\n",
      "test loss is 0.0007226636058898729\n",
      "Batch: 33600,train loss is: 0.0005899050634681302\n",
      "test loss is 0.0007172132499522987\n",
      "Batch: 33700,train loss is: 0.0006154509044824071\n",
      "test loss is 0.0007334791146740669\n",
      "Batch: 33800,train loss is: 0.0007994774651597259\n",
      "test loss is 0.0007120135820314995\n",
      "Batch: 33900,train loss is: 0.0006989576138934822\n",
      "test loss is 0.0007193677359163387\n",
      "-----------------------Epoch: 10----------------------------------\n",
      "Batch: 0,train loss is: 0.0003503193700836821\n",
      "test loss is 0.0007291986223193904\n",
      "Batch: 100,train loss is: 0.0014532805361759402\n",
      "test loss is 0.0007224937705606434\n",
      "Batch: 200,train loss is: 0.0005624727002785418\n",
      "test loss is 0.0007296506024985199\n",
      "Batch: 300,train loss is: 0.0003014219554924284\n",
      "test loss is 0.0007197764506057454\n",
      "Batch: 400,train loss is: 0.0004937393995714316\n",
      "test loss is 0.0007133645826836088\n",
      "Batch: 500,train loss is: 0.0005709844736008382\n",
      "test loss is 0.0007463084441762008\n",
      "Batch: 600,train loss is: 0.0002491455067912238\n",
      "test loss is 0.0007524338202280402\n",
      "Batch: 700,train loss is: 0.0003283283557310996\n",
      "test loss is 0.0007133307423612384\n",
      "Batch: 800,train loss is: 0.0004552337543721286\n",
      "test loss is 0.0007183343184199085\n",
      "Batch: 900,train loss is: 0.00031455219048353824\n",
      "test loss is 0.0007148747054504949\n",
      "Batch: 1000,train loss is: 0.0004656590691783294\n",
      "test loss is 0.0007524044159677692\n",
      "Batch: 1100,train loss is: 0.001763817250607884\n",
      "test loss is 0.0007103859089544381\n",
      "Batch: 1200,train loss is: 0.00033814674479208635\n",
      "test loss is 0.000725386736316374\n",
      "Batch: 1300,train loss is: 0.0005272102879962461\n",
      "test loss is 0.0007164982323314432\n",
      "Batch: 1400,train loss is: 0.0004641438522164323\n",
      "test loss is 0.0007362776980715448\n",
      "Batch: 1500,train loss is: 0.0005078707367333921\n",
      "test loss is 0.0007415713619806677\n",
      "Batch: 1600,train loss is: 0.0006673775453343751\n",
      "test loss is 0.0007939028021860276\n",
      "Batch: 1700,train loss is: 0.0012854227926908238\n",
      "test loss is 0.0007073857189054655\n",
      "Batch: 1800,train loss is: 0.00043118852238835603\n",
      "test loss is 0.000725967289258481\n",
      "Batch: 1900,train loss is: 0.00038911779957771266\n",
      "test loss is 0.0007531116776011682\n",
      "Batch: 2000,train loss is: 0.0006384864053244392\n",
      "test loss is 0.0008484117097613551\n",
      "Batch: 2100,train loss is: 0.0006567195487655759\n",
      "test loss is 0.0008614916049140271\n",
      "Batch: 2200,train loss is: 0.0008361504548147598\n",
      "test loss is 0.0007206393843353598\n",
      "Batch: 2300,train loss is: 0.0004255946150978879\n",
      "test loss is 0.0007536411093388042\n",
      "Batch: 2400,train loss is: 0.0005429551034346758\n",
      "test loss is 0.0007214382127572942\n",
      "Batch: 2500,train loss is: 0.0006776577730092238\n",
      "test loss is 0.000715070072551321\n",
      "Batch: 2600,train loss is: 0.0003826671342179823\n",
      "test loss is 0.0007323357308895891\n",
      "Batch: 2700,train loss is: 0.000669648365903528\n",
      "test loss is 0.0007044346002233065\n",
      "Batch: 2800,train loss is: 0.0004782633857008746\n",
      "test loss is 0.0007454538175833519\n",
      "Batch: 2900,train loss is: 0.00047567984945272773\n",
      "test loss is 0.0007069914556210116\n",
      "Batch: 3000,train loss is: 0.0005173972515590386\n",
      "test loss is 0.0007063754801896543\n",
      "Batch: 3100,train loss is: 0.0006819503843415634\n",
      "test loss is 0.0007627403239534591\n",
      "Batch: 3200,train loss is: 0.000988926312067223\n",
      "test loss is 0.0007316003066261402\n",
      "Batch: 3300,train loss is: 0.00048464963039928646\n",
      "test loss is 0.0007210336716373237\n",
      "Batch: 3400,train loss is: 0.00048057580262105367\n",
      "test loss is 0.0007172410790915701\n",
      "Batch: 3500,train loss is: 0.00022982018866421014\n",
      "test loss is 0.0007172455567383739\n",
      "Batch: 3600,train loss is: 0.0005371569845536112\n",
      "test loss is 0.0007526517495695592\n",
      "Batch: 3700,train loss is: 0.0003911677692829618\n",
      "test loss is 0.000710484518714064\n",
      "Batch: 3800,train loss is: 0.0004613667661211143\n",
      "test loss is 0.0007484516950231767\n",
      "Batch: 3900,train loss is: 0.000645347145982267\n",
      "test loss is 0.0007190523152926806\n",
      "Batch: 4000,train loss is: 0.0004386043602251475\n",
      "test loss is 0.0007217013702487948\n",
      "Batch: 4100,train loss is: 0.0007379670755457315\n",
      "test loss is 0.0007398732636962207\n",
      "Batch: 4200,train loss is: 0.0003245563355637327\n",
      "test loss is 0.0007491714865537402\n",
      "Batch: 4300,train loss is: 0.00042186420556729933\n",
      "test loss is 0.0007106521884254025\n",
      "Batch: 4400,train loss is: 0.00038965661867613186\n",
      "test loss is 0.0007213238263009648\n",
      "Batch: 4500,train loss is: 0.0006944808577976305\n",
      "test loss is 0.0007428925528822401\n",
      "Batch: 4600,train loss is: 0.0005873062013102841\n",
      "test loss is 0.0007180018205969622\n",
      "Batch: 4700,train loss is: 0.0006151649138942923\n",
      "test loss is 0.0007485608482015366\n",
      "Batch: 4800,train loss is: 0.0012416601556892695\n",
      "test loss is 0.0007155151610603553\n",
      "Batch: 4900,train loss is: 0.0005532818343312228\n",
      "test loss is 0.0007216606024940675\n",
      "Batch: 5000,train loss is: 0.000606171653649146\n",
      "test loss is 0.0007295203481914278\n",
      "Batch: 5100,train loss is: 0.0005806019417100513\n",
      "test loss is 0.0007841255628162353\n",
      "Batch: 5200,train loss is: 0.0003258237278322974\n",
      "test loss is 0.0007248632999317487\n",
      "Batch: 5300,train loss is: 0.000640841127072889\n",
      "test loss is 0.0007441379345989479\n",
      "Batch: 5400,train loss is: 0.0007535831469126512\n",
      "test loss is 0.0007095896189912695\n",
      "Batch: 5500,train loss is: 0.0004619632538653696\n",
      "test loss is 0.0007356583265418232\n",
      "Batch: 5600,train loss is: 0.000604406819830747\n",
      "test loss is 0.0007308920669244997\n",
      "Batch: 5700,train loss is: 0.0006620283989280853\n",
      "test loss is 0.000720282862698111\n",
      "Batch: 5800,train loss is: 0.0005150144481648285\n",
      "test loss is 0.0007178837039704318\n",
      "Batch: 5900,train loss is: 0.00343768040197055\n",
      "test loss is 0.0007219577991137094\n",
      "Batch: 6000,train loss is: 0.0016361858250302445\n",
      "test loss is 0.0007309365679359265\n",
      "Batch: 6100,train loss is: 0.00043634774593771157\n",
      "test loss is 0.0007077616799407852\n",
      "Batch: 6200,train loss is: 0.0005963585159279864\n",
      "test loss is 0.0007240343248854183\n",
      "Batch: 6300,train loss is: 0.0012244361831425918\n",
      "test loss is 0.0007367758570072181\n",
      "Batch: 6400,train loss is: 0.0004974752432704654\n",
      "test loss is 0.0007690325693722197\n",
      "Batch: 6500,train loss is: 0.0003395270008109107\n",
      "test loss is 0.0007371835567437743\n",
      "Batch: 6600,train loss is: 0.0021702639097224566\n",
      "test loss is 0.0007162832148741683\n",
      "Batch: 6700,train loss is: 0.0003785016954354778\n",
      "test loss is 0.0007343404540495248\n",
      "Batch: 6800,train loss is: 0.0008224355291539587\n",
      "test loss is 0.0007141845206336051\n",
      "Batch: 6900,train loss is: 0.0006917596658702955\n",
      "test loss is 0.0007200715974008947\n",
      "Batch: 7000,train loss is: 0.0005903540493213366\n",
      "test loss is 0.0007438302592464711\n",
      "Batch: 7100,train loss is: 0.0011035337143498866\n",
      "test loss is 0.0007302220921211562\n",
      "Batch: 7200,train loss is: 0.0004175019059509439\n",
      "test loss is 0.0007334153718735053\n",
      "Batch: 7300,train loss is: 0.000829897390679508\n",
      "test loss is 0.0007709285016271661\n",
      "Batch: 7400,train loss is: 0.00037169521385757\n",
      "test loss is 0.0007157504727064726\n",
      "Batch: 7500,train loss is: 0.000681204459752805\n",
      "test loss is 0.0007353947434835093\n",
      "Batch: 7600,train loss is: 0.00041063838188521866\n",
      "test loss is 0.0007050968501714085\n",
      "Batch: 7700,train loss is: 0.0007783202456472008\n",
      "test loss is 0.0007192686676196757\n",
      "Batch: 7800,train loss is: 0.0002883578746812398\n",
      "test loss is 0.0007109819753564417\n",
      "Batch: 7900,train loss is: 0.0006104615780096983\n",
      "test loss is 0.0007281863187023773\n",
      "Batch: 8000,train loss is: 0.0004702882207056849\n",
      "test loss is 0.0007528116365572292\n",
      "Batch: 8100,train loss is: 0.0005871061818488537\n",
      "test loss is 0.0007360535604645643\n",
      "Batch: 8200,train loss is: 0.0012103899790227178\n",
      "test loss is 0.0007343807071744969\n",
      "Batch: 8300,train loss is: 0.0004782132747288499\n",
      "test loss is 0.0007374477765908653\n",
      "Batch: 8400,train loss is: 0.0005767396462171367\n",
      "test loss is 0.0007174643912066676\n",
      "Batch: 8500,train loss is: 0.000812425241390326\n",
      "test loss is 0.0007350465338413948\n",
      "Batch: 8600,train loss is: 0.000696932155266131\n",
      "test loss is 0.0007404654435247178\n",
      "Batch: 8700,train loss is: 0.0005094530079395326\n",
      "test loss is 0.0007588304031369095\n",
      "Batch: 8800,train loss is: 0.00032863999079963465\n",
      "test loss is 0.0007097537928624677\n",
      "Batch: 8900,train loss is: 0.001099891128028168\n",
      "test loss is 0.0007118577114396194\n",
      "Batch: 9000,train loss is: 0.0002773799016332252\n",
      "test loss is 0.0007264462328098429\n",
      "Batch: 9100,train loss is: 0.00036676012960778554\n",
      "test loss is 0.0007104565764902623\n",
      "Batch: 9200,train loss is: 0.0008364617429675877\n",
      "test loss is 0.0007854397864355115\n",
      "Batch: 9300,train loss is: 0.0005875924460641067\n",
      "test loss is 0.0007412017034616744\n",
      "Batch: 9400,train loss is: 0.00044603293196061454\n",
      "test loss is 0.0007455098935055086\n",
      "Batch: 9500,train loss is: 0.0004014042772124759\n",
      "test loss is 0.0007400408109097803\n",
      "Batch: 9600,train loss is: 0.00044465205891099037\n",
      "test loss is 0.0007082756296100942\n",
      "Batch: 9700,train loss is: 0.0003477335502347778\n",
      "test loss is 0.0007337754400438096\n",
      "Batch: 9800,train loss is: 0.00036363212037992333\n",
      "test loss is 0.0007154746748613644\n",
      "Batch: 9900,train loss is: 0.000525240501049104\n",
      "test loss is 0.0007487347675153429\n",
      "Batch: 10000,train loss is: 0.0004681651571533995\n",
      "test loss is 0.000711460152289869\n",
      "Batch: 10100,train loss is: 0.001104296946246658\n",
      "test loss is 0.0007239337554690831\n",
      "Batch: 10200,train loss is: 0.0017326815190440228\n",
      "test loss is 0.000804553296035776\n",
      "Batch: 10300,train loss is: 0.0004977258577835045\n",
      "test loss is 0.0007209081806802023\n",
      "Batch: 10400,train loss is: 0.00032489113530274216\n",
      "test loss is 0.00071544040395508\n",
      "Batch: 10500,train loss is: 0.0007462760455263614\n",
      "test loss is 0.0007168292294587642\n",
      "Batch: 10600,train loss is: 0.00033840952984735465\n",
      "test loss is 0.000768441285609406\n",
      "Batch: 10700,train loss is: 0.0006475641658301993\n",
      "test loss is 0.0007157671745555348\n",
      "Batch: 10800,train loss is: 0.0005559812450183437\n",
      "test loss is 0.0007262702978688694\n",
      "Batch: 10900,train loss is: 0.0007048894994942735\n",
      "test loss is 0.0007205922701163537\n",
      "Batch: 11000,train loss is: 0.000803581827741635\n",
      "test loss is 0.0007453221082570857\n",
      "Batch: 11100,train loss is: 0.0010639842219165815\n",
      "test loss is 0.0007124221185198467\n",
      "Batch: 11200,train loss is: 0.0006885751474503077\n",
      "test loss is 0.0007554679328096314\n",
      "Batch: 11300,train loss is: 0.0003217561585830012\n",
      "test loss is 0.0007341698298651699\n",
      "Batch: 11400,train loss is: 0.0006114589004563557\n",
      "test loss is 0.0007252579046769868\n",
      "Batch: 11500,train loss is: 0.0005544446080179888\n",
      "test loss is 0.0007237432006397823\n",
      "Batch: 11600,train loss is: 0.0019203698179144175\n",
      "test loss is 0.0007271912296819904\n",
      "Batch: 11700,train loss is: 0.0002996523088613853\n",
      "test loss is 0.0007031607855295995\n",
      "Batch: 11800,train loss is: 0.0006140347906152699\n",
      "test loss is 0.0007138992636549756\n",
      "Batch: 11900,train loss is: 0.0006065568978689469\n",
      "test loss is 0.0007305794991472603\n",
      "Batch: 12000,train loss is: 0.00037426988490916384\n",
      "test loss is 0.0007182041972280434\n",
      "Batch: 12100,train loss is: 0.001634381956888293\n",
      "test loss is 0.0007236075499250956\n",
      "Batch: 12200,train loss is: 0.0007075400503054065\n",
      "test loss is 0.0007426927918402949\n",
      "Batch: 12300,train loss is: 0.00045597110608191333\n",
      "test loss is 0.0007352194735481684\n",
      "Batch: 12400,train loss is: 0.000502890351678922\n",
      "test loss is 0.0007185830103022578\n",
      "Batch: 12500,train loss is: 0.000407449386226099\n",
      "test loss is 0.0007423882017087689\n",
      "Batch: 12600,train loss is: 0.0004527439491086344\n",
      "test loss is 0.0007339041239494973\n",
      "Batch: 12700,train loss is: 0.0004398845981602038\n",
      "test loss is 0.0007077429520044397\n",
      "Batch: 12800,train loss is: 0.0003824322574291801\n",
      "test loss is 0.0007132936061521306\n",
      "Batch: 12900,train loss is: 0.000682540618394506\n",
      "test loss is 0.000731297453037359\n",
      "Batch: 13000,train loss is: 0.00023001923535368097\n",
      "test loss is 0.0007044028867163408\n",
      "Batch: 13100,train loss is: 0.00224116749227712\n",
      "test loss is 0.000708364233596206\n",
      "Batch: 13200,train loss is: 0.0007898437890499918\n",
      "test loss is 0.0008289264151954446\n",
      "Batch: 13300,train loss is: 0.0006257385106935735\n",
      "test loss is 0.0007394603960977574\n",
      "Batch: 13400,train loss is: 0.0003912722534716641\n",
      "test loss is 0.0007449497185655786\n",
      "Batch: 13500,train loss is: 0.000625407357323748\n",
      "test loss is 0.000748546044865062\n",
      "Batch: 13600,train loss is: 0.000542077823785908\n",
      "test loss is 0.0007287470203254105\n",
      "Batch: 13700,train loss is: 0.0007087450419606376\n",
      "test loss is 0.0007114115976049546\n",
      "Batch: 13800,train loss is: 0.0004820092212486389\n",
      "test loss is 0.0007145098751243672\n",
      "Batch: 13900,train loss is: 0.00048214600375598134\n",
      "test loss is 0.000763639425061205\n",
      "Batch: 14000,train loss is: 0.0004326871456088174\n",
      "test loss is 0.0007313956501818804\n",
      "Batch: 14100,train loss is: 0.0007961942005913567\n",
      "test loss is 0.0007705859759616422\n",
      "Batch: 14200,train loss is: 0.0005981063132991632\n",
      "test loss is 0.0007012274221453082\n",
      "Batch: 14300,train loss is: 0.0002957753146985917\n",
      "test loss is 0.0007113579636597949\n",
      "Batch: 14400,train loss is: 0.001327289456704715\n",
      "test loss is 0.0007138007483793738\n",
      "Batch: 14500,train loss is: 0.0004452232462756911\n",
      "test loss is 0.0007166172776020508\n",
      "Batch: 14600,train loss is: 0.0013211387289117928\n",
      "test loss is 0.0007149735080172858\n",
      "Batch: 14700,train loss is: 0.00047512933580013837\n",
      "test loss is 0.0007649279417924082\n",
      "Batch: 14800,train loss is: 0.0008535371186461111\n",
      "test loss is 0.0007249401132370964\n",
      "Batch: 14900,train loss is: 0.00041280498888645304\n",
      "test loss is 0.0007570850109468883\n",
      "Batch: 15000,train loss is: 0.0007679812482296912\n",
      "test loss is 0.0007073938855475303\n",
      "Batch: 15100,train loss is: 0.0006644356290541585\n",
      "test loss is 0.0007393347688468993\n",
      "Batch: 15200,train loss is: 0.0009920894176661278\n",
      "test loss is 0.0007238514776750601\n",
      "Batch: 15300,train loss is: 0.00042479830849996163\n",
      "test loss is 0.0007095934035345259\n",
      "Batch: 15400,train loss is: 0.0004721752843746185\n",
      "test loss is 0.0007278192666219408\n",
      "Batch: 15500,train loss is: 0.0007572106095555281\n",
      "test loss is 0.0007034024668438782\n",
      "Batch: 15600,train loss is: 0.0009296704971520957\n",
      "test loss is 0.0007467893692288367\n",
      "Batch: 15700,train loss is: 0.0004927054402114486\n",
      "test loss is 0.0007139112443993625\n",
      "Batch: 15800,train loss is: 0.0007130960320829879\n",
      "test loss is 0.0007062492908626337\n",
      "Batch: 15900,train loss is: 0.0009290284307044683\n",
      "test loss is 0.0007275844878071313\n",
      "Batch: 16000,train loss is: 0.0003888598686663674\n",
      "test loss is 0.0007338609640512709\n",
      "Batch: 16100,train loss is: 0.0005563215307945828\n",
      "test loss is 0.0007147125010889429\n",
      "Batch: 16200,train loss is: 0.0004011121373737416\n",
      "test loss is 0.000713560301552003\n",
      "Batch: 16300,train loss is: 0.0010720069710441606\n",
      "test loss is 0.0007461693180873678\n",
      "Batch: 16400,train loss is: 0.0003920843002479479\n",
      "test loss is 0.0007065380122456453\n",
      "Batch: 16500,train loss is: 0.000617589919445861\n",
      "test loss is 0.0007392875940590005\n",
      "Batch: 16600,train loss is: 0.00031392864196966844\n",
      "test loss is 0.0007066422974009999\n",
      "Batch: 16700,train loss is: 0.0009386092983293319\n",
      "test loss is 0.0007262811838428561\n",
      "Batch: 16800,train loss is: 0.0005849752813396363\n",
      "test loss is 0.0007416345638708472\n",
      "Batch: 16900,train loss is: 0.0006890530235245965\n",
      "test loss is 0.0007230733985775553\n",
      "Batch: 17000,train loss is: 0.0006982563744033573\n",
      "test loss is 0.0007007351110216272\n",
      "Batch: 17100,train loss is: 0.0005606731188759645\n",
      "test loss is 0.000716267712667472\n",
      "Batch: 17200,train loss is: 0.0005313294131164987\n",
      "test loss is 0.0007200469894744854\n",
      "Batch: 17300,train loss is: 0.0009151806899971814\n",
      "test loss is 0.000739362765933407\n",
      "Batch: 17400,train loss is: 0.0004257628773291495\n",
      "test loss is 0.0007182633257697093\n",
      "Batch: 17500,train loss is: 0.0005097089618714433\n",
      "test loss is 0.000713444324003311\n",
      "Batch: 17600,train loss is: 0.0004242120086971987\n",
      "test loss is 0.0007316737125909675\n",
      "Batch: 17700,train loss is: 0.0003578103514363044\n",
      "test loss is 0.0007482548082152746\n",
      "Batch: 17800,train loss is: 0.0013782703873219848\n",
      "test loss is 0.0007190169577943392\n",
      "Batch: 17900,train loss is: 0.0004920606558512599\n",
      "test loss is 0.0007504266586112498\n",
      "Batch: 18000,train loss is: 0.0003413612620964915\n",
      "test loss is 0.0007019676505631941\n",
      "Batch: 18100,train loss is: 0.0004448181244354698\n",
      "test loss is 0.0007128135337407565\n",
      "Batch: 18200,train loss is: 0.000575151746071393\n",
      "test loss is 0.0007218427921559672\n",
      "Batch: 18300,train loss is: 0.0010584113538732668\n",
      "test loss is 0.0007172413183898661\n",
      "Batch: 18400,train loss is: 0.00042876778570989193\n",
      "test loss is 0.0007114056306657715\n",
      "Batch: 18500,train loss is: 0.0009037307227938228\n",
      "test loss is 0.0007249310086658063\n",
      "Batch: 18600,train loss is: 0.0005039698296943921\n",
      "test loss is 0.0007132882714765385\n",
      "Batch: 18700,train loss is: 0.0005648185000333209\n",
      "test loss is 0.0007262215069359829\n",
      "Batch: 18800,train loss is: 0.000566698357470767\n",
      "test loss is 0.0007137685326457392\n",
      "Batch: 18900,train loss is: 0.0006409958634243942\n",
      "test loss is 0.0007458335112004252\n",
      "Batch: 19000,train loss is: 0.000661378595784354\n",
      "test loss is 0.0007156154357488664\n",
      "Batch: 19100,train loss is: 0.0003094145799199325\n",
      "test loss is 0.0007392778381201601\n",
      "Batch: 19200,train loss is: 0.0005424197868747635\n",
      "test loss is 0.0007216442186540895\n",
      "Batch: 19300,train loss is: 0.000772943113924549\n",
      "test loss is 0.0007046561378193239\n",
      "Batch: 19400,train loss is: 0.00029553442138487656\n",
      "test loss is 0.0007044184512460887\n",
      "Batch: 19500,train loss is: 0.0005481677527960311\n",
      "test loss is 0.0007489887971918442\n",
      "Batch: 19600,train loss is: 0.0004460554671325676\n",
      "test loss is 0.0007264897807951061\n",
      "Batch: 19700,train loss is: 0.000760262496509817\n",
      "test loss is 0.0007927703481411214\n",
      "Batch: 19800,train loss is: 0.0006040111770668156\n",
      "test loss is 0.000722132221608214\n",
      "Batch: 19900,train loss is: 0.0002634235990193438\n",
      "test loss is 0.0007504127439590392\n",
      "Batch: 20000,train loss is: 0.0007469228702848312\n",
      "test loss is 0.0007182147662750755\n",
      "Batch: 20100,train loss is: 0.0004713333876393996\n",
      "test loss is 0.0007213336016393818\n",
      "Batch: 20200,train loss is: 0.0008767639040136805\n",
      "test loss is 0.000733460323800992\n",
      "Batch: 20300,train loss is: 0.00034568377329958865\n",
      "test loss is 0.0007379934472134727\n",
      "Batch: 20400,train loss is: 0.0004821334862432163\n",
      "test loss is 0.0007603720529192799\n",
      "Batch: 20500,train loss is: 0.0006115567811472583\n",
      "test loss is 0.0007157034936922264\n",
      "Batch: 20600,train loss is: 0.0005208339160133056\n",
      "test loss is 0.0007462171590207912\n",
      "Batch: 20700,train loss is: 0.000684738955846616\n",
      "test loss is 0.0007234090123789876\n",
      "Batch: 20800,train loss is: 0.0003548545038289233\n",
      "test loss is 0.0007339594603838228\n",
      "Batch: 20900,train loss is: 0.0006313716631612715\n",
      "test loss is 0.0007663254232164503\n",
      "Batch: 21000,train loss is: 0.00026115195517301137\n",
      "test loss is 0.0007060449954894312\n",
      "Batch: 21100,train loss is: 0.0006977704622032932\n",
      "test loss is 0.0007384281971124902\n",
      "Batch: 21200,train loss is: 0.0003475269610215487\n",
      "test loss is 0.0007412840079378368\n",
      "Batch: 21300,train loss is: 0.0003463422658123385\n",
      "test loss is 0.0007130511837065849\n",
      "Batch: 21400,train loss is: 0.0008266964659866662\n",
      "test loss is 0.0007234993891225611\n",
      "Batch: 21500,train loss is: 0.0002231765788035066\n",
      "test loss is 0.0007313109624305604\n",
      "Batch: 21600,train loss is: 0.0007202643715232129\n",
      "test loss is 0.0007286287766999763\n",
      "Batch: 21700,train loss is: 0.0003321791538383831\n",
      "test loss is 0.0007083672771615604\n",
      "Batch: 21800,train loss is: 0.0006754822507926225\n",
      "test loss is 0.0007712341243021402\n",
      "Batch: 21900,train loss is: 0.0005221112947826376\n",
      "test loss is 0.0007982924124989025\n",
      "Batch: 22000,train loss is: 0.0006909131327605593\n",
      "test loss is 0.0007122805290788398\n",
      "Batch: 22100,train loss is: 0.000348160960664313\n",
      "test loss is 0.0007177967269245531\n",
      "Batch: 22200,train loss is: 0.0005178116446257022\n",
      "test loss is 0.0007166658203248201\n",
      "Batch: 22300,train loss is: 0.0006448291629307204\n",
      "test loss is 0.0007213269215315855\n",
      "Batch: 22400,train loss is: 0.0004992917085686634\n",
      "test loss is 0.0007199108648144812\n",
      "Batch: 22500,train loss is: 0.0004745497344197878\n",
      "test loss is 0.000707315159657396\n",
      "Batch: 22600,train loss is: 0.0010318188586342868\n",
      "test loss is 0.0007523274094095846\n",
      "Batch: 22700,train loss is: 0.0004037272336638443\n",
      "test loss is 0.0007069934932133024\n",
      "Batch: 22800,train loss is: 0.0005513938737592008\n",
      "test loss is 0.0007359777154522816\n",
      "Batch: 22900,train loss is: 0.0003402466170737604\n",
      "test loss is 0.000784985204432209\n",
      "Batch: 23000,train loss is: 0.0004293797744885801\n",
      "test loss is 0.0007166821296563746\n",
      "Batch: 23100,train loss is: 0.0005115477215858719\n",
      "test loss is 0.0006992915599514418\n",
      "Batch: 23200,train loss is: 0.0003148876876360597\n",
      "test loss is 0.0006959361718549251\n",
      "Batch: 23300,train loss is: 0.0005097732793698832\n",
      "test loss is 0.0007076100552560889\n",
      "Batch: 23400,train loss is: 0.0006531063847986969\n",
      "test loss is 0.0007407784280433413\n",
      "Batch: 23500,train loss is: 0.002063059809377945\n",
      "test loss is 0.0007093029957187737\n",
      "Batch: 23600,train loss is: 0.0002918164256417617\n",
      "test loss is 0.0007064200561665148\n",
      "Batch: 23700,train loss is: 0.0009156312870092445\n",
      "test loss is 0.0007204506630859248\n",
      "Batch: 23800,train loss is: 0.0008429731562191387\n",
      "test loss is 0.000712566268559594\n",
      "Batch: 23900,train loss is: 0.0008939091353885732\n",
      "test loss is 0.0007213160708945762\n",
      "Batch: 24000,train loss is: 0.0007891065223330203\n",
      "test loss is 0.000764651612807361\n",
      "Batch: 24100,train loss is: 0.0003563648457219324\n",
      "test loss is 0.0006984930955534214\n",
      "Batch: 24200,train loss is: 0.0015156866564175878\n",
      "test loss is 0.0007285011817380899\n",
      "Batch: 24300,train loss is: 0.0005149002620077572\n",
      "test loss is 0.0008548061745872768\n",
      "Batch: 24400,train loss is: 0.001333598660492971\n",
      "test loss is 0.0007106326585167536\n",
      "Batch: 24500,train loss is: 0.0006623681525375527\n",
      "test loss is 0.0007043525951149263\n",
      "Batch: 24600,train loss is: 0.00047777716191080006\n",
      "test loss is 0.0007407982570773851\n",
      "Batch: 24700,train loss is: 0.00029621936961877685\n",
      "test loss is 0.0007138851848985659\n",
      "Batch: 24800,train loss is: 0.0008951438413486063\n",
      "test loss is 0.0007174143926212292\n",
      "Batch: 24900,train loss is: 0.00018497701500682265\n",
      "test loss is 0.0007133624071645831\n",
      "Batch: 25000,train loss is: 0.0007217095138760765\n",
      "test loss is 0.0007102283275814969\n",
      "Batch: 25100,train loss is: 0.0005921770499204454\n",
      "test loss is 0.000722492267149546\n",
      "Batch: 25200,train loss is: 0.0007581515432752441\n",
      "test loss is 0.0007257625716776543\n",
      "Batch: 25300,train loss is: 0.000802609917524842\n",
      "test loss is 0.0007487538911089385\n",
      "Batch: 25400,train loss is: 0.0004115211769656922\n",
      "test loss is 0.000707377012626614\n",
      "Batch: 25500,train loss is: 0.0005878861030360358\n",
      "test loss is 0.0007161000859457511\n",
      "Batch: 25600,train loss is: 0.0006555728925541302\n",
      "test loss is 0.0007153171623214441\n",
      "Batch: 25700,train loss is: 0.0005729481155827302\n",
      "test loss is 0.0007345881268045858\n",
      "Batch: 25800,train loss is: 0.00035987008473762467\n",
      "test loss is 0.0007028128716595881\n",
      "Batch: 25900,train loss is: 0.0003061767026623779\n",
      "test loss is 0.000756953682881245\n",
      "Batch: 26000,train loss is: 0.0016300034293137403\n",
      "test loss is 0.0007723554853445315\n",
      "Batch: 26100,train loss is: 0.0008714591060504986\n",
      "test loss is 0.0007008153474613324\n",
      "Batch: 26200,train loss is: 0.0006519119547335287\n",
      "test loss is 0.0007001920569832982\n",
      "Batch: 26300,train loss is: 0.0011643470817960305\n",
      "test loss is 0.0007261933044352885\n",
      "Batch: 26400,train loss is: 0.0005952191761695042\n",
      "test loss is 0.000708627912709525\n",
      "Batch: 26500,train loss is: 0.00039206821686306177\n",
      "test loss is 0.0007191315096733474\n",
      "Batch: 26600,train loss is: 0.001392818004023621\n",
      "test loss is 0.0007919511311915207\n",
      "Batch: 26700,train loss is: 0.00044489169677455634\n",
      "test loss is 0.0008146841892082411\n",
      "Batch: 26800,train loss is: 0.0012008320998177066\n",
      "test loss is 0.0007182894914285347\n",
      "Batch: 26900,train loss is: 0.0004881200737934326\n",
      "test loss is 0.0007171477504004322\n",
      "Batch: 27000,train loss is: 0.0006802787703917845\n",
      "test loss is 0.000844993557418271\n",
      "Batch: 27100,train loss is: 0.0006106496557061647\n",
      "test loss is 0.0007676564513025754\n",
      "Batch: 27200,train loss is: 0.0006991857455819225\n",
      "test loss is 0.0007555588460073761\n",
      "Batch: 27300,train loss is: 0.0005279253819290289\n",
      "test loss is 0.0007218078776217131\n",
      "Batch: 27400,train loss is: 0.000909179777007374\n",
      "test loss is 0.0007160352587982859\n",
      "Batch: 27500,train loss is: 0.0005132144049073021\n",
      "test loss is 0.0007908446300976024\n",
      "Batch: 27600,train loss is: 0.000590479971133001\n",
      "test loss is 0.0007559253939718566\n",
      "Batch: 27700,train loss is: 0.0016583329162558039\n",
      "test loss is 0.0007260436550047424\n",
      "Batch: 27800,train loss is: 0.0007857805980138806\n",
      "test loss is 0.0007450616393247162\n",
      "Batch: 27900,train loss is: 0.001362459506189393\n",
      "test loss is 0.0007135932806528133\n",
      "Batch: 28000,train loss is: 0.0004181861691520856\n",
      "test loss is 0.0007124375587010652\n",
      "Batch: 28100,train loss is: 0.0008886645500097722\n",
      "test loss is 0.0007342180333413267\n",
      "Batch: 28200,train loss is: 0.0005984755802943109\n",
      "test loss is 0.0006941932777259592\n",
      "Batch: 28300,train loss is: 0.0018638145107686185\n",
      "test loss is 0.0007220485662791904\n",
      "Batch: 28400,train loss is: 0.0005362610365219257\n",
      "test loss is 0.0007046674876747573\n",
      "Batch: 28500,train loss is: 0.0014968106917099034\n",
      "test loss is 0.0007405674997684515\n",
      "Batch: 28600,train loss is: 0.00038090254174513293\n",
      "test loss is 0.0007245855618577969\n",
      "Batch: 28700,train loss is: 0.0005337842032603848\n",
      "test loss is 0.0007124340934901258\n",
      "Batch: 28800,train loss is: 0.0011079131184891448\n",
      "test loss is 0.0007188884421931253\n",
      "Batch: 28900,train loss is: 0.00046954256134537365\n",
      "test loss is 0.0007046661869822261\n",
      "Batch: 29000,train loss is: 0.0005209800857000428\n",
      "test loss is 0.0007073432143268691\n",
      "Batch: 29100,train loss is: 0.0006068131630898535\n",
      "test loss is 0.0007184977343246675\n",
      "Batch: 29200,train loss is: 0.00043545194401087625\n",
      "test loss is 0.0007252643551804367\n",
      "Batch: 29300,train loss is: 0.001056004727587224\n",
      "test loss is 0.0007054156191458682\n",
      "Batch: 29400,train loss is: 0.0007281530052073148\n",
      "test loss is 0.0007840015147272262\n",
      "Batch: 29500,train loss is: 0.00047234677510435137\n",
      "test loss is 0.0007317008042502264\n",
      "Batch: 29600,train loss is: 0.0005042112504170964\n",
      "test loss is 0.0007029990631196852\n",
      "Batch: 29700,train loss is: 0.00034989829400898113\n",
      "test loss is 0.0007226713857535026\n",
      "Batch: 29800,train loss is: 0.000830355166669899\n",
      "test loss is 0.0007003231746347239\n",
      "Batch: 29900,train loss is: 0.0031636332330618935\n",
      "test loss is 0.0007080284132577555\n",
      "Batch: 30000,train loss is: 0.0003490829534049213\n",
      "test loss is 0.0007598214076298083\n",
      "Batch: 30100,train loss is: 0.0005141236861333917\n",
      "test loss is 0.0007488553159332776\n",
      "Batch: 30200,train loss is: 0.0003712252783930088\n",
      "test loss is 0.0007131020746514539\n",
      "Batch: 30300,train loss is: 0.00028400104408646823\n",
      "test loss is 0.0007012781880841542\n",
      "Batch: 30400,train loss is: 0.0003634946321444316\n",
      "test loss is 0.0007137642711143536\n",
      "Batch: 30500,train loss is: 0.00036750714919308913\n",
      "test loss is 0.0007611969426754187\n",
      "Batch: 30600,train loss is: 0.00024411472669615727\n",
      "test loss is 0.0007077476798304745\n",
      "Batch: 30700,train loss is: 0.00039442484118176757\n",
      "test loss is 0.0008221499686148837\n",
      "Batch: 30800,train loss is: 0.00028181740352064774\n",
      "test loss is 0.0007767581794248368\n",
      "Batch: 30900,train loss is: 0.0003580041489299406\n",
      "test loss is 0.0007115250003795637\n",
      "Batch: 31000,train loss is: 0.0005152660743331539\n",
      "test loss is 0.0007233613864535737\n",
      "Batch: 31100,train loss is: 0.00037753082071414893\n",
      "test loss is 0.000792875476549294\n",
      "Batch: 31200,train loss is: 0.0005723741066529502\n",
      "test loss is 0.0007172726781280837\n",
      "Batch: 31300,train loss is: 0.0005563213320479929\n",
      "test loss is 0.0007207348681382421\n",
      "Batch: 31400,train loss is: 0.0008038352936436707\n",
      "test loss is 0.0007120548580375435\n",
      "Batch: 31500,train loss is: 0.0004927258686676241\n",
      "test loss is 0.0007157660839484504\n",
      "Batch: 31600,train loss is: 0.00045677799613781206\n",
      "test loss is 0.0007175467154219942\n",
      "Batch: 31700,train loss is: 0.0005785840727917807\n",
      "test loss is 0.0007410990563166592\n",
      "Batch: 31800,train loss is: 0.001298575084619487\n",
      "test loss is 0.0007135712036173276\n",
      "Batch: 31900,train loss is: 0.0008050589495628947\n",
      "test loss is 0.0007310787698608715\n",
      "Batch: 32000,train loss is: 0.004160589242159566\n",
      "test loss is 0.0007153652418645912\n",
      "Batch: 32100,train loss is: 0.0004665120046280743\n",
      "test loss is 0.0007178552916338926\n",
      "Batch: 32200,train loss is: 0.00036945481697744345\n",
      "test loss is 0.0007243349234914355\n",
      "Batch: 32300,train loss is: 0.00035975957674318963\n",
      "test loss is 0.0007089119563098878\n",
      "Batch: 32400,train loss is: 0.00041649337611873727\n",
      "test loss is 0.0006982766481519355\n",
      "Batch: 32500,train loss is: 0.000971111480956311\n",
      "test loss is 0.00074297917319005\n",
      "Batch: 32600,train loss is: 0.0004787536751331551\n",
      "test loss is 0.0007228148664080384\n",
      "Batch: 32700,train loss is: 0.0004835998513129781\n",
      "test loss is 0.0007298211632576814\n",
      "Batch: 32800,train loss is: 0.0004191773272570766\n",
      "test loss is 0.000742158523726125\n",
      "Batch: 32900,train loss is: 0.00065141192641834\n",
      "test loss is 0.0007024097160140949\n",
      "Batch: 33000,train loss is: 0.0005083372322687843\n",
      "test loss is 0.0007036427667645415\n",
      "Batch: 33100,train loss is: 0.0004383026028501956\n",
      "test loss is 0.0007366391915775469\n",
      "Batch: 33200,train loss is: 0.000496370515774724\n",
      "test loss is 0.0007951373773577272\n",
      "Batch: 33300,train loss is: 0.000447718262168489\n",
      "test loss is 0.0006993903004589373\n",
      "Batch: 33400,train loss is: 0.000492009989864877\n",
      "test loss is 0.0007197556283843674\n",
      "Batch: 33500,train loss is: 0.00038769698461854364\n",
      "test loss is 0.000713648261636661\n",
      "Batch: 33600,train loss is: 0.0005841404271161281\n",
      "test loss is 0.0007079903707426711\n",
      "Batch: 33700,train loss is: 0.0006007737243029295\n",
      "test loss is 0.0007243069720188468\n",
      "Batch: 33800,train loss is: 0.000795812936814164\n",
      "test loss is 0.0007033095369796277\n",
      "Batch: 33900,train loss is: 0.0006926799832545147\n",
      "test loss is 0.0007102801219083546\n",
      "-----------------------Epoch: 11----------------------------------\n",
      "Batch: 0,train loss is: 0.000333655770918343\n",
      "test loss is 0.00072046331911969\n",
      "Batch: 100,train loss is: 0.0014260460480438463\n",
      "test loss is 0.0007136810476849582\n",
      "Batch: 200,train loss is: 0.0005275582008936298\n",
      "test loss is 0.000722462832884399\n",
      "Batch: 300,train loss is: 0.0002935669526415397\n",
      "test loss is 0.0007110399522036904\n",
      "Batch: 400,train loss is: 0.0004826972052453538\n",
      "test loss is 0.0007035962371147775\n",
      "Batch: 500,train loss is: 0.0005507889728910996\n",
      "test loss is 0.0007360661439997697\n",
      "Batch: 600,train loss is: 0.0002478507671537389\n",
      "test loss is 0.0007452035088474686\n",
      "Batch: 700,train loss is: 0.00033736788389687625\n",
      "test loss is 0.0007038916746960875\n",
      "Batch: 800,train loss is: 0.00044904872491489283\n",
      "test loss is 0.0007105764648568362\n",
      "Batch: 900,train loss is: 0.00030726337109301727\n",
      "test loss is 0.0007071192707290434\n",
      "Batch: 1000,train loss is: 0.0004576160942924196\n",
      "test loss is 0.0007438665278819637\n",
      "Batch: 1100,train loss is: 0.0017610365459233936\n",
      "test loss is 0.0007015959027461777\n",
      "Batch: 1200,train loss is: 0.0003269389824936008\n",
      "test loss is 0.0007175313748175314\n",
      "Batch: 1300,train loss is: 0.0005255963131689758\n",
      "test loss is 0.0007075972727598538\n",
      "Batch: 1400,train loss is: 0.0004531430982398022\n",
      "test loss is 0.0007277701772113095\n",
      "Batch: 1500,train loss is: 0.0005002024101133291\n",
      "test loss is 0.0007331179357566302\n",
      "Batch: 1600,train loss is: 0.0006798642712290411\n",
      "test loss is 0.0007844158062182353\n",
      "Batch: 1700,train loss is: 0.0012689944753915949\n",
      "test loss is 0.0006986353495891812\n",
      "Batch: 1800,train loss is: 0.00041140055960778353\n",
      "test loss is 0.0007148937661128673\n",
      "Batch: 1900,train loss is: 0.0003913775039262377\n",
      "test loss is 0.000744145383334776\n",
      "Batch: 2000,train loss is: 0.0006456391723737621\n",
      "test loss is 0.000843247441684746\n",
      "Batch: 2100,train loss is: 0.0006607647327153639\n",
      "test loss is 0.0008555955542819138\n",
      "Batch: 2200,train loss is: 0.0008156074735867022\n",
      "test loss is 0.0007120886210956431\n",
      "Batch: 2300,train loss is: 0.0004269068119213185\n",
      "test loss is 0.0007434397167646109\n",
      "Batch: 2400,train loss is: 0.0005270632187732841\n",
      "test loss is 0.0007121793180820447\n",
      "Batch: 2500,train loss is: 0.0006559983366001491\n",
      "test loss is 0.0007060060747484268\n",
      "Batch: 2600,train loss is: 0.00038517818420080675\n",
      "test loss is 0.0007210495667556027\n",
      "Batch: 2700,train loss is: 0.0006589169479114497\n",
      "test loss is 0.0006953439190224221\n",
      "Batch: 2800,train loss is: 0.0004704622619852208\n",
      "test loss is 0.0007342806702504363\n",
      "Batch: 2900,train loss is: 0.0004671132726584636\n",
      "test loss is 0.0006988735514972138\n",
      "Batch: 3000,train loss is: 0.0005146041941556187\n",
      "test loss is 0.0006979117957698369\n",
      "Batch: 3100,train loss is: 0.000683716689058317\n",
      "test loss is 0.0007536404949021676\n",
      "Batch: 3200,train loss is: 0.0009846948735558127\n",
      "test loss is 0.0007227333670120544\n",
      "Batch: 3300,train loss is: 0.0004960185329745552\n",
      "test loss is 0.0007133033849167709\n",
      "Batch: 3400,train loss is: 0.00047197305569956284\n",
      "test loss is 0.0007081695774745637\n",
      "Batch: 3500,train loss is: 0.00022713056933723483\n",
      "test loss is 0.0007082319559101836\n",
      "Batch: 3600,train loss is: 0.0005474951204061544\n",
      "test loss is 0.0007446006325841157\n",
      "Batch: 3700,train loss is: 0.0003762009061178005\n",
      "test loss is 0.0007031935994108369\n",
      "Batch: 3800,train loss is: 0.0004560577040155911\n",
      "test loss is 0.0007389758959194488\n",
      "Batch: 3900,train loss is: 0.0006358613478826995\n",
      "test loss is 0.0007110439463483844\n",
      "Batch: 4000,train loss is: 0.0004386324990044375\n",
      "test loss is 0.0007127784027374127\n",
      "Batch: 4100,train loss is: 0.0007298623187738692\n",
      "test loss is 0.0007310189156774646\n",
      "Batch: 4200,train loss is: 0.0003136453608349195\n",
      "test loss is 0.0007397454283392436\n",
      "Batch: 4300,train loss is: 0.0004189446473268445\n",
      "test loss is 0.000701088117601919\n",
      "Batch: 4400,train loss is: 0.0003655671997184183\n",
      "test loss is 0.0007143866175361318\n",
      "Batch: 4500,train loss is: 0.0007072264829851819\n",
      "test loss is 0.0007331684953348033\n",
      "Batch: 4600,train loss is: 0.0005814120255202253\n",
      "test loss is 0.0007096766714191362\n",
      "Batch: 4700,train loss is: 0.0005978333567422593\n",
      "test loss is 0.0007396608498916238\n",
      "Batch: 4800,train loss is: 0.0012477005060661955\n",
      "test loss is 0.000705441012419363\n",
      "Batch: 4900,train loss is: 0.0005414323736854225\n",
      "test loss is 0.0007133624280232057\n",
      "Batch: 5000,train loss is: 0.0006016160402224174\n",
      "test loss is 0.0007214472852381058\n",
      "Batch: 5100,train loss is: 0.0005571290350697789\n",
      "test loss is 0.0007740639999516782\n",
      "Batch: 5200,train loss is: 0.00031944476614831075\n",
      "test loss is 0.0007166402585660947\n",
      "Batch: 5300,train loss is: 0.0006109742855576459\n",
      "test loss is 0.0007342968574519276\n",
      "Batch: 5400,train loss is: 0.000747274079662398\n",
      "test loss is 0.0007003351992919155\n",
      "Batch: 5500,train loss is: 0.00045237443786762846\n",
      "test loss is 0.0007271914581476864\n",
      "Batch: 5600,train loss is: 0.0005984842578253615\n",
      "test loss is 0.0007216655679976792\n",
      "Batch: 5700,train loss is: 0.0006297552579997166\n",
      "test loss is 0.0007118242772090214\n",
      "Batch: 5800,train loss is: 0.0005075105452332275\n",
      "test loss is 0.0007092577192114214\n",
      "Batch: 5900,train loss is: 0.0034379845603406327\n",
      "test loss is 0.0007135505838008928\n",
      "Batch: 6000,train loss is: 0.0016494623011351587\n",
      "test loss is 0.00072213666454685\n",
      "Batch: 6100,train loss is: 0.0004306478688675313\n",
      "test loss is 0.0006987998931523854\n",
      "Batch: 6200,train loss is: 0.0005900325409646185\n",
      "test loss is 0.0007153700259773458\n",
      "Batch: 6300,train loss is: 0.001185732350180261\n",
      "test loss is 0.0007287344389880397\n",
      "Batch: 6400,train loss is: 0.0005093162142886022\n",
      "test loss is 0.0007590890167660366\n",
      "Batch: 6500,train loss is: 0.00032703045515498105\n",
      "test loss is 0.000726681493008895\n",
      "Batch: 6600,train loss is: 0.00213413754258971\n",
      "test loss is 0.0007068318240021421\n",
      "Batch: 6700,train loss is: 0.0003635081845687362\n",
      "test loss is 0.0007242487805638347\n",
      "Batch: 6800,train loss is: 0.0008028259159193632\n",
      "test loss is 0.0007056926435286534\n",
      "Batch: 6900,train loss is: 0.0006873869910310088\n",
      "test loss is 0.0007122850561216798\n",
      "Batch: 7000,train loss is: 0.0005877919819160582\n",
      "test loss is 0.0007342736707023345\n",
      "Batch: 7100,train loss is: 0.0011154143291782913\n",
      "test loss is 0.0007209031950700551\n",
      "Batch: 7200,train loss is: 0.0004219138723376748\n",
      "test loss is 0.0007244976791674103\n",
      "Batch: 7300,train loss is: 0.0008182835357128101\n",
      "test loss is 0.0007590816983498727\n",
      "Batch: 7400,train loss is: 0.00036902947645204627\n",
      "test loss is 0.0007069451012490602\n",
      "Batch: 7500,train loss is: 0.0006777690676172401\n",
      "test loss is 0.0007281644062720544\n",
      "Batch: 7600,train loss is: 0.0003991734249993012\n",
      "test loss is 0.000695824780608742\n",
      "Batch: 7700,train loss is: 0.0008024629982509586\n",
      "test loss is 0.0007112659983129731\n",
      "Batch: 7800,train loss is: 0.00028708332555643466\n",
      "test loss is 0.0007019774873105439\n",
      "Batch: 7900,train loss is: 0.0006070643221769226\n",
      "test loss is 0.0007201667912815242\n",
      "Batch: 8000,train loss is: 0.0004589043526860474\n",
      "test loss is 0.0007439294760108681\n",
      "Batch: 8100,train loss is: 0.0005851439749953686\n",
      "test loss is 0.0007283180601863028\n",
      "Batch: 8200,train loss is: 0.0011909076038788557\n",
      "test loss is 0.0007259753164129593\n",
      "Batch: 8300,train loss is: 0.00044990506371608006\n",
      "test loss is 0.0007288589216005121\n",
      "Batch: 8400,train loss is: 0.0005741363599748933\n",
      "test loss is 0.0007087893888492954\n",
      "Batch: 8500,train loss is: 0.0007720894731945516\n",
      "test loss is 0.000725426162353653\n",
      "Batch: 8600,train loss is: 0.0006932397390587241\n",
      "test loss is 0.0007320399357317304\n",
      "Batch: 8700,train loss is: 0.0005028518825050596\n",
      "test loss is 0.0007525282404043963\n",
      "Batch: 8800,train loss is: 0.0003305781856330653\n",
      "test loss is 0.0007021014750993889\n",
      "Batch: 8900,train loss is: 0.0010946294699218837\n",
      "test loss is 0.0007033023058799758\n",
      "Batch: 9000,train loss is: 0.0002709313504872969\n",
      "test loss is 0.0007186831289763359\n",
      "Batch: 9100,train loss is: 0.0003521454557140915\n",
      "test loss is 0.0007015622193814983\n",
      "Batch: 9200,train loss is: 0.0008239186904250909\n",
      "test loss is 0.0007771538375288421\n",
      "Batch: 9300,train loss is: 0.0005746819367760703\n",
      "test loss is 0.00073700622397098\n",
      "Batch: 9400,train loss is: 0.00044443369297530647\n",
      "test loss is 0.0007362149390620541\n",
      "Batch: 9500,train loss is: 0.00039180275201281927\n",
      "test loss is 0.0007342512429762554\n",
      "Batch: 9600,train loss is: 0.00044116061125564957\n",
      "test loss is 0.0006994603567595399\n",
      "Batch: 9700,train loss is: 0.0003356790073907285\n",
      "test loss is 0.0007232517053565756\n",
      "Batch: 9800,train loss is: 0.0003428428097444989\n",
      "test loss is 0.0007066006034028435\n",
      "Batch: 9900,train loss is: 0.0005172236973712661\n",
      "test loss is 0.0007392624235851685\n",
      "Batch: 10000,train loss is: 0.00045744147725244703\n",
      "test loss is 0.0007022331510708282\n",
      "Batch: 10100,train loss is: 0.001102585038822555\n",
      "test loss is 0.0007178075578968805\n",
      "Batch: 10200,train loss is: 0.0017319802149775262\n",
      "test loss is 0.0007955379694838421\n",
      "Batch: 10300,train loss is: 0.00048698700657811666\n",
      "test loss is 0.0007118996746876581\n",
      "Batch: 10400,train loss is: 0.00031260494188921936\n",
      "test loss is 0.0007065891747037775\n",
      "Batch: 10500,train loss is: 0.0007508720742849382\n",
      "test loss is 0.0007090378793774209\n",
      "Batch: 10600,train loss is: 0.0003323278233487128\n",
      "test loss is 0.0007608948383718315\n",
      "Batch: 10700,train loss is: 0.0006358383036648758\n",
      "test loss is 0.0007071408421614961\n",
      "Batch: 10800,train loss is: 0.0005637125315851646\n",
      "test loss is 0.0007175020544907635\n",
      "Batch: 10900,train loss is: 0.0006946604879336127\n",
      "test loss is 0.0007120393571265108\n",
      "Batch: 11000,train loss is: 0.0007940867614429567\n",
      "test loss is 0.0007369481330526882\n",
      "Batch: 11100,train loss is: 0.001062335621139477\n",
      "test loss is 0.0007041030654876239\n",
      "Batch: 11200,train loss is: 0.0006861118314863333\n",
      "test loss is 0.0007489782858589776\n",
      "Batch: 11300,train loss is: 0.0003192191119391519\n",
      "test loss is 0.0007236472340042994\n",
      "Batch: 11400,train loss is: 0.000629182304879099\n",
      "test loss is 0.0007156971519185208\n",
      "Batch: 11500,train loss is: 0.000557360138758025\n",
      "test loss is 0.0007152333311448905\n",
      "Batch: 11600,train loss is: 0.0019191141472731641\n",
      "test loss is 0.0007172322081025852\n",
      "Batch: 11700,train loss is: 0.0002972609645400866\n",
      "test loss is 0.000694606229553157\n",
      "Batch: 11800,train loss is: 0.0006142092357996159\n",
      "test loss is 0.00070536530525265\n",
      "Batch: 11900,train loss is: 0.0005995891086879058\n",
      "test loss is 0.0007216919628864649\n",
      "Batch: 12000,train loss is: 0.00036152998382252917\n",
      "test loss is 0.0007089418809568482\n",
      "Batch: 12100,train loss is: 0.0016096040118281818\n",
      "test loss is 0.0007158267079906013\n",
      "Batch: 12200,train loss is: 0.0006978411871077089\n",
      "test loss is 0.0007342213987748303\n",
      "Batch: 12300,train loss is: 0.0004540054811583838\n",
      "test loss is 0.000725615148807523\n",
      "Batch: 12400,train loss is: 0.0004852628767522225\n",
      "test loss is 0.0007103078805635174\n",
      "Batch: 12500,train loss is: 0.000413812902481956\n",
      "test loss is 0.0007359784685687251\n",
      "Batch: 12600,train loss is: 0.0004610993049203759\n",
      "test loss is 0.0007263910347391214\n",
      "Batch: 12700,train loss is: 0.000424214634635008\n",
      "test loss is 0.00069961779358766\n",
      "Batch: 12800,train loss is: 0.00037258792915251536\n",
      "test loss is 0.0007039147870187124\n",
      "Batch: 12900,train loss is: 0.0006794376670418079\n",
      "test loss is 0.0007212785835949319\n",
      "Batch: 13000,train loss is: 0.00022347551678528898\n",
      "test loss is 0.0006969383537494091\n",
      "Batch: 13100,train loss is: 0.0022330522360817055\n",
      "test loss is 0.0007002328478699614\n",
      "Batch: 13200,train loss is: 0.0007997900572576103\n",
      "test loss is 0.0008224483741330139\n",
      "Batch: 13300,train loss is: 0.0006352480595561467\n",
      "test loss is 0.0007274617158141002\n",
      "Batch: 13400,train loss is: 0.00038346188854943973\n",
      "test loss is 0.0007367759221303086\n",
      "Batch: 13500,train loss is: 0.0006244564703629166\n",
      "test loss is 0.0007397012167078117\n",
      "Batch: 13600,train loss is: 0.0005512158337189264\n",
      "test loss is 0.0007211039449786487\n",
      "Batch: 13700,train loss is: 0.0007061040618968763\n",
      "test loss is 0.0007018114389946954\n",
      "Batch: 13800,train loss is: 0.00045189032929899095\n",
      "test loss is 0.0007033832408085634\n",
      "Batch: 13900,train loss is: 0.0004484327702955817\n",
      "test loss is 0.0007582673389326912\n",
      "Batch: 14000,train loss is: 0.00042793928650121366\n",
      "test loss is 0.000724422886045286\n",
      "Batch: 14100,train loss is: 0.0007833100922228624\n",
      "test loss is 0.000760627598229882\n",
      "Batch: 14200,train loss is: 0.0006023230133437778\n",
      "test loss is 0.0006931971379641152\n",
      "Batch: 14300,train loss is: 0.00028246446632726425\n",
      "test loss is 0.0007031457673826035\n",
      "Batch: 14400,train loss is: 0.00131863101769265\n",
      "test loss is 0.0007053837404869376\n",
      "Batch: 14500,train loss is: 0.00046027448607404944\n",
      "test loss is 0.0007068093618791197\n",
      "Batch: 14600,train loss is: 0.0012463414264728468\n",
      "test loss is 0.0007065905090200302\n",
      "Batch: 14700,train loss is: 0.0004716023410254883\n",
      "test loss is 0.0007578357896086376\n",
      "Batch: 14800,train loss is: 0.0008461026531664452\n",
      "test loss is 0.0007178868893803947\n",
      "Batch: 14900,train loss is: 0.0004160889911958761\n",
      "test loss is 0.0007494460803539745\n",
      "Batch: 15000,train loss is: 0.0007483893360022895\n",
      "test loss is 0.0006986264471498435\n",
      "Batch: 15100,train loss is: 0.0006581677054590082\n",
      "test loss is 0.0007296906418875348\n",
      "Batch: 15200,train loss is: 0.0009570804212110337\n",
      "test loss is 0.0007148783889818854\n",
      "Batch: 15300,train loss is: 0.00041287924766130135\n",
      "test loss is 0.0007015204766719349\n",
      "Batch: 15400,train loss is: 0.00047030406655281616\n",
      "test loss is 0.0007189153955671979\n",
      "Batch: 15500,train loss is: 0.0007401833724762287\n",
      "test loss is 0.0006947627435531225\n",
      "Batch: 15600,train loss is: 0.0009150517313015183\n",
      "test loss is 0.0007365784502915554\n",
      "Batch: 15700,train loss is: 0.00046894943369462055\n",
      "test loss is 0.0007058213988721707\n",
      "Batch: 15800,train loss is: 0.0007106463894978094\n",
      "test loss is 0.0006968785073718952\n",
      "Batch: 15900,train loss is: 0.0009098737943638353\n",
      "test loss is 0.0007178281643308286\n",
      "Batch: 16000,train loss is: 0.00038915921363063074\n",
      "test loss is 0.0007241609926119441\n",
      "Batch: 16100,train loss is: 0.000540932097620867\n",
      "test loss is 0.0007058957111360875\n",
      "Batch: 16200,train loss is: 0.00039755244446163597\n",
      "test loss is 0.0007037952716456807\n",
      "Batch: 16300,train loss is: 0.0010429524189838053\n",
      "test loss is 0.0007373638831978064\n",
      "Batch: 16400,train loss is: 0.000381444889450251\n",
      "test loss is 0.0006981974686238472\n",
      "Batch: 16500,train loss is: 0.0006250822769890695\n",
      "test loss is 0.0007281866579677115\n",
      "Batch: 16600,train loss is: 0.00031108350666132176\n",
      "test loss is 0.0006976333410901193\n",
      "Batch: 16700,train loss is: 0.0009130541215726139\n",
      "test loss is 0.0007181179108436504\n",
      "Batch: 16800,train loss is: 0.0005684712716231335\n",
      "test loss is 0.0007322080462582103\n",
      "Batch: 16900,train loss is: 0.0006913935132691853\n",
      "test loss is 0.0007141980159132938\n",
      "Batch: 17000,train loss is: 0.0006744262892535625\n",
      "test loss is 0.0006928285536691342\n",
      "Batch: 17100,train loss is: 0.0005473139836185469\n",
      "test loss is 0.0007080537160585694\n",
      "Batch: 17200,train loss is: 0.0005244721770093421\n",
      "test loss is 0.0007122914470055677\n",
      "Batch: 17300,train loss is: 0.0008966934618592416\n",
      "test loss is 0.0007310433611380007\n",
      "Batch: 17400,train loss is: 0.00041794228396481025\n",
      "test loss is 0.0007101825988988671\n",
      "Batch: 17500,train loss is: 0.0005237650535875498\n",
      "test loss is 0.0007048164051403841\n",
      "Batch: 17600,train loss is: 0.0004236316447170497\n",
      "test loss is 0.0007225750211756087\n",
      "Batch: 17700,train loss is: 0.00035808838581008684\n",
      "test loss is 0.0007399245023431305\n",
      "Batch: 17800,train loss is: 0.001375919187191016\n",
      "test loss is 0.0007105957916749595\n",
      "Batch: 17900,train loss is: 0.0004960875848286083\n",
      "test loss is 0.0007413059726366899\n",
      "Batch: 18000,train loss is: 0.00032963166090221527\n",
      "test loss is 0.0006934589531487044\n",
      "Batch: 18100,train loss is: 0.00043108812560273664\n",
      "test loss is 0.0007033788441908871\n",
      "Batch: 18200,train loss is: 0.0005751567044716973\n",
      "test loss is 0.0007141304099634291\n",
      "Batch: 18300,train loss is: 0.0010826833262391252\n",
      "test loss is 0.0007091316807388326\n",
      "Batch: 18400,train loss is: 0.00041726693529530026\n",
      "test loss is 0.0007023654558643617\n",
      "Batch: 18500,train loss is: 0.0009118668598130372\n",
      "test loss is 0.0007154404092734818\n",
      "Batch: 18600,train loss is: 0.0004905182183481796\n",
      "test loss is 0.0007048690937922158\n",
      "Batch: 18700,train loss is: 0.0005690597615079483\n",
      "test loss is 0.0007207945808032138\n",
      "Batch: 18800,train loss is: 0.000555689023538724\n",
      "test loss is 0.0007048445917045845\n",
      "Batch: 18900,train loss is: 0.0006281495751980072\n",
      "test loss is 0.0007363105758525071\n",
      "Batch: 19000,train loss is: 0.0006504080391099521\n",
      "test loss is 0.000707725245197762\n",
      "Batch: 19100,train loss is: 0.00032087708276359547\n",
      "test loss is 0.0007302985067547563\n",
      "Batch: 19200,train loss is: 0.0005381251403795061\n",
      "test loss is 0.0007132315259600655\n",
      "Batch: 19300,train loss is: 0.0007531190693161511\n",
      "test loss is 0.000695260689702067\n",
      "Batch: 19400,train loss is: 0.0002893414906119575\n",
      "test loss is 0.0006967919673297233\n",
      "Batch: 19500,train loss is: 0.0005357340845724198\n",
      "test loss is 0.0007389951751537897\n",
      "Batch: 19600,train loss is: 0.0004364919821057668\n",
      "test loss is 0.0007180119125355031\n",
      "Batch: 19700,train loss is: 0.0007513161511691426\n",
      "test loss is 0.000786770624855936\n",
      "Batch: 19800,train loss is: 0.0006033639590033076\n",
      "test loss is 0.0007134478468815034\n",
      "Batch: 19900,train loss is: 0.00027192580831301175\n",
      "test loss is 0.0007395282801498411\n",
      "Batch: 20000,train loss is: 0.0007325936852220196\n",
      "test loss is 0.0007089867199845118\n",
      "Batch: 20100,train loss is: 0.0004761315814607029\n",
      "test loss is 0.0007122082479596202\n",
      "Batch: 20200,train loss is: 0.0008602438393462737\n",
      "test loss is 0.0007253894785849543\n",
      "Batch: 20300,train loss is: 0.0003314792911397068\n",
      "test loss is 0.0007289853184399026\n",
      "Batch: 20400,train loss is: 0.00048289247890700553\n",
      "test loss is 0.0007483978320049981\n",
      "Batch: 20500,train loss is: 0.0006084756178288394\n",
      "test loss is 0.0007059626807342894\n",
      "Batch: 20600,train loss is: 0.0005007173028026093\n",
      "test loss is 0.0007372796802236615\n",
      "Batch: 20700,train loss is: 0.0006974823140380709\n",
      "test loss is 0.0007141475067314169\n",
      "Batch: 20800,train loss is: 0.0003532063176547134\n",
      "test loss is 0.0007281496064283342\n",
      "Batch: 20900,train loss is: 0.0006164505260633356\n",
      "test loss is 0.0007577278656234683\n",
      "Batch: 21000,train loss is: 0.0002523300691394289\n",
      "test loss is 0.0006975624425358962\n",
      "Batch: 21100,train loss is: 0.0006923372373508262\n",
      "test loss is 0.0007302232964024808\n",
      "Batch: 21200,train loss is: 0.00035131667103841294\n",
      "test loss is 0.0007344805181810289\n",
      "Batch: 21300,train loss is: 0.00033981936770452513\n",
      "test loss is 0.000704843703844331\n",
      "Batch: 21400,train loss is: 0.0008162890479667963\n",
      "test loss is 0.0007131017375592733\n",
      "Batch: 21500,train loss is: 0.00022450942964521861\n",
      "test loss is 0.0007220058750557426\n",
      "Batch: 21600,train loss is: 0.0007222960970173372\n",
      "test loss is 0.0007198733597418693\n",
      "Batch: 21700,train loss is: 0.00033249155455816943\n",
      "test loss is 0.0006995709933098805\n",
      "Batch: 21800,train loss is: 0.000704474148235404\n",
      "test loss is 0.0007622132163840209\n",
      "Batch: 21900,train loss is: 0.0005166968617326553\n",
      "test loss is 0.0007915530375525259\n",
      "Batch: 22000,train loss is: 0.0006836815634285263\n",
      "test loss is 0.0007025007843483222\n",
      "Batch: 22100,train loss is: 0.0003512216439625041\n",
      "test loss is 0.0007080440362769776\n",
      "Batch: 22200,train loss is: 0.0005140001853870369\n",
      "test loss is 0.0007083080986789621\n",
      "Batch: 22300,train loss is: 0.0006224092248773796\n",
      "test loss is 0.0007139339249039\n",
      "Batch: 22400,train loss is: 0.000473512157005752\n",
      "test loss is 0.0007127764167762115\n",
      "Batch: 22500,train loss is: 0.00047698935955264025\n",
      "test loss is 0.0006986990805970752\n",
      "Batch: 22600,train loss is: 0.001022886823668033\n",
      "test loss is 0.0007422185041851437\n",
      "Batch: 22700,train loss is: 0.00039624793308500603\n",
      "test loss is 0.0006982713732916597\n",
      "Batch: 22800,train loss is: 0.0005506477582967856\n",
      "test loss is 0.0007284401407123149\n",
      "Batch: 22900,train loss is: 0.0003347291895712926\n",
      "test loss is 0.0007787006226921608\n",
      "Batch: 23000,train loss is: 0.0004317478320349488\n",
      "test loss is 0.0007090955147771064\n",
      "Batch: 23100,train loss is: 0.0004993200609551085\n",
      "test loss is 0.0006900915844089184\n",
      "Batch: 23200,train loss is: 0.00030914291566836425\n",
      "test loss is 0.000688194242282598\n",
      "Batch: 23300,train loss is: 0.000498146581946406\n",
      "test loss is 0.0007004273831169618\n",
      "Batch: 23400,train loss is: 0.0006406221864977831\n",
      "test loss is 0.0007326663290012634\n",
      "Batch: 23500,train loss is: 0.001999204712840878\n",
      "test loss is 0.0006994478455683325\n",
      "Batch: 23600,train loss is: 0.0002928440427378614\n",
      "test loss is 0.0006986688911972521\n",
      "Batch: 23700,train loss is: 0.0009231980680616137\n",
      "test loss is 0.0007099595577462033\n",
      "Batch: 23800,train loss is: 0.0008323301229143228\n",
      "test loss is 0.0007047885912449322\n",
      "Batch: 23900,train loss is: 0.0008941556414744451\n",
      "test loss is 0.0007136837215805735\n",
      "Batch: 24000,train loss is: 0.0007781963892115759\n",
      "test loss is 0.0007574431377173639\n",
      "Batch: 24100,train loss is: 0.0003426929919510732\n",
      "test loss is 0.0006894110206953269\n",
      "Batch: 24200,train loss is: 0.0014964072262344471\n",
      "test loss is 0.0007198554713347029\n",
      "Batch: 24300,train loss is: 0.0005288269210777438\n",
      "test loss is 0.0008482665520854346\n",
      "Batch: 24400,train loss is: 0.00132400741563407\n",
      "test loss is 0.0007021207845233128\n",
      "Batch: 24500,train loss is: 0.0006581263514297147\n",
      "test loss is 0.0006952817975622187\n",
      "Batch: 24600,train loss is: 0.0004610312211275727\n",
      "test loss is 0.000730779250985656\n",
      "Batch: 24700,train loss is: 0.0002952922704649663\n",
      "test loss is 0.0007057979970403277\n",
      "Batch: 24800,train loss is: 0.0008591418667410832\n",
      "test loss is 0.0007098765267234105\n",
      "Batch: 24900,train loss is: 0.00019164664080434181\n",
      "test loss is 0.0007060929937092942\n",
      "Batch: 25000,train loss is: 0.0006890145791604796\n",
      "test loss is 0.0007020837825241222\n",
      "Batch: 25100,train loss is: 0.0005742500389563846\n",
      "test loss is 0.0007137399935785199\n",
      "Batch: 25200,train loss is: 0.0007598478181305156\n",
      "test loss is 0.000716473376146912\n",
      "Batch: 25300,train loss is: 0.000788625671348313\n",
      "test loss is 0.0007433778487791321\n",
      "Batch: 25400,train loss is: 0.00041153364638681376\n",
      "test loss is 0.0006995315492339671\n",
      "Batch: 25500,train loss is: 0.0005884675200791288\n",
      "test loss is 0.0007091975640668839\n",
      "Batch: 25600,train loss is: 0.000646487609909582\n",
      "test loss is 0.0007057652530879405\n",
      "Batch: 25700,train loss is: 0.0005733142740276721\n",
      "test loss is 0.0007262639662967883\n",
      "Batch: 25800,train loss is: 0.00036705034564851383\n",
      "test loss is 0.0006939790829163338\n",
      "Batch: 25900,train loss is: 0.00030165863903265107\n",
      "test loss is 0.0007503680487948678\n",
      "Batch: 26000,train loss is: 0.001617973649153371\n",
      "test loss is 0.000762965585280576\n",
      "Batch: 26100,train loss is: 0.000857037141773985\n",
      "test loss is 0.0006923953958713248\n",
      "Batch: 26200,train loss is: 0.0006394132532252562\n",
      "test loss is 0.0006918162449799639\n",
      "Batch: 26300,train loss is: 0.001145603826027819\n",
      "test loss is 0.0007174345390163619\n",
      "Batch: 26400,train loss is: 0.0005828253632604028\n",
      "test loss is 0.0006983480499722554\n",
      "Batch: 26500,train loss is: 0.0004126084809554052\n",
      "test loss is 0.0007129604500130473\n",
      "Batch: 26600,train loss is: 0.0013938512521013435\n",
      "test loss is 0.0007834929467989235\n",
      "Batch: 26700,train loss is: 0.0004542563384968265\n",
      "test loss is 0.0008043444583703615\n",
      "Batch: 26800,train loss is: 0.0011906542364273486\n",
      "test loss is 0.0007110771663291343\n",
      "Batch: 26900,train loss is: 0.0004775901351616748\n",
      "test loss is 0.0007093113700475715\n",
      "Batch: 27000,train loss is: 0.0006765525123529521\n",
      "test loss is 0.0008331679768216857\n",
      "Batch: 27100,train loss is: 0.0006257651670360977\n",
      "test loss is 0.0007602084001435247\n",
      "Batch: 27200,train loss is: 0.0007020570669881307\n",
      "test loss is 0.000746758998511615\n",
      "Batch: 27300,train loss is: 0.0005178687211857396\n",
      "test loss is 0.0007146998244268936\n",
      "Batch: 27400,train loss is: 0.0008746981807507741\n",
      "test loss is 0.0007087507700092321\n",
      "Batch: 27500,train loss is: 0.0004992365268956851\n",
      "test loss is 0.0007807192622137196\n",
      "Batch: 27600,train loss is: 0.0005829750884332095\n",
      "test loss is 0.0007452729277318439\n",
      "Batch: 27700,train loss is: 0.0016343138102386163\n",
      "test loss is 0.0007187440544600347\n",
      "Batch: 27800,train loss is: 0.0007872022543297784\n",
      "test loss is 0.0007356407257625533\n",
      "Batch: 27900,train loss is: 0.0013715291201567405\n",
      "test loss is 0.0007061472485254314\n",
      "Batch: 28000,train loss is: 0.00041286227029256606\n",
      "test loss is 0.0007029757389858827\n",
      "Batch: 28100,train loss is: 0.000879963502555451\n",
      "test loss is 0.000725850262039141\n",
      "Batch: 28200,train loss is: 0.0005827162126300294\n",
      "test loss is 0.0006858490872708028\n",
      "Batch: 28300,train loss is: 0.0018279302997536884\n",
      "test loss is 0.0007154341851372082\n",
      "Batch: 28400,train loss is: 0.0005284731739201202\n",
      "test loss is 0.0006958992957242472\n",
      "Batch: 28500,train loss is: 0.0015005664393730818\n",
      "test loss is 0.0007352820526406848\n",
      "Batch: 28600,train loss is: 0.00037827942267271045\n",
      "test loss is 0.0007150278714858445\n",
      "Batch: 28700,train loss is: 0.0005461526310846359\n",
      "test loss is 0.0007046041764447726\n",
      "Batch: 28800,train loss is: 0.0010963171211894028\n",
      "test loss is 0.0007108452398513299\n",
      "Batch: 28900,train loss is: 0.000464087990131838\n",
      "test loss is 0.0006959812599830262\n",
      "Batch: 29000,train loss is: 0.0005101598998743643\n",
      "test loss is 0.0007000415368670013\n",
      "Batch: 29100,train loss is: 0.00058292116864016\n",
      "test loss is 0.0007086153168976292\n",
      "Batch: 29200,train loss is: 0.0004288050330037146\n",
      "test loss is 0.0007160059778454469\n",
      "Batch: 29300,train loss is: 0.0010440247844116968\n",
      "test loss is 0.0006974226222339848\n",
      "Batch: 29400,train loss is: 0.0007316519509869487\n",
      "test loss is 0.0007761062322688532\n",
      "Batch: 29500,train loss is: 0.0004699222183647021\n",
      "test loss is 0.0007244565627272546\n",
      "Batch: 29600,train loss is: 0.0004956971253736444\n",
      "test loss is 0.0006928572215866909\n",
      "Batch: 29700,train loss is: 0.0003473023171675756\n",
      "test loss is 0.0007129935578474136\n",
      "Batch: 29800,train loss is: 0.0008175438815350603\n",
      "test loss is 0.0006913011613895693\n",
      "Batch: 29900,train loss is: 0.0031223500880236234\n",
      "test loss is 0.0006992502920084274\n",
      "Batch: 30000,train loss is: 0.0003480657885077928\n",
      "test loss is 0.0007503023288608735\n",
      "Batch: 30100,train loss is: 0.0005219585512074613\n",
      "test loss is 0.0007413640596463752\n",
      "Batch: 30200,train loss is: 0.00037331760550194027\n",
      "test loss is 0.0007060909641033138\n",
      "Batch: 30300,train loss is: 0.00027959050818510736\n",
      "test loss is 0.0006930886818597942\n",
      "Batch: 30400,train loss is: 0.0003553936828956255\n",
      "test loss is 0.0007052855167465732\n",
      "Batch: 30500,train loss is: 0.00035195363935213494\n",
      "test loss is 0.0007514255335194209\n",
      "Batch: 30600,train loss is: 0.00024250345372402922\n",
      "test loss is 0.0007003032943444132\n",
      "Batch: 30700,train loss is: 0.00039108545926742775\n",
      "test loss is 0.000808822970644238\n",
      "Batch: 30800,train loss is: 0.00028096281280951\n",
      "test loss is 0.0007669293974153301\n",
      "Batch: 30900,train loss is: 0.0003560385851792748\n",
      "test loss is 0.0007030333080264262\n",
      "Batch: 31000,train loss is: 0.0005090586656862441\n",
      "test loss is 0.0007157784205579101\n",
      "Batch: 31100,train loss is: 0.00036786966164996\n",
      "test loss is 0.0007852829012104342\n",
      "Batch: 31200,train loss is: 0.0005522514135040838\n",
      "test loss is 0.000708521315136016\n",
      "Batch: 31300,train loss is: 0.0005513285521553331\n",
      "test loss is 0.000711157547804339\n",
      "Batch: 31400,train loss is: 0.0007945590138020462\n",
      "test loss is 0.0007027851469808144\n",
      "Batch: 31500,train loss is: 0.0004765676693574166\n",
      "test loss is 0.0007057726253696036\n",
      "Batch: 31600,train loss is: 0.00044783529215013915\n",
      "test loss is 0.0007091974760805348\n",
      "Batch: 31700,train loss is: 0.0005798882516400601\n",
      "test loss is 0.000732548519138553\n",
      "Batch: 31800,train loss is: 0.001291773824935171\n",
      "test loss is 0.0007053761391550535\n",
      "Batch: 31900,train loss is: 0.0007884247457772782\n",
      "test loss is 0.0007205415114707164\n",
      "Batch: 32000,train loss is: 0.004121221299818893\n",
      "test loss is 0.0007078871793865242\n",
      "Batch: 32100,train loss is: 0.0004522292979710429\n",
      "test loss is 0.000713154936610979\n",
      "Batch: 32200,train loss is: 0.000359803622276186\n",
      "test loss is 0.000715067092231567\n",
      "Batch: 32300,train loss is: 0.0003581167388300778\n",
      "test loss is 0.000699461685443007\n",
      "Batch: 32400,train loss is: 0.0004096117082289649\n",
      "test loss is 0.0006901876361281455\n",
      "Batch: 32500,train loss is: 0.0009546121717589953\n",
      "test loss is 0.0007351699497059571\n",
      "Batch: 32600,train loss is: 0.00048106787584361254\n",
      "test loss is 0.0007157928597933754\n",
      "Batch: 32700,train loss is: 0.0004829322530688812\n",
      "test loss is 0.0007241506515909884\n",
      "Batch: 32800,train loss is: 0.0004212587140065824\n",
      "test loss is 0.000736003573256665\n",
      "Batch: 32900,train loss is: 0.0006551404346995034\n",
      "test loss is 0.0006949910614304434\n",
      "Batch: 33000,train loss is: 0.0004965802670884086\n",
      "test loss is 0.0006966395521792821\n",
      "Batch: 33100,train loss is: 0.00043480088830001406\n",
      "test loss is 0.0007265056736152203\n",
      "Batch: 33200,train loss is: 0.0004809815945026002\n",
      "test loss is 0.0007851228313454595\n",
      "Batch: 33300,train loss is: 0.00046243178864558223\n",
      "test loss is 0.0006919515869491554\n",
      "Batch: 33400,train loss is: 0.0004817272969474461\n",
      "test loss is 0.0007106299746637646\n",
      "Batch: 33500,train loss is: 0.0003783286819382348\n",
      "test loss is 0.0007050659486010536\n",
      "Batch: 33600,train loss is: 0.000574996307803166\n",
      "test loss is 0.0006994425184286122\n",
      "Batch: 33700,train loss is: 0.0005866438491454678\n",
      "test loss is 0.0007162294947409691\n",
      "Batch: 33800,train loss is: 0.0007987188704224768\n",
      "test loss is 0.0006959018491479772\n",
      "Batch: 33900,train loss is: 0.0006853681330934638\n",
      "test loss is 0.0007017164625350023\n",
      "-----------------------Epoch: 12----------------------------------\n",
      "Batch: 0,train loss is: 0.00032391523804735275\n",
      "test loss is 0.0007127866608240916\n",
      "Batch: 100,train loss is: 0.0013960663040477577\n",
      "test loss is 0.0007063200799758624\n",
      "Batch: 200,train loss is: 0.0005015463146434847\n",
      "test loss is 0.0007156172118089171\n",
      "Batch: 300,train loss is: 0.00028819342724334195\n",
      "test loss is 0.0007032801674031531\n",
      "Batch: 400,train loss is: 0.00047637333934280824\n",
      "test loss is 0.0006946866108209864\n",
      "Batch: 500,train loss is: 0.0005306257643350598\n",
      "test loss is 0.0007265401886295274\n",
      "Batch: 600,train loss is: 0.0002470710129336259\n",
      "test loss is 0.0007378538019049254\n",
      "Batch: 700,train loss is: 0.0003438288963932088\n",
      "test loss is 0.0006950167751446892\n",
      "Batch: 800,train loss is: 0.00044627468652589655\n",
      "test loss is 0.0007023904171522717\n",
      "Batch: 900,train loss is: 0.00030229972740230903\n",
      "test loss is 0.0006992509115969256\n",
      "Batch: 1000,train loss is: 0.0004537444088592452\n",
      "test loss is 0.0007368318232766881\n",
      "Batch: 1100,train loss is: 0.0017687584414843702\n",
      "test loss is 0.0006928808087055921\n",
      "Batch: 1200,train loss is: 0.00031876043076732656\n",
      "test loss is 0.0007109435171175257\n",
      "Batch: 1300,train loss is: 0.0005245630773234039\n",
      "test loss is 0.0006993358059595276\n",
      "Batch: 1400,train loss is: 0.0004420248922585547\n",
      "test loss is 0.0007205307209897479\n",
      "Batch: 1500,train loss is: 0.00048456487735147396\n",
      "test loss is 0.0007255310483761671\n",
      "Batch: 1600,train loss is: 0.0006848388184319097\n",
      "test loss is 0.0007765709380223737\n",
      "Batch: 1700,train loss is: 0.0012491992767335492\n",
      "test loss is 0.0006906460569572032\n",
      "Batch: 1800,train loss is: 0.00039314700049926756\n",
      "test loss is 0.0007045776780810147\n",
      "Batch: 1900,train loss is: 0.0003946382983217219\n",
      "test loss is 0.0007351314183859956\n",
      "Batch: 2000,train loss is: 0.0006537191249525843\n",
      "test loss is 0.0008380118917255733\n",
      "Batch: 2100,train loss is: 0.000659195272969036\n",
      "test loss is 0.0008500512252594418\n",
      "Batch: 2200,train loss is: 0.0007951496966088381\n",
      "test loss is 0.0007047819849051591\n",
      "Batch: 2300,train loss is: 0.0004338903502101157\n",
      "test loss is 0.0007338008467851257\n",
      "Batch: 2400,train loss is: 0.0005062388549631456\n",
      "test loss is 0.0007036351481799455\n",
      "Batch: 2500,train loss is: 0.0006339981110280932\n",
      "test loss is 0.0006978581877408465\n",
      "Batch: 2600,train loss is: 0.0003896833156908852\n",
      "test loss is 0.0007106254484488972\n",
      "Batch: 2700,train loss is: 0.000642781247900932\n",
      "test loss is 0.0006867368106064156\n",
      "Batch: 2800,train loss is: 0.00046218109639918826\n",
      "test loss is 0.0007236480038683986\n",
      "Batch: 2900,train loss is: 0.00045448558269750805\n",
      "test loss is 0.0006911382135857981\n",
      "Batch: 3000,train loss is: 0.0005147056080654061\n",
      "test loss is 0.0006908137156812498\n",
      "Batch: 3100,train loss is: 0.0006826106450764313\n",
      "test loss is 0.0007445904540208168\n",
      "Batch: 3200,train loss is: 0.0009963805770078283\n",
      "test loss is 0.0007148099126754098\n",
      "Batch: 3300,train loss is: 0.0005156012964744046\n",
      "test loss is 0.0007063602255187597\n",
      "Batch: 3400,train loss is: 0.00046115848718535996\n",
      "test loss is 0.0007002165506206081\n",
      "Batch: 3500,train loss is: 0.00022535261172866643\n",
      "test loss is 0.0006993023111254739\n",
      "Batch: 3600,train loss is: 0.0005547472575065239\n",
      "test loss is 0.0007362596611054185\n",
      "Batch: 3700,train loss is: 0.00036185221671320523\n",
      "test loss is 0.0006963775570028894\n",
      "Batch: 3800,train loss is: 0.0004534396324812841\n",
      "test loss is 0.0007298142218791524\n",
      "Batch: 3900,train loss is: 0.000619075674423061\n",
      "test loss is 0.0007035553767009818\n",
      "Batch: 4000,train loss is: 0.00043790277159337204\n",
      "test loss is 0.0007043840363659177\n",
      "Batch: 4100,train loss is: 0.0007224049636679041\n",
      "test loss is 0.0007224904964735117\n",
      "Batch: 4200,train loss is: 0.0003006605432804019\n",
      "test loss is 0.0007305310748651543\n",
      "Batch: 4300,train loss is: 0.00041313074302558626\n",
      "test loss is 0.0006924228001231414\n",
      "Batch: 4400,train loss is: 0.00034277104834794367\n",
      "test loss is 0.0007091605156773085\n",
      "Batch: 4500,train loss is: 0.0007177386405157779\n",
      "test loss is 0.0007237970259920611\n",
      "Batch: 4600,train loss is: 0.0005780729815349864\n",
      "test loss is 0.0007024752288008324\n",
      "Batch: 4700,train loss is: 0.000581165632184054\n",
      "test loss is 0.00073116398467676\n",
      "Batch: 4800,train loss is: 0.0012567986161435758\n",
      "test loss is 0.0006958488654516175\n",
      "Batch: 4900,train loss is: 0.0005367515767368047\n",
      "test loss is 0.000705432055858653\n",
      "Batch: 5000,train loss is: 0.0005973783497639411\n",
      "test loss is 0.0007135406323439774\n",
      "Batch: 5100,train loss is: 0.0005319165071677688\n",
      "test loss is 0.0007636133829384174\n",
      "Batch: 5200,train loss is: 0.0003131006776769343\n",
      "test loss is 0.0007091560760182508\n",
      "Batch: 5300,train loss is: 0.0005868049691468163\n",
      "test loss is 0.0007249344348174768\n",
      "Batch: 5400,train loss is: 0.0007452194752999877\n",
      "test loss is 0.000691879463120623\n",
      "Batch: 5500,train loss is: 0.00044599698860612095\n",
      "test loss is 0.000719867817941052\n",
      "Batch: 5600,train loss is: 0.0005862824812811414\n",
      "test loss is 0.0007120791378297383\n",
      "Batch: 5700,train loss is: 0.0006067887253732474\n",
      "test loss is 0.0007048155012534509\n",
      "Batch: 5800,train loss is: 0.0005030514850198705\n",
      "test loss is 0.0007008602778433221\n",
      "Batch: 5900,train loss is: 0.0034466206032155054\n",
      "test loss is 0.0007054609008849\n",
      "Batch: 6000,train loss is: 0.0016508737793550531\n",
      "test loss is 0.0007143816905759375\n",
      "Batch: 6100,train loss is: 0.000424627510705805\n",
      "test loss is 0.0006910147410103987\n",
      "Batch: 6200,train loss is: 0.0005866782145336826\n",
      "test loss is 0.0007073056167591754\n",
      "Batch: 6300,train loss is: 0.0011585943098793175\n",
      "test loss is 0.0007212758409509667\n",
      "Batch: 6400,train loss is: 0.0005205337397240968\n",
      "test loss is 0.0007494101365739553\n",
      "Batch: 6500,train loss is: 0.00031838954912341317\n",
      "test loss is 0.0007159186272509072\n",
      "Batch: 6600,train loss is: 0.0021118587302100065\n",
      "test loss is 0.0006985510966580258\n",
      "Batch: 6700,train loss is: 0.00035140975113931444\n",
      "test loss is 0.0007152201668694394\n",
      "Batch: 6800,train loss is: 0.0007842162696483196\n",
      "test loss is 0.0006976730673354415\n",
      "Batch: 6900,train loss is: 0.0006796124849053691\n",
      "test loss is 0.0007056429294441027\n",
      "Batch: 7000,train loss is: 0.0005847534013764272\n",
      "test loss is 0.0007243936100387049\n",
      "Batch: 7100,train loss is: 0.0011191721062086969\n",
      "test loss is 0.0007118283433844329\n",
      "Batch: 7200,train loss is: 0.00042993280249385924\n",
      "test loss is 0.0007161935835825488\n",
      "Batch: 7300,train loss is: 0.0007982273284019343\n",
      "test loss is 0.0007468574733135204\n",
      "Batch: 7400,train loss is: 0.00036826188994392475\n",
      "test loss is 0.0006989375755346479\n",
      "Batch: 7500,train loss is: 0.0006711666313009833\n",
      "test loss is 0.0007206795413162112\n",
      "Batch: 7600,train loss is: 0.00038798813718158383\n",
      "test loss is 0.0006874671181883378\n",
      "Batch: 7700,train loss is: 0.0008216432443468844\n",
      "test loss is 0.0007041575268366498\n",
      "Batch: 7800,train loss is: 0.0002882544855562021\n",
      "test loss is 0.0006935353725588708\n",
      "Batch: 7900,train loss is: 0.0005974818617644983\n",
      "test loss is 0.0007130512828644512\n",
      "Batch: 8000,train loss is: 0.0004493127694720787\n",
      "test loss is 0.0007356625848455601\n",
      "Batch: 8100,train loss is: 0.0005838445852651721\n",
      "test loss is 0.0007203209092236371\n",
      "Batch: 8200,train loss is: 0.0011508064516692644\n",
      "test loss is 0.0007173488234658471\n",
      "Batch: 8300,train loss is: 0.0004203917723641593\n",
      "test loss is 0.0007199596746683208\n",
      "Batch: 8400,train loss is: 0.0005699668067272278\n",
      "test loss is 0.0007009614421985129\n",
      "Batch: 8500,train loss is: 0.000737358470088376\n",
      "test loss is 0.0007168138084148238\n",
      "Batch: 8600,train loss is: 0.0006871002168036922\n",
      "test loss is 0.0007234125243165355\n",
      "Batch: 8700,train loss is: 0.0004955631722517957\n",
      "test loss is 0.0007480193703819866\n",
      "Batch: 8800,train loss is: 0.0003300502125224895\n",
      "test loss is 0.0006945555360263968\n",
      "Batch: 8900,train loss is: 0.0010883318099043885\n",
      "test loss is 0.000694826988471011\n",
      "Batch: 9000,train loss is: 0.0002649592988642778\n",
      "test loss is 0.0007115086602610073\n",
      "Batch: 9100,train loss is: 0.00034126895880277045\n",
      "test loss is 0.0006929133602589179\n",
      "Batch: 9200,train loss is: 0.0008182686745118145\n",
      "test loss is 0.0007682874198909517\n",
      "Batch: 9300,train loss is: 0.0005670540713973461\n",
      "test loss is 0.0007327336346814867\n",
      "Batch: 9400,train loss is: 0.0004476998265291319\n",
      "test loss is 0.0007278050671154671\n",
      "Batch: 9500,train loss is: 0.00038619253465941406\n",
      "test loss is 0.0007267038660378131\n",
      "Batch: 9600,train loss is: 0.0004355727488689188\n",
      "test loss is 0.0006910862553950645\n",
      "Batch: 9700,train loss is: 0.00033555918372214083\n",
      "test loss is 0.0007136647603146099\n",
      "Batch: 9800,train loss is: 0.0003238040938609372\n",
      "test loss is 0.0006989948612042717\n",
      "Batch: 9900,train loss is: 0.0005097509517286062\n",
      "test loss is 0.0007300979346400959\n",
      "Batch: 10000,train loss is: 0.0004454908314282425\n",
      "test loss is 0.0006926777408626304\n",
      "Batch: 10100,train loss is: 0.0011117109906404025\n",
      "test loss is 0.0007101946931803788\n",
      "Batch: 10200,train loss is: 0.0017342905191286334\n",
      "test loss is 0.000787160307263008\n",
      "Batch: 10300,train loss is: 0.00047961269831235805\n",
      "test loss is 0.0007033550537150581\n",
      "Batch: 10400,train loss is: 0.000304609649701214\n",
      "test loss is 0.000698348057362125\n",
      "Batch: 10500,train loss is: 0.0007483806977941096\n",
      "test loss is 0.0007017964113430713\n",
      "Batch: 10600,train loss is: 0.00032971602524571223\n",
      "test loss is 0.0007541690988410411\n",
      "Batch: 10700,train loss is: 0.0006269436421335867\n",
      "test loss is 0.0006988669701542765\n",
      "Batch: 10800,train loss is: 0.0005714817150759555\n",
      "test loss is 0.0007095971083650099\n",
      "Batch: 10900,train loss is: 0.0006910614854979662\n",
      "test loss is 0.0007037434243487653\n",
      "Batch: 11000,train loss is: 0.000786130825513406\n",
      "test loss is 0.0007277445972899153\n",
      "Batch: 11100,train loss is: 0.0010503151012358155\n",
      "test loss is 0.0006954471570280269\n",
      "Batch: 11200,train loss is: 0.000682357574203342\n",
      "test loss is 0.0007415830230551231\n",
      "Batch: 11300,train loss is: 0.00031807579991143334\n",
      "test loss is 0.0007135960603574055\n",
      "Batch: 11400,train loss is: 0.0006441688398013649\n",
      "test loss is 0.0007065950143040681\n",
      "Batch: 11500,train loss is: 0.0005497312811436399\n",
      "test loss is 0.0007075967125789896\n",
      "Batch: 11600,train loss is: 0.0019105143250022064\n",
      "test loss is 0.0007083305775785657\n",
      "Batch: 11700,train loss is: 0.00030091198740053916\n",
      "test loss is 0.0006869404466140691\n",
      "Batch: 11800,train loss is: 0.0006179920440379897\n",
      "test loss is 0.000697297315590643\n",
      "Batch: 11900,train loss is: 0.000578699589449383\n",
      "test loss is 0.0007126067690955269\n",
      "Batch: 12000,train loss is: 0.00034762641554546823\n",
      "test loss is 0.0007010382051656304\n",
      "Batch: 12100,train loss is: 0.0015994706098829282\n",
      "test loss is 0.0007088583582160612\n",
      "Batch: 12200,train loss is: 0.0006847420070942868\n",
      "test loss is 0.0007266475744524025\n",
      "Batch: 12300,train loss is: 0.0004557018742653415\n",
      "test loss is 0.0007171047447823607\n",
      "Batch: 12400,train loss is: 0.0004657806164807969\n",
      "test loss is 0.0007024888953820234\n",
      "Batch: 12500,train loss is: 0.00042984037260624714\n",
      "test loss is 0.0007275784580792914\n",
      "Batch: 12600,train loss is: 0.00047144460899871984\n",
      "test loss is 0.0007189147740209453\n",
      "Batch: 12700,train loss is: 0.0004123640163624438\n",
      "test loss is 0.0006926131142402085\n",
      "Batch: 12800,train loss is: 0.00037461569271362514\n",
      "test loss is 0.0006958850185842562\n",
      "Batch: 12900,train loss is: 0.0006695884933005552\n",
      "test loss is 0.0007121237536193942\n",
      "Batch: 13000,train loss is: 0.0002203365041469195\n",
      "test loss is 0.0006901510361663739\n",
      "Batch: 13100,train loss is: 0.002240043348611292\n",
      "test loss is 0.0006919319864897995\n",
      "Batch: 13200,train loss is: 0.000807897590431279\n",
      "test loss is 0.0008164203327379678\n",
      "Batch: 13300,train loss is: 0.0006492538574747463\n",
      "test loss is 0.0007162132093942139\n",
      "Batch: 13400,train loss is: 0.0003779995575635868\n",
      "test loss is 0.0007295683931705611\n",
      "Batch: 13500,train loss is: 0.0006180110767146365\n",
      "test loss is 0.0007313715962029511\n",
      "Batch: 13600,train loss is: 0.0005556493809127044\n",
      "test loss is 0.0007134457693169057\n",
      "Batch: 13700,train loss is: 0.000698848200666726\n",
      "test loss is 0.0006933249540774337\n",
      "Batch: 13800,train loss is: 0.0004354388487816427\n",
      "test loss is 0.0006947034845117421\n",
      "Batch: 13900,train loss is: 0.00041547506056895643\n",
      "test loss is 0.0007526949543048805\n",
      "Batch: 14000,train loss is: 0.0004217705205500888\n",
      "test loss is 0.0007180440839499234\n",
      "Batch: 14100,train loss is: 0.0007707947974735056\n",
      "test loss is 0.0007520568755763219\n",
      "Batch: 14200,train loss is: 0.000600863527134864\n",
      "test loss is 0.0006856782853262277\n",
      "Batch: 14300,train loss is: 0.0002680213890360326\n",
      "test loss is 0.0006955394224508331\n",
      "Batch: 14400,train loss is: 0.0013098130793600647\n",
      "test loss is 0.0006972763180106595\n",
      "Batch: 14500,train loss is: 0.00048524724458649424\n",
      "test loss is 0.0006979907716892493\n",
      "Batch: 14600,train loss is: 0.001178879876423789\n",
      "test loss is 0.0006992488936734003\n",
      "Batch: 14700,train loss is: 0.0004677088257411353\n",
      "test loss is 0.0007507858195155795\n",
      "Batch: 14800,train loss is: 0.0008374441521757105\n",
      "test loss is 0.000711825734626452\n",
      "Batch: 14900,train loss is: 0.0004181696843755645\n",
      "test loss is 0.0007435065575455477\n",
      "Batch: 15000,train loss is: 0.0007311968937112979\n",
      "test loss is 0.0006907582082802715\n",
      "Batch: 15100,train loss is: 0.0006570129869164679\n",
      "test loss is 0.0007218798557320184\n",
      "Batch: 15200,train loss is: 0.0009226561316763169\n",
      "test loss is 0.0007062971030389453\n",
      "Batch: 15300,train loss is: 0.0004009004572530073\n",
      "test loss is 0.0006940880559845211\n",
      "Batch: 15400,train loss is: 0.0004687638978709686\n",
      "test loss is 0.0007098457703251095\n",
      "Batch: 15500,train loss is: 0.0007246090202360087\n",
      "test loss is 0.0006866567653523911\n",
      "Batch: 15600,train loss is: 0.0008991768557573873\n",
      "test loss is 0.0007264366669916341\n",
      "Batch: 15700,train loss is: 0.00044742036034898093\n",
      "test loss is 0.0006975047354962279\n",
      "Batch: 15800,train loss is: 0.0007046786360544133\n",
      "test loss is 0.0006881382494663244\n",
      "Batch: 15900,train loss is: 0.0008879870501823566\n",
      "test loss is 0.0007082588516929794\n",
      "Batch: 16000,train loss is: 0.00038648454373598433\n",
      "test loss is 0.0007144321506670671\n",
      "Batch: 16100,train loss is: 0.0005290962842492627\n",
      "test loss is 0.0006986045332282857\n",
      "Batch: 16200,train loss is: 0.00039542329239662286\n",
      "test loss is 0.0006942163933689316\n",
      "Batch: 16300,train loss is: 0.0010107543363124729\n",
      "test loss is 0.0007288314810440232\n",
      "Batch: 16400,train loss is: 0.00036975413698163976\n",
      "test loss is 0.0006905586691912276\n",
      "Batch: 16500,train loss is: 0.0006322268985556398\n",
      "test loss is 0.0007177946022138411\n",
      "Batch: 16600,train loss is: 0.0003091528054720386\n",
      "test loss is 0.000689613242037875\n",
      "Batch: 16700,train loss is: 0.0008963499667171346\n",
      "test loss is 0.0007101490557880878\n",
      "Batch: 16800,train loss is: 0.0005575780012986713\n",
      "test loss is 0.0007246548584517156\n",
      "Batch: 16900,train loss is: 0.0006997568762778096\n",
      "test loss is 0.0007064240229974585\n",
      "Batch: 17000,train loss is: 0.0006615671410960546\n",
      "test loss is 0.0006855294407066334\n",
      "Batch: 17100,train loss is: 0.0005395498890620468\n",
      "test loss is 0.0007010067757383531\n",
      "Batch: 17200,train loss is: 0.0005123521796711141\n",
      "test loss is 0.0007048352635453245\n",
      "Batch: 17300,train loss is: 0.0008625306079591803\n",
      "test loss is 0.0007252901751693821\n",
      "Batch: 17400,train loss is: 0.0004110354536812103\n",
      "test loss is 0.0007027774643760629\n",
      "Batch: 17500,train loss is: 0.000539921995876555\n",
      "test loss is 0.0006980395263212466\n",
      "Batch: 17600,train loss is: 0.00042326363201562083\n",
      "test loss is 0.0007140834568677271\n",
      "Batch: 17700,train loss is: 0.0003610211710516929\n",
      "test loss is 0.0007320281815215723\n",
      "Batch: 17800,train loss is: 0.0013770618101953782\n",
      "test loss is 0.000703219291347027\n",
      "Batch: 17900,train loss is: 0.0005006172921110435\n",
      "test loss is 0.0007341701502390225\n",
      "Batch: 18000,train loss is: 0.00032080014359071104\n",
      "test loss is 0.0006854132690890898\n",
      "Batch: 18100,train loss is: 0.00042109495721898003\n",
      "test loss is 0.0006952279273836897\n",
      "Batch: 18200,train loss is: 0.0005750977543342035\n",
      "test loss is 0.0007063473347335724\n",
      "Batch: 18300,train loss is: 0.0011049492954917377\n",
      "test loss is 0.00070173556683498\n",
      "Batch: 18400,train loss is: 0.00040815338505963323\n",
      "test loss is 0.0006944711503387773\n",
      "Batch: 18500,train loss is: 0.0009155641709554831\n",
      "test loss is 0.0007070176971884357\n",
      "Batch: 18600,train loss is: 0.00048274173361719164\n",
      "test loss is 0.0006972901517739376\n",
      "Batch: 18700,train loss is: 0.0005751724074176379\n",
      "test loss is 0.0007139977308276195\n",
      "Batch: 18800,train loss is: 0.0005495509043609846\n",
      "test loss is 0.0006966786447820809\n",
      "Batch: 18900,train loss is: 0.0006152860717814189\n",
      "test loss is 0.0007280058396322475\n",
      "Batch: 19000,train loss is: 0.0006441103478914605\n",
      "test loss is 0.0006998411012386314\n",
      "Batch: 19100,train loss is: 0.00032698507423094183\n",
      "test loss is 0.0007237176882851873\n",
      "Batch: 19200,train loss is: 0.0005321881820632761\n",
      "test loss is 0.0007051541109157248\n",
      "Batch: 19300,train loss is: 0.0007324588379672899\n",
      "test loss is 0.0006870914019864007\n",
      "Batch: 19400,train loss is: 0.00028565089408879126\n",
      "test loss is 0.0006894970039241179\n",
      "Batch: 19500,train loss is: 0.0005269841219906747\n",
      "test loss is 0.0007296075457820302\n",
      "Batch: 19600,train loss is: 0.0004237448662553456\n",
      "test loss is 0.0007092378540398793\n",
      "Batch: 19700,train loss is: 0.0007376245646660035\n",
      "test loss is 0.0007808068916262169\n",
      "Batch: 19800,train loss is: 0.0006052099777568963\n",
      "test loss is 0.0007059899535470748\n",
      "Batch: 19900,train loss is: 0.0002820142388911686\n",
      "test loss is 0.0007302962108202205\n",
      "Batch: 20000,train loss is: 0.0007198926377194493\n",
      "test loss is 0.0007005277018990875\n",
      "Batch: 20100,train loss is: 0.00048206865582678756\n",
      "test loss is 0.0007050800922335592\n",
      "Batch: 20200,train loss is: 0.0008376150747110163\n",
      "test loss is 0.0007175050303335686\n",
      "Batch: 20300,train loss is: 0.0003154569000476752\n",
      "test loss is 0.0007208505791578326\n",
      "Batch: 20400,train loss is: 0.0004893244376129206\n",
      "test loss is 0.0007413474783744018\n",
      "Batch: 20500,train loss is: 0.0006073620886554664\n",
      "test loss is 0.0006977847163183045\n",
      "Batch: 20600,train loss is: 0.0004879400032972214\n",
      "test loss is 0.0007283314358514809\n",
      "Batch: 20700,train loss is: 0.0006988672243209848\n",
      "test loss is 0.0007058470303927483\n",
      "Batch: 20800,train loss is: 0.00035186583595857095\n",
      "test loss is 0.0007220018620960318\n",
      "Batch: 20900,train loss is: 0.0006066923967678967\n",
      "test loss is 0.0007495505841619842\n",
      "Batch: 21000,train loss is: 0.0002466381084892921\n",
      "test loss is 0.0006896938148817138\n",
      "Batch: 21100,train loss is: 0.0006880251549485291\n",
      "test loss is 0.0007223418842916166\n",
      "Batch: 21200,train loss is: 0.0003537969077449323\n",
      "test loss is 0.0007283997726540283\n",
      "Batch: 21300,train loss is: 0.00033811702895670604\n",
      "test loss is 0.0006970085771215196\n",
      "Batch: 21400,train loss is: 0.0008069183259916303\n",
      "test loss is 0.0007030812753223909\n",
      "Batch: 21500,train loss is: 0.0002309156716305378\n",
      "test loss is 0.00071322459382054\n",
      "Batch: 21600,train loss is: 0.0007244764967106025\n",
      "test loss is 0.0007135375186400007\n",
      "Batch: 21700,train loss is: 0.0003271323777485914\n",
      "test loss is 0.000690406916429216\n",
      "Batch: 21800,train loss is: 0.0007273377539083628\n",
      "test loss is 0.0007536314560492481\n",
      "Batch: 21900,train loss is: 0.000505832473144082\n",
      "test loss is 0.0007857249077397897\n",
      "Batch: 22000,train loss is: 0.0006765921084870149\n",
      "test loss is 0.0006944386341052523\n",
      "Batch: 22100,train loss is: 0.00035535463484397643\n",
      "test loss is 0.0006986176190886756\n",
      "Batch: 22200,train loss is: 0.00050813788037638\n",
      "test loss is 0.0007000854214623522\n",
      "Batch: 22300,train loss is: 0.000603890148155369\n",
      "test loss is 0.000707216284136407\n",
      "Batch: 22400,train loss is: 0.00046644533995866144\n",
      "test loss is 0.0007064042715995669\n",
      "Batch: 22500,train loss is: 0.0004809374265052155\n",
      "test loss is 0.0006898783793194482\n",
      "Batch: 22600,train loss is: 0.0010118566975244156\n",
      "test loss is 0.0007343440489196584\n",
      "Batch: 22700,train loss is: 0.000386876399153622\n",
      "test loss is 0.0006909020772365506\n",
      "Batch: 22800,train loss is: 0.0005447596270929429\n",
      "test loss is 0.0007209924372307216\n",
      "Batch: 22900,train loss is: 0.0003260772943930209\n",
      "test loss is 0.0007710073607825294\n",
      "Batch: 23000,train loss is: 0.00043739184002117895\n",
      "test loss is 0.0007010531476704366\n",
      "Batch: 23100,train loss is: 0.00048613171527864247\n",
      "test loss is 0.0006818794940122854\n",
      "Batch: 23200,train loss is: 0.00030432500225624487\n",
      "test loss is 0.0006808773074654611\n",
      "Batch: 23300,train loss is: 0.0004892604019935575\n",
      "test loss is 0.0006931416136744033\n",
      "Batch: 23400,train loss is: 0.0006353072459562292\n",
      "test loss is 0.0007234557192717655\n",
      "Batch: 23500,train loss is: 0.0018989464415305455\n",
      "test loss is 0.0006907572946759402\n",
      "Batch: 23600,train loss is: 0.00029285732058820685\n",
      "test loss is 0.0006915916518864261\n",
      "Batch: 23700,train loss is: 0.0009318367202474257\n",
      "test loss is 0.0007004054881839201\n",
      "Batch: 23800,train loss is: 0.0008237270124233981\n",
      "test loss is 0.0006982294808277888\n",
      "Batch: 23900,train loss is: 0.0008868057719616395\n",
      "test loss is 0.0007068746380342604\n",
      "Batch: 24000,train loss is: 0.0007597508841284105\n",
      "test loss is 0.0007492641637898587\n",
      "Batch: 24100,train loss is: 0.0003296038171922795\n",
      "test loss is 0.0006808250282806684\n",
      "Batch: 24200,train loss is: 0.0014816066178994357\n",
      "test loss is 0.0007117457449774726\n",
      "Batch: 24300,train loss is: 0.0005323469726334222\n",
      "test loss is 0.000837854139853475\n",
      "Batch: 24400,train loss is: 0.0013185920852494862\n",
      "test loss is 0.0006940791951768103\n",
      "Batch: 24500,train loss is: 0.00065854492394658\n",
      "test loss is 0.0006864226590446305\n",
      "Batch: 24600,train loss is: 0.0004449506020006642\n",
      "test loss is 0.0007227506103016706\n",
      "Batch: 24700,train loss is: 0.00029450590027275215\n",
      "test loss is 0.0006981552608952614\n",
      "Batch: 24800,train loss is: 0.0008253524836339367\n",
      "test loss is 0.0007036747158105025\n",
      "Batch: 24900,train loss is: 0.0001945585787550247\n",
      "test loss is 0.0006979902832544579\n",
      "Batch: 25000,train loss is: 0.0006581745839361052\n",
      "test loss is 0.0006942096731606235\n",
      "Batch: 25100,train loss is: 0.0005539710803789792\n",
      "test loss is 0.0007047837539586299\n",
      "Batch: 25200,train loss is: 0.0007602166892513534\n",
      "test loss is 0.000708165811761018\n",
      "Batch: 25300,train loss is: 0.0007706694399192348\n",
      "test loss is 0.000737819416487265\n",
      "Batch: 25400,train loss is: 0.000416261438653529\n",
      "test loss is 0.0006926322524854434\n",
      "Batch: 25500,train loss is: 0.0005844179774791872\n",
      "test loss is 0.0007019176306115721\n",
      "Batch: 25600,train loss is: 0.0006414487447105152\n",
      "test loss is 0.0006982531423651995\n",
      "Batch: 25700,train loss is: 0.0005733596736661068\n",
      "test loss is 0.0007182971832770539\n",
      "Batch: 25800,train loss is: 0.00036550890963844677\n",
      "test loss is 0.0006861358239928666\n",
      "Batch: 25900,train loss is: 0.00029803828455923466\n",
      "test loss is 0.0007424081441364003\n",
      "Batch: 26000,train loss is: 0.0015970583756959617\n",
      "test loss is 0.0007532999585598262\n",
      "Batch: 26100,train loss is: 0.0008428819601761667\n",
      "test loss is 0.0006846814679762767\n",
      "Batch: 26200,train loss is: 0.0006326004741155187\n",
      "test loss is 0.0006841729479296926\n",
      "Batch: 26300,train loss is: 0.0011300779405508034\n",
      "test loss is 0.0007083454487403068\n",
      "Batch: 26400,train loss is: 0.0005783274720264939\n",
      "test loss is 0.0006890987185947669\n",
      "Batch: 26500,train loss is: 0.0004331258489898923\n",
      "test loss is 0.000707118799284434\n",
      "Batch: 26600,train loss is: 0.0013864010221996917\n",
      "test loss is 0.0007764772325553137\n",
      "Batch: 26700,train loss is: 0.0004645631971962417\n",
      "test loss is 0.000794958051158994\n",
      "Batch: 26800,train loss is: 0.0011708001596778905\n",
      "test loss is 0.000704751926544575\n",
      "Batch: 26900,train loss is: 0.0004730073979425333\n",
      "test loss is 0.0007015593771418145\n",
      "Batch: 27000,train loss is: 0.0006748948536346801\n",
      "test loss is 0.0008227604615344826\n",
      "Batch: 27100,train loss is: 0.000635680511391451\n",
      "test loss is 0.0007539211749461869\n",
      "Batch: 27200,train loss is: 0.0006988903363593222\n",
      "test loss is 0.0007400254798884663\n",
      "Batch: 27300,train loss is: 0.0005054346478055463\n",
      "test loss is 0.0007086246777000388\n",
      "Batch: 27400,train loss is: 0.0008425031330119517\n",
      "test loss is 0.0007016224276752314\n",
      "Batch: 27500,train loss is: 0.0004835708722291531\n",
      "test loss is 0.0007711302853529337\n",
      "Batch: 27600,train loss is: 0.000573346608604729\n",
      "test loss is 0.000734030946999814\n",
      "Batch: 27700,train loss is: 0.0016119656541155288\n",
      "test loss is 0.000710469979423342\n",
      "Batch: 27800,train loss is: 0.0007920689964364948\n",
      "test loss is 0.0007283253044817535\n",
      "Batch: 27900,train loss is: 0.001370377618323429\n",
      "test loss is 0.0006992464902829398\n",
      "Batch: 28000,train loss is: 0.0004064695789937315\n",
      "test loss is 0.0006949632188429677\n",
      "Batch: 28100,train loss is: 0.0008680442600394726\n",
      "test loss is 0.0007151284295740533\n",
      "Batch: 28200,train loss is: 0.0005696218913872682\n",
      "test loss is 0.000677910624758435\n",
      "Batch: 28300,train loss is: 0.0017956667244236166\n",
      "test loss is 0.0007086508882812325\n",
      "Batch: 28400,train loss is: 0.0005194541056111043\n",
      "test loss is 0.0006877533682832113\n",
      "Batch: 28500,train loss is: 0.0015048866865832102\n",
      "test loss is 0.0007294689561298537\n",
      "Batch: 28600,train loss is: 0.0003766930978036275\n",
      "test loss is 0.0007060417404431774\n",
      "Batch: 28700,train loss is: 0.0005504544414762245\n",
      "test loss is 0.0006965591253056017\n",
      "Batch: 28800,train loss is: 0.001098326605484786\n",
      "test loss is 0.0007038119445601358\n",
      "Batch: 28900,train loss is: 0.00046442982374465136\n",
      "test loss is 0.0006878749445595869\n",
      "Batch: 29000,train loss is: 0.0005050626730284118\n",
      "test loss is 0.0006933363037183611\n",
      "Batch: 29100,train loss is: 0.0005608848088902029\n",
      "test loss is 0.0007001333597394487\n",
      "Batch: 29200,train loss is: 0.0004210437776360808\n",
      "test loss is 0.0007070758439092527\n",
      "Batch: 29300,train loss is: 0.0010225052077505235\n",
      "test loss is 0.0006903527461461282\n",
      "Batch: 29400,train loss is: 0.0007265807584150285\n",
      "test loss is 0.00076965226412105\n",
      "Batch: 29500,train loss is: 0.0004678449490585429\n",
      "test loss is 0.000717413407151247\n",
      "Batch: 29600,train loss is: 0.0004967795383550342\n",
      "test loss is 0.0006837904650774045\n",
      "Batch: 29700,train loss is: 0.0003461799570981645\n",
      "test loss is 0.0007037027148660862\n",
      "Batch: 29800,train loss is: 0.0008112783856206098\n",
      "test loss is 0.0006834119752037048\n",
      "Batch: 29900,train loss is: 0.0030943130133237727\n",
      "test loss is 0.0006908455685495248\n",
      "Batch: 30000,train loss is: 0.0003461936944464436\n",
      "test loss is 0.0007435267031566412\n",
      "Batch: 30100,train loss is: 0.0005321724220356969\n",
      "test loss is 0.0007329012213670337\n",
      "Batch: 30200,train loss is: 0.0003798947995464608\n",
      "test loss is 0.0006989613667014891\n",
      "Batch: 30300,train loss is: 0.00027631350912643685\n",
      "test loss is 0.0006854874427861538\n",
      "Batch: 30400,train loss is: 0.0003473285443425868\n",
      "test loss is 0.0006965229437200547\n",
      "Batch: 30500,train loss is: 0.000340708254289613\n",
      "test loss is 0.0007416571441814441\n",
      "Batch: 30600,train loss is: 0.0002450091405528442\n",
      "test loss is 0.0006934951918827015\n",
      "Batch: 30700,train loss is: 0.000391657322617677\n",
      "test loss is 0.0007946522712603677\n",
      "Batch: 30800,train loss is: 0.00027951110705725605\n",
      "test loss is 0.0007577759732997685\n",
      "Batch: 30900,train loss is: 0.0003523384266789317\n",
      "test loss is 0.0006963732025944122\n",
      "Batch: 31000,train loss is: 0.0004987762715642728\n",
      "test loss is 0.0007086863111472995\n",
      "Batch: 31100,train loss is: 0.00036107968399046896\n",
      "test loss is 0.0007796349263188631\n",
      "Batch: 31200,train loss is: 0.0005417173600692642\n",
      "test loss is 0.0007002070759030932\n",
      "Batch: 31300,train loss is: 0.0005383572546218583\n",
      "test loss is 0.000701374476965289\n",
      "Batch: 31400,train loss is: 0.0007870749528386022\n",
      "test loss is 0.0006941205410561812\n",
      "Batch: 31500,train loss is: 0.00046624263187770273\n",
      "test loss is 0.0006979596026454051\n",
      "Batch: 31600,train loss is: 0.0004403797254537166\n",
      "test loss is 0.0007012206063407369\n",
      "Batch: 31700,train loss is: 0.0005769662732990748\n",
      "test loss is 0.0007240480522155176\n",
      "Batch: 31800,train loss is: 0.0012928721186998813\n",
      "test loss is 0.0006980749817189679\n",
      "Batch: 31900,train loss is: 0.0007724217443839005\n",
      "test loss is 0.0007110731711140925\n",
      "Batch: 32000,train loss is: 0.004084368787549228\n",
      "test loss is 0.0007003728884361553\n",
      "Batch: 32100,train loss is: 0.00043389805230858287\n",
      "test loss is 0.0007081933954216263\n",
      "Batch: 32200,train loss is: 0.00034799581194044893\n",
      "test loss is 0.0007064244570671069\n",
      "Batch: 32300,train loss is: 0.00035267704822553394\n",
      "test loss is 0.0006907421657472069\n",
      "Batch: 32400,train loss is: 0.00040341373034843015\n",
      "test loss is 0.0006823936488446906\n",
      "Batch: 32500,train loss is: 0.0009413474885695861\n",
      "test loss is 0.0007277466697461012\n",
      "Batch: 32600,train loss is: 0.000477713820328062\n",
      "test loss is 0.0007080960498269835\n",
      "Batch: 32700,train loss is: 0.00048230374096821\n",
      "test loss is 0.0007207253714556998\n",
      "Batch: 32800,train loss is: 0.00042621291292389233\n",
      "test loss is 0.0007315200157545726\n",
      "Batch: 32900,train loss is: 0.0006578817224146331\n",
      "test loss is 0.0006873567668326409\n",
      "Batch: 33000,train loss is: 0.00048608481931230985\n",
      "test loss is 0.0006902795135832663\n",
      "Batch: 33100,train loss is: 0.0004304959253916032\n",
      "test loss is 0.0007162990946726285\n",
      "Batch: 33200,train loss is: 0.00047003465275468074\n",
      "test loss is 0.0007744179673040409\n",
      "Batch: 33300,train loss is: 0.0004761363351652808\n",
      "test loss is 0.0006845437437571746\n",
      "Batch: 33400,train loss is: 0.0004782378141038934\n",
      "test loss is 0.0007026298624011832\n",
      "Batch: 33500,train loss is: 0.0003717937872354897\n",
      "test loss is 0.0006971211393942424\n",
      "Batch: 33600,train loss is: 0.0005721166414083467\n",
      "test loss is 0.0006913336221930485\n",
      "Batch: 33700,train loss is: 0.0005695498579444474\n",
      "test loss is 0.0007089891244549324\n",
      "Batch: 33800,train loss is: 0.0007964449419348532\n",
      "test loss is 0.0006888105498807396\n",
      "Batch: 33900,train loss is: 0.0006745706652874421\n",
      "test loss is 0.0006937508669984259\n",
      "-----------------------Epoch: 13----------------------------------\n",
      "Batch: 0,train loss is: 0.00031144142466679254\n",
      "test loss is 0.0007064002760220811\n",
      "Batch: 100,train loss is: 0.0013675214565376432\n",
      "test loss is 0.0006992284715282481\n",
      "Batch: 200,train loss is: 0.0004665396432752875\n",
      "test loss is 0.000710675022035651\n",
      "Batch: 300,train loss is: 0.00028268572267771375\n",
      "test loss is 0.0006955785385246456\n",
      "Batch: 400,train loss is: 0.00047269068556215295\n",
      "test loss is 0.0006865185817370192\n",
      "Batch: 500,train loss is: 0.0005131431909994867\n",
      "test loss is 0.000716738935598104\n",
      "Batch: 600,train loss is: 0.0002462027259852789\n",
      "test loss is 0.000730580181725479\n",
      "Batch: 700,train loss is: 0.00034907403953995676\n",
      "test loss is 0.0006868647107198904\n",
      "Batch: 800,train loss is: 0.0004399316030933915\n",
      "test loss is 0.0006946028224583686\n",
      "Batch: 900,train loss is: 0.0002970797508892101\n",
      "test loss is 0.0006919523762197991\n",
      "Batch: 1000,train loss is: 0.00045220716039299667\n",
      "test loss is 0.0007294258302154613\n",
      "Batch: 1100,train loss is: 0.0017600654550141218\n",
      "test loss is 0.000685735160069841\n",
      "Batch: 1200,train loss is: 0.00030934637875669886\n",
      "test loss is 0.0007036347268314692\n",
      "Batch: 1300,train loss is: 0.0005232725294245572\n",
      "test loss is 0.0006910405705403567\n",
      "Batch: 1400,train loss is: 0.0004358117354517163\n",
      "test loss is 0.0007130622802261966\n",
      "Batch: 1500,train loss is: 0.00046412217923820373\n",
      "test loss is 0.0007168426740235687\n",
      "Batch: 1600,train loss is: 0.0006860515232885485\n",
      "test loss is 0.0007686532131265363\n",
      "Batch: 1700,train loss is: 0.0012328401550255018\n",
      "test loss is 0.0006832556272379393\n",
      "Batch: 1800,train loss is: 0.00038265543553596214\n",
      "test loss is 0.0006951234345221175\n",
      "Batch: 1900,train loss is: 0.0003987590287532306\n",
      "test loss is 0.000727677023616399\n",
      "Batch: 2000,train loss is: 0.000666419968268042\n",
      "test loss is 0.0008325005750442662\n",
      "Batch: 2100,train loss is: 0.0006604879983710853\n",
      "test loss is 0.0008432578282712989\n",
      "Batch: 2200,train loss is: 0.0007792995683093775\n",
      "test loss is 0.0006969381345480916\n",
      "Batch: 2300,train loss is: 0.0004408766856753279\n",
      "test loss is 0.00072373185443018\n",
      "Batch: 2400,train loss is: 0.0004897974703453094\n",
      "test loss is 0.0006957440881723471\n",
      "Batch: 2500,train loss is: 0.0006182896504626976\n",
      "test loss is 0.0006899958643337271\n",
      "Batch: 2600,train loss is: 0.00039475841674471083\n",
      "test loss is 0.000701158013997647\n",
      "Batch: 2700,train loss is: 0.000632238949038009\n",
      "test loss is 0.0006788383586290956\n",
      "Batch: 2800,train loss is: 0.00045630172566991575\n",
      "test loss is 0.0007137789155134246\n",
      "Batch: 2900,train loss is: 0.000447081580913599\n",
      "test loss is 0.000684257530370019\n",
      "Batch: 3000,train loss is: 0.0005113143758862018\n",
      "test loss is 0.0006840293603539006\n",
      "Batch: 3100,train loss is: 0.0006822830222873945\n",
      "test loss is 0.0007359823553162806\n",
      "Batch: 3200,train loss is: 0.0009974956111416602\n",
      "test loss is 0.0007077776198607234\n",
      "Batch: 3300,train loss is: 0.0005277527597015659\n",
      "test loss is 0.0006992777721662069\n",
      "Batch: 3400,train loss is: 0.00043514342885285116\n",
      "test loss is 0.0006919192093875826\n",
      "Batch: 3500,train loss is: 0.00022551563132653434\n",
      "test loss is 0.0006908264530166242\n",
      "Batch: 3600,train loss is: 0.0005579781039591485\n",
      "test loss is 0.0007300599326448174\n",
      "Batch: 3700,train loss is: 0.00035036657728894366\n",
      "test loss is 0.0006897595659503814\n",
      "Batch: 3800,train loss is: 0.0004495493804608286\n",
      "test loss is 0.0007205481897150203\n",
      "Batch: 3900,train loss is: 0.000611273223081784\n",
      "test loss is 0.0006968806367842119\n",
      "Batch: 4000,train loss is: 0.0004393596165950031\n",
      "test loss is 0.0006965960963257659\n",
      "Batch: 4100,train loss is: 0.0007255084398857439\n",
      "test loss is 0.0007152596105522497\n",
      "Batch: 4200,train loss is: 0.0002907557788566351\n",
      "test loss is 0.0007228913046622784\n",
      "Batch: 4300,train loss is: 0.00041102445542618246\n",
      "test loss is 0.0006835488983824151\n",
      "Batch: 4400,train loss is: 0.0003226274144089723\n",
      "test loss is 0.0007049257220605401\n",
      "Batch: 4500,train loss is: 0.0007306497594046721\n",
      "test loss is 0.0007159792900498328\n",
      "Batch: 4600,train loss is: 0.0005791829081338732\n",
      "test loss is 0.0006958926985891345\n",
      "Batch: 4700,train loss is: 0.0005647130674519455\n",
      "test loss is 0.0007230766737100456\n",
      "Batch: 4800,train loss is: 0.0012597317433635194\n",
      "test loss is 0.0006877043775665774\n",
      "Batch: 4900,train loss is: 0.0005322343372513521\n",
      "test loss is 0.0006979316006175096\n",
      "Batch: 5000,train loss is: 0.0005912038097750551\n",
      "test loss is 0.0007065806152698436\n",
      "Batch: 5100,train loss is: 0.0005109011441849417\n",
      "test loss is 0.0007530911059672597\n",
      "Batch: 5200,train loss is: 0.000310004900160635\n",
      "test loss is 0.0007019163000044768\n",
      "Batch: 5300,train loss is: 0.0005534588201697064\n",
      "test loss is 0.0007147671014206586\n",
      "Batch: 5400,train loss is: 0.0007335196374192839\n",
      "test loss is 0.0006845453962974243\n",
      "Batch: 5500,train loss is: 0.00044306319459627865\n",
      "test loss is 0.0007127673631181678\n",
      "Batch: 5600,train loss is: 0.0005762117283318191\n",
      "test loss is 0.0007031381371044393\n",
      "Batch: 5700,train loss is: 0.0005889476021308809\n",
      "test loss is 0.0006977903763927209\n",
      "Batch: 5800,train loss is: 0.0004978849582687528\n",
      "test loss is 0.0006928725342716907\n",
      "Batch: 5900,train loss is: 0.003424551988044187\n",
      "test loss is 0.0006978101751261206\n",
      "Batch: 6000,train loss is: 0.0016582132646472407\n",
      "test loss is 0.0007061787257052016\n",
      "Batch: 6100,train loss is: 0.0004189346976424271\n",
      "test loss is 0.0006835236481924196\n",
      "Batch: 6200,train loss is: 0.0005783216512816193\n",
      "test loss is 0.0006992646306017028\n",
      "Batch: 6300,train loss is: 0.0011319148478783376\n",
      "test loss is 0.0007134592417533063\n",
      "Batch: 6400,train loss is: 0.0005237351050552911\n",
      "test loss is 0.0007411434155056701\n",
      "Batch: 6500,train loss is: 0.00031478521936238767\n",
      "test loss is 0.0007062316849545077\n",
      "Batch: 6600,train loss is: 0.002089932755673304\n",
      "test loss is 0.0006903467996028746\n",
      "Batch: 6700,train loss is: 0.00033804867589103474\n",
      "test loss is 0.0007062175143086234\n",
      "Batch: 6800,train loss is: 0.0007647438767834915\n",
      "test loss is 0.0006900441323178169\n",
      "Batch: 6900,train loss is: 0.0006827805634409903\n",
      "test loss is 0.0006992864066377121\n",
      "Batch: 7000,train loss is: 0.0005833071437367539\n",
      "test loss is 0.0007160440108761922\n",
      "Batch: 7100,train loss is: 0.00112804924005571\n",
      "test loss is 0.0007035681092715358\n",
      "Batch: 7200,train loss is: 0.00043545884711654044\n",
      "test loss is 0.0007090366572719805\n",
      "Batch: 7300,train loss is: 0.0007879238186793282\n",
      "test loss is 0.0007335891925606467\n",
      "Batch: 7400,train loss is: 0.00036172671514522227\n",
      "test loss is 0.0006913293869969772\n",
      "Batch: 7500,train loss is: 0.0006725144250378332\n",
      "test loss is 0.0007144474759243879\n",
      "Batch: 7600,train loss is: 0.0003811098301644463\n",
      "test loss is 0.0006785618123611129\n",
      "Batch: 7700,train loss is: 0.0008395140198574905\n",
      "test loss is 0.0006979454179949126\n",
      "Batch: 7800,train loss is: 0.0002846475526323786\n",
      "test loss is 0.0006856135962559658\n",
      "Batch: 7900,train loss is: 0.000584987586782618\n",
      "test loss is 0.0007070220015112079\n",
      "Batch: 8000,train loss is: 0.00043829363163075863\n",
      "test loss is 0.000728728727075762\n",
      "Batch: 8100,train loss is: 0.0005839320634082188\n",
      "test loss is 0.0007140785683907114\n",
      "Batch: 8200,train loss is: 0.001119946742761989\n",
      "test loss is 0.0007098802210962575\n",
      "Batch: 8300,train loss is: 0.0003952118140788538\n",
      "test loss is 0.0007118536725854111\n",
      "Batch: 8400,train loss is: 0.0005653119601613697\n",
      "test loss is 0.0006936573128263995\n",
      "Batch: 8500,train loss is: 0.0007038187167758666\n",
      "test loss is 0.0007089722362709389\n",
      "Batch: 8600,train loss is: 0.0006772900428421585\n",
      "test loss is 0.0007161937247055001\n",
      "Batch: 8700,train loss is: 0.0004858650134141167\n",
      "test loss is 0.0007428401367970873\n",
      "Batch: 8800,train loss is: 0.000333466825587324\n",
      "test loss is 0.0006871703700775394\n",
      "Batch: 8900,train loss is: 0.0010840300557650405\n",
      "test loss is 0.0006874515999963594\n",
      "Batch: 9000,train loss is: 0.00026082017242147346\n",
      "test loss is 0.0007061074083374062\n",
      "Batch: 9100,train loss is: 0.00033253816207483636\n",
      "test loss is 0.0006856312816319936\n",
      "Batch: 9200,train loss is: 0.0008075366935239118\n",
      "test loss is 0.0007608106388319759\n",
      "Batch: 9300,train loss is: 0.0005584730674589959\n",
      "test loss is 0.0007278069127184176\n",
      "Batch: 9400,train loss is: 0.00044858986522657826\n",
      "test loss is 0.0007193513417573591\n",
      "Batch: 9500,train loss is: 0.00038082066451685385\n",
      "test loss is 0.0007203018022728399\n",
      "Batch: 9600,train loss is: 0.0004297102920723795\n",
      "test loss is 0.0006831729439220145\n",
      "Batch: 9700,train loss is: 0.00033676790162335114\n",
      "test loss is 0.0007042764463123801\n",
      "Batch: 9800,train loss is: 0.00030451661445423663\n",
      "test loss is 0.0006918143848568666\n",
      "Batch: 9900,train loss is: 0.0005025392473953376\n",
      "test loss is 0.0007211629900424032\n",
      "Batch: 10000,train loss is: 0.0004393022047745479\n",
      "test loss is 0.0006839015298230651\n",
      "Batch: 10100,train loss is: 0.001093111723217283\n",
      "test loss is 0.0007045525263320897\n",
      "Batch: 10200,train loss is: 0.0017259494870863713\n",
      "test loss is 0.0007832982672920634\n",
      "Batch: 10300,train loss is: 0.0004690388998737966\n",
      "test loss is 0.0006953132825057683\n",
      "Batch: 10400,train loss is: 0.0002991922433471628\n",
      "test loss is 0.0006909843534834766\n",
      "Batch: 10500,train loss is: 0.0007439751086354275\n",
      "test loss is 0.0006950167947798447\n",
      "Batch: 10600,train loss is: 0.0003313306791271762\n",
      "test loss is 0.0007492420870495121\n",
      "Batch: 10700,train loss is: 0.0006183195348480475\n",
      "test loss is 0.0006908332594151487\n",
      "Batch: 10800,train loss is: 0.0005777428556671389\n",
      "test loss is 0.0007011359011217915\n",
      "Batch: 10900,train loss is: 0.0006924111631865153\n",
      "test loss is 0.0006964556092715033\n",
      "Batch: 11000,train loss is: 0.0007830456601806923\n",
      "test loss is 0.0007198915111931882\n",
      "Batch: 11100,train loss is: 0.0010334732800861086\n",
      "test loss is 0.0006868080617732525\n",
      "Batch: 11200,train loss is: 0.0006789399349475819\n",
      "test loss is 0.0007346658151464205\n",
      "Batch: 11300,train loss is: 0.00031912887655310095\n",
      "test loss is 0.0007051090349617268\n",
      "Batch: 11400,train loss is: 0.0006666017799681235\n",
      "test loss is 0.0006982165892453898\n",
      "Batch: 11500,train loss is: 0.0005487969882914333\n",
      "test loss is 0.000700844289668593\n",
      "Batch: 11600,train loss is: 0.001895996039652558\n",
      "test loss is 0.0006993254044017046\n",
      "Batch: 11700,train loss is: 0.0002989356418215924\n",
      "test loss is 0.0006798619774103369\n",
      "Batch: 11800,train loss is: 0.0006137991099189221\n",
      "test loss is 0.000689830789719792\n",
      "Batch: 11900,train loss is: 0.0005612988484001453\n",
      "test loss is 0.0007042475895294702\n",
      "Batch: 12000,train loss is: 0.00034208654934894894\n",
      "test loss is 0.0006926218849672415\n",
      "Batch: 12100,train loss is: 0.0015877135921215791\n",
      "test loss is 0.000701250103067887\n",
      "Batch: 12200,train loss is: 0.0006785814501813649\n",
      "test loss is 0.0007187500161688524\n",
      "Batch: 12300,train loss is: 0.00046059409612091514\n",
      "test loss is 0.000708499431243478\n",
      "Batch: 12400,train loss is: 0.00044803106846165415\n",
      "test loss is 0.0006951486749679928\n",
      "Batch: 12500,train loss is: 0.0004515611768094222\n",
      "test loss is 0.0007185668718546223\n",
      "Batch: 12600,train loss is: 0.0004783306321368504\n",
      "test loss is 0.0007100843881297482\n",
      "Batch: 12700,train loss is: 0.0004030300375233287\n",
      "test loss is 0.0006857832697105102\n",
      "Batch: 12800,train loss is: 0.0003735115736523626\n",
      "test loss is 0.0006878446582104192\n",
      "Batch: 12900,train loss is: 0.0006650317577745661\n",
      "test loss is 0.0007033084710570073\n",
      "Batch: 13000,train loss is: 0.00021604076399075916\n",
      "test loss is 0.000683611840980404\n",
      "Batch: 13100,train loss is: 0.002227997246575587\n",
      "test loss is 0.0006842164559023934\n",
      "Batch: 13200,train loss is: 0.0008090466003945073\n",
      "test loss is 0.0008094461872324226\n",
      "Batch: 13300,train loss is: 0.0006594170279954493\n",
      "test loss is 0.000707619941205105\n",
      "Batch: 13400,train loss is: 0.0003685539841270206\n",
      "test loss is 0.0007230258780949707\n",
      "Batch: 13500,train loss is: 0.0006104662234642642\n",
      "test loss is 0.0007231241527065879\n",
      "Batch: 13600,train loss is: 0.0005639588565929806\n",
      "test loss is 0.0007066278013295678\n",
      "Batch: 13700,train loss is: 0.0006979023046816857\n",
      "test loss is 0.0006849923471582243\n",
      "Batch: 13800,train loss is: 0.00042420153292223035\n",
      "test loss is 0.0006863839375729781\n",
      "Batch: 13900,train loss is: 0.0003892458223201869\n",
      "test loss is 0.0007465164075096929\n",
      "Batch: 14000,train loss is: 0.00041600211117761215\n",
      "test loss is 0.0007110218152399051\n",
      "Batch: 14100,train loss is: 0.0007607587795302741\n",
      "test loss is 0.0007433982124943547\n",
      "Batch: 14200,train loss is: 0.0005989205146035569\n",
      "test loss is 0.0006787688231274471\n",
      "Batch: 14300,train loss is: 0.00025898236358189375\n",
      "test loss is 0.0006882730256561367\n",
      "Batch: 14400,train loss is: 0.001302914180632216\n",
      "test loss is 0.0006892355030411545\n",
      "Batch: 14500,train loss is: 0.0005109865948221356\n",
      "test loss is 0.000689411241143105\n",
      "Batch: 14600,train loss is: 0.001116804498654062\n",
      "test loss is 0.0006930594254836889\n",
      "Batch: 14700,train loss is: 0.0004653589277276144\n",
      "test loss is 0.0007437946141571859\n",
      "Batch: 14800,train loss is: 0.0008264162308162758\n",
      "test loss is 0.0007051499869084744\n",
      "Batch: 14900,train loss is: 0.00042390185333749606\n",
      "test loss is 0.000737904605071111\n",
      "Batch: 15000,train loss is: 0.0007299275889579071\n",
      "test loss is 0.0006828496621833134\n",
      "Batch: 15100,train loss is: 0.0006639184585212728\n",
      "test loss is 0.0007136282077596323\n",
      "Batch: 15200,train loss is: 0.0008951493640272775\n",
      "test loss is 0.0006988291879829197\n",
      "Batch: 15300,train loss is: 0.0003870801825703078\n",
      "test loss is 0.0006871701550853417\n",
      "Batch: 15400,train loss is: 0.0004655889812246729\n",
      "test loss is 0.0007017354183525583\n",
      "Batch: 15500,train loss is: 0.0007077775524516863\n",
      "test loss is 0.000679471848914954\n",
      "Batch: 15600,train loss is: 0.0008866031176001432\n",
      "test loss is 0.0007173350121263644\n",
      "Batch: 15700,train loss is: 0.00042961107670631615\n",
      "test loss is 0.0006900917506485056\n",
      "Batch: 15800,train loss is: 0.0006948717206417965\n",
      "test loss is 0.0006800895422997544\n",
      "Batch: 15900,train loss is: 0.0008687479748290431\n",
      "test loss is 0.0006997535479072397\n",
      "Batch: 16000,train loss is: 0.00038546356363725086\n",
      "test loss is 0.0007052448349776141\n",
      "Batch: 16100,train loss is: 0.0005280021760894404\n",
      "test loss is 0.0006911267887261238\n",
      "Batch: 16200,train loss is: 0.00039513833181504426\n",
      "test loss is 0.0006856775225983706\n",
      "Batch: 16300,train loss is: 0.000987789389444204\n",
      "test loss is 0.0007214340585703148\n",
      "Batch: 16400,train loss is: 0.00036456751240946213\n",
      "test loss is 0.000683879933811895\n",
      "Batch: 16500,train loss is: 0.0006359906706597522\n",
      "test loss is 0.0007087662188726971\n",
      "Batch: 16600,train loss is: 0.0003087517925831166\n",
      "test loss is 0.0006819553979684674\n",
      "Batch: 16700,train loss is: 0.0008867583615253319\n",
      "test loss is 0.0007024391694113974\n",
      "Batch: 16800,train loss is: 0.0005461608425388258\n",
      "test loss is 0.0007183038113090477\n",
      "Batch: 16900,train loss is: 0.0007010351457647404\n",
      "test loss is 0.0006991248765746077\n",
      "Batch: 17000,train loss is: 0.0006447421279260976\n",
      "test loss is 0.0006782993299375495\n",
      "Batch: 17100,train loss is: 0.0005364951924168086\n",
      "test loss is 0.0006940192899766168\n",
      "Batch: 17200,train loss is: 0.0004998391598745925\n",
      "test loss is 0.0006971067100781511\n",
      "Batch: 17300,train loss is: 0.0008401780581389461\n",
      "test loss is 0.0007186185660966149\n",
      "Batch: 17400,train loss is: 0.0004093305985875886\n",
      "test loss is 0.0006957111405232349\n",
      "Batch: 17500,train loss is: 0.0005469655761975471\n",
      "test loss is 0.0006912270765956727\n",
      "Batch: 17600,train loss is: 0.0004249828743857261\n",
      "test loss is 0.0007056559552249687\n",
      "Batch: 17700,train loss is: 0.00035502724846956984\n",
      "test loss is 0.0007260304685309926\n",
      "Batch: 17800,train loss is: 0.0013619928942569435\n",
      "test loss is 0.0006958572798825182\n",
      "Batch: 17900,train loss is: 0.0005003325792558587\n",
      "test loss is 0.000724868135039552\n",
      "Batch: 18000,train loss is: 0.0003102093331811449\n",
      "test loss is 0.0006778747568029394\n",
      "Batch: 18100,train loss is: 0.0004086631428153955\n",
      "test loss is 0.0006867377407681\n",
      "Batch: 18200,train loss is: 0.0005729112206211012\n",
      "test loss is 0.0006990168269330339\n",
      "Batch: 18300,train loss is: 0.001121796183502454\n",
      "test loss is 0.0006947967298880243\n",
      "Batch: 18400,train loss is: 0.00040173446373208686\n",
      "test loss is 0.0006868836816807164\n",
      "Batch: 18500,train loss is: 0.0009185279959980856\n",
      "test loss is 0.0006990075753579038\n",
      "Batch: 18600,train loss is: 0.0004760284149811321\n",
      "test loss is 0.0006900699687342661\n",
      "Batch: 18700,train loss is: 0.000577955761036444\n",
      "test loss is 0.0007086788967357212\n",
      "Batch: 18800,train loss is: 0.0005418534346167008\n",
      "test loss is 0.0006886944620797431\n",
      "Batch: 18900,train loss is: 0.000600989239889135\n",
      "test loss is 0.0007182511241283915\n",
      "Batch: 19000,train loss is: 0.0006315439888348365\n",
      "test loss is 0.0006919217728302532\n",
      "Batch: 19100,train loss is: 0.00033481743793095214\n",
      "test loss is 0.0007165216903327082\n",
      "Batch: 19200,train loss is: 0.0005245822934147693\n",
      "test loss is 0.0006975521404946831\n",
      "Batch: 19300,train loss is: 0.0007143901652959573\n",
      "test loss is 0.0006791919083264397\n",
      "Batch: 19400,train loss is: 0.000279549867665834\n",
      "test loss is 0.0006824995454511679\n",
      "Batch: 19500,train loss is: 0.0005210654778071866\n",
      "test loss is 0.0007217137905590629\n",
      "Batch: 19600,train loss is: 0.0004141536147526312\n",
      "test loss is 0.0007014634207649967\n",
      "Batch: 19700,train loss is: 0.0007247007187691038\n",
      "test loss is 0.0007739373127789525\n",
      "Batch: 19800,train loss is: 0.000599222414410316\n",
      "test loss is 0.0006989246686220759\n",
      "Batch: 19900,train loss is: 0.000293797576289344\n",
      "test loss is 0.0007199826542466336\n",
      "Batch: 20000,train loss is: 0.000714039426359314\n",
      "test loss is 0.0006931869044321359\n",
      "Batch: 20100,train loss is: 0.0004963839830549046\n",
      "test loss is 0.0006990997718996456\n",
      "Batch: 20200,train loss is: 0.0008150169299162267\n",
      "test loss is 0.0007100012184368436\n",
      "Batch: 20300,train loss is: 0.00030382134149153564\n",
      "test loss is 0.0007132764877889407\n",
      "Batch: 20400,train loss is: 0.0004916630498469709\n",
      "test loss is 0.000731897455024684\n",
      "Batch: 20500,train loss is: 0.0006044132437316741\n",
      "test loss is 0.0006893422912345093\n",
      "Batch: 20600,train loss is: 0.00048082833044544096\n",
      "test loss is 0.0007207756418380997\n",
      "Batch: 20700,train loss is: 0.000702522689754655\n",
      "test loss is 0.0006972606744950591\n",
      "Batch: 20800,train loss is: 0.0003472868910239741\n",
      "test loss is 0.0007162092348215991\n",
      "Batch: 20900,train loss is: 0.0005961136467881537\n",
      "test loss is 0.0007414414778318182\n",
      "Batch: 21000,train loss is: 0.000243044672380536\n",
      "test loss is 0.0006819597223264549\n",
      "Batch: 21100,train loss is: 0.0006863185185604084\n",
      "test loss is 0.000715708044907827\n",
      "Batch: 21200,train loss is: 0.0003548439809820326\n",
      "test loss is 0.0007223850668402805\n",
      "Batch: 21300,train loss is: 0.00033142605154400844\n",
      "test loss is 0.0006897615136487502\n",
      "Batch: 21400,train loss is: 0.000800558713249506\n",
      "test loss is 0.0006934647748987134\n",
      "Batch: 21500,train loss is: 0.00023761275878191659\n",
      "test loss is 0.0007053740156286311\n",
      "Batch: 21600,train loss is: 0.0007178835498565981\n",
      "test loss is 0.0007060554403582611\n",
      "Batch: 21700,train loss is: 0.00032361724893912176\n",
      "test loss is 0.0006820555793059745\n",
      "Batch: 21800,train loss is: 0.0007482122008156583\n",
      "test loss is 0.0007437186718142277\n",
      "Batch: 21900,train loss is: 0.0004922545025394676\n",
      "test loss is 0.00077970667860878\n",
      "Batch: 22000,train loss is: 0.0006662301197205636\n",
      "test loss is 0.0006867839543944826\n",
      "Batch: 22100,train loss is: 0.00035729419008952813\n",
      "test loss is 0.0006908770813505021\n",
      "Batch: 22200,train loss is: 0.000493777019883248\n",
      "test loss is 0.0006918277961189304\n",
      "Batch: 22300,train loss is: 0.0005869683382498569\n",
      "test loss is 0.0007001980546950637\n",
      "Batch: 22400,train loss is: 0.0004560833561202994\n",
      "test loss is 0.0007000208391450169\n",
      "Batch: 22500,train loss is: 0.00048752528903034354\n",
      "test loss is 0.0006820070865418034\n",
      "Batch: 22600,train loss is: 0.00100574313471901\n",
      "test loss is 0.0007265007470148656\n",
      "Batch: 22700,train loss is: 0.0003746125883360226\n",
      "test loss is 0.0006819772886770417\n",
      "Batch: 22800,train loss is: 0.0005274800093474193\n",
      "test loss is 0.0007154452265870165\n",
      "Batch: 22900,train loss is: 0.0003193934471661543\n",
      "test loss is 0.0007645183113519661\n",
      "Batch: 23000,train loss is: 0.00043845188529166595\n",
      "test loss is 0.0006938172651329047\n",
      "Batch: 23100,train loss is: 0.0004783924336806934\n",
      "test loss is 0.0006750346498357834\n",
      "Batch: 23200,train loss is: 0.0002965714537645306\n",
      "test loss is 0.000673858239146084\n",
      "Batch: 23300,train loss is: 0.0004808871067299816\n",
      "test loss is 0.0006858203064574945\n",
      "Batch: 23400,train loss is: 0.0006235866629952611\n",
      "test loss is 0.0007161986672061131\n",
      "Batch: 23500,train loss is: 0.0018032981874095554\n",
      "test loss is 0.0006824389436157895\n",
      "Batch: 23600,train loss is: 0.00029503503936091635\n",
      "test loss is 0.000685264030899506\n",
      "Batch: 23700,train loss is: 0.0009332482105128176\n",
      "test loss is 0.0006909068931968231\n",
      "Batch: 23800,train loss is: 0.0008112438495050296\n",
      "test loss is 0.0006900945710098691\n",
      "Batch: 23900,train loss is: 0.0008880061364535709\n",
      "test loss is 0.0006995477569489607\n",
      "Batch: 24000,train loss is: 0.0007479397456503902\n",
      "test loss is 0.0007409987344729596\n",
      "Batch: 24100,train loss is: 0.00032099677756438846\n",
      "test loss is 0.0006730052220956962\n",
      "Batch: 24200,train loss is: 0.0014529712065383849\n",
      "test loss is 0.0007030503728172224\n",
      "Batch: 24300,train loss is: 0.000536018884011893\n",
      "test loss is 0.0008290860386989045\n",
      "Batch: 24400,train loss is: 0.001313442675169491\n",
      "test loss is 0.000686559439799514\n",
      "Batch: 24500,train loss is: 0.0006653956815572862\n",
      "test loss is 0.0006776874576805527\n",
      "Batch: 24600,train loss is: 0.0004335464877588693\n",
      "test loss is 0.0007142273169820171\n",
      "Batch: 24700,train loss is: 0.0002924459627815182\n",
      "test loss is 0.0006914286619097908\n",
      "Batch: 24800,train loss is: 0.0007855896127254644\n",
      "test loss is 0.0006970368612587833\n",
      "Batch: 24900,train loss is: 0.0001966740051175024\n",
      "test loss is 0.0006893671593861681\n",
      "Batch: 25000,train loss is: 0.0006293393146598301\n",
      "test loss is 0.0006860480516242728\n",
      "Batch: 25100,train loss is: 0.0005360337944953864\n",
      "test loss is 0.0006966501690930875\n",
      "Batch: 25200,train loss is: 0.0007619617520978061\n",
      "test loss is 0.0007000027690650934\n",
      "Batch: 25300,train loss is: 0.0007623649757313146\n",
      "test loss is 0.00073259044485914\n",
      "Batch: 25400,train loss is: 0.0004163634786953412\n",
      "test loss is 0.0006854183995817117\n",
      "Batch: 25500,train loss is: 0.0005852039107568047\n",
      "test loss is 0.0006944217837071124\n",
      "Batch: 25600,train loss is: 0.0006420124030386313\n",
      "test loss is 0.0006904381547419699\n",
      "Batch: 25700,train loss is: 0.0005792604458511031\n",
      "test loss is 0.0007109481987630671\n",
      "Batch: 25800,train loss is: 0.0003622287460346696\n",
      "test loss is 0.00067890250775372\n",
      "Batch: 25900,train loss is: 0.0002947476289788418\n",
      "test loss is 0.0007355227409021926\n",
      "Batch: 26000,train loss is: 0.001584069280794722\n",
      "test loss is 0.0007445726133162051\n",
      "Batch: 26100,train loss is: 0.0008346192114512402\n",
      "test loss is 0.0006776162859075104\n",
      "Batch: 26200,train loss is: 0.0006291509860568035\n",
      "test loss is 0.0006770141895154544\n",
      "Batch: 26300,train loss is: 0.0011039517462708523\n",
      "test loss is 0.000699257097758713\n",
      "Batch: 26400,train loss is: 0.0005689699027139581\n",
      "test loss is 0.0006811061957893243\n",
      "Batch: 26500,train loss is: 0.00045189104422557275\n",
      "test loss is 0.0007030694430084461\n",
      "Batch: 26600,train loss is: 0.001379679505402411\n",
      "test loss is 0.000770222014470609\n",
      "Batch: 26700,train loss is: 0.0004679439394194993\n",
      "test loss is 0.0007845100736970512\n",
      "Batch: 26800,train loss is: 0.001162212480709159\n",
      "test loss is 0.0006984406424395373\n",
      "Batch: 26900,train loss is: 0.00046594105292699725\n",
      "test loss is 0.000693329144732656\n",
      "Batch: 27000,train loss is: 0.0006636482482446938\n",
      "test loss is 0.0008112246784134238\n",
      "Batch: 27100,train loss is: 0.000643096588651601\n",
      "test loss is 0.0007444273598676421\n",
      "Batch: 27200,train loss is: 0.0006980806280301555\n",
      "test loss is 0.0007318626478406025\n",
      "Batch: 27300,train loss is: 0.0004989870980549916\n",
      "test loss is 0.000702532725854565\n",
      "Batch: 27400,train loss is: 0.0008068752332475433\n",
      "test loss is 0.0006957604493603297\n",
      "Batch: 27500,train loss is: 0.00047099285852859044\n",
      "test loss is 0.000762297068492356\n",
      "Batch: 27600,train loss is: 0.0005598771384007766\n",
      "test loss is 0.0007246124310596428\n",
      "Batch: 27700,train loss is: 0.0015883861907738466\n",
      "test loss is 0.0007021623333174022\n",
      "Batch: 27800,train loss is: 0.0007826674943588094\n",
      "test loss is 0.0007201573363797552\n",
      "Batch: 27900,train loss is: 0.001374680045704263\n",
      "test loss is 0.0006920660452290873\n",
      "Batch: 28000,train loss is: 0.00040021190202850704\n",
      "test loss is 0.000686615613571478\n",
      "Batch: 28100,train loss is: 0.0008621718808508721\n",
      "test loss is 0.000705855157987518\n",
      "Batch: 28200,train loss is: 0.0005512473265756537\n",
      "test loss is 0.000670432297125276\n",
      "Batch: 28300,train loss is: 0.0017697140974024497\n",
      "test loss is 0.0007019657936022177\n",
      "Batch: 28400,train loss is: 0.0005156025161861418\n",
      "test loss is 0.0006801441693769764\n",
      "Batch: 28500,train loss is: 0.0015018222564386942\n",
      "test loss is 0.0007244934518858294\n",
      "Batch: 28600,train loss is: 0.00037535177985479717\n",
      "test loss is 0.0006976674178728111\n",
      "Batch: 28700,train loss is: 0.0005519257065251155\n",
      "test loss is 0.0006887032105187393\n",
      "Batch: 28800,train loss is: 0.0010925390814931703\n",
      "test loss is 0.000696367630794505\n",
      "Batch: 28900,train loss is: 0.0004649562917083748\n",
      "test loss is 0.000679735298946576\n",
      "Batch: 29000,train loss is: 0.0004992661249559702\n",
      "test loss is 0.0006872919901512538\n",
      "Batch: 29100,train loss is: 0.0005426123702863116\n",
      "test loss is 0.0006913155040838728\n",
      "Batch: 29200,train loss is: 0.000417554750095659\n",
      "test loss is 0.0006990858266804245\n",
      "Batch: 29300,train loss is: 0.0010071870099135636\n",
      "test loss is 0.0006830789413343128\n",
      "Batch: 29400,train loss is: 0.0007163088567138625\n",
      "test loss is 0.0007638573593605794\n",
      "Batch: 29500,train loss is: 0.00046777407238573696\n",
      "test loss is 0.000711861687777595\n",
      "Batch: 29600,train loss is: 0.000491865047291834\n",
      "test loss is 0.0006752681901819134\n",
      "Batch: 29700,train loss is: 0.00034595065217190137\n",
      "test loss is 0.0006952961362046392\n",
      "Batch: 29800,train loss is: 0.000802000471435696\n",
      "test loss is 0.0006759824958331607\n",
      "Batch: 29900,train loss is: 0.003054028808548478\n",
      "test loss is 0.0006832620429829276\n",
      "Batch: 30000,train loss is: 0.0003475902749031001\n",
      "test loss is 0.0007349451344336482\n",
      "Batch: 30100,train loss is: 0.0005363648514106162\n",
      "test loss is 0.0007236136271235932\n",
      "Batch: 30200,train loss is: 0.0003805197789268931\n",
      "test loss is 0.0006929665957496774\n",
      "Batch: 30300,train loss is: 0.0002749562978247893\n",
      "test loss is 0.0006783948271675235\n",
      "Batch: 30400,train loss is: 0.00033902471839171585\n",
      "test loss is 0.0006886798558036775\n",
      "Batch: 30500,train loss is: 0.0003352581763302465\n",
      "test loss is 0.0007300882594109082\n",
      "Batch: 30600,train loss is: 0.00024388078197693956\n",
      "test loss is 0.0006865674163096383\n",
      "Batch: 30700,train loss is: 0.00038758923204289627\n",
      "test loss is 0.0007834694967639071\n",
      "Batch: 30800,train loss is: 0.0002782097421629733\n",
      "test loss is 0.0007492081671833322\n",
      "Batch: 30900,train loss is: 0.0003513488405976723\n",
      "test loss is 0.0006894555931731982\n",
      "Batch: 31000,train loss is: 0.0004931876700546031\n",
      "test loss is 0.0007017334331970777\n",
      "Batch: 31100,train loss is: 0.00035621693019406134\n",
      "test loss is 0.0007718884512548335\n",
      "Batch: 31200,train loss is: 0.0005309098641545763\n",
      "test loss is 0.0006932335790324895\n",
      "Batch: 31300,train loss is: 0.0005161781239744828\n",
      "test loss is 0.0006922929470921019\n",
      "Batch: 31400,train loss is: 0.000777836150726276\n",
      "test loss is 0.000686213041818011\n",
      "Batch: 31500,train loss is: 0.0004580472453682867\n",
      "test loss is 0.0006897362969561541\n",
      "Batch: 31600,train loss is: 0.00043196532682374843\n",
      "test loss is 0.0006940181760056972\n",
      "Batch: 31700,train loss is: 0.0005755892719911161\n",
      "test loss is 0.0007165869366488533\n",
      "Batch: 31800,train loss is: 0.0012939248406910013\n",
      "test loss is 0.0006909982730687178\n",
      "Batch: 31900,train loss is: 0.0007503576024137467\n",
      "test loss is 0.0007026256343012434\n",
      "Batch: 32000,train loss is: 0.0040276499343658905\n",
      "test loss is 0.0006938910023586252\n",
      "Batch: 32100,train loss is: 0.000418332344010598\n",
      "test loss is 0.000704033066021287\n",
      "Batch: 32200,train loss is: 0.00034236732087783157\n",
      "test loss is 0.0006977272013859298\n",
      "Batch: 32300,train loss is: 0.0003516997021967465\n",
      "test loss is 0.0006821648120898355\n",
      "Batch: 32400,train loss is: 0.00039598349654110606\n",
      "test loss is 0.000675454070521968\n",
      "Batch: 32500,train loss is: 0.0009220959850667867\n",
      "test loss is 0.0007212831519624964\n",
      "Batch: 32600,train loss is: 0.0004753800923516257\n",
      "test loss is 0.0007013241344290733\n",
      "Batch: 32700,train loss is: 0.0004909875538401617\n",
      "test loss is 0.0007169600124221028\n",
      "Batch: 32800,train loss is: 0.0004341022973396349\n",
      "test loss is 0.0007269226307174435\n",
      "Batch: 32900,train loss is: 0.0006595648089471455\n",
      "test loss is 0.0006806715960620475\n",
      "Batch: 33000,train loss is: 0.0004786152209899427\n",
      "test loss is 0.0006841857739909228\n",
      "Batch: 33100,train loss is: 0.00042659759501112747\n",
      "test loss is 0.0007061374306823588\n",
      "Batch: 33200,train loss is: 0.0004562718048948052\n",
      "test loss is 0.000762142300000302\n",
      "Batch: 33300,train loss is: 0.0004911082276959771\n",
      "test loss is 0.0006779220885088045\n",
      "Batch: 33400,train loss is: 0.0004737002398233664\n",
      "test loss is 0.0006947496317481522\n",
      "Batch: 33500,train loss is: 0.00036643470545488045\n",
      "test loss is 0.000689189934209491\n",
      "Batch: 33600,train loss is: 0.0005662692593032182\n",
      "test loss is 0.000684127779237115\n",
      "Batch: 33700,train loss is: 0.0005601799187756851\n",
      "test loss is 0.0007016802759096718\n",
      "Batch: 33800,train loss is: 0.0007979443277465822\n",
      "test loss is 0.0006821656050733018\n",
      "Batch: 33900,train loss is: 0.0006617008068125997\n",
      "test loss is 0.0006862868600679982\n",
      "-----------------------Epoch: 14----------------------------------\n",
      "Batch: 0,train loss is: 0.0003066078492540474\n",
      "test loss is 0.000700240614124529\n",
      "Batch: 100,train loss is: 0.0013418940137351573\n",
      "test loss is 0.0006920130842972223\n",
      "Batch: 200,train loss is: 0.00044121727710722463\n",
      "test loss is 0.000705296564214513\n",
      "Batch: 300,train loss is: 0.00028158003609705197\n",
      "test loss is 0.0006885614990760505\n",
      "Batch: 400,train loss is: 0.0004679611885402382\n",
      "test loss is 0.0006780604398027441\n",
      "Batch: 500,train loss is: 0.00049909132140337\n",
      "test loss is 0.0007080660999696504\n",
      "Batch: 600,train loss is: 0.0002454882474325248\n",
      "test loss is 0.0007234563289368719\n",
      "Batch: 700,train loss is: 0.00035466323579160793\n",
      "test loss is 0.0006791700899164415\n",
      "Batch: 800,train loss is: 0.0004355887342671638\n",
      "test loss is 0.0006875560193198978\n",
      "Batch: 900,train loss is: 0.00029253253395849035\n",
      "test loss is 0.0006852443685337678\n",
      "Batch: 1000,train loss is: 0.0004528501942499104\n",
      "test loss is 0.000722125942874906\n",
      "Batch: 1100,train loss is: 0.0017546316870929908\n",
      "test loss is 0.0006784348026987337\n",
      "Batch: 1200,train loss is: 0.00030045053839144656\n",
      "test loss is 0.0006974756954613335\n",
      "Batch: 1300,train loss is: 0.0005274189862430824\n",
      "test loss is 0.0006841707841379281\n",
      "Batch: 1400,train loss is: 0.0004296944497063472\n",
      "test loss is 0.0007060408266073222\n",
      "Batch: 1500,train loss is: 0.0004487394596080539\n",
      "test loss is 0.000710574151398726\n",
      "Batch: 1600,train loss is: 0.0006844889894068485\n",
      "test loss is 0.0007601125703663949\n",
      "Batch: 1700,train loss is: 0.001218389717674944\n",
      "test loss is 0.0006761253222912936\n",
      "Batch: 1800,train loss is: 0.0003742269305495312\n",
      "test loss is 0.000685864301337541\n",
      "Batch: 1900,train loss is: 0.00040317617810596074\n",
      "test loss is 0.0007223393518536444\n",
      "Batch: 2000,train loss is: 0.0006729986709684811\n",
      "test loss is 0.0008288148385228206\n",
      "Batch: 2100,train loss is: 0.0006616667900981125\n",
      "test loss is 0.000837464588734144\n",
      "Batch: 2200,train loss is: 0.0007650214660129283\n",
      "test loss is 0.0006897756729858455\n",
      "Batch: 2300,train loss is: 0.00044799297186526104\n",
      "test loss is 0.0007148965591036821\n",
      "Batch: 2400,train loss is: 0.00047237547691165327\n",
      "test loss is 0.0006886423957561538\n",
      "Batch: 2500,train loss is: 0.0006024305496395734\n",
      "test loss is 0.0006828187429337854\n",
      "Batch: 2600,train loss is: 0.00039971597661919825\n",
      "test loss is 0.0006920283562583613\n",
      "Batch: 2700,train loss is: 0.0006209742068219481\n",
      "test loss is 0.0006712759123955093\n",
      "Batch: 2800,train loss is: 0.0004538139484106222\n",
      "test loss is 0.0007044850958116434\n",
      "Batch: 2900,train loss is: 0.00043788802370928084\n",
      "test loss is 0.0006778969424580681\n",
      "Batch: 3000,train loss is: 0.0005091940037290064\n",
      "test loss is 0.000677599943759171\n",
      "Batch: 3100,train loss is: 0.0006771569980331855\n",
      "test loss is 0.0007270864887292787\n",
      "Batch: 3200,train loss is: 0.0009992920603412556\n",
      "test loss is 0.0007007433428708015\n",
      "Batch: 3300,train loss is: 0.0005404076975116565\n",
      "test loss is 0.000693052052591372\n",
      "Batch: 3400,train loss is: 0.000414840827159359\n",
      "test loss is 0.000684852903375565\n",
      "Batch: 3500,train loss is: 0.00022445933797996494\n",
      "test loss is 0.0006825223025025006\n",
      "Batch: 3600,train loss is: 0.0005564644508493487\n",
      "test loss is 0.0007231870539188248\n",
      "Batch: 3700,train loss is: 0.0003471054491037783\n",
      "test loss is 0.0006835352504691689\n",
      "Batch: 3800,train loss is: 0.0004459206490072195\n",
      "test loss is 0.0007128691964045169\n",
      "Batch: 3900,train loss is: 0.0006065610249364701\n",
      "test loss is 0.0006896708337776193\n",
      "Batch: 4000,train loss is: 0.0004374151449008523\n",
      "test loss is 0.0006884027063312915\n",
      "Batch: 4100,train loss is: 0.0007253549975313956\n",
      "test loss is 0.0007074780460876545\n",
      "Batch: 4200,train loss is: 0.000280942059696309\n",
      "test loss is 0.0007142478579512345\n",
      "Batch: 4300,train loss is: 0.00040749184257014425\n",
      "test loss is 0.0006758810471737808\n",
      "Batch: 4400,train loss is: 0.00031471622078817256\n",
      "test loss is 0.0007001795978441513\n",
      "Batch: 4500,train loss is: 0.0007415297225016447\n",
      "test loss is 0.0007073713165694536\n",
      "Batch: 4600,train loss is: 0.0005786769461279317\n",
      "test loss is 0.0006898723992517318\n",
      "Batch: 4700,train loss is: 0.0005571241052829229\n",
      "test loss is 0.000714970370563749\n",
      "Batch: 4800,train loss is: 0.0012717383666448062\n",
      "test loss is 0.0006797610804100449\n",
      "Batch: 4900,train loss is: 0.0005256906002937166\n",
      "test loss is 0.0006901571798200718\n",
      "Batch: 5000,train loss is: 0.000583215317877766\n",
      "test loss is 0.0007011071442344601\n",
      "Batch: 5100,train loss is: 0.0004869240767403923\n",
      "test loss is 0.0007434892013077387\n",
      "Batch: 5200,train loss is: 0.00030824799298355616\n",
      "test loss is 0.0006954017967454444\n",
      "Batch: 5300,train loss is: 0.0005257457022907633\n",
      "test loss is 0.0007068476855440905\n",
      "Batch: 5400,train loss is: 0.0007264669564686323\n",
      "test loss is 0.0006773807734880281\n",
      "Batch: 5500,train loss is: 0.0004374221202358631\n",
      "test loss is 0.0007060392493137774\n",
      "Batch: 5600,train loss is: 0.0005748203470695082\n",
      "test loss is 0.0006935533629837658\n",
      "Batch: 5700,train loss is: 0.0005676149587929789\n",
      "test loss is 0.0006915164716820382\n",
      "Batch: 5800,train loss is: 0.0004932501298079569\n",
      "test loss is 0.0006850862565549945\n",
      "Batch: 5900,train loss is: 0.0033852599588341816\n",
      "test loss is 0.0006906927226446181\n",
      "Batch: 6000,train loss is: 0.0016472279619319428\n",
      "test loss is 0.00069947568594172\n",
      "Batch: 6100,train loss is: 0.0004163120722473792\n",
      "test loss is 0.0006764802397230552\n",
      "Batch: 6200,train loss is: 0.0005721448916582758\n",
      "test loss is 0.000692182271891214\n",
      "Batch: 6300,train loss is: 0.0011165977016442223\n",
      "test loss is 0.0007060985795439767\n",
      "Batch: 6400,train loss is: 0.0005263779773214956\n",
      "test loss is 0.0007333296998388787\n",
      "Batch: 6500,train loss is: 0.0003103541635445366\n",
      "test loss is 0.0006970661737446829\n",
      "Batch: 6600,train loss is: 0.0020728658167054577\n",
      "test loss is 0.0006824222011245774\n",
      "Batch: 6700,train loss is: 0.0003288852939467955\n",
      "test loss is 0.0006980606472647375\n",
      "Batch: 6800,train loss is: 0.0007477571866775864\n",
      "test loss is 0.0006834586538082219\n",
      "Batch: 6900,train loss is: 0.0006843325372069058\n",
      "test loss is 0.0006935523551018646\n",
      "Batch: 7000,train loss is: 0.0005787495210792428\n",
      "test loss is 0.0007080222706529185\n",
      "Batch: 7100,train loss is: 0.0011254435999882677\n",
      "test loss is 0.0006958624018918262\n",
      "Batch: 7200,train loss is: 0.00044238155215675596\n",
      "test loss is 0.000701814960679996\n",
      "Batch: 7300,train loss is: 0.0007689465618637873\n",
      "test loss is 0.0007236381437424305\n",
      "Batch: 7400,train loss is: 0.0003620033718194155\n",
      "test loss is 0.0006838247117864841\n",
      "Batch: 7500,train loss is: 0.0006643594301881461\n",
      "test loss is 0.0007070262758405937\n",
      "Batch: 7600,train loss is: 0.0003811654655068415\n",
      "test loss is 0.0006707672309735565\n",
      "Batch: 7700,train loss is: 0.000843999369906647\n",
      "test loss is 0.0006917803539131387\n",
      "Batch: 7800,train loss is: 0.0002835487056151253\n",
      "test loss is 0.0006780749780073189\n",
      "Batch: 7900,train loss is: 0.000576125628692702\n",
      "test loss is 0.0007013658629633744\n",
      "Batch: 8000,train loss is: 0.0004254189559618319\n",
      "test loss is 0.0007212767920710573\n",
      "Batch: 8100,train loss is: 0.000584833255091643\n",
      "test loss is 0.000707641583859436\n",
      "Batch: 8200,train loss is: 0.001097422262473032\n",
      "test loss is 0.0007024368692163696\n",
      "Batch: 8300,train loss is: 0.00037388509443751284\n",
      "test loss is 0.0007041507253074035\n",
      "Batch: 8400,train loss is: 0.0005608626056327578\n",
      "test loss is 0.000686347031344264\n",
      "Batch: 8500,train loss is: 0.0006798056393664025\n",
      "test loss is 0.0007014451912303122\n",
      "Batch: 8600,train loss is: 0.0006655051364205753\n",
      "test loss is 0.0007086899937847636\n",
      "Batch: 8700,train loss is: 0.0004760761895260363\n",
      "test loss is 0.0007387968823284297\n",
      "Batch: 8800,train loss is: 0.00034032683625224736\n",
      "test loss is 0.0006801211506436322\n",
      "Batch: 8900,train loss is: 0.0010683789997978554\n",
      "test loss is 0.0006800261860263595\n",
      "Batch: 9000,train loss is: 0.0002556395015555466\n",
      "test loss is 0.0006997649861959546\n",
      "Batch: 9100,train loss is: 0.00032652098197960403\n",
      "test loss is 0.0006783197863641395\n",
      "Batch: 9200,train loss is: 0.0007944273220786062\n",
      "test loss is 0.0007528183408263976\n",
      "Batch: 9300,train loss is: 0.0005515306107547936\n",
      "test loss is 0.000723936322800395\n",
      "Batch: 9400,train loss is: 0.000448476769821333\n",
      "test loss is 0.0007112648366005539\n",
      "Batch: 9500,train loss is: 0.0003752854342517568\n",
      "test loss is 0.0007134511141423648\n",
      "Batch: 9600,train loss is: 0.0004236217351183902\n",
      "test loss is 0.0006756901237340185\n",
      "Batch: 9700,train loss is: 0.0003447825149844784\n",
      "test loss is 0.0006955460964008582\n",
      "Batch: 9800,train loss is: 0.0002902895957322073\n",
      "test loss is 0.0006852930461908451\n",
      "Batch: 9900,train loss is: 0.0004966426495658857\n",
      "test loss is 0.0007135527578442576\n",
      "Batch: 10000,train loss is: 0.0004344553601166738\n",
      "test loss is 0.0006758160493829167\n",
      "Batch: 10100,train loss is: 0.001064530101437346\n",
      "test loss is 0.0006976672695440805\n",
      "Batch: 10200,train loss is: 0.0017145425964728977\n",
      "test loss is 0.0007767683221442152\n",
      "Batch: 10300,train loss is: 0.0004594157505054849\n",
      "test loss is 0.0006878742555423199\n",
      "Batch: 10400,train loss is: 0.0002929047465072056\n",
      "test loss is 0.0006838895794924431\n",
      "Batch: 10500,train loss is: 0.0007458392139326084\n",
      "test loss is 0.0006891477254093224\n",
      "Batch: 10600,train loss is: 0.0003284040156895183\n",
      "test loss is 0.0007443586262456994\n",
      "Batch: 10700,train loss is: 0.0006151277269499968\n",
      "test loss is 0.0006835270356104212\n",
      "Batch: 10800,train loss is: 0.0005834121062355742\n",
      "test loss is 0.0006937292502831442\n",
      "Batch: 10900,train loss is: 0.000702676865221526\n",
      "test loss is 0.0006891279679102363\n",
      "Batch: 11000,train loss is: 0.0007778357718582336\n",
      "test loss is 0.0007118666736058131\n",
      "Batch: 11100,train loss is: 0.001011986430979611\n",
      "test loss is 0.0006786429488798829\n",
      "Batch: 11200,train loss is: 0.0006805245162616016\n",
      "test loss is 0.0007287266245599466\n",
      "Batch: 11300,train loss is: 0.0003160121325170913\n",
      "test loss is 0.0006973293095741638\n",
      "Batch: 11400,train loss is: 0.0006827403144108317\n",
      "test loss is 0.000689922804996305\n",
      "Batch: 11500,train loss is: 0.0005465681402873481\n",
      "test loss is 0.0006943313524163837\n",
      "Batch: 11600,train loss is: 0.0018881949004478284\n",
      "test loss is 0.0006902904961771477\n",
      "Batch: 11700,train loss is: 0.00029807139314172573\n",
      "test loss is 0.0006731210632537792\n",
      "Batch: 11800,train loss is: 0.0006054878588080138\n",
      "test loss is 0.0006816621541468117\n",
      "Batch: 11900,train loss is: 0.0005463920403247273\n",
      "test loss is 0.0006965565280424293\n",
      "Batch: 12000,train loss is: 0.0003340075830920154\n",
      "test loss is 0.0006851768205674973\n",
      "Batch: 12100,train loss is: 0.0015734947787768134\n",
      "test loss is 0.0006947633641505382\n",
      "Batch: 12200,train loss is: 0.0006692870277761955\n",
      "test loss is 0.0007113952312284539\n",
      "Batch: 12300,train loss is: 0.0004631462218530451\n",
      "test loss is 0.0007006568533046879\n",
      "Batch: 12400,train loss is: 0.0004311686517638106\n",
      "test loss is 0.0006874997533625986\n",
      "Batch: 12500,train loss is: 0.0004654523558426975\n",
      "test loss is 0.0007114397984205722\n",
      "Batch: 12600,train loss is: 0.00048612489408379886\n",
      "test loss is 0.0007028592893532012\n",
      "Batch: 12700,train loss is: 0.00039437051823055646\n",
      "test loss is 0.0006797828663855036\n",
      "Batch: 12800,train loss is: 0.00037407400886345967\n",
      "test loss is 0.0006801887053952479\n",
      "Batch: 12900,train loss is: 0.0006507566826973\n",
      "test loss is 0.0006965191808278473\n",
      "Batch: 13000,train loss is: 0.00021308155934760136\n",
      "test loss is 0.0006780459186979029\n",
      "Batch: 13100,train loss is: 0.0021897017368933632\n",
      "test loss is 0.0006766876923959771\n",
      "Batch: 13200,train loss is: 0.0008149935298864648\n",
      "test loss is 0.0008032620348574497\n",
      "Batch: 13300,train loss is: 0.0006577238543085961\n",
      "test loss is 0.0006979065581830585\n",
      "Batch: 13400,train loss is: 0.00036706947070153773\n",
      "test loss is 0.0007148730044581666\n",
      "Batch: 13500,train loss is: 0.000605121982600217\n",
      "test loss is 0.000715304260047404\n",
      "Batch: 13600,train loss is: 0.0005680771605593873\n",
      "test loss is 0.0006994011965648811\n",
      "Batch: 13700,train loss is: 0.0006783539990434828\n",
      "test loss is 0.0006768941656169017\n",
      "Batch: 13800,train loss is: 0.00041442738513328223\n",
      "test loss is 0.0006787888175936868\n",
      "Batch: 13900,train loss is: 0.00036190917383784933\n",
      "test loss is 0.0007391973283359767\n",
      "Batch: 14000,train loss is: 0.00040465219020024057\n",
      "test loss is 0.0007060418817300254\n",
      "Batch: 14100,train loss is: 0.0007540426453389634\n",
      "test loss is 0.000734843758885082\n",
      "Batch: 14200,train loss is: 0.0005893490468431433\n",
      "test loss is 0.0006717871070008522\n",
      "Batch: 14300,train loss is: 0.00025076321121680114\n",
      "test loss is 0.000680632024956347\n",
      "Batch: 14400,train loss is: 0.0012962978234599622\n",
      "test loss is 0.0006818407582918913\n",
      "Batch: 14500,train loss is: 0.0005349215667290078\n",
      "test loss is 0.0006817612237557403\n",
      "Batch: 14600,train loss is: 0.0010505642884007878\n",
      "test loss is 0.0006868845965355343\n",
      "Batch: 14700,train loss is: 0.00046128675905653864\n",
      "test loss is 0.0007368816027201182\n",
      "Batch: 14800,train loss is: 0.0008098588543963655\n",
      "test loss is 0.0006981300927708426\n",
      "Batch: 14900,train loss is: 0.0004329954747399484\n",
      "test loss is 0.0007316083268809365\n",
      "Batch: 15000,train loss is: 0.0007136651242624483\n",
      "test loss is 0.0006751138904272305\n",
      "Batch: 15100,train loss is: 0.0006681147848854914\n",
      "test loss is 0.0007068814616874918\n",
      "Batch: 15200,train loss is: 0.0008744867137075694\n",
      "test loss is 0.0006909488306477428\n",
      "Batch: 15300,train loss is: 0.0003781243326729603\n",
      "test loss is 0.0006808834283744421\n",
      "Batch: 15400,train loss is: 0.00046007239937652475\n",
      "test loss is 0.0006943391323555299\n",
      "Batch: 15500,train loss is: 0.0006970527470018477\n",
      "test loss is 0.0006726832691171578\n",
      "Batch: 15600,train loss is: 0.000883610219301488\n",
      "test loss is 0.0007082883775098465\n",
      "Batch: 15700,train loss is: 0.0004118152616744397\n",
      "test loss is 0.0006825215670254268\n",
      "Batch: 15800,train loss is: 0.0006774846910813648\n",
      "test loss is 0.000671523749938967\n",
      "Batch: 15900,train loss is: 0.0008573992856689863\n",
      "test loss is 0.0006916947768944234\n",
      "Batch: 16000,train loss is: 0.0003810559206830561\n",
      "test loss is 0.0006963371597071552\n",
      "Batch: 16100,train loss is: 0.0005257462462347853\n",
      "test loss is 0.0006842029334469294\n",
      "Batch: 16200,train loss is: 0.0003911984562220107\n",
      "test loss is 0.0006773842419799314\n",
      "Batch: 16300,train loss is: 0.000963298474228845\n",
      "test loss is 0.0007144287895489601\n",
      "Batch: 16400,train loss is: 0.00035313962698395\n",
      "test loss is 0.0006765076846821515\n",
      "Batch: 16500,train loss is: 0.0006418033244698713\n",
      "test loss is 0.000700106534349777\n",
      "Batch: 16600,train loss is: 0.00030841102980336347\n",
      "test loss is 0.0006751404948321434\n",
      "Batch: 16700,train loss is: 0.000876751733387144\n",
      "test loss is 0.0006950890754325815\n",
      "Batch: 16800,train loss is: 0.0005383546889393663\n",
      "test loss is 0.0007116908175607764\n",
      "Batch: 16900,train loss is: 0.0007071599481786868\n",
      "test loss is 0.0006921086909315862\n",
      "Batch: 17000,train loss is: 0.0006242641923672511\n",
      "test loss is 0.0006709417643950825\n",
      "Batch: 17100,train loss is: 0.0005293639013036245\n",
      "test loss is 0.0006863384205235714\n",
      "Batch: 17200,train loss is: 0.0004959534227769609\n",
      "test loss is 0.0006885727405497612\n",
      "Batch: 17300,train loss is: 0.0008254143255448723\n",
      "test loss is 0.0007126518321581687\n",
      "Batch: 17400,train loss is: 0.0004063409351172893\n",
      "test loss is 0.0006891306776494983\n",
      "Batch: 17500,train loss is: 0.0005516282728053083\n",
      "test loss is 0.0006847131533813045\n",
      "Batch: 17600,train loss is: 0.00042596174338369286\n",
      "test loss is 0.0006978203434286403\n",
      "Batch: 17700,train loss is: 0.0003506490583770931\n",
      "test loss is 0.0007194156970689469\n",
      "Batch: 17800,train loss is: 0.0013960376951479597\n",
      "test loss is 0.0006914107290511576\n",
      "Batch: 17900,train loss is: 0.0004996444994576853\n",
      "test loss is 0.0007191451289133841\n",
      "Batch: 18000,train loss is: 0.00030578097078320585\n",
      "test loss is 0.0006703161902583416\n",
      "Batch: 18100,train loss is: 0.0003996744199739001\n",
      "test loss is 0.000678948929368651\n",
      "Batch: 18200,train loss is: 0.000572835542961992\n",
      "test loss is 0.0006924905446521883\n",
      "Batch: 18300,train loss is: 0.0011521139101602264\n",
      "test loss is 0.0006886348806188952\n",
      "Batch: 18400,train loss is: 0.00040240199745874785\n",
      "test loss is 0.0006803973433886043\n",
      "Batch: 18500,train loss is: 0.0009100281915528398\n",
      "test loss is 0.000691889370287663\n",
      "Batch: 18600,train loss is: 0.0004682274008283065\n",
      "test loss is 0.0006830358117717406\n",
      "Batch: 18700,train loss is: 0.0005802686092476568\n",
      "test loss is 0.000702666378437\n",
      "Batch: 18800,train loss is: 0.000544656858873074\n",
      "test loss is 0.0006808319927350847\n",
      "Batch: 18900,train loss is: 0.0005900761117054121\n",
      "test loss is 0.0007092093851067574\n",
      "Batch: 19000,train loss is: 0.0006184870375996268\n",
      "test loss is 0.0006842147737719558\n",
      "Batch: 19100,train loss is: 0.00034971497341980444\n",
      "test loss is 0.0007087607109390167\n",
      "Batch: 19200,train loss is: 0.0005139219121037318\n",
      "test loss is 0.0006905128424855856\n",
      "Batch: 19300,train loss is: 0.0006930382820512829\n",
      "test loss is 0.0006711588756011577\n",
      "Batch: 19400,train loss is: 0.0002755489118578647\n",
      "test loss is 0.0006758071568737598\n",
      "Batch: 19500,train loss is: 0.0005167608636019429\n",
      "test loss is 0.0007143123250396585\n",
      "Batch: 19600,train loss is: 0.00040558707402422474\n",
      "test loss is 0.000693890475084347\n",
      "Batch: 19700,train loss is: 0.0007151883803277307\n",
      "test loss is 0.0007671993506747759\n",
      "Batch: 19800,train loss is: 0.0005935754690070628\n",
      "test loss is 0.0006925452726567589\n",
      "Batch: 19900,train loss is: 0.00030571778507987947\n",
      "test loss is 0.0007095501074300032\n",
      "Batch: 20000,train loss is: 0.0007072888865194097\n",
      "test loss is 0.0006850012705795674\n",
      "Batch: 20100,train loss is: 0.000516617666393828\n",
      "test loss is 0.0006935028997876566\n",
      "Batch: 20200,train loss is: 0.0007970004063007325\n",
      "test loss is 0.0007036763727197425\n",
      "Batch: 20300,train loss is: 0.0002957113500886464\n",
      "test loss is 0.0007051979746069507\n",
      "Batch: 20400,train loss is: 0.0004900194214425845\n",
      "test loss is 0.0007231752612293168\n",
      "Batch: 20500,train loss is: 0.0006056828439194811\n",
      "test loss is 0.0006817407437878254\n",
      "Batch: 20600,train loss is: 0.0004751688283417009\n",
      "test loss is 0.0007141128347046492\n",
      "Batch: 20700,train loss is: 0.0007161134977448923\n",
      "test loss is 0.0006894805766631015\n",
      "Batch: 20800,train loss is: 0.0003380587612173941\n",
      "test loss is 0.000711390630897479\n",
      "Batch: 20900,train loss is: 0.0005883087671336676\n",
      "test loss is 0.0007337645206113676\n",
      "Batch: 21000,train loss is: 0.0002375428442259706\n",
      "test loss is 0.0006748584495287361\n",
      "Batch: 21100,train loss is: 0.0006850085918689613\n",
      "test loss is 0.0007090690815594082\n",
      "Batch: 21200,train loss is: 0.00036238380406447194\n",
      "test loss is 0.0007150670963632415\n",
      "Batch: 21300,train loss is: 0.00033094749229685487\n",
      "test loss is 0.0006832134582507562\n",
      "Batch: 21400,train loss is: 0.000797048559210985\n",
      "test loss is 0.0006850463366598114\n",
      "Batch: 21500,train loss is: 0.000240851975013341\n",
      "test loss is 0.0006978543744165951\n",
      "Batch: 21600,train loss is: 0.0007153047927580656\n",
      "test loss is 0.000699538610382605\n",
      "Batch: 21700,train loss is: 0.00031471408646987516\n",
      "test loss is 0.0006746911954911667\n",
      "Batch: 21800,train loss is: 0.0007739280662786796\n",
      "test loss is 0.0007344934938752893\n",
      "Batch: 21900,train loss is: 0.0004780899177164432\n",
      "test loss is 0.0007717808695924956\n",
      "Batch: 22000,train loss is: 0.0006533754042318777\n",
      "test loss is 0.0006799392139030193\n",
      "Batch: 22100,train loss is: 0.00036408564099321033\n",
      "test loss is 0.0006831377351505634\n",
      "Batch: 22200,train loss is: 0.0004918621933949999\n",
      "test loss is 0.0006859243991076833\n",
      "Batch: 22300,train loss is: 0.0005699262057286602\n",
      "test loss is 0.0006939624474144716\n",
      "Batch: 22400,train loss is: 0.00045286360052335757\n",
      "test loss is 0.0006949294848995974\n",
      "Batch: 22500,train loss is: 0.000492447705988486\n",
      "test loss is 0.0006748981790119939\n",
      "Batch: 22600,train loss is: 0.0009973944298411136\n",
      "test loss is 0.0007196390934069075\n",
      "Batch: 22700,train loss is: 0.0003679054837365502\n",
      "test loss is 0.0006747486346376088\n",
      "Batch: 22800,train loss is: 0.0005188114365709155\n",
      "test loss is 0.0007095924408015673\n",
      "Batch: 22900,train loss is: 0.00031446179589569124\n",
      "test loss is 0.0007583743002140274\n",
      "Batch: 23000,train loss is: 0.0004479287892986298\n",
      "test loss is 0.0006869121375645636\n",
      "Batch: 23100,train loss is: 0.0004658863863562947\n",
      "test loss is 0.0006678486493935647\n",
      "Batch: 23200,train loss is: 0.00029224267038685205\n",
      "test loss is 0.0006674763908414266\n",
      "Batch: 23300,train loss is: 0.0004716101164045496\n",
      "test loss is 0.0006785943793696453\n",
      "Batch: 23400,train loss is: 0.0006148136341214254\n",
      "test loss is 0.0007084257589519303\n",
      "Batch: 23500,train loss is: 0.001708560503412526\n",
      "test loss is 0.0006740807340017894\n",
      "Batch: 23600,train loss is: 0.00029962316443765924\n",
      "test loss is 0.0006792860358282264\n",
      "Batch: 23700,train loss is: 0.0009374111751420764\n",
      "test loss is 0.0006821002253614506\n",
      "Batch: 23800,train loss is: 0.0008063086162479405\n",
      "test loss is 0.0006823173371642695\n",
      "Batch: 23900,train loss is: 0.0008885384532153567\n",
      "test loss is 0.0006928078519058639\n",
      "Batch: 24000,train loss is: 0.0007372939042784059\n",
      "test loss is 0.0007333255713534637\n",
      "Batch: 24100,train loss is: 0.00031721167243508973\n",
      "test loss is 0.0006654931265288891\n",
      "Batch: 24200,train loss is: 0.001444006472746119\n",
      "test loss is 0.0006949779859203927\n",
      "Batch: 24300,train loss is: 0.0005379520927230031\n",
      "test loss is 0.0008191378835140226\n",
      "Batch: 24400,train loss is: 0.0013059607916903672\n",
      "test loss is 0.000679951398618265\n",
      "Batch: 24500,train loss is: 0.0006633899192609627\n",
      "test loss is 0.0006696624880861503\n",
      "Batch: 24600,train loss is: 0.00042418151726301073\n",
      "test loss is 0.0007071183202280256\n",
      "Batch: 24700,train loss is: 0.00028997782565576914\n",
      "test loss is 0.0006856038462709154\n",
      "Batch: 24800,train loss is: 0.0007650823805646161\n",
      "test loss is 0.0006915744341282435\n",
      "Batch: 24900,train loss is: 0.0001964654247988413\n",
      "test loss is 0.0006813488112570806\n",
      "Batch: 25000,train loss is: 0.0005992888165221452\n",
      "test loss is 0.0006784330271630045\n",
      "Batch: 25100,train loss is: 0.0005122618543996766\n",
      "test loss is 0.0006886942622946679\n",
      "Batch: 25200,train loss is: 0.0007538710903610224\n",
      "test loss is 0.0006916597649975927\n",
      "Batch: 25300,train loss is: 0.0007646262086006581\n",
      "test loss is 0.0007285719694216467\n",
      "Batch: 25400,train loss is: 0.0004127212822150347\n",
      "test loss is 0.0006786185948449867\n",
      "Batch: 25500,train loss is: 0.0005858963687281876\n",
      "test loss is 0.0006880847683583978\n",
      "Batch: 25600,train loss is: 0.0006393040675758343\n",
      "test loss is 0.0006838676203056201\n",
      "Batch: 25700,train loss is: 0.0005838166349364937\n",
      "test loss is 0.0007026911080626475\n",
      "Batch: 25800,train loss is: 0.00036055609521502753\n",
      "test loss is 0.0006717579880062307\n",
      "Batch: 25900,train loss is: 0.0002941403207802931\n",
      "test loss is 0.0007281734442604218\n",
      "Batch: 26000,train loss is: 0.0015731661733819978\n",
      "test loss is 0.0007356271232361252\n",
      "Batch: 26100,train loss is: 0.0008185141722397974\n",
      "test loss is 0.0006706136252696315\n",
      "Batch: 26200,train loss is: 0.0006335399176925094\n",
      "test loss is 0.0006699923052725565\n",
      "Batch: 26300,train loss is: 0.0010783103827929792\n",
      "test loss is 0.0006922668333488685\n",
      "Batch: 26400,train loss is: 0.0005670312802773368\n",
      "test loss is 0.000672284856850166\n",
      "Batch: 26500,train loss is: 0.0004700607683200427\n",
      "test loss is 0.0006998776944918625\n",
      "Batch: 26600,train loss is: 0.0013645671225790538\n",
      "test loss is 0.0007636233623254223\n",
      "Batch: 26700,train loss is: 0.0004715885045616229\n",
      "test loss is 0.0007762245372692652\n",
      "Batch: 26800,train loss is: 0.0011622222601898409\n",
      "test loss is 0.0006924546209901963\n",
      "Batch: 26900,train loss is: 0.00046208046771834335\n",
      "test loss is 0.000685872624682518\n",
      "Batch: 27000,train loss is: 0.0006606512298980293\n",
      "test loss is 0.0007999945566225494\n",
      "Batch: 27100,train loss is: 0.0006456047161223611\n",
      "test loss is 0.0007369837786541008\n",
      "Batch: 27200,train loss is: 0.0006871265359586636\n",
      "test loss is 0.0007227855427073833\n",
      "Batch: 27300,train loss is: 0.0004906338427407229\n",
      "test loss is 0.0006964312854040199\n",
      "Batch: 27400,train loss is: 0.0007715065451947608\n",
      "test loss is 0.0006895248045682231\n",
      "Batch: 27500,train loss is: 0.0004622852363391057\n",
      "test loss is 0.0007524602968963418\n",
      "Batch: 27600,train loss is: 0.0005473695598212265\n",
      "test loss is 0.0007159758879311263\n",
      "Batch: 27700,train loss is: 0.001569723179249794\n",
      "test loss is 0.0006942156082979014\n",
      "Batch: 27800,train loss is: 0.0007701046650878254\n",
      "test loss is 0.000712423383996591\n",
      "Batch: 27900,train loss is: 0.0013763511324778803\n",
      "test loss is 0.0006857497508677578\n",
      "Batch: 28000,train loss is: 0.0003994272790115713\n",
      "test loss is 0.0006797298325025933\n",
      "Batch: 28100,train loss is: 0.0008511605630317434\n",
      "test loss is 0.0006982046002703932\n",
      "Batch: 28200,train loss is: 0.0005414215888663073\n",
      "test loss is 0.0006635828871893427\n",
      "Batch: 28300,train loss is: 0.0017507815845835884\n",
      "test loss is 0.0006962142583014318\n",
      "Batch: 28400,train loss is: 0.0005154378517199944\n",
      "test loss is 0.0006733418299970833\n",
      "Batch: 28500,train loss is: 0.0014861956565556623\n",
      "test loss is 0.0007191074532418686\n",
      "Batch: 28600,train loss is: 0.00036941038262627705\n",
      "test loss is 0.0006899688151923053\n",
      "Batch: 28700,train loss is: 0.0005547035440384166\n",
      "test loss is 0.0006808328141040752\n",
      "Batch: 28800,train loss is: 0.001087170836828555\n",
      "test loss is 0.0006900141937822414\n",
      "Batch: 28900,train loss is: 0.00046249857643161624\n",
      "test loss is 0.0006725061832417926\n",
      "Batch: 29000,train loss is: 0.0004980834631106761\n",
      "test loss is 0.0006803877069721906\n",
      "Batch: 29100,train loss is: 0.0005327855663951105\n",
      "test loss is 0.0006832621764270686\n",
      "Batch: 29200,train loss is: 0.00041956423232004085\n",
      "test loss is 0.0006915468751430376\n",
      "Batch: 29300,train loss is: 0.0009925877147781057\n",
      "test loss is 0.0006760966339722248\n",
      "Batch: 29400,train loss is: 0.000711410120072495\n",
      "test loss is 0.0007598477922681814\n",
      "Batch: 29500,train loss is: 0.00046716843706647926\n",
      "test loss is 0.000706048633104294\n",
      "Batch: 29600,train loss is: 0.0004903999723564875\n",
      "test loss is 0.0006673506913527444\n",
      "Batch: 29700,train loss is: 0.000346147176130211\n",
      "test loss is 0.000688407540644298\n",
      "Batch: 29800,train loss is: 0.0007980085623669548\n",
      "test loss is 0.000669010325952394\n",
      "Batch: 29900,train loss is: 0.002999065087852663\n",
      "test loss is 0.0006758557030542721\n",
      "Batch: 30000,train loss is: 0.0003502363295508167\n",
      "test loss is 0.0007275379536549042\n",
      "Batch: 30100,train loss is: 0.0005508841755227276\n",
      "test loss is 0.0007170201841230376\n",
      "Batch: 30200,train loss is: 0.000378550156787247\n",
      "test loss is 0.0006870962015670358\n",
      "Batch: 30300,train loss is: 0.00027543172109481203\n",
      "test loss is 0.0006720247183813492\n",
      "Batch: 30400,train loss is: 0.0003354702606439053\n",
      "test loss is 0.0006814376360070434\n",
      "Batch: 30500,train loss is: 0.00032798130276960304\n",
      "test loss is 0.0007197232542442472\n",
      "Batch: 30600,train loss is: 0.0002461522272215713\n",
      "test loss is 0.0006795089725349478\n",
      "Batch: 30700,train loss is: 0.00038744784572437244\n",
      "test loss is 0.0007712012786612758\n",
      "Batch: 30800,train loss is: 0.0002740662568327543\n",
      "test loss is 0.0007416378482063585\n",
      "Batch: 30900,train loss is: 0.00034847243710968927\n",
      "test loss is 0.0006821507039416228\n",
      "Batch: 31000,train loss is: 0.0004885222391320799\n",
      "test loss is 0.0006947075988311883\n",
      "Batch: 31100,train loss is: 0.0003456569298630051\n",
      "test loss is 0.0007639973713584838\n",
      "Batch: 31200,train loss is: 0.0005208584517282213\n",
      "test loss is 0.0006854712328220274\n",
      "Batch: 31300,train loss is: 0.000494622817020427\n",
      "test loss is 0.0006836207304494006\n",
      "Batch: 31400,train loss is: 0.0007642079594546064\n",
      "test loss is 0.0006785894320168128\n",
      "Batch: 31500,train loss is: 0.0004540754783036331\n",
      "test loss is 0.0006835699544550136\n",
      "Batch: 31600,train loss is: 0.0004229151062721688\n",
      "test loss is 0.0006869084566361632\n",
      "Batch: 31700,train loss is: 0.0005709954658622538\n",
      "test loss is 0.0007087747044802428\n",
      "Batch: 31800,train loss is: 0.001292203701836537\n",
      "test loss is 0.0006846599389361737\n",
      "Batch: 31900,train loss is: 0.0007370172675525515\n",
      "test loss is 0.0006928819536214137\n",
      "Batch: 32000,train loss is: 0.003955708202653887\n",
      "test loss is 0.0006869146350291253\n",
      "Batch: 32100,train loss is: 0.00040494329988654805\n",
      "test loss is 0.0006992660286468836\n",
      "Batch: 32200,train loss is: 0.0003357396128696146\n",
      "test loss is 0.0006898598370840912\n",
      "Batch: 32300,train loss is: 0.00034990503715942795\n",
      "test loss is 0.0006738112794028689\n",
      "Batch: 32400,train loss is: 0.00038895304670471723\n",
      "test loss is 0.000668651266385783\n",
      "Batch: 32500,train loss is: 0.0009077303716734104\n",
      "test loss is 0.000714545680022034\n",
      "Batch: 32600,train loss is: 0.0004713295530695377\n",
      "test loss is 0.0006952164931386556\n",
      "Batch: 32700,train loss is: 0.000495935721927162\n",
      "test loss is 0.0007138924699628197\n",
      "Batch: 32800,train loss is: 0.0004436631965009544\n",
      "test loss is 0.0007221346424155242\n",
      "Batch: 32900,train loss is: 0.000666358250205228\n",
      "test loss is 0.00067458905217512\n",
      "Batch: 33000,train loss is: 0.00046607651364116503\n",
      "test loss is 0.0006789601182702964\n",
      "Batch: 33100,train loss is: 0.0004254777343329165\n",
      "test loss is 0.000697255569654906\n",
      "Batch: 33200,train loss is: 0.0004484827730006128\n",
      "test loss is 0.0007529492585180656\n",
      "Batch: 33300,train loss is: 0.0004994041770590516\n",
      "test loss is 0.0006722067046706281\n",
      "Batch: 33400,train loss is: 0.00047498642396530573\n",
      "test loss is 0.0006876153289227361\n",
      "Batch: 33500,train loss is: 0.00035992604348870666\n",
      "test loss is 0.0006818773550204114\n",
      "Batch: 33600,train loss is: 0.0005612205638403113\n",
      "test loss is 0.000677074289915963\n",
      "Batch: 33700,train loss is: 0.0005473761128710046\n",
      "test loss is 0.0006941346469899449\n",
      "Batch: 33800,train loss is: 0.0007927448710619309\n",
      "test loss is 0.000675582717175855\n",
      "Batch: 33900,train loss is: 0.0006554888342710534\n",
      "test loss is 0.0006791522032511242\n",
      "-----------------------Epoch: 15----------------------------------\n",
      "Batch: 0,train loss is: 0.0003037126383068708\n",
      "test loss is 0.000694797506817734\n",
      "Batch: 100,train loss is: 0.0012959975019344791\n",
      "test loss is 0.0006861127196988278\n",
      "Batch: 200,train loss is: 0.00041717464375584597\n",
      "test loss is 0.000700474599583562\n",
      "Batch: 300,train loss is: 0.0002830019827130985\n",
      "test loss is 0.0006818263147576599\n",
      "Batch: 400,train loss is: 0.00045782502605241774\n",
      "test loss is 0.0006702719965716416\n",
      "Batch: 500,train loss is: 0.00048750588544469657\n",
      "test loss is 0.0006998164948366513\n",
      "Batch: 600,train loss is: 0.00024056549891128174\n",
      "test loss is 0.0007154908189840006\n",
      "Batch: 700,train loss is: 0.00035928774738410753\n",
      "test loss is 0.0006722939299350815\n",
      "Batch: 800,train loss is: 0.00042991043377802195\n",
      "test loss is 0.0006799634849046443\n",
      "Batch: 900,train loss is: 0.00029388362736764117\n",
      "test loss is 0.0006790667290004315\n",
      "Batch: 1000,train loss is: 0.00044518605277580383\n",
      "test loss is 0.000714283987773097\n",
      "Batch: 1100,train loss is: 0.0017646621171416507\n",
      "test loss is 0.0006713953028240169\n",
      "Batch: 1200,train loss is: 0.00029102131342164436\n",
      "test loss is 0.0006926245440866854\n",
      "Batch: 1300,train loss is: 0.0005274591095882662\n",
      "test loss is 0.0006767939097570121\n",
      "Batch: 1400,train loss is: 0.00042227882934917224\n",
      "test loss is 0.0007001203172531693\n",
      "Batch: 1500,train loss is: 0.0004300195225993435\n",
      "test loss is 0.0007050981026640295\n",
      "Batch: 1600,train loss is: 0.0006919629351052714\n",
      "test loss is 0.0007515611323841174\n",
      "Batch: 1700,train loss is: 0.001198014008247775\n",
      "test loss is 0.000669503207468783\n",
      "Batch: 1800,train loss is: 0.0003670558081802886\n",
      "test loss is 0.0006770508295052791\n",
      "Batch: 1900,train loss is: 0.0004035712909088478\n",
      "test loss is 0.0007143502179865282\n",
      "Batch: 2000,train loss is: 0.0006761063031871748\n",
      "test loss is 0.0008233960879176208\n",
      "Batch: 2100,train loss is: 0.0006602195345941491\n",
      "test loss is 0.0008304611743487144\n",
      "Batch: 2200,train loss is: 0.0007444288630382089\n",
      "test loss is 0.0006829528789816945\n",
      "Batch: 2300,train loss is: 0.0004535787225558115\n",
      "test loss is 0.0007062616297607202\n",
      "Batch: 2400,train loss is: 0.00045580656651348256\n",
      "test loss is 0.0006822474210371697\n",
      "Batch: 2500,train loss is: 0.000584817124013446\n",
      "test loss is 0.0006756328992956178\n",
      "Batch: 2600,train loss is: 0.0003990910017005808\n",
      "test loss is 0.000683547083967942\n",
      "Batch: 2700,train loss is: 0.000606090947301275\n",
      "test loss is 0.0006639562318120874\n",
      "Batch: 2800,train loss is: 0.0004507365386894162\n",
      "test loss is 0.0006952809302590632\n",
      "Batch: 2900,train loss is: 0.000431941191733251\n",
      "test loss is 0.0006716792412060388\n",
      "Batch: 3000,train loss is: 0.0005054870969967228\n",
      "test loss is 0.0006711051823225628\n",
      "Batch: 3100,train loss is: 0.0006599655072320561\n",
      "test loss is 0.000718181705287116\n",
      "Batch: 3200,train loss is: 0.0010025318191288532\n",
      "test loss is 0.0006927431046790725\n",
      "Batch: 3300,train loss is: 0.0005490606984017228\n",
      "test loss is 0.0006863361350162559\n",
      "Batch: 3400,train loss is: 0.00040177518496231006\n",
      "test loss is 0.0006779285094439378\n",
      "Batch: 3500,train loss is: 0.00022547461416755717\n",
      "test loss is 0.000674739229718436\n",
      "Batch: 3600,train loss is: 0.0005485050068347975\n",
      "test loss is 0.0007163030445422177\n",
      "Batch: 3700,train loss is: 0.0003450093218875036\n",
      "test loss is 0.0006778219406015455\n",
      "Batch: 3800,train loss is: 0.00044334322105515943\n",
      "test loss is 0.0007041915861489195\n",
      "Batch: 3900,train loss is: 0.000594609799719108\n",
      "test loss is 0.0006828141307687085\n",
      "Batch: 4000,train loss is: 0.0004313128841075802\n",
      "test loss is 0.0006809629538572614\n",
      "Batch: 4100,train loss is: 0.000726992987213482\n",
      "test loss is 0.0007014965651462918\n",
      "Batch: 4200,train loss is: 0.00027448773807984335\n",
      "test loss is 0.0007060057134743995\n",
      "Batch: 4300,train loss is: 0.00040906197745313596\n",
      "test loss is 0.0006689173660430238\n",
      "Batch: 4400,train loss is: 0.0003096583541833555\n",
      "test loss is 0.0006954730396371959\n",
      "Batch: 4500,train loss is: 0.0007420497275263487\n",
      "test loss is 0.0006990934483780115\n",
      "Batch: 4600,train loss is: 0.0005731695631065825\n",
      "test loss is 0.0006840765638032897\n",
      "Batch: 4700,train loss is: 0.0005552767278500512\n",
      "test loss is 0.0007082876776174977\n",
      "Batch: 4800,train loss is: 0.0012837229307091933\n",
      "test loss is 0.0006721470675700839\n",
      "Batch: 4900,train loss is: 0.0005047146851183325\n",
      "test loss is 0.0006833101201945658\n",
      "Batch: 5000,train loss is: 0.0005699430863265635\n",
      "test loss is 0.0006945391705172838\n",
      "Batch: 5100,train loss is: 0.00047239032923773044\n",
      "test loss is 0.0007352940972832578\n",
      "Batch: 5200,train loss is: 0.0003067806638142514\n",
      "test loss is 0.0006900185537768037\n",
      "Batch: 5300,train loss is: 0.0004946619220463823\n",
      "test loss is 0.0006992902022963542\n",
      "Batch: 5400,train loss is: 0.0007288302143028957\n",
      "test loss is 0.0006709109155267631\n",
      "Batch: 5500,train loss is: 0.0004306876805353864\n",
      "test loss is 0.0006991946026161875\n",
      "Batch: 5600,train loss is: 0.0005701060514184148\n",
      "test loss is 0.000684977674306773\n",
      "Batch: 5700,train loss is: 0.000544307024467839\n",
      "test loss is 0.0006863739836878442\n",
      "Batch: 5800,train loss is: 0.0004830208050713538\n",
      "test loss is 0.0006783130482953904\n",
      "Batch: 5900,train loss is: 0.00337757303988018\n",
      "test loss is 0.0006839539588995179\n",
      "Batch: 6000,train loss is: 0.0016483812075526448\n",
      "test loss is 0.0006948089279381663\n",
      "Batch: 6100,train loss is: 0.0004138935341963001\n",
      "test loss is 0.0006696235236870828\n",
      "Batch: 6200,train loss is: 0.0005625320642052452\n",
      "test loss is 0.0006844404985603525\n",
      "Batch: 6300,train loss is: 0.0010949341622925794\n",
      "test loss is 0.0006983524992266692\n",
      "Batch: 6400,train loss is: 0.0005287911887879587\n",
      "test loss is 0.0007256364057924269\n",
      "Batch: 6500,train loss is: 0.000306853010394729\n",
      "test loss is 0.0006878941005111199\n",
      "Batch: 6600,train loss is: 0.002053490168093217\n",
      "test loss is 0.0006754588061669954\n",
      "Batch: 6700,train loss is: 0.00032192623230764013\n",
      "test loss is 0.0006896308086194766\n",
      "Batch: 6800,train loss is: 0.0007383312848374106\n",
      "test loss is 0.0006769143248275655\n",
      "Batch: 6900,train loss is: 0.000686641680984262\n",
      "test loss is 0.0006877957079423824\n",
      "Batch: 7000,train loss is: 0.0005755394024737917\n",
      "test loss is 0.0006996497570362366\n",
      "Batch: 7100,train loss is: 0.0011150818204758142\n",
      "test loss is 0.0006879729163455228\n",
      "Batch: 7200,train loss is: 0.00044303358241229576\n",
      "test loss is 0.0006947248033989824\n",
      "Batch: 7300,train loss is: 0.0007576394203702177\n",
      "test loss is 0.0007129992988201384\n",
      "Batch: 7400,train loss is: 0.0003568018726580258\n",
      "test loss is 0.0006776260408340289\n",
      "Batch: 7500,train loss is: 0.0006583461442851243\n",
      "test loss is 0.0007016200994317852\n",
      "Batch: 7600,train loss is: 0.0003896206692827245\n",
      "test loss is 0.0006637054082943193\n",
      "Batch: 7700,train loss is: 0.0008612029821321945\n",
      "test loss is 0.0006860244190122222\n",
      "Batch: 7800,train loss is: 0.00028521444361487394\n",
      "test loss is 0.0006713745477911757\n",
      "Batch: 7900,train loss is: 0.000563126950223608\n",
      "test loss is 0.0006950274280883005\n",
      "Batch: 8000,train loss is: 0.0004105020440719333\n",
      "test loss is 0.0007136340046558301\n",
      "Batch: 8100,train loss is: 0.0005783137143369981\n",
      "test loss is 0.0007020968341715589\n",
      "Batch: 8200,train loss is: 0.0010907189779578047\n",
      "test loss is 0.0006964486248752313\n",
      "Batch: 8300,train loss is: 0.00035582750444039995\n",
      "test loss is 0.0006965571850424098\n",
      "Batch: 8400,train loss is: 0.0005616231179186742\n",
      "test loss is 0.0006799873232291511\n",
      "Batch: 8500,train loss is: 0.0006476824249634332\n",
      "test loss is 0.0006950067173139764\n",
      "Batch: 8600,train loss is: 0.0006641712165343414\n",
      "test loss is 0.0007019247516048736\n",
      "Batch: 8700,train loss is: 0.0004665340695651001\n",
      "test loss is 0.0007345625795246954\n",
      "Batch: 8800,train loss is: 0.0003499498272510667\n",
      "test loss is 0.0006736820118449102\n",
      "Batch: 8900,train loss is: 0.0010586751356482565\n",
      "test loss is 0.0006731567425723698\n",
      "Batch: 9000,train loss is: 0.00025429558836809576\n",
      "test loss is 0.0006938217426952906\n",
      "Batch: 9100,train loss is: 0.0003207238122566724\n",
      "test loss is 0.0006716591707040145\n",
      "Batch: 9200,train loss is: 0.0007853361227401579\n",
      "test loss is 0.000744541960810563\n",
      "Batch: 9300,train loss is: 0.0005465298157666262\n",
      "test loss is 0.0007197581645187717\n",
      "Batch: 9400,train loss is: 0.0004466602434058311\n",
      "test loss is 0.0007044632908699599\n",
      "Batch: 9500,train loss is: 0.00037067044633471986\n",
      "test loss is 0.0007077749655662079\n",
      "Batch: 9600,train loss is: 0.0004164949066762657\n",
      "test loss is 0.000669299418671478\n",
      "Batch: 9700,train loss is: 0.00034680951067953247\n",
      "test loss is 0.000687245675645272\n",
      "Batch: 9800,train loss is: 0.00027917200294241313\n",
      "test loss is 0.000678676613477948\n",
      "Batch: 9900,train loss is: 0.0004887249456895378\n",
      "test loss is 0.0007068010415227552\n",
      "Batch: 10000,train loss is: 0.0004325517883386995\n",
      "test loss is 0.000668095984466506\n",
      "Batch: 10100,train loss is: 0.0010505716011979878\n",
      "test loss is 0.0006906129203394783\n",
      "Batch: 10200,train loss is: 0.0016951981299045118\n",
      "test loss is 0.0007697398028421833\n",
      "Batch: 10300,train loss is: 0.00044609759754808654\n",
      "test loss is 0.0006804846127095772\n",
      "Batch: 10400,train loss is: 0.00028729427718875833\n",
      "test loss is 0.0006772075663654581\n",
      "Batch: 10500,train loss is: 0.0007451567989909962\n",
      "test loss is 0.0006829667584472692\n",
      "Batch: 10600,train loss is: 0.0003219400185100896\n",
      "test loss is 0.000738775147445124\n",
      "Batch: 10700,train loss is: 0.0006084237594591566\n",
      "test loss is 0.0006760897997510773\n",
      "Batch: 10800,train loss is: 0.0005881417994321648\n",
      "test loss is 0.0006868293566677069\n",
      "Batch: 10900,train loss is: 0.0007098435340082723\n",
      "test loss is 0.000682017805793419\n",
      "Batch: 11000,train loss is: 0.0007697781299249435\n",
      "test loss is 0.0007045115428721236\n",
      "Batch: 11100,train loss is: 0.0009875881677095075\n",
      "test loss is 0.0006711016583180776\n",
      "Batch: 11200,train loss is: 0.0006814691878545088\n",
      "test loss is 0.0007227186534680289\n",
      "Batch: 11300,train loss is: 0.00031202832897275675\n",
      "test loss is 0.0006898981748937568\n",
      "Batch: 11400,train loss is: 0.0006928368250856072\n",
      "test loss is 0.0006822377968284299\n",
      "Batch: 11500,train loss is: 0.0005400312625036555\n",
      "test loss is 0.0006879559257981444\n",
      "Batch: 11600,train loss is: 0.0018776970859139027\n",
      "test loss is 0.00068235420174567\n",
      "Batch: 11700,train loss is: 0.0002945620842064104\n",
      "test loss is 0.000666543498580065\n",
      "Batch: 11800,train loss is: 0.0006040997483410724\n",
      "test loss is 0.0006753542175396194\n",
      "Batch: 11900,train loss is: 0.0005303315265465438\n",
      "test loss is 0.0006889479405060818\n",
      "Batch: 12000,train loss is: 0.0003242049877865631\n",
      "test loss is 0.0006781061842777625\n",
      "Batch: 12100,train loss is: 0.0015630806905906486\n",
      "test loss is 0.0006883806994123857\n",
      "Batch: 12200,train loss is: 0.000657362138719202\n",
      "test loss is 0.0007042420107492368\n",
      "Batch: 12300,train loss is: 0.0004637165140443799\n",
      "test loss is 0.0006932351702352637\n",
      "Batch: 12400,train loss is: 0.00041453844405696307\n",
      "test loss is 0.0006806325873273993\n",
      "Batch: 12500,train loss is: 0.0004622822527910107\n",
      "test loss is 0.000705175872474309\n",
      "Batch: 12600,train loss is: 0.0004938220226131638\n",
      "test loss is 0.0006953091408658688\n",
      "Batch: 12700,train loss is: 0.000390297653949254\n",
      "test loss is 0.0006735742373361024\n",
      "Batch: 12800,train loss is: 0.0003714741147394349\n",
      "test loss is 0.000672944294257831\n",
      "Batch: 12900,train loss is: 0.000637007840277931\n",
      "test loss is 0.0006896703308675957\n",
      "Batch: 13000,train loss is: 0.00020939655228780193\n",
      "test loss is 0.0006721885480456617\n",
      "Batch: 13100,train loss is: 0.002149188623718833\n",
      "test loss is 0.0006700360368560965\n",
      "Batch: 13200,train loss is: 0.0008155384095468492\n",
      "test loss is 0.0007981531430778699\n",
      "Batch: 13300,train loss is: 0.000653606925289066\n",
      "test loss is 0.000689897939708752\n",
      "Batch: 13400,train loss is: 0.0003668597922517256\n",
      "test loss is 0.0007071080457594986\n",
      "Batch: 13500,train loss is: 0.0005931980496023814\n",
      "test loss is 0.0007079487611522795\n",
      "Batch: 13600,train loss is: 0.0005674123426013328\n",
      "test loss is 0.0006935724673479082\n",
      "Batch: 13700,train loss is: 0.0006691618964443232\n",
      "test loss is 0.0006696016039669407\n",
      "Batch: 13800,train loss is: 0.00040392273848254776\n",
      "test loss is 0.0006708170570320641\n",
      "Batch: 13900,train loss is: 0.0003411222983440865\n",
      "test loss is 0.0007346203958387221\n",
      "Batch: 14000,train loss is: 0.0003951696848809045\n",
      "test loss is 0.000699507292846024\n",
      "Batch: 14100,train loss is: 0.0007433327436491058\n",
      "test loss is 0.0007261414493257813\n",
      "Batch: 14200,train loss is: 0.0005839786023894385\n",
      "test loss is 0.0006654398794042327\n",
      "Batch: 14300,train loss is: 0.0002456912303554729\n",
      "test loss is 0.0006737019637700236\n",
      "Batch: 14400,train loss is: 0.0012885620253983753\n",
      "test loss is 0.0006739889247841252\n",
      "Batch: 14500,train loss is: 0.0005657854575986226\n",
      "test loss is 0.0006747209629617178\n",
      "Batch: 14600,train loss is: 0.0009902091305580014\n",
      "test loss is 0.0006821588837478415\n",
      "Batch: 14700,train loss is: 0.00045882771001601715\n",
      "test loss is 0.0007306122960984484\n",
      "Batch: 14800,train loss is: 0.0007889260087872217\n",
      "test loss is 0.0006912492122181205\n",
      "Batch: 14900,train loss is: 0.00044207459969938155\n",
      "test loss is 0.0007257482247448005\n",
      "Batch: 15000,train loss is: 0.0006979255536206183\n",
      "test loss is 0.0006680962226507802\n",
      "Batch: 15100,train loss is: 0.0006724657975577543\n",
      "test loss is 0.0006988586351655009\n",
      "Batch: 15200,train loss is: 0.0008481486159917374\n",
      "test loss is 0.0006835499356534043\n",
      "Batch: 15300,train loss is: 0.00037057134719429663\n",
      "test loss is 0.0006745682157964729\n",
      "Batch: 15400,train loss is: 0.00045974385163365485\n",
      "test loss is 0.0006884938624128948\n",
      "Batch: 15500,train loss is: 0.0006905072122421658\n",
      "test loss is 0.000666654542081558\n",
      "Batch: 15600,train loss is: 0.0008604087009831857\n",
      "test loss is 0.0006999211928754946\n",
      "Batch: 15700,train loss is: 0.00040314843042730896\n",
      "test loss is 0.0006753505494827923\n",
      "Batch: 15800,train loss is: 0.0006625272518019674\n",
      "test loss is 0.0006644102187281529\n",
      "Batch: 15900,train loss is: 0.000844846923503928\n",
      "test loss is 0.0006845748889522506\n",
      "Batch: 16000,train loss is: 0.00037709564704659147\n",
      "test loss is 0.0006874797561203609\n",
      "Batch: 16100,train loss is: 0.0005289150508646221\n",
      "test loss is 0.0006781452778666789\n",
      "Batch: 16200,train loss is: 0.00038827039320417465\n",
      "test loss is 0.0006697870245125529\n",
      "Batch: 16300,train loss is: 0.000942409849227543\n",
      "test loss is 0.0007081278613201001\n",
      "Batch: 16400,train loss is: 0.00034101579501264715\n",
      "test loss is 0.0006705785850687718\n",
      "Batch: 16500,train loss is: 0.0006506598748183481\n",
      "test loss is 0.0006934734392662883\n",
      "Batch: 16600,train loss is: 0.0003103790602037651\n",
      "test loss is 0.0006691637896696218\n",
      "Batch: 16700,train loss is: 0.0008606753339305986\n",
      "test loss is 0.0006876709433233706\n",
      "Batch: 16800,train loss is: 0.0005304890249872403\n",
      "test loss is 0.0007053720789991579\n",
      "Batch: 16900,train loss is: 0.0007027227684099115\n",
      "test loss is 0.0006843129056301752\n",
      "Batch: 17000,train loss is: 0.0006102290687262275\n",
      "test loss is 0.0006645958054421663\n",
      "Batch: 17100,train loss is: 0.0005234374315975227\n",
      "test loss is 0.0006794384004846276\n",
      "Batch: 17200,train loss is: 0.0004889746586613155\n",
      "test loss is 0.000682159751585311\n",
      "Batch: 17300,train loss is: 0.0008099861707842936\n",
      "test loss is 0.0007058345930369685\n",
      "Batch: 17400,train loss is: 0.0003946180471427663\n",
      "test loss is 0.0006831210719676707\n",
      "Batch: 17500,train loss is: 0.0005554722472468733\n",
      "test loss is 0.0006790788933150959\n",
      "Batch: 17600,train loss is: 0.0004245985737827883\n",
      "test loss is 0.0006906136935487208\n",
      "Batch: 17700,train loss is: 0.0003458599938219629\n",
      "test loss is 0.0007120647084338084\n",
      "Batch: 17800,train loss is: 0.0014318670744804082\n",
      "test loss is 0.0006850207066281491\n",
      "Batch: 17900,train loss is: 0.0004929453171143352\n",
      "test loss is 0.0007123796573661267\n",
      "Batch: 18000,train loss is: 0.00030063898557065714\n",
      "test loss is 0.0006638814780991626\n",
      "Batch: 18100,train loss is: 0.00039311089868155583\n",
      "test loss is 0.0006717260771378424\n",
      "Batch: 18200,train loss is: 0.000573331565240734\n",
      "test loss is 0.0006859194907844238\n",
      "Batch: 18300,train loss is: 0.001182392843659863\n",
      "test loss is 0.0006820606645032533\n",
      "Batch: 18400,train loss is: 0.0003958706711167001\n",
      "test loss is 0.0006734543148408372\n",
      "Batch: 18500,train loss is: 0.0009095272072089429\n",
      "test loss is 0.0006843105678649491\n",
      "Batch: 18600,train loss is: 0.0004612874098824901\n",
      "test loss is 0.0006766451568731403\n",
      "Batch: 18700,train loss is: 0.0005727679216070737\n",
      "test loss is 0.0006964830302926558\n",
      "Batch: 18800,train loss is: 0.0005485933109463633\n",
      "test loss is 0.0006731530167894483\n",
      "Batch: 18900,train loss is: 0.0005793172321920296\n",
      "test loss is 0.0007017997949664767\n",
      "Batch: 19000,train loss is: 0.0006128059434845082\n",
      "test loss is 0.0006765778524075005\n",
      "Batch: 19100,train loss is: 0.0003584780047058209\n",
      "test loss is 0.0007020829761467347\n",
      "Batch: 19200,train loss is: 0.0005074283640027207\n",
      "test loss is 0.0006835192624891056\n",
      "Batch: 19300,train loss is: 0.0006665910690482902\n",
      "test loss is 0.0006644012058795196\n",
      "Batch: 19400,train loss is: 0.00027326630513821616\n",
      "test loss is 0.0006693847662258163\n",
      "Batch: 19500,train loss is: 0.0005127146634246075\n",
      "test loss is 0.0007090549072824561\n",
      "Batch: 19600,train loss is: 0.0004000980869532071\n",
      "test loss is 0.0006872630235701232\n",
      "Batch: 19700,train loss is: 0.0007129647388674989\n",
      "test loss is 0.0007593981536160328\n",
      "Batch: 19800,train loss is: 0.0005853037190624094\n",
      "test loss is 0.0006865190856530846\n",
      "Batch: 19900,train loss is: 0.00032030716878113944\n",
      "test loss is 0.0007004194856865847\n",
      "Batch: 20000,train loss is: 0.0006965518672569836\n",
      "test loss is 0.0006781228425752744\n",
      "Batch: 20100,train loss is: 0.0005381158589473103\n",
      "test loss is 0.00068837179687464\n",
      "Batch: 20200,train loss is: 0.0007786810498493672\n",
      "test loss is 0.0006963263902743782\n",
      "Batch: 20300,train loss is: 0.00029066681188740515\n",
      "test loss is 0.000697820863898864\n",
      "Batch: 20400,train loss is: 0.0004816402653555753\n",
      "test loss is 0.0007141708248623831\n",
      "Batch: 20500,train loss is: 0.0006061397480267236\n",
      "test loss is 0.0006745512700237748\n",
      "Batch: 20600,train loss is: 0.0004731484072742384\n",
      "test loss is 0.000708008325607502\n",
      "Batch: 20700,train loss is: 0.000726905079998691\n",
      "test loss is 0.0006825617040881554\n",
      "Batch: 20800,train loss is: 0.0003354939505547166\n",
      "test loss is 0.0007067149535267012\n",
      "Batch: 20900,train loss is: 0.0005801146740868009\n",
      "test loss is 0.0007289780275607737\n",
      "Batch: 21000,train loss is: 0.00023254631697245025\n",
      "test loss is 0.0006678470611881762\n",
      "Batch: 21100,train loss is: 0.0006809744917187773\n",
      "test loss is 0.0007033006500682951\n",
      "Batch: 21200,train loss is: 0.0003653093593935702\n",
      "test loss is 0.0007097845686215712\n",
      "Batch: 21300,train loss is: 0.0003268721367361266\n",
      "test loss is 0.0006768229388591107\n",
      "Batch: 21400,train loss is: 0.0007881832310708005\n",
      "test loss is 0.0006763789666265604\n",
      "Batch: 21500,train loss is: 0.0002338254127359728\n",
      "test loss is 0.0006918044426797659\n",
      "Batch: 21600,train loss is: 0.0007261878972134515\n",
      "test loss is 0.0006938166950476217\n",
      "Batch: 21700,train loss is: 0.0003062152271260084\n",
      "test loss is 0.0006679597681998186\n",
      "Batch: 21800,train loss is: 0.0007938971698336306\n",
      "test loss is 0.0007260605001417748\n",
      "Batch: 21900,train loss is: 0.0004589868721472273\n",
      "test loss is 0.0007636141402970093\n",
      "Batch: 22000,train loss is: 0.0006442966148470947\n",
      "test loss is 0.0006732495675996156\n",
      "Batch: 22100,train loss is: 0.00036736492371126586\n",
      "test loss is 0.0006758827969534967\n",
      "Batch: 22200,train loss is: 0.0004925202353680031\n",
      "test loss is 0.0006805521743483939\n",
      "Batch: 22300,train loss is: 0.0005549715932613812\n",
      "test loss is 0.0006876425336395238\n",
      "Batch: 22400,train loss is: 0.0004502008788385208\n",
      "test loss is 0.0006896066179274766\n",
      "Batch: 22500,train loss is: 0.00048794187358000107\n",
      "test loss is 0.0006678484009813647\n",
      "Batch: 22600,train loss is: 0.0009813559039012006\n",
      "test loss is 0.0007108666164247259\n",
      "Batch: 22700,train loss is: 0.0003598904906125812\n",
      "test loss is 0.0006685680831091345\n",
      "Batch: 22800,train loss is: 0.0005067622437346552\n",
      "test loss is 0.0007044325987677154\n",
      "Batch: 22900,train loss is: 0.00030918948108715267\n",
      "test loss is 0.0007520231138401142\n",
      "Batch: 23000,train loss is: 0.00045897223217374224\n",
      "test loss is 0.0006810052775519578\n",
      "Batch: 23100,train loss is: 0.00045499705664560954\n",
      "test loss is 0.0006615122321688604\n",
      "Batch: 23200,train loss is: 0.00028972668755431483\n",
      "test loss is 0.0006613843366377569\n",
      "Batch: 23300,train loss is: 0.0004632367553387821\n",
      "test loss is 0.0006713953353695213\n",
      "Batch: 23400,train loss is: 0.0006098314404179181\n",
      "test loss is 0.0007008681111782826\n",
      "Batch: 23500,train loss is: 0.0016331402039045115\n",
      "test loss is 0.0006666124503890762\n",
      "Batch: 23600,train loss is: 0.00030204820613796213\n",
      "test loss is 0.0006727599496309448\n",
      "Batch: 23700,train loss is: 0.0009372193547722727\n",
      "test loss is 0.000673579606417418\n",
      "Batch: 23800,train loss is: 0.0008015583237210038\n",
      "test loss is 0.0006749858898909973\n",
      "Batch: 23900,train loss is: 0.0008914954655729708\n",
      "test loss is 0.0006871375703598672\n",
      "Batch: 24000,train loss is: 0.0007295362725663258\n",
      "test loss is 0.0007260883153199088\n",
      "Batch: 24100,train loss is: 0.0003112175477340536\n",
      "test loss is 0.0006589637720584935\n",
      "Batch: 24200,train loss is: 0.0014178957087959893\n",
      "test loss is 0.0006880625301188487\n",
      "Batch: 24300,train loss is: 0.0005375771831359875\n",
      "test loss is 0.0008092529929570807\n",
      "Batch: 24400,train loss is: 0.001303617294004961\n",
      "test loss is 0.0006729499228145296\n",
      "Batch: 24500,train loss is: 0.0006679122136359492\n",
      "test loss is 0.0006621749647993361\n",
      "Batch: 24600,train loss is: 0.000411745017329782\n",
      "test loss is 0.0006995131184176875\n",
      "Batch: 24700,train loss is: 0.00028684301174664603\n",
      "test loss is 0.0006791244760793136\n",
      "Batch: 24800,train loss is: 0.0007505641752413479\n",
      "test loss is 0.0006855024236903856\n",
      "Batch: 24900,train loss is: 0.00019726176520374337\n",
      "test loss is 0.0006747445595980489\n",
      "Batch: 25000,train loss is: 0.0005778472150037652\n",
      "test loss is 0.0006715944190489626\n",
      "Batch: 25100,train loss is: 0.0004864523674025672\n",
      "test loss is 0.0006817979736335197\n",
      "Batch: 25200,train loss is: 0.0007429967572530396\n",
      "test loss is 0.0006843351705020308\n",
      "Batch: 25300,train loss is: 0.000759246840438222\n",
      "test loss is 0.0007235817209570529\n",
      "Batch: 25400,train loss is: 0.00040962190805393417\n",
      "test loss is 0.0006722987200322121\n",
      "Batch: 25500,train loss is: 0.0005851651987697424\n",
      "test loss is 0.0006822600557090054\n",
      "Batch: 25600,train loss is: 0.000635464399008702\n",
      "test loss is 0.0006771262058124798\n",
      "Batch: 25700,train loss is: 0.0005811005411918247\n",
      "test loss is 0.0006968251101107174\n",
      "Batch: 25800,train loss is: 0.0003569898700158213\n",
      "test loss is 0.0006652787341503114\n",
      "Batch: 25900,train loss is: 0.00029314847141765005\n",
      "test loss is 0.0007216964334396848\n",
      "Batch: 26000,train loss is: 0.001545237339076887\n",
      "test loss is 0.00072753711078819\n",
      "Batch: 26100,train loss is: 0.0007977882787301018\n",
      "test loss is 0.0006630431718878853\n",
      "Batch: 26200,train loss is: 0.0006235890866844491\n",
      "test loss is 0.0006628373065677029\n",
      "Batch: 26300,train loss is: 0.0010614509792593837\n",
      "test loss is 0.0006845482801754412\n",
      "Batch: 26400,train loss is: 0.0005654266906965524\n",
      "test loss is 0.0006649883641032669\n",
      "Batch: 26500,train loss is: 0.0004895833400054803\n",
      "test loss is 0.000696176480824785\n",
      "Batch: 26600,train loss is: 0.0013529457316895932\n",
      "test loss is 0.0007570631004844276\n",
      "Batch: 26700,train loss is: 0.0004818771303515732\n",
      "test loss is 0.0007650639281317637\n",
      "Batch: 26800,train loss is: 0.0011519297324666764\n",
      "test loss is 0.0006873635937244581\n",
      "Batch: 26900,train loss is: 0.0004594855220545781\n",
      "test loss is 0.0006787686123189794\n",
      "Batch: 27000,train loss is: 0.0006502345359646889\n",
      "test loss is 0.0007891656934453439\n",
      "Batch: 27100,train loss is: 0.0006544847442397709\n",
      "test loss is 0.0007312831527885741\n",
      "Batch: 27200,train loss is: 0.0006722868851048544\n",
      "test loss is 0.0007142818979925244\n",
      "Batch: 27300,train loss is: 0.0004836897792786955\n",
      "test loss is 0.0006899513877598814\n",
      "Batch: 27400,train loss is: 0.0007436813548807431\n",
      "test loss is 0.0006827476034533082\n",
      "Batch: 27500,train loss is: 0.000460643856178391\n",
      "test loss is 0.0007451593269063745\n",
      "Batch: 27600,train loss is: 0.0005330417667549264\n",
      "test loss is 0.0007064648052731328\n",
      "Batch: 27700,train loss is: 0.0015532566661367341\n",
      "test loss is 0.0006864679220755393\n",
      "Batch: 27800,train loss is: 0.0007629242472847092\n",
      "test loss is 0.0007058283559730958\n",
      "Batch: 27900,train loss is: 0.0013517001165666163\n",
      "test loss is 0.0006777315888139378\n",
      "Batch: 28000,train loss is: 0.00039535228678524596\n",
      "test loss is 0.0006730222514698108\n",
      "Batch: 28100,train loss is: 0.0008425009355392283\n",
      "test loss is 0.0006896536029584026\n",
      "Batch: 28200,train loss is: 0.0005414997313488979\n",
      "test loss is 0.0006568759896661962\n",
      "Batch: 28300,train loss is: 0.001710602441604301\n",
      "test loss is 0.0006899706192453554\n",
      "Batch: 28400,train loss is: 0.0005202451708961125\n",
      "test loss is 0.0006667531885838239\n",
      "Batch: 28500,train loss is: 0.001463801030294045\n",
      "test loss is 0.0007131651122565613\n",
      "Batch: 28600,train loss is: 0.00036068684083535515\n",
      "test loss is 0.0006820230946706638\n",
      "Batch: 28700,train loss is: 0.00056255051799898\n",
      "test loss is 0.0006735650902637721\n",
      "Batch: 28800,train loss is: 0.0010752527930557915\n",
      "test loss is 0.0006834274711582973\n",
      "Batch: 28900,train loss is: 0.000460387254492914\n",
      "test loss is 0.0006651976520248849\n",
      "Batch: 29000,train loss is: 0.0004993279412199653\n",
      "test loss is 0.0006738962212745513\n",
      "Batch: 29100,train loss is: 0.0005190293922067581\n",
      "test loss is 0.0006760664216940112\n",
      "Batch: 29200,train loss is: 0.00041875744649637163\n",
      "test loss is 0.0006852370600250827\n",
      "Batch: 29300,train loss is: 0.0009710812749298906\n",
      "test loss is 0.0006685249368685902\n",
      "Batch: 29400,train loss is: 0.0007101467330253235\n",
      "test loss is 0.0007556602692456311\n",
      "Batch: 29500,train loss is: 0.00046962213484467806\n",
      "test loss is 0.0007003776599317527\n",
      "Batch: 29600,train loss is: 0.0004990139907806043\n",
      "test loss is 0.000659614054554262\n",
      "Batch: 29700,train loss is: 0.0003473616757528988\n",
      "test loss is 0.0006806206238809891\n",
      "Batch: 29800,train loss is: 0.0007979353510695962\n",
      "test loss is 0.0006620562462815845\n",
      "Batch: 29900,train loss is: 0.0029463257040218795\n",
      "test loss is 0.0006691778616851355\n",
      "Batch: 30000,train loss is: 0.0003563307988161294\n",
      "test loss is 0.000718569531022595\n",
      "Batch: 30100,train loss is: 0.0005594425985696714\n",
      "test loss is 0.0007099541475540551\n",
      "Batch: 30200,train loss is: 0.00037799305888127315\n",
      "test loss is 0.0006804076835060475\n",
      "Batch: 30300,train loss is: 0.00027681571765565037\n",
      "test loss is 0.000665274977268673\n",
      "Batch: 30400,train loss is: 0.00032773941054894864\n",
      "test loss is 0.0006739111509219633\n",
      "Batch: 30500,train loss is: 0.000322474974352916\n",
      "test loss is 0.0007096268537764341\n",
      "Batch: 30600,train loss is: 0.0002507256139726418\n",
      "test loss is 0.0006732739972253613\n",
      "Batch: 30700,train loss is: 0.0003835986683260814\n",
      "test loss is 0.0007602290665507431\n",
      "Batch: 30800,train loss is: 0.00026811703617801003\n",
      "test loss is 0.0007336389804713108\n",
      "Batch: 30900,train loss is: 0.00034487280933372566\n",
      "test loss is 0.0006760150582206099\n",
      "Batch: 31000,train loss is: 0.0004840910072083077\n",
      "test loss is 0.0006874862399360691\n",
      "Batch: 31100,train loss is: 0.00034089920851841285\n",
      "test loss is 0.0007606751331386786\n",
      "Batch: 31200,train loss is: 0.0004996325777675954\n",
      "test loss is 0.0006779624727085392\n",
      "Batch: 31300,train loss is: 0.00047295325644866213\n",
      "test loss is 0.0006764955865152902\n",
      "Batch: 31400,train loss is: 0.0007558471511436203\n",
      "test loss is 0.0006709847659295548\n",
      "Batch: 31500,train loss is: 0.0004508128830722763\n",
      "test loss is 0.0006763683198616246\n",
      "Batch: 31600,train loss is: 0.00041814272735863793\n",
      "test loss is 0.0006796630467453981\n",
      "Batch: 31700,train loss is: 0.0005658690507187273\n",
      "test loss is 0.0007011886051189584\n",
      "Batch: 31800,train loss is: 0.0012988846204175948\n",
      "test loss is 0.0006770111243493066\n",
      "Batch: 31900,train loss is: 0.0007268558781747457\n",
      "test loss is 0.0006850722146292775\n",
      "Batch: 32000,train loss is: 0.0038778388865199874\n",
      "test loss is 0.0006806673050025448\n",
      "Batch: 32100,train loss is: 0.00038964135207077556\n",
      "test loss is 0.0006940655872321543\n",
      "Batch: 32200,train loss is: 0.00033261668239830663\n",
      "test loss is 0.0006818032696098241\n",
      "Batch: 32300,train loss is: 0.0003452353176843761\n",
      "test loss is 0.0006658379786533921\n",
      "Batch: 32400,train loss is: 0.0003794769740019568\n",
      "test loss is 0.0006614652323573442\n",
      "Batch: 32500,train loss is: 0.0008874118814095745\n",
      "test loss is 0.0007076739356410132\n",
      "Batch: 32600,train loss is: 0.000472509889948622\n",
      "test loss is 0.0006893219267847566\n",
      "Batch: 32700,train loss is: 0.0005040245733502885\n",
      "test loss is 0.0007115819128506251\n",
      "Batch: 32800,train loss is: 0.0004505888414130521\n",
      "test loss is 0.0007165215136243239\n",
      "Batch: 32900,train loss is: 0.0006702837170971714\n",
      "test loss is 0.0006690326718046784\n",
      "Batch: 33000,train loss is: 0.00045130551286206986\n",
      "test loss is 0.0006735873738945967\n",
      "Batch: 33100,train loss is: 0.0004288074840985634\n",
      "test loss is 0.0006888134918318288\n",
      "Batch: 33200,train loss is: 0.00044182453524713644\n",
      "test loss is 0.0007445017733394544\n",
      "Batch: 33300,train loss is: 0.0005076365304076871\n",
      "test loss is 0.0006668998707130616\n",
      "Batch: 33400,train loss is: 0.00047530829274005146\n",
      "test loss is 0.0006811775846405133\n",
      "Batch: 33500,train loss is: 0.00035330517286181833\n",
      "test loss is 0.0006745263406391462\n",
      "Batch: 33600,train loss is: 0.0005541045397838021\n",
      "test loss is 0.0006699438845063706\n",
      "Batch: 33700,train loss is: 0.0005379195998840476\n",
      "test loss is 0.0006865733293122574\n",
      "Batch: 33800,train loss is: 0.0007767556839203907\n",
      "test loss is 0.0006686798096656683\n",
      "Batch: 33900,train loss is: 0.0006435897982655196\n",
      "test loss is 0.0006713368246837912\n",
      "-----------------------Epoch: 16----------------------------------\n",
      "Batch: 0,train loss is: 0.00029537558713171793\n",
      "test loss is 0.0006889534080374079\n",
      "Batch: 100,train loss is: 0.001257353491655124\n",
      "test loss is 0.0006813004201508739\n",
      "Batch: 200,train loss is: 0.0004018381629053317\n",
      "test loss is 0.0006961314457429466\n",
      "Batch: 300,train loss is: 0.0002803765818926433\n",
      "test loss is 0.0006755421364435477\n",
      "Batch: 400,train loss is: 0.0004517733711377749\n",
      "test loss is 0.0006634206450175134\n",
      "Batch: 500,train loss is: 0.00047772641805563735\n",
      "test loss is 0.00069233183402225\n",
      "Batch: 600,train loss is: 0.00023955846103098426\n",
      "test loss is 0.0007092275243389162\n",
      "Batch: 700,train loss is: 0.00036331483811571463\n",
      "test loss is 0.0006660574218352261\n",
      "Batch: 800,train loss is: 0.0004272924887950381\n",
      "test loss is 0.0006726865185475807\n",
      "Batch: 900,train loss is: 0.0002950006999482745\n",
      "test loss is 0.00067326930555724\n",
      "Batch: 1000,train loss is: 0.0004387750080004662\n",
      "test loss is 0.0007063240144280202\n",
      "Batch: 1100,train loss is: 0.00176595904907252\n",
      "test loss is 0.000664751863765936\n",
      "Batch: 1200,train loss is: 0.0002803749171022099\n",
      "test loss is 0.0006875876752499926\n",
      "Batch: 1300,train loss is: 0.000527888951964605\n",
      "test loss is 0.0006701714329063102\n",
      "Batch: 1400,train loss is: 0.0004172740258370857\n",
      "test loss is 0.0006944734892001645\n",
      "Batch: 1500,train loss is: 0.00041195362395496095\n",
      "test loss is 0.0007006176559817187\n",
      "Batch: 1600,train loss is: 0.0006941951337836119\n",
      "test loss is 0.0007451396032987884\n",
      "Batch: 1700,train loss is: 0.0011826762685986487\n",
      "test loss is 0.00066303975660245\n",
      "Batch: 1800,train loss is: 0.0003615632554033786\n",
      "test loss is 0.0006694309305382497\n",
      "Batch: 1900,train loss is: 0.00040491077428943375\n",
      "test loss is 0.0007069802033500647\n",
      "Batch: 2000,train loss is: 0.0006733160867782847\n",
      "test loss is 0.0008154865133777895\n",
      "Batch: 2100,train loss is: 0.0006627217040281315\n",
      "test loss is 0.0008224643278904873\n",
      "Batch: 2200,train loss is: 0.0007310261542162621\n",
      "test loss is 0.0006764989923481359\n",
      "Batch: 2300,train loss is: 0.00045979235438444436\n",
      "test loss is 0.0006970582708870972\n",
      "Batch: 2400,train loss is: 0.0004444359447144422\n",
      "test loss is 0.0006758846936769886\n",
      "Batch: 2500,train loss is: 0.0005724376197524336\n",
      "test loss is 0.0006684031729763767\n",
      "Batch: 2600,train loss is: 0.00040460311061429465\n",
      "test loss is 0.0006763146337804743\n",
      "Batch: 2700,train loss is: 0.0005925104201888574\n",
      "test loss is 0.0006566610998463579\n",
      "Batch: 2800,train loss is: 0.00045094996913660297\n",
      "test loss is 0.0006859504934042988\n",
      "Batch: 2900,train loss is: 0.00042907861733587193\n",
      "test loss is 0.0006659205240247844\n",
      "Batch: 3000,train loss is: 0.00049613775792689\n",
      "test loss is 0.0006651137157725301\n",
      "Batch: 3100,train loss is: 0.0006345649761990733\n",
      "test loss is 0.0007091022164501154\n",
      "Batch: 3200,train loss is: 0.0010152146783452246\n",
      "test loss is 0.0006872559966784738\n",
      "Batch: 3300,train loss is: 0.0005638782498022486\n",
      "test loss is 0.0006810899688161897\n",
      "Batch: 3400,train loss is: 0.00039006410614261106\n",
      "test loss is 0.0006718406112989147\n",
      "Batch: 3500,train loss is: 0.00022560114332485545\n",
      "test loss is 0.0006671825378311854\n",
      "Batch: 3600,train loss is: 0.0005411609588083255\n",
      "test loss is 0.0007126372135120301\n",
      "Batch: 3700,train loss is: 0.00034560923016025514\n",
      "test loss is 0.0006738703021823227\n",
      "Batch: 3800,train loss is: 0.00043989418835948895\n",
      "test loss is 0.0006951124727276819\n",
      "Batch: 3900,train loss is: 0.0005818188142002001\n",
      "test loss is 0.0006766742458082484\n",
      "Batch: 4000,train loss is: 0.0004254259578146399\n",
      "test loss is 0.0006732327835626556\n",
      "Batch: 4100,train loss is: 0.0007270522112596475\n",
      "test loss is 0.000692963081200199\n",
      "Batch: 4200,train loss is: 0.0002679580751111224\n",
      "test loss is 0.0006982189863592916\n",
      "Batch: 4300,train loss is: 0.00040798028946580143\n",
      "test loss is 0.0006621562172453467\n",
      "Batch: 4400,train loss is: 0.0002990513590713597\n",
      "test loss is 0.0006908603889237779\n",
      "Batch: 4500,train loss is: 0.0007424529649703419\n",
      "test loss is 0.0006914727379053281\n",
      "Batch: 4600,train loss is: 0.0005714402565986106\n",
      "test loss is 0.0006787460974007362\n",
      "Batch: 4700,train loss is: 0.0005503207400285395\n",
      "test loss is 0.0007012871203361901\n",
      "Batch: 4800,train loss is: 0.001303073691503847\n",
      "test loss is 0.0006646948101662339\n",
      "Batch: 4900,train loss is: 0.0004900678106773625\n",
      "test loss is 0.0006766063039930972\n",
      "Batch: 5000,train loss is: 0.0005595630911008608\n",
      "test loss is 0.0006881479859215335\n",
      "Batch: 5100,train loss is: 0.00046284096931919344\n",
      "test loss is 0.0007274228693964316\n",
      "Batch: 5200,train loss is: 0.00030442355034203367\n",
      "test loss is 0.0006854236502946299\n",
      "Batch: 5300,train loss is: 0.0004782241015133097\n",
      "test loss is 0.00069250562506459\n",
      "Batch: 5400,train loss is: 0.0007221246792356978\n",
      "test loss is 0.0006647430696909349\n",
      "Batch: 5500,train loss is: 0.00042567534317628343\n",
      "test loss is 0.0006928311010453352\n",
      "Batch: 5600,train loss is: 0.0005588771223492369\n",
      "test loss is 0.0006769274555797293\n",
      "Batch: 5700,train loss is: 0.0005184941993040834\n",
      "test loss is 0.0006809552817435872\n",
      "Batch: 5800,train loss is: 0.00047050294115522355\n",
      "test loss is 0.0006720696475602629\n",
      "Batch: 5900,train loss is: 0.0033699561904083743\n",
      "test loss is 0.000677282161715147\n",
      "Batch: 6000,train loss is: 0.0016497570650430122\n",
      "test loss is 0.0006891632560641509\n",
      "Batch: 6100,train loss is: 0.00041150893988654163\n",
      "test loss is 0.0006635282966902394\n",
      "Batch: 6200,train loss is: 0.0005524808670027307\n",
      "test loss is 0.0006765049380482585\n",
      "Batch: 6300,train loss is: 0.0010774884168364188\n",
      "test loss is 0.0006905528719537675\n",
      "Batch: 6400,train loss is: 0.0005245528253217797\n",
      "test loss is 0.0007176180965572435\n",
      "Batch: 6500,train loss is: 0.00030463279856454716\n",
      "test loss is 0.0006798962690559839\n",
      "Batch: 6600,train loss is: 0.0020211861595556438\n",
      "test loss is 0.0006680340465720354\n",
      "Batch: 6700,train loss is: 0.0003175736981004297\n",
      "test loss is 0.0006816288323271564\n",
      "Batch: 6800,train loss is: 0.0007274402443033956\n",
      "test loss is 0.000670744737721648\n",
      "Batch: 6900,train loss is: 0.0006780416436305824\n",
      "test loss is 0.0006823067542260649\n",
      "Batch: 7000,train loss is: 0.0005627862114051344\n",
      "test loss is 0.0006916061199212485\n",
      "Batch: 7100,train loss is: 0.0011076773274970111\n",
      "test loss is 0.0006809284779428314\n",
      "Batch: 7200,train loss is: 0.00044688847977526286\n",
      "test loss is 0.0006879081701339728\n",
      "Batch: 7300,train loss is: 0.0007457200750652185\n",
      "test loss is 0.000703002554988104\n",
      "Batch: 7400,train loss is: 0.00035686178499763974\n",
      "test loss is 0.0006710584422050504\n",
      "Batch: 7500,train loss is: 0.0006509162035971754\n",
      "test loss is 0.0006946169490283967\n",
      "Batch: 7600,train loss is: 0.00039624617702418156\n",
      "test loss is 0.0006564294569837608\n",
      "Batch: 7700,train loss is: 0.000861201112620923\n",
      "test loss is 0.0006805487514595635\n",
      "Batch: 7800,train loss is: 0.00028557367556940463\n",
      "test loss is 0.0006648049373489639\n",
      "Batch: 7900,train loss is: 0.0005474396198093638\n",
      "test loss is 0.0006888639229862094\n",
      "Batch: 8000,train loss is: 0.0004019987783012681\n",
      "test loss is 0.0007052586098039858\n",
      "Batch: 8100,train loss is: 0.0005684803119718181\n",
      "test loss is 0.000693111037479755\n",
      "Batch: 8200,train loss is: 0.001073531375370569\n",
      "test loss is 0.0006897411748092488\n",
      "Batch: 8300,train loss is: 0.00034310852367042935\n",
      "test loss is 0.0006891393384129121\n",
      "Batch: 8400,train loss is: 0.0005583763765288652\n",
      "test loss is 0.0006737874539310942\n",
      "Batch: 8500,train loss is: 0.0006210816163663334\n",
      "test loss is 0.0006873621570703779\n",
      "Batch: 8600,train loss is: 0.0006638783387186133\n",
      "test loss is 0.0006953617953132697\n",
      "Batch: 8700,train loss is: 0.0004603256669597263\n",
      "test loss is 0.0007305767106246627\n",
      "Batch: 8800,train loss is: 0.0003627336481154401\n",
      "test loss is 0.0006673702310203499\n",
      "Batch: 8900,train loss is: 0.0010462816969881632\n",
      "test loss is 0.0006666569579285801\n",
      "Batch: 9000,train loss is: 0.0002520747920838997\n",
      "test loss is 0.0006884374364326097\n",
      "Batch: 9100,train loss is: 0.0003145608863662891\n",
      "test loss is 0.0006653210740357824\n",
      "Batch: 9200,train loss is: 0.0007733453543526515\n",
      "test loss is 0.0007373431596936563\n",
      "Batch: 9300,train loss is: 0.0005442435527765791\n",
      "test loss is 0.0007146892311201357\n",
      "Batch: 9400,train loss is: 0.00045227747363517037\n",
      "test loss is 0.0006979048604193074\n",
      "Batch: 9500,train loss is: 0.00036504460668513764\n",
      "test loss is 0.0007018293989190445\n",
      "Batch: 9600,train loss is: 0.0004086522146230427\n",
      "test loss is 0.0006626463391038912\n",
      "Batch: 9700,train loss is: 0.00035460112612633774\n",
      "test loss is 0.0006789085637751592\n",
      "Batch: 9800,train loss is: 0.0002697907700555287\n",
      "test loss is 0.000672062463816536\n",
      "Batch: 9900,train loss is: 0.0004784178729389734\n",
      "test loss is 0.0006990973702779358\n",
      "Batch: 10000,train loss is: 0.00043329032001331936\n",
      "test loss is 0.0006607115719969446\n",
      "Batch: 10100,train loss is: 0.001031290606848644\n",
      "test loss is 0.000682887097746519\n",
      "Batch: 10200,train loss is: 0.0017205268596495681\n",
      "test loss is 0.0007658380794573693\n",
      "Batch: 10300,train loss is: 0.00043739770078587715\n",
      "test loss is 0.0006737270577751217\n",
      "Batch: 10400,train loss is: 0.0002811300425290836\n",
      "test loss is 0.000670582791671277\n",
      "Batch: 10500,train loss is: 0.0007505562945111242\n",
      "test loss is 0.0006770170255660274\n",
      "Batch: 10600,train loss is: 0.0003157038328709217\n",
      "test loss is 0.0007322355959128106\n",
      "Batch: 10700,train loss is: 0.0006044680210309662\n",
      "test loss is 0.0006692629632841008\n",
      "Batch: 10800,train loss is: 0.0005956647834464166\n",
      "test loss is 0.0006801581677111163\n",
      "Batch: 10900,train loss is: 0.0007033036396152791\n",
      "test loss is 0.0006753149548614006\n",
      "Batch: 11000,train loss is: 0.0007645976408397078\n",
      "test loss is 0.0006975715997150846\n",
      "Batch: 11100,train loss is: 0.0009690042117072013\n",
      "test loss is 0.0006639320802902147\n",
      "Batch: 11200,train loss is: 0.0006799277315588422\n",
      "test loss is 0.0007162464506595587\n",
      "Batch: 11300,train loss is: 0.00031029533702917083\n",
      "test loss is 0.0006822960671902831\n",
      "Batch: 11400,train loss is: 0.0007077759187305506\n",
      "test loss is 0.0006746436170887723\n",
      "Batch: 11500,train loss is: 0.0005308910470498139\n",
      "test loss is 0.0006804490455539723\n",
      "Batch: 11600,train loss is: 0.001853589731718954\n",
      "test loss is 0.0006744162357846218\n",
      "Batch: 11700,train loss is: 0.000295443579113318\n",
      "test loss is 0.0006603959203320222\n",
      "Batch: 11800,train loss is: 0.0006004635069696382\n",
      "test loss is 0.0006692499735681071\n",
      "Batch: 11900,train loss is: 0.0005165515174445884\n",
      "test loss is 0.0006817798646069014\n",
      "Batch: 12000,train loss is: 0.0003171525531677839\n",
      "test loss is 0.0006718831391994168\n",
      "Batch: 12100,train loss is: 0.0015410291549154637\n",
      "test loss is 0.0006820772715964009\n",
      "Batch: 12200,train loss is: 0.0006470750369113366\n",
      "test loss is 0.0006980490437455086\n",
      "Batch: 12300,train loss is: 0.0004640575107002958\n",
      "test loss is 0.0006863105701431954\n",
      "Batch: 12400,train loss is: 0.0004005593086946904\n",
      "test loss is 0.0006738374314785158\n",
      "Batch: 12500,train loss is: 0.00047631631309226377\n",
      "test loss is 0.0006990069671513664\n",
      "Batch: 12600,train loss is: 0.0004989370276405821\n",
      "test loss is 0.0006885918584995087\n",
      "Batch: 12700,train loss is: 0.0003809089531837045\n",
      "test loss is 0.0006667770390753898\n",
      "Batch: 12800,train loss is: 0.00037202969399270614\n",
      "test loss is 0.0006660840896018658\n",
      "Batch: 12900,train loss is: 0.0006242771654813527\n",
      "test loss is 0.0006826461710930932\n",
      "Batch: 13000,train loss is: 0.0002059504259556439\n",
      "test loss is 0.0006667516910603905\n",
      "Batch: 13100,train loss is: 0.002105785527262256\n",
      "test loss is 0.0006634709940861504\n",
      "Batch: 13200,train loss is: 0.0008163511888368022\n",
      "test loss is 0.0007921746649866388\n",
      "Batch: 13300,train loss is: 0.0006514268880099703\n",
      "test loss is 0.0006826451490772777\n",
      "Batch: 13400,train loss is: 0.0003646956940184579\n",
      "test loss is 0.0006997190204608114\n",
      "Batch: 13500,train loss is: 0.0005805601660248664\n",
      "test loss is 0.0006994248270161081\n",
      "Batch: 13600,train loss is: 0.0005651235604247145\n",
      "test loss is 0.0006879825962087121\n",
      "Batch: 13700,train loss is: 0.0006645631001665965\n",
      "test loss is 0.0006625160057694403\n",
      "Batch: 13800,train loss is: 0.0003983311226095246\n",
      "test loss is 0.0006634757639297078\n",
      "Batch: 13900,train loss is: 0.00032384711334071127\n",
      "test loss is 0.0007291836471920557\n",
      "Batch: 14000,train loss is: 0.00038485049435893286\n",
      "test loss is 0.0006934970775580834\n",
      "Batch: 14100,train loss is: 0.0007331649148785655\n",
      "test loss is 0.0007178430715409761\n",
      "Batch: 14200,train loss is: 0.0005763786078871357\n",
      "test loss is 0.00065915744955278\n",
      "Batch: 14300,train loss is: 0.00024113106859030656\n",
      "test loss is 0.0006661848332247238\n",
      "Batch: 14400,train loss is: 0.0012772176554258848\n",
      "test loss is 0.000666475236211846\n",
      "Batch: 14500,train loss is: 0.0005765902980373407\n",
      "test loss is 0.000667263824156012\n",
      "Batch: 14600,train loss is: 0.0009494884599318426\n",
      "test loss is 0.0006767557944480503\n",
      "Batch: 14700,train loss is: 0.000460624757511799\n",
      "test loss is 0.0007244158901890173\n",
      "Batch: 14800,train loss is: 0.0007774517260304853\n",
      "test loss is 0.0006849201559019769\n",
      "Batch: 14900,train loss is: 0.00045357343642510745\n",
      "test loss is 0.0007188626934164736\n",
      "Batch: 15000,train loss is: 0.0006799824514233107\n",
      "test loss is 0.0006611059293000699\n",
      "Batch: 15100,train loss is: 0.0006762531734601762\n",
      "test loss is 0.0006915689307114356\n",
      "Batch: 15200,train loss is: 0.0008300073709530492\n",
      "test loss is 0.0006764888763020942\n",
      "Batch: 15300,train loss is: 0.0003604596958965704\n",
      "test loss is 0.0006679016218717596\n",
      "Batch: 15400,train loss is: 0.00046155084521488086\n",
      "test loss is 0.000682563715413621\n",
      "Batch: 15500,train loss is: 0.0006796659270527516\n",
      "test loss is 0.000660544392661427\n",
      "Batch: 15600,train loss is: 0.0008484530875893577\n",
      "test loss is 0.0006909442906859292\n",
      "Batch: 15700,train loss is: 0.0003956083596056639\n",
      "test loss is 0.0006684010582754124\n",
      "Batch: 15800,train loss is: 0.0006442173430651956\n",
      "test loss is 0.0006569545518410706\n",
      "Batch: 15900,train loss is: 0.0008373505879987012\n",
      "test loss is 0.0006771031603850414\n",
      "Batch: 16000,train loss is: 0.0003725917622450317\n",
      "test loss is 0.0006785870534108914\n",
      "Batch: 16100,train loss is: 0.0005241220012236397\n",
      "test loss is 0.0006710884063624141\n",
      "Batch: 16200,train loss is: 0.00038553081777664716\n",
      "test loss is 0.0006622651352222858\n",
      "Batch: 16300,train loss is: 0.0009258385499258864\n",
      "test loss is 0.0007000590157767558\n",
      "Batch: 16400,train loss is: 0.0003307879477328099\n",
      "test loss is 0.0006636270778628114\n",
      "Batch: 16500,train loss is: 0.0006589065878305808\n",
      "test loss is 0.0006857654763388354\n",
      "Batch: 16600,train loss is: 0.00031337405198800017\n",
      "test loss is 0.0006640151396368039\n",
      "Batch: 16700,train loss is: 0.0008462548452789383\n",
      "test loss is 0.0006805788427304406\n",
      "Batch: 16800,train loss is: 0.0005248663541107495\n",
      "test loss is 0.0006998486953298279\n",
      "Batch: 16900,train loss is: 0.0006988858206708953\n",
      "test loss is 0.0006768839700550957\n",
      "Batch: 17000,train loss is: 0.0005962389286515299\n",
      "test loss is 0.0006587618689641005\n",
      "Batch: 17100,train loss is: 0.0005138609875745819\n",
      "test loss is 0.0006719417204149207\n",
      "Batch: 17200,train loss is: 0.0004688890201083796\n",
      "test loss is 0.000675634472725164\n",
      "Batch: 17300,train loss is: 0.0007898828614335509\n",
      "test loss is 0.0006994114780207858\n",
      "Batch: 17400,train loss is: 0.0003872831491051612\n",
      "test loss is 0.0006776448821671903\n",
      "Batch: 17500,train loss is: 0.0005599598848041856\n",
      "test loss is 0.0006733242766790703\n",
      "Batch: 17600,train loss is: 0.00042620794814770565\n",
      "test loss is 0.0006834778914459394\n",
      "Batch: 17700,train loss is: 0.000343644110733431\n",
      "test loss is 0.000706434505045566\n",
      "Batch: 17800,train loss is: 0.0014432195896949057\n",
      "test loss is 0.0006808503447508566\n",
      "Batch: 17900,train loss is: 0.00048423585903594574\n",
      "test loss is 0.000706816509070203\n",
      "Batch: 18000,train loss is: 0.00029392975604955126\n",
      "test loss is 0.0006573928800908406\n",
      "Batch: 18100,train loss is: 0.00038817036913328\n",
      "test loss is 0.0006657358770749776\n",
      "Batch: 18200,train loss is: 0.0005756248177973474\n",
      "test loss is 0.0006797102318711877\n",
      "Batch: 18300,train loss is: 0.0012093195156136244\n",
      "test loss is 0.0006756798690287669\n",
      "Batch: 18400,train loss is: 0.0003926718125611738\n",
      "test loss is 0.0006666593957475303\n",
      "Batch: 18500,train loss is: 0.000907557561271086\n",
      "test loss is 0.0006771236944697577\n",
      "Batch: 18600,train loss is: 0.0004571912809215612\n",
      "test loss is 0.0006701470386766403\n",
      "Batch: 18700,train loss is: 0.0005689935220021161\n",
      "test loss is 0.0006895732011466126\n",
      "Batch: 18800,train loss is: 0.0005480194771814612\n",
      "test loss is 0.0006657972805559\n",
      "Batch: 18900,train loss is: 0.0005673836201266566\n",
      "test loss is 0.0006932709903885207\n",
      "Batch: 19000,train loss is: 0.000596213737631188\n",
      "test loss is 0.0006697409286635339\n",
      "Batch: 19100,train loss is: 0.0003658763645326707\n",
      "test loss is 0.0006954512113723504\n",
      "Batch: 19200,train loss is: 0.000502969209168906\n",
      "test loss is 0.0006773143191474573\n",
      "Batch: 19300,train loss is: 0.0006506406524680197\n",
      "test loss is 0.0006578071017662651\n",
      "Batch: 19400,train loss is: 0.0002694114426895296\n",
      "test loss is 0.0006633267017314122\n",
      "Batch: 19500,train loss is: 0.0005135434622164311\n",
      "test loss is 0.0007009080054939926\n",
      "Batch: 19600,train loss is: 0.0003948508301922405\n",
      "test loss is 0.0006806048881549852\n",
      "Batch: 19700,train loss is: 0.0007060622530677254\n",
      "test loss is 0.000751697829779398\n",
      "Batch: 19800,train loss is: 0.0005722717100276127\n",
      "test loss is 0.0006804741804383324\n",
      "Batch: 19900,train loss is: 0.00033394764710853973\n",
      "test loss is 0.0006931409271404122\n",
      "Batch: 20000,train loss is: 0.0006845210733017637\n",
      "test loss is 0.0006712035851843602\n",
      "Batch: 20100,train loss is: 0.0005567858063117913\n",
      "test loss is 0.0006821839901543433\n",
      "Batch: 20200,train loss is: 0.0007605972450211794\n",
      "test loss is 0.0006894695893348565\n",
      "Batch: 20300,train loss is: 0.00028500952355657816\n",
      "test loss is 0.0006908948991203535\n",
      "Batch: 20400,train loss is: 0.0004775346828000365\n",
      "test loss is 0.0007049643603622526\n",
      "Batch: 20500,train loss is: 0.0006065162910951461\n",
      "test loss is 0.000667014424941768\n",
      "Batch: 20600,train loss is: 0.0004735719514297057\n",
      "test loss is 0.0007012293390845549\n",
      "Batch: 20700,train loss is: 0.0007323689806648457\n",
      "test loss is 0.0006751892498028422\n",
      "Batch: 20800,train loss is: 0.0003320799229842205\n",
      "test loss is 0.0007021184558213267\n",
      "Batch: 20900,train loss is: 0.0005656422650980024\n",
      "test loss is 0.0007207732543752061\n",
      "Batch: 21000,train loss is: 0.00022629019805938097\n",
      "test loss is 0.0006613546444410076\n",
      "Batch: 21100,train loss is: 0.0006787550413232401\n",
      "test loss is 0.0006975505448492369\n",
      "Batch: 21200,train loss is: 0.000372261430524792\n",
      "test loss is 0.0007041054409622497\n",
      "Batch: 21300,train loss is: 0.0003182408458135071\n",
      "test loss is 0.0006711993056179445\n",
      "Batch: 21400,train loss is: 0.0007782562075199054\n",
      "test loss is 0.0006695460315823953\n",
      "Batch: 21500,train loss is: 0.00023060609674249618\n",
      "test loss is 0.0006855868849846449\n",
      "Batch: 21600,train loss is: 0.000734193073703096\n",
      "test loss is 0.0006890997884554906\n",
      "Batch: 21700,train loss is: 0.0002968800847272493\n",
      "test loss is 0.0006616065379514658\n",
      "Batch: 21800,train loss is: 0.0008075955733801633\n",
      "test loss is 0.0007161353356063585\n",
      "Batch: 21900,train loss is: 0.00044249751633797235\n",
      "test loss is 0.0007559051049869778\n",
      "Batch: 22000,train loss is: 0.0006384569916469857\n",
      "test loss is 0.0006670098507032526\n",
      "Batch: 22100,train loss is: 0.0003689435052783027\n",
      "test loss is 0.0006691333408899397\n",
      "Batch: 22200,train loss is: 0.0004861711457687162\n",
      "test loss is 0.0006747231383489017\n",
      "Batch: 22300,train loss is: 0.0005408089236829316\n",
      "test loss is 0.0006821995890377093\n",
      "Batch: 22400,train loss is: 0.0004488597988939045\n",
      "test loss is 0.0006836035724446687\n",
      "Batch: 22500,train loss is: 0.00048765186850099966\n",
      "test loss is 0.000661279933930632\n",
      "Batch: 22600,train loss is: 0.0009663590320485699\n",
      "test loss is 0.0007031542477529609\n",
      "Batch: 22700,train loss is: 0.0003504140407485701\n",
      "test loss is 0.0006619626593909705\n",
      "Batch: 22800,train loss is: 0.00050001688588367\n",
      "test loss is 0.0006976654758475405\n",
      "Batch: 22900,train loss is: 0.00030419294362072524\n",
      "test loss is 0.0007464916950576614\n",
      "Batch: 23000,train loss is: 0.00047442814786907543\n",
      "test loss is 0.0006748447140956542\n",
      "Batch: 23100,train loss is: 0.0004479879780732203\n",
      "test loss is 0.0006558714215662507\n",
      "Batch: 23200,train loss is: 0.00028877892459386426\n",
      "test loss is 0.0006550286534265483\n",
      "Batch: 23300,train loss is: 0.0004560057249212575\n",
      "test loss is 0.0006650719780539605\n",
      "Batch: 23400,train loss is: 0.0006027401968804946\n",
      "test loss is 0.0006931509811373221\n",
      "Batch: 23500,train loss is: 0.001572222503653268\n",
      "test loss is 0.0006591460666761997\n",
      "Batch: 23600,train loss is: 0.00030333934614914875\n",
      "test loss is 0.000666691673818317\n",
      "Batch: 23700,train loss is: 0.0009340566319558819\n",
      "test loss is 0.0006661790023033201\n",
      "Batch: 23800,train loss is: 0.0008028777168804384\n",
      "test loss is 0.0006691198811892456\n",
      "Batch: 23900,train loss is: 0.0008982443147729688\n",
      "test loss is 0.000681394087332184\n",
      "Batch: 24000,train loss is: 0.0007232827179833968\n",
      "test loss is 0.000716513776623694\n",
      "Batch: 24100,train loss is: 0.0003050569828755466\n",
      "test loss is 0.0006528612240376628\n",
      "Batch: 24200,train loss is: 0.0013952827529626066\n",
      "test loss is 0.0006806338604780551\n",
      "Batch: 24300,train loss is: 0.0005379008643272719\n",
      "test loss is 0.000799885058662998\n",
      "Batch: 24400,train loss is: 0.0013007313550836232\n",
      "test loss is 0.0006661856154770957\n",
      "Batch: 24500,train loss is: 0.0006707052737534622\n",
      "test loss is 0.0006552021381527875\n",
      "Batch: 24600,train loss is: 0.0003998508323723504\n",
      "test loss is 0.000692735069346085\n",
      "Batch: 24700,train loss is: 0.00028031214076658747\n",
      "test loss is 0.0006720829331520216\n",
      "Batch: 24800,train loss is: 0.0007276123791168219\n",
      "test loss is 0.0006797863444324417\n",
      "Batch: 24900,train loss is: 0.00019602544448808816\n",
      "test loss is 0.0006682300268509397\n",
      "Batch: 25000,train loss is: 0.0005661942903945961\n",
      "test loss is 0.0006648458530696139\n",
      "Batch: 25100,train loss is: 0.0004619843603932276\n",
      "test loss is 0.0006749116268370474\n",
      "Batch: 25200,train loss is: 0.0007404351452737957\n",
      "test loss is 0.0006769887797705629\n",
      "Batch: 25300,train loss is: 0.0007610925116081073\n",
      "test loss is 0.0007194255645384969\n",
      "Batch: 25400,train loss is: 0.0004114397349687226\n",
      "test loss is 0.0006655906600427942\n",
      "Batch: 25500,train loss is: 0.0005864585250207544\n",
      "test loss is 0.0006760582818255299\n",
      "Batch: 25600,train loss is: 0.0006291710980661945\n",
      "test loss is 0.0006715316863859657\n",
      "Batch: 25700,train loss is: 0.0005784395832886914\n",
      "test loss is 0.0006913420165847333\n",
      "Batch: 25800,train loss is: 0.0003518852430226607\n",
      "test loss is 0.0006591932748141667\n",
      "Batch: 25900,train loss is: 0.0002885172208665338\n",
      "test loss is 0.0007162782952350796\n",
      "Batch: 26000,train loss is: 0.0015280514257453323\n",
      "test loss is 0.0007206884698678778\n",
      "Batch: 26100,train loss is: 0.0007928305198619804\n",
      "test loss is 0.0006572046014089322\n",
      "Batch: 26200,train loss is: 0.0006173132731061885\n",
      "test loss is 0.0006561761823082915\n",
      "Batch: 26300,train loss is: 0.0010379922472167412\n",
      "test loss is 0.000676252906483229\n",
      "Batch: 26400,train loss is: 0.000563788969327511\n",
      "test loss is 0.0006578567983378276\n",
      "Batch: 26500,train loss is: 0.0005099584244056086\n",
      "test loss is 0.0006929379247664647\n",
      "Batch: 26600,train loss is: 0.001338991226802265\n",
      "test loss is 0.0007506550613301533\n",
      "Batch: 26700,train loss is: 0.0004920478120653962\n",
      "test loss is 0.0007539131264706015\n",
      "Batch: 26800,train loss is: 0.001142615363274573\n",
      "test loss is 0.0006819089308460152\n",
      "Batch: 26900,train loss is: 0.00045777899350212865\n",
      "test loss is 0.0006724384841467996\n",
      "Batch: 27000,train loss is: 0.0006406722559486575\n",
      "test loss is 0.0007809227839534819\n",
      "Batch: 27100,train loss is: 0.0006499913450613566\n",
      "test loss is 0.0007235528965402533\n",
      "Batch: 27200,train loss is: 0.0006558734094788322\n",
      "test loss is 0.0007061003856962591\n",
      "Batch: 27300,train loss is: 0.0004714519526991955\n",
      "test loss is 0.0006837385439408048\n",
      "Batch: 27400,train loss is: 0.000695175129707586\n",
      "test loss is 0.0006750099456555184\n",
      "Batch: 27500,train loss is: 0.0004557324833713672\n",
      "test loss is 0.0007380097262530563\n",
      "Batch: 27600,train loss is: 0.0005190485840492002\n",
      "test loss is 0.0006977512418721599\n",
      "Batch: 27700,train loss is: 0.0015345525264751669\n",
      "test loss is 0.0006782147915906043\n",
      "Batch: 27800,train loss is: 0.0007669199725765197\n",
      "test loss is 0.0006985057294132905\n",
      "Batch: 27900,train loss is: 0.001320641164031728\n",
      "test loss is 0.000668726833294855\n",
      "Batch: 28000,train loss is: 0.0003914619109325442\n",
      "test loss is 0.0006666680755816825\n",
      "Batch: 28100,train loss is: 0.0008295632164498193\n",
      "test loss is 0.0006824869695414709\n",
      "Batch: 28200,train loss is: 0.000537053697203327\n",
      "test loss is 0.0006510466206257378\n",
      "Batch: 28300,train loss is: 0.001690870326833483\n",
      "test loss is 0.0006826490460476156\n",
      "Batch: 28400,train loss is: 0.0005274993117439413\n",
      "test loss is 0.0006602409417200746\n",
      "Batch: 28500,train loss is: 0.001430848520511853\n",
      "test loss is 0.0007080133085195872\n",
      "Batch: 28600,train loss is: 0.0003520845282436886\n",
      "test loss is 0.0006748522518576612\n",
      "Batch: 28700,train loss is: 0.000559696671820594\n",
      "test loss is 0.0006657211355913268\n",
      "Batch: 28800,train loss is: 0.0010699621049159785\n",
      "test loss is 0.0006771916983551081\n",
      "Batch: 28900,train loss is: 0.00045469837269706046\n",
      "test loss is 0.0006580828678490363\n",
      "Batch: 29000,train loss is: 0.000492681651725433\n",
      "test loss is 0.0006665914663663985\n",
      "Batch: 29100,train loss is: 0.0005101681668955555\n",
      "test loss is 0.0006685160857277504\n",
      "Batch: 29200,train loss is: 0.00041653684981752273\n",
      "test loss is 0.0006787652840622984\n",
      "Batch: 29300,train loss is: 0.000959104666062109\n",
      "test loss is 0.0006622715174669161\n",
      "Batch: 29400,train loss is: 0.0007095577102333944\n",
      "test loss is 0.000752975078121588\n",
      "Batch: 29500,train loss is: 0.0004734820708994605\n",
      "test loss is 0.000695823917706933\n",
      "Batch: 29600,train loss is: 0.0005124994977718227\n",
      "test loss is 0.0006522733956543306\n",
      "Batch: 29700,train loss is: 0.00034973216638426477\n",
      "test loss is 0.000673847472812926\n",
      "Batch: 29800,train loss is: 0.0007932799171120782\n",
      "test loss is 0.0006561202887282984\n",
      "Batch: 29900,train loss is: 0.002894535009522277\n",
      "test loss is 0.0006624984946411235\n",
      "Batch: 30000,train loss is: 0.0003640099687642994\n",
      "test loss is 0.0007130998729950336\n",
      "Batch: 30100,train loss is: 0.0005702263357703398\n",
      "test loss is 0.0007020135248188911\n",
      "Batch: 30200,train loss is: 0.0003770031753392449\n",
      "test loss is 0.0006737121037036799\n",
      "Batch: 30300,train loss is: 0.00027698986105212023\n",
      "test loss is 0.0006586945797915664\n",
      "Batch: 30400,train loss is: 0.00031932735439932906\n",
      "test loss is 0.0006669645440656961\n",
      "Batch: 30500,train loss is: 0.0003188724990416637\n",
      "test loss is 0.0007016896167439628\n",
      "Batch: 30600,train loss is: 0.0002556737380570321\n",
      "test loss is 0.0006670482144132156\n",
      "Batch: 30700,train loss is: 0.00037714912790771677\n",
      "test loss is 0.0007495130670389464\n",
      "Batch: 30800,train loss is: 0.0002625303795366478\n",
      "test loss is 0.0007258921276597757\n",
      "Batch: 30900,train loss is: 0.00033998874987935766\n",
      "test loss is 0.0006696519470578263\n",
      "Batch: 31000,train loss is: 0.0004746827697157382\n",
      "test loss is 0.00068115956685182\n",
      "Batch: 31100,train loss is: 0.0003277725966013174\n",
      "test loss is 0.000753006785839129\n",
      "Batch: 31200,train loss is: 0.00047749918871267926\n",
      "test loss is 0.0006708503699753899\n",
      "Batch: 31300,train loss is: 0.0004553787235986215\n",
      "test loss is 0.0006694639750053268\n",
      "Batch: 31400,train loss is: 0.0007474168763063758\n",
      "test loss is 0.0006643470683129258\n",
      "Batch: 31500,train loss is: 0.00045212374974915064\n",
      "test loss is 0.0006724028868878296\n",
      "Batch: 31600,train loss is: 0.0004139751746239255\n",
      "test loss is 0.0006730301721363404\n",
      "Batch: 31700,train loss is: 0.000564163978336171\n",
      "test loss is 0.0006939495675916295\n",
      "Batch: 31800,train loss is: 0.0012872866253138924\n",
      "test loss is 0.0006708074865017352\n",
      "Batch: 31900,train loss is: 0.0007123796798034616\n",
      "test loss is 0.0006783564719256276\n",
      "Batch: 32000,train loss is: 0.0037803798572993218\n",
      "test loss is 0.0006749859357733856\n",
      "Batch: 32100,train loss is: 0.00037937061136074677\n",
      "test loss is 0.0006901547807651934\n",
      "Batch: 32200,train loss is: 0.0003250146230562909\n",
      "test loss is 0.0006737390245196682\n",
      "Batch: 32300,train loss is: 0.0003426282427906111\n",
      "test loss is 0.0006587987062250665\n",
      "Batch: 32400,train loss is: 0.00036955536524664714\n",
      "test loss is 0.000654942985924162\n",
      "Batch: 32500,train loss is: 0.00087371403266655\n",
      "test loss is 0.0007013948084442948\n",
      "Batch: 32600,train loss is: 0.00046544395051077155\n",
      "test loss is 0.000682744027735637\n",
      "Batch: 32700,train loss is: 0.00051224958495511\n",
      "test loss is 0.000709426532216533\n",
      "Batch: 32800,train loss is: 0.00045933848082624953\n",
      "test loss is 0.0007113073356268638\n",
      "Batch: 32900,train loss is: 0.0006609537293249872\n",
      "test loss is 0.0006641531321053644\n",
      "Batch: 33000,train loss is: 0.0004341531825369126\n",
      "test loss is 0.0006680659404962386\n",
      "Batch: 33100,train loss is: 0.00042509896136524745\n",
      "test loss is 0.0006808066665359477\n",
      "Batch: 33200,train loss is: 0.000438315155933522\n",
      "test loss is 0.0007377340303061083\n",
      "Batch: 33300,train loss is: 0.000518153890664057\n",
      "test loss is 0.0006620787410683068\n",
      "Batch: 33400,train loss is: 0.00047709294882210073\n",
      "test loss is 0.0006750976301972491\n",
      "Batch: 33500,train loss is: 0.00034574053333176187\n",
      "test loss is 0.000667474323541105\n",
      "Batch: 33600,train loss is: 0.0005465101875099388\n",
      "test loss is 0.0006632195056849926\n",
      "Batch: 33700,train loss is: 0.0005302404648815706\n",
      "test loss is 0.0006807756099013608\n",
      "Batch: 33800,train loss is: 0.0007597151425542459\n",
      "test loss is 0.0006620485522731361\n",
      "Batch: 33900,train loss is: 0.0006338385683339745\n",
      "test loss is 0.0006646977648325238\n",
      "-----------------------Epoch: 17----------------------------------\n",
      "Batch: 0,train loss is: 0.00029292560645118173\n",
      "test loss is 0.0006827436033062036\n",
      "Batch: 100,train loss is: 0.0012165601760418564\n",
      "test loss is 0.0006767219759886781\n",
      "Batch: 200,train loss is: 0.0003878413783733554\n",
      "test loss is 0.0006920558787504199\n",
      "Batch: 300,train loss is: 0.00028151977749610584\n",
      "test loss is 0.000669912508102088\n",
      "Batch: 400,train loss is: 0.0004453074665217143\n",
      "test loss is 0.0006561463445577527\n",
      "Batch: 500,train loss is: 0.00046808244398563703\n",
      "test loss is 0.000684635464453931\n",
      "Batch: 600,train loss is: 0.00023268561526106147\n",
      "test loss is 0.0007007272506439822\n",
      "Batch: 700,train loss is: 0.0003662022778395697\n",
      "test loss is 0.0006598638533348906\n",
      "Batch: 800,train loss is: 0.0004236790847474631\n",
      "test loss is 0.000665754252393494\n",
      "Batch: 900,train loss is: 0.0003047993068905918\n",
      "test loss is 0.0006682468847861242\n",
      "Batch: 1000,train loss is: 0.0004360762585816671\n",
      "test loss is 0.0006989560378775481\n",
      "Batch: 1100,train loss is: 0.0017585131662846427\n",
      "test loss is 0.0006582522675116041\n",
      "Batch: 1200,train loss is: 0.0002713664892959558\n",
      "test loss is 0.000681582306882461\n",
      "Batch: 1300,train loss is: 0.0005253266389435712\n",
      "test loss is 0.0006641311315097127\n",
      "Batch: 1400,train loss is: 0.00041475039026623786\n",
      "test loss is 0.0006885325137896132\n",
      "Batch: 1500,train loss is: 0.0003947496460015017\n",
      "test loss is 0.0006950202647107637\n",
      "Batch: 1600,train loss is: 0.0006830947313183633\n",
      "test loss is 0.0007369355144161998\n",
      "Batch: 1700,train loss is: 0.0011729956431458656\n",
      "test loss is 0.0006568637587381114\n",
      "Batch: 1800,train loss is: 0.0003556032847217141\n",
      "test loss is 0.0006603978648427832\n",
      "Batch: 1900,train loss is: 0.0004038588057447777\n",
      "test loss is 0.000700413659174557\n",
      "Batch: 2000,train loss is: 0.0006712651483442994\n",
      "test loss is 0.0008099082894414067\n",
      "Batch: 2100,train loss is: 0.0006565456929634455\n",
      "test loss is 0.0008171602418175013\n",
      "Batch: 2200,train loss is: 0.0007162582413989448\n",
      "test loss is 0.0006707400288300667\n",
      "Batch: 2300,train loss is: 0.0004650682491213886\n",
      "test loss is 0.0006882557662072473\n",
      "Batch: 2400,train loss is: 0.0004368584278884509\n",
      "test loss is 0.0006700109224541377\n",
      "Batch: 2500,train loss is: 0.000558155420094206\n",
      "test loss is 0.000661619932612645\n",
      "Batch: 2600,train loss is: 0.00040723923496573144\n",
      "test loss is 0.0006689107777542015\n",
      "Batch: 2700,train loss is: 0.0005795024314976576\n",
      "test loss is 0.0006492907429529605\n",
      "Batch: 2800,train loss is: 0.0004482125683675939\n",
      "test loss is 0.0006768731962831447\n",
      "Batch: 2900,train loss is: 0.00042039733725113265\n",
      "test loss is 0.0006608900633839822\n",
      "Batch: 3000,train loss is: 0.0004846433901268798\n",
      "test loss is 0.0006592755852537483\n",
      "Batch: 3100,train loss is: 0.000613599095795719\n",
      "test loss is 0.0007011279054618516\n",
      "Batch: 3200,train loss is: 0.00101605886441005\n",
      "test loss is 0.0006800356551000706\n",
      "Batch: 3300,train loss is: 0.000565111739990698\n",
      "test loss is 0.000674386611752543\n",
      "Batch: 3400,train loss is: 0.00038033046819501315\n",
      "test loss is 0.0006655404142127929\n",
      "Batch: 3500,train loss is: 0.00022489407592451896\n",
      "test loss is 0.0006601070259842594\n",
      "Batch: 3600,train loss is: 0.000532292845852948\n",
      "test loss is 0.0007081085675984302\n",
      "Batch: 3700,train loss is: 0.0003426586069668833\n",
      "test loss is 0.0006688653393139374\n",
      "Batch: 3800,train loss is: 0.0004330731247109938\n",
      "test loss is 0.000686203834248536\n",
      "Batch: 3900,train loss is: 0.0005752524430123153\n",
      "test loss is 0.0006699682092474885\n",
      "Batch: 4000,train loss is: 0.00041760072796847225\n",
      "test loss is 0.0006656179373138877\n",
      "Batch: 4100,train loss is: 0.0007375034711205336\n",
      "test loss is 0.0006863671204117348\n",
      "Batch: 4200,train loss is: 0.00026153765796724053\n",
      "test loss is 0.0006901863036106069\n",
      "Batch: 4300,train loss is: 0.0004126881141905807\n",
      "test loss is 0.0006553318907137722\n",
      "Batch: 4400,train loss is: 0.00028876652648620287\n",
      "test loss is 0.0006858548760469516\n",
      "Batch: 4500,train loss is: 0.0007396881911361524\n",
      "test loss is 0.0006839914763574385\n",
      "Batch: 4600,train loss is: 0.0005711185818682348\n",
      "test loss is 0.0006736770494436731\n",
      "Batch: 4700,train loss is: 0.0005539341850731487\n",
      "test loss is 0.0006937299584625136\n",
      "Batch: 4800,train loss is: 0.0013171555082899455\n",
      "test loss is 0.0006575452518571254\n",
      "Batch: 4900,train loss is: 0.00047526802186831244\n",
      "test loss is 0.0006699654406828906\n",
      "Batch: 5000,train loss is: 0.0005468981912255605\n",
      "test loss is 0.0006835184989284816\n",
      "Batch: 5100,train loss is: 0.0004529301063750716\n",
      "test loss is 0.000719323157120631\n",
      "Batch: 5200,train loss is: 0.0003023078977030462\n",
      "test loss is 0.0006806478847306141\n",
      "Batch: 5300,train loss is: 0.0004628986209324975\n",
      "test loss is 0.000685670641365743\n",
      "Batch: 5400,train loss is: 0.0007154207870594537\n",
      "test loss is 0.0006587335055946135\n",
      "Batch: 5500,train loss is: 0.00041908423120590367\n",
      "test loss is 0.0006862043832371456\n",
      "Batch: 5600,train loss is: 0.0005573744756824508\n",
      "test loss is 0.0006687074030474809\n",
      "Batch: 5700,train loss is: 0.0004925746654963954\n",
      "test loss is 0.0006741772107639492\n",
      "Batch: 5800,train loss is: 0.00045385968314044767\n",
      "test loss is 0.0006662243368464729\n",
      "Batch: 5900,train loss is: 0.0033413287958539402\n",
      "test loss is 0.0006708706163763397\n",
      "Batch: 6000,train loss is: 0.001645799047092973\n",
      "test loss is 0.0006833780374832045\n",
      "Batch: 6100,train loss is: 0.00040834782068397774\n",
      "test loss is 0.0006567724033932435\n",
      "Batch: 6200,train loss is: 0.0005476696574063365\n",
      "test loss is 0.0006693131534799481\n",
      "Batch: 6300,train loss is: 0.0010567683342167491\n",
      "test loss is 0.0006832224003725724\n",
      "Batch: 6400,train loss is: 0.0005236770792705007\n",
      "test loss is 0.000709794277966985\n",
      "Batch: 6500,train loss is: 0.00030308826716288396\n",
      "test loss is 0.0006721042330921507\n",
      "Batch: 6600,train loss is: 0.0019888646456996655\n",
      "test loss is 0.0006609141234105495\n",
      "Batch: 6700,train loss is: 0.0003105049261920685\n",
      "test loss is 0.0006737771041946617\n",
      "Batch: 6800,train loss is: 0.0007127648485651248\n",
      "test loss is 0.0006654238607800725\n",
      "Batch: 6900,train loss is: 0.0006664932125459309\n",
      "test loss is 0.0006763581754891972\n",
      "Batch: 7000,train loss is: 0.0005551158940482594\n",
      "test loss is 0.0006841285402752415\n",
      "Batch: 7100,train loss is: 0.0011057148902196528\n",
      "test loss is 0.000674050765851979\n",
      "Batch: 7200,train loss is: 0.00045080949872514195\n",
      "test loss is 0.0006806601790825494\n",
      "Batch: 7300,train loss is: 0.0007283071123765139\n",
      "test loss is 0.000694012863886718\n",
      "Batch: 7400,train loss is: 0.0003585220579494308\n",
      "test loss is 0.0006649002752315999\n",
      "Batch: 7500,train loss is: 0.0006423859190953816\n",
      "test loss is 0.0006878130813324213\n",
      "Batch: 7600,train loss is: 0.0003991324989471535\n",
      "test loss is 0.0006501261694379713\n",
      "Batch: 7700,train loss is: 0.0008669534909939441\n",
      "test loss is 0.0006748702985772166\n",
      "Batch: 7800,train loss is: 0.000279759510448992\n",
      "test loss is 0.0006578926453414939\n",
      "Batch: 7900,train loss is: 0.0005291744392656492\n",
      "test loss is 0.0006822373364742813\n",
      "Batch: 8000,train loss is: 0.00038969728958746457\n",
      "test loss is 0.000698483115799816\n",
      "Batch: 8100,train loss is: 0.0005694770533771515\n",
      "test loss is 0.0006862579976203085\n",
      "Batch: 8200,train loss is: 0.0010653747700155487\n",
      "test loss is 0.0006824394774026485\n",
      "Batch: 8300,train loss is: 0.0003329964675886686\n",
      "test loss is 0.000681014008395416\n",
      "Batch: 8400,train loss is: 0.0005552881881115787\n",
      "test loss is 0.0006684283177028944\n",
      "Batch: 8500,train loss is: 0.0006037054540601924\n",
      "test loss is 0.0006810369567685665\n",
      "Batch: 8600,train loss is: 0.0006613698448981048\n",
      "test loss is 0.0006876594853104483\n",
      "Batch: 8700,train loss is: 0.0004564305318234667\n",
      "test loss is 0.0007235235496544114\n",
      "Batch: 8800,train loss is: 0.0003656170907971067\n",
      "test loss is 0.0006618583819888104\n",
      "Batch: 8900,train loss is: 0.0010374824599979357\n",
      "test loss is 0.0006607600375411819\n",
      "Batch: 9000,train loss is: 0.0002554632484254584\n",
      "test loss is 0.0006829752670497652\n",
      "Batch: 9100,train loss is: 0.00031281317289730785\n",
      "test loss is 0.0006591294429144039\n",
      "Batch: 9200,train loss is: 0.0007640792384647892\n",
      "test loss is 0.0007278596590019863\n",
      "Batch: 9300,train loss is: 0.0005437345663193002\n",
      "test loss is 0.0007099376360850175\n",
      "Batch: 9400,train loss is: 0.00045355349358140176\n",
      "test loss is 0.0006904626280646085\n",
      "Batch: 9500,train loss is: 0.0003609093020970719\n",
      "test loss is 0.0006947981533673566\n",
      "Batch: 9600,train loss is: 0.00040001099428035343\n",
      "test loss is 0.0006558859245570384\n",
      "Batch: 9700,train loss is: 0.0003585251666346021\n",
      "test loss is 0.0006708497059169405\n",
      "Batch: 9800,train loss is: 0.00026308159776850844\n",
      "test loss is 0.0006652100957727014\n",
      "Batch: 9900,train loss is: 0.00046989964448812554\n",
      "test loss is 0.0006914660984711459\n",
      "Batch: 10000,train loss is: 0.0004344710022289397\n",
      "test loss is 0.0006537445041617531\n",
      "Batch: 10100,train loss is: 0.0009805912775022931\n",
      "test loss is 0.0006761757878912141\n",
      "Batch: 10200,train loss is: 0.0017000653980750445\n",
      "test loss is 0.0007572210792841782\n",
      "Batch: 10300,train loss is: 0.00042980667954971237\n",
      "test loss is 0.0006669840860928087\n",
      "Batch: 10400,train loss is: 0.00027671992843923487\n",
      "test loss is 0.0006638989443636408\n",
      "Batch: 10500,train loss is: 0.0007644674672518304\n",
      "test loss is 0.0006712344167947812\n",
      "Batch: 10600,train loss is: 0.0003143643656985139\n",
      "test loss is 0.0007266823645603235\n",
      "Batch: 10700,train loss is: 0.0006005787766316498\n",
      "test loss is 0.0006627998414471893\n",
      "Batch: 10800,train loss is: 0.0006027656399264968\n",
      "test loss is 0.0006728750470365233\n",
      "Batch: 10900,train loss is: 0.0007023697490267937\n",
      "test loss is 0.0006689446796724728\n",
      "Batch: 11000,train loss is: 0.0007585770325615175\n",
      "test loss is 0.0006905000401112962\n",
      "Batch: 11100,train loss is: 0.0009477649216923941\n",
      "test loss is 0.0006571489769957992\n",
      "Batch: 11200,train loss is: 0.0006804617112530295\n",
      "test loss is 0.0007092844339749053\n",
      "Batch: 11300,train loss is: 0.00030190172793054286\n",
      "test loss is 0.0006760747276876603\n",
      "Batch: 11400,train loss is: 0.00071532038599044\n",
      "test loss is 0.0006679938212931638\n",
      "Batch: 11500,train loss is: 0.0005152459243553756\n",
      "test loss is 0.0006741161999910517\n",
      "Batch: 11600,train loss is: 0.0018269287983292976\n",
      "test loss is 0.0006667897369408366\n",
      "Batch: 11700,train loss is: 0.00029831528752517474\n",
      "test loss is 0.0006541658081914683\n",
      "Batch: 11800,train loss is: 0.0005932980253482985\n",
      "test loss is 0.0006632202649372837\n",
      "Batch: 11900,train loss is: 0.0005078959831449881\n",
      "test loss is 0.0006750850620095818\n",
      "Batch: 12000,train loss is: 0.0003133360353383567\n",
      "test loss is 0.00066564690374783\n",
      "Batch: 12100,train loss is: 0.001538069361689123\n",
      "test loss is 0.000676703231504131\n",
      "Batch: 12200,train loss is: 0.0006359598910013373\n",
      "test loss is 0.0006919468280490216\n",
      "Batch: 12300,train loss is: 0.00046037478844371027\n",
      "test loss is 0.0006789520830181015\n",
      "Batch: 12400,train loss is: 0.00039420661607921084\n",
      "test loss is 0.0006665543520507673\n",
      "Batch: 12500,train loss is: 0.00048142975947460025\n",
      "test loss is 0.000692548612300405\n",
      "Batch: 12600,train loss is: 0.0005022949340958648\n",
      "test loss is 0.0006810444062506019\n",
      "Batch: 12700,train loss is: 0.000369878930216117\n",
      "test loss is 0.0006602503835227677\n",
      "Batch: 12800,train loss is: 0.0003639395746376245\n",
      "test loss is 0.0006595428153035385\n",
      "Batch: 12900,train loss is: 0.0006035521806127267\n",
      "test loss is 0.000675205991289645\n",
      "Batch: 13000,train loss is: 0.0002037756831596774\n",
      "test loss is 0.0006616778343589989\n",
      "Batch: 13100,train loss is: 0.002066816335841452\n",
      "test loss is 0.0006577359278743235\n",
      "Batch: 13200,train loss is: 0.0008242274568191397\n",
      "test loss is 0.0007884524172828483\n",
      "Batch: 13300,train loss is: 0.0006347394316501032\n",
      "test loss is 0.0006732693568760829\n",
      "Batch: 13400,train loss is: 0.0003633153199995583\n",
      "test loss is 0.0006921440130489262\n",
      "Batch: 13500,train loss is: 0.0005656493202496522\n",
      "test loss is 0.0006918360166805393\n",
      "Batch: 13600,train loss is: 0.0005596501329110905\n",
      "test loss is 0.0006825535456611283\n",
      "Batch: 13700,train loss is: 0.0006538811812403022\n",
      "test loss is 0.0006552701614094129\n",
      "Batch: 13800,train loss is: 0.00040285411943594173\n",
      "test loss is 0.0006577087410556458\n",
      "Batch: 13900,train loss is: 0.000305309104575834\n",
      "test loss is 0.0007244086825983899\n",
      "Batch: 14000,train loss is: 0.0003725068459945776\n",
      "test loss is 0.0006870062654246924\n",
      "Batch: 14100,train loss is: 0.0007249907649049165\n",
      "test loss is 0.0007085129842806329\n",
      "Batch: 14200,train loss is: 0.0005704961426881487\n",
      "test loss is 0.0006535021933668323\n",
      "Batch: 14300,train loss is: 0.00024027999481825908\n",
      "test loss is 0.0006596978905224459\n",
      "Batch: 14400,train loss is: 0.0012609725190198145\n",
      "test loss is 0.0006591425501447208\n",
      "Batch: 14500,train loss is: 0.0005894440244569989\n",
      "test loss is 0.0006607637251512743\n",
      "Batch: 14600,train loss is: 0.0009194491025602002\n",
      "test loss is 0.0006714127016564136\n",
      "Batch: 14700,train loss is: 0.00046131330419879845\n",
      "test loss is 0.0007199885786802927\n",
      "Batch: 14800,train loss is: 0.0007748727972540763\n",
      "test loss is 0.0006791958795229285\n",
      "Batch: 14900,train loss is: 0.0004526597779917897\n",
      "test loss is 0.0007125379065612391\n",
      "Batch: 15000,train loss is: 0.0006588121968007573\n",
      "test loss is 0.000654254287152689\n",
      "Batch: 15100,train loss is: 0.0006924231170132302\n",
      "test loss is 0.0006851512946576114\n",
      "Batch: 15200,train loss is: 0.0008159366845878455\n",
      "test loss is 0.0006697012855155687\n",
      "Batch: 15300,train loss is: 0.000353085718073363\n",
      "test loss is 0.0006615506468355711\n",
      "Batch: 15400,train loss is: 0.0004583561992356114\n",
      "test loss is 0.0006766930404627117\n",
      "Batch: 15500,train loss is: 0.0006744737933836646\n",
      "test loss is 0.0006545906227611037\n",
      "Batch: 15600,train loss is: 0.0008405572853818447\n",
      "test loss is 0.0006830997938505032\n",
      "Batch: 15700,train loss is: 0.000389506880690838\n",
      "test loss is 0.0006619397606856248\n",
      "Batch: 15800,train loss is: 0.0006273132332869836\n",
      "test loss is 0.000649806712198794\n",
      "Batch: 15900,train loss is: 0.000819854198137694\n",
      "test loss is 0.0006709010205618171\n",
      "Batch: 16000,train loss is: 0.00036930605256112306\n",
      "test loss is 0.000669972970107537\n",
      "Batch: 16100,train loss is: 0.0005267348136925251\n",
      "test loss is 0.0006644598588188345\n",
      "Batch: 16200,train loss is: 0.0003811991622787908\n",
      "test loss is 0.0006548713532380529\n",
      "Batch: 16300,train loss is: 0.0009093866584576931\n",
      "test loss is 0.0006929868988933635\n",
      "Batch: 16400,train loss is: 0.00032009783103206546\n",
      "test loss is 0.0006569140618896863\n",
      "Batch: 16500,train loss is: 0.0006623769345129673\n",
      "test loss is 0.0006777770636661663\n",
      "Batch: 16600,train loss is: 0.00031988383172540825\n",
      "test loss is 0.0006583486958499919\n",
      "Batch: 16700,train loss is: 0.0008305456017340201\n",
      "test loss is 0.0006727364262884693\n",
      "Batch: 16800,train loss is: 0.0005123271930927071\n",
      "test loss is 0.0006937407215669323\n",
      "Batch: 16900,train loss is: 0.0007051533691215905\n",
      "test loss is 0.0006708472381380838\n",
      "Batch: 17000,train loss is: 0.0005909413771225425\n",
      "test loss is 0.0006524862413525363\n",
      "Batch: 17100,train loss is: 0.0005074465270023919\n",
      "test loss is 0.0006649053444429312\n",
      "Batch: 17200,train loss is: 0.0004539717329007181\n",
      "test loss is 0.0006688087457397803\n",
      "Batch: 17300,train loss is: 0.0007809609998053125\n",
      "test loss is 0.0006930809080431457\n",
      "Batch: 17400,train loss is: 0.00037886885094062203\n",
      "test loss is 0.0006728728224918237\n",
      "Batch: 17500,train loss is: 0.0005615100230364701\n",
      "test loss is 0.0006677577115622437\n",
      "Batch: 17600,train loss is: 0.0004215665667883273\n",
      "test loss is 0.0006753015794771075\n",
      "Batch: 17700,train loss is: 0.0003392028493922774\n",
      "test loss is 0.0007009921247796688\n",
      "Batch: 17800,train loss is: 0.0014623464444755388\n",
      "test loss is 0.0006772716537201789\n",
      "Batch: 17900,train loss is: 0.0004861569026945683\n",
      "test loss is 0.0006998310128989568\n",
      "Batch: 18000,train loss is: 0.00028543217635302893\n",
      "test loss is 0.0006508753422770771\n",
      "Batch: 18100,train loss is: 0.0003809528770761531\n",
      "test loss is 0.0006591982853345962\n",
      "Batch: 18200,train loss is: 0.0005792392089694349\n",
      "test loss is 0.0006721651572990138\n",
      "Batch: 18300,train loss is: 0.0012387537028925068\n",
      "test loss is 0.0006696110123105604\n",
      "Batch: 18400,train loss is: 0.0003891984079222511\n",
      "test loss is 0.0006607971041696433\n",
      "Batch: 18500,train loss is: 0.0009030841701013387\n",
      "test loss is 0.0006704050187175319\n",
      "Batch: 18600,train loss is: 0.00045110619323362805\n",
      "test loss is 0.0006642479788396281\n",
      "Batch: 18700,train loss is: 0.0005641703888671094\n",
      "test loss is 0.0006830220708962587\n",
      "Batch: 18800,train loss is: 0.000552860072703419\n",
      "test loss is 0.0006588261347372754\n",
      "Batch: 18900,train loss is: 0.0005501321855378465\n",
      "test loss is 0.000684930702336375\n",
      "Batch: 19000,train loss is: 0.000576390096703508\n",
      "test loss is 0.0006629394880407791\n",
      "Batch: 19100,train loss is: 0.00037818612639534203\n",
      "test loss is 0.0006889022755189217\n",
      "Batch: 19200,train loss is: 0.0005027653210149112\n",
      "test loss is 0.0006711322565318173\n",
      "Batch: 19300,train loss is: 0.0006325711875372624\n",
      "test loss is 0.0006515018615047517\n",
      "Batch: 19400,train loss is: 0.00026741003034691263\n",
      "test loss is 0.0006570411435772258\n",
      "Batch: 19500,train loss is: 0.0005093332161693122\n",
      "test loss is 0.0006939294907135989\n",
      "Batch: 19600,train loss is: 0.00038894030346995376\n",
      "test loss is 0.000673642428732244\n",
      "Batch: 19700,train loss is: 0.000692303542860482\n",
      "test loss is 0.0007431006655307351\n",
      "Batch: 19800,train loss is: 0.0005658960469908565\n",
      "test loss is 0.0006745457065267775\n",
      "Batch: 19900,train loss is: 0.00035190818398199224\n",
      "test loss is 0.0006867570058247201\n",
      "Batch: 20000,train loss is: 0.0006752011832005634\n",
      "test loss is 0.0006639294955165244\n",
      "Batch: 20100,train loss is: 0.0005708200462617799\n",
      "test loss is 0.0006775277304560374\n",
      "Batch: 20200,train loss is: 0.000744903897863581\n",
      "test loss is 0.0006833527398767972\n",
      "Batch: 20300,train loss is: 0.00028201884719191334\n",
      "test loss is 0.0006846106194457936\n",
      "Batch: 20400,train loss is: 0.00047073057133353955\n",
      "test loss is 0.0006958050733704465\n",
      "Batch: 20500,train loss is: 0.0006070689696962726\n",
      "test loss is 0.0006607175488081266\n",
      "Batch: 20600,train loss is: 0.00047276207343431017\n",
      "test loss is 0.0006944639659610812\n",
      "Batch: 20700,train loss is: 0.0007524218327469279\n",
      "test loss is 0.0006672847321150491\n",
      "Batch: 20800,train loss is: 0.0003283208747771287\n",
      "test loss is 0.0006986383074945339\n",
      "Batch: 20900,train loss is: 0.0005544031126038567\n",
      "test loss is 0.0007131197426716624\n",
      "Batch: 21000,train loss is: 0.000222085673817563\n",
      "test loss is 0.0006545785774160027\n",
      "Batch: 21100,train loss is: 0.0006782712828337511\n",
      "test loss is 0.0006923564531857122\n",
      "Batch: 21200,train loss is: 0.0003766147838306733\n",
      "test loss is 0.0006984739396968727\n",
      "Batch: 21300,train loss is: 0.00031260031390916397\n",
      "test loss is 0.0006656888784994343\n",
      "Batch: 21400,train loss is: 0.0007761979586353532\n",
      "test loss is 0.0006622968586677384\n",
      "Batch: 21500,train loss is: 0.00022802212952613781\n",
      "test loss is 0.0006790811938523578\n",
      "Batch: 21600,train loss is: 0.0007482680453398335\n",
      "test loss is 0.0006838073151800778\n",
      "Batch: 21700,train loss is: 0.00028986301622499537\n",
      "test loss is 0.0006550038737094705\n",
      "Batch: 21800,train loss is: 0.0008149354968165746\n",
      "test loss is 0.000710465893471066\n",
      "Batch: 21900,train loss is: 0.0004274133985858079\n",
      "test loss is 0.0007475284858427082\n",
      "Batch: 22000,train loss is: 0.0006595146691317495\n",
      "test loss is 0.0006618964484571441\n",
      "Batch: 22100,train loss is: 0.00036694014409627946\n",
      "test loss is 0.0006614613157171865\n",
      "Batch: 22200,train loss is: 0.00048468072027822853\n",
      "test loss is 0.0006688897790666425\n",
      "Batch: 22300,train loss is: 0.0005283818027438889\n",
      "test loss is 0.0006768296159424443\n",
      "Batch: 22400,train loss is: 0.0004500819729586082\n",
      "test loss is 0.0006777306730148104\n",
      "Batch: 22500,train loss is: 0.0004889969440866302\n",
      "test loss is 0.0006546028230144205\n",
      "Batch: 22600,train loss is: 0.000940009018025932\n",
      "test loss is 0.0006960052597991052\n",
      "Batch: 22700,train loss is: 0.00034103531816699183\n",
      "test loss is 0.0006557105201087772\n",
      "Batch: 22800,train loss is: 0.0004965199650526511\n",
      "test loss is 0.0006915490169050059\n",
      "Batch: 22900,train loss is: 0.0002998535705357717\n",
      "test loss is 0.0007415312076684921\n",
      "Batch: 23000,train loss is: 0.0004894699789698608\n",
      "test loss is 0.000668924082227623\n",
      "Batch: 23100,train loss is: 0.0004377254711293339\n",
      "test loss is 0.0006494887091885923\n",
      "Batch: 23200,train loss is: 0.00028880686822218806\n",
      "test loss is 0.0006483405709330234\n",
      "Batch: 23300,train loss is: 0.00044475320235125813\n",
      "test loss is 0.0006571681788130285\n",
      "Batch: 23400,train loss is: 0.000598505189476004\n",
      "test loss is 0.0006868243129366869\n",
      "Batch: 23500,train loss is: 0.0014993556268734884\n",
      "test loss is 0.0006511301253329033\n",
      "Batch: 23600,train loss is: 0.0003084048632804144\n",
      "test loss is 0.0006612165089177578\n",
      "Batch: 23700,train loss is: 0.0009556554767683157\n",
      "test loss is 0.0006589056869091695\n",
      "Batch: 23800,train loss is: 0.0008062263305156463\n",
      "test loss is 0.0006625066812182627\n",
      "Batch: 23900,train loss is: 0.000910263779063342\n",
      "test loss is 0.0006765367782996556\n",
      "Batch: 24000,train loss is: 0.0007229775733966055\n",
      "test loss is 0.0007095755971225174\n",
      "Batch: 24100,train loss is: 0.00029559103261059066\n",
      "test loss is 0.0006476500408276703\n",
      "Batch: 24200,train loss is: 0.0013685276031177034\n",
      "test loss is 0.000673797518335317\n",
      "Batch: 24300,train loss is: 0.0005285696148821248\n",
      "test loss is 0.0007883924219453634\n",
      "Batch: 24400,train loss is: 0.0012962603207158396\n",
      "test loss is 0.0006603270689861547\n",
      "Batch: 24500,train loss is: 0.0006717800529050558\n",
      "test loss is 0.0006486951241843138\n",
      "Batch: 24600,train loss is: 0.00038716816263159663\n",
      "test loss is 0.0006858365635463496\n",
      "Batch: 24700,train loss is: 0.0002789308895860013\n",
      "test loss is 0.0006671896079040254\n",
      "Batch: 24800,train loss is: 0.000698647043468195\n",
      "test loss is 0.0006732747166955427\n",
      "Batch: 24900,train loss is: 0.00019401693150601423\n",
      "test loss is 0.0006613621528249829\n",
      "Batch: 25000,train loss is: 0.0005434485319202414\n",
      "test loss is 0.0006581959051212285\n",
      "Batch: 25100,train loss is: 0.0004410889879733523\n",
      "test loss is 0.0006683398769720068\n",
      "Batch: 25200,train loss is: 0.0007250426199169791\n",
      "test loss is 0.000670423694878573\n",
      "Batch: 25300,train loss is: 0.0007690127440698173\n",
      "test loss is 0.000715553169541515\n",
      "Batch: 25400,train loss is: 0.00040998240397304147\n",
      "test loss is 0.0006595203661337058\n",
      "Batch: 25500,train loss is: 0.0005852586573093357\n",
      "test loss is 0.0006696426596275764\n",
      "Batch: 25600,train loss is: 0.000621103320189936\n",
      "test loss is 0.0006660717397944974\n",
      "Batch: 25700,train loss is: 0.0005740281837327709\n",
      "test loss is 0.0006857360027199552\n",
      "Batch: 25800,train loss is: 0.00034591758752474755\n",
      "test loss is 0.0006535749036407533\n",
      "Batch: 25900,train loss is: 0.00028290900830222325\n",
      "test loss is 0.0007103157014053175\n",
      "Batch: 26000,train loss is: 0.0015025978970422391\n",
      "test loss is 0.0007122370933830127\n",
      "Batch: 26100,train loss is: 0.000782876469616131\n",
      "test loss is 0.0006524992351972808\n",
      "Batch: 26200,train loss is: 0.0006111159465067076\n",
      "test loss is 0.0006501453219595984\n",
      "Batch: 26300,train loss is: 0.001015603237967169\n",
      "test loss is 0.0006701620761272344\n",
      "Batch: 26400,train loss is: 0.0005624118164123291\n",
      "test loss is 0.000650938082592194\n",
      "Batch: 26500,train loss is: 0.0005336017504372118\n",
      "test loss is 0.0006892024098134475\n",
      "Batch: 26600,train loss is: 0.0013015582488491906\n",
      "test loss is 0.0007446286081949059\n",
      "Batch: 26700,train loss is: 0.0005020037475620211\n",
      "test loss is 0.0007414685363696726\n",
      "Batch: 26800,train loss is: 0.0011437247488919563\n",
      "test loss is 0.0006758849088256461\n",
      "Batch: 26900,train loss is: 0.00045773779628072815\n",
      "test loss is 0.0006664582659209051\n",
      "Batch: 27000,train loss is: 0.0006308269930101247\n",
      "test loss is 0.0007725590680272989\n",
      "Batch: 27100,train loss is: 0.0006562825702482265\n",
      "test loss is 0.0007172651767370354\n",
      "Batch: 27200,train loss is: 0.0006425391679120411\n",
      "test loss is 0.000695742811709613\n",
      "Batch: 27300,train loss is: 0.00045611905973212555\n",
      "test loss is 0.0006770508068704605\n",
      "Batch: 27400,train loss is: 0.0006647240895754437\n",
      "test loss is 0.0006679625582776628\n",
      "Batch: 27500,train loss is: 0.00045788112226975236\n",
      "test loss is 0.0007297053860399931\n",
      "Batch: 27600,train loss is: 0.0005028600523108012\n",
      "test loss is 0.0006883883073174192\n",
      "Batch: 27700,train loss is: 0.0015197765994670227\n",
      "test loss is 0.0006707268258072807\n",
      "Batch: 27800,train loss is: 0.0007675694365300103\n",
      "test loss is 0.0006892457532890771\n",
      "Batch: 27900,train loss is: 0.0012787274418094585\n",
      "test loss is 0.0006606862591444982\n",
      "Batch: 28000,train loss is: 0.0003878217783313623\n",
      "test loss is 0.0006607838077094693\n",
      "Batch: 28100,train loss is: 0.000817539785205653\n",
      "test loss is 0.0006747676989706512\n",
      "Batch: 28200,train loss is: 0.0005381608902138769\n",
      "test loss is 0.0006457179791794102\n",
      "Batch: 28300,train loss is: 0.0016756056852562172\n",
      "test loss is 0.0006755688083718537\n",
      "Batch: 28400,train loss is: 0.0005397445120266477\n",
      "test loss is 0.0006536606945725096\n",
      "Batch: 28500,train loss is: 0.0014007176702479057\n",
      "test loss is 0.0007032301426793752\n",
      "Batch: 28600,train loss is: 0.00034467554213884333\n",
      "test loss is 0.0006676623372688441\n",
      "Batch: 28700,train loss is: 0.0005597543996629698\n",
      "test loss is 0.0006591224573109239\n",
      "Batch: 28800,train loss is: 0.0010529645083126918\n",
      "test loss is 0.0006720670953160647\n",
      "Batch: 28900,train loss is: 0.00045279294392695053\n",
      "test loss is 0.0006513999361075901\n",
      "Batch: 29000,train loss is: 0.00048737017764873024\n",
      "test loss is 0.0006586908384921019\n",
      "Batch: 29100,train loss is: 0.0004954237469147568\n",
      "test loss is 0.0006618868033560239\n",
      "Batch: 29200,train loss is: 0.000414811140414536\n",
      "test loss is 0.0006725907934124585\n",
      "Batch: 29300,train loss is: 0.0009395266960828349\n",
      "test loss is 0.0006560065031676187\n",
      "Batch: 29400,train loss is: 0.0007012428269400982\n",
      "test loss is 0.0007487283978160229\n",
      "Batch: 29500,train loss is: 0.0004792967884053213\n",
      "test loss is 0.0006911648049016384\n",
      "Batch: 29600,train loss is: 0.0005168903299994092\n",
      "test loss is 0.0006456948830363558\n",
      "Batch: 29700,train loss is: 0.0003510974096233063\n",
      "test loss is 0.0006670720263036408\n",
      "Batch: 29800,train loss is: 0.000793535431249424\n",
      "test loss is 0.0006500923856919624\n",
      "Batch: 29900,train loss is: 0.0028572827851949495\n",
      "test loss is 0.0006564311227435208\n",
      "Batch: 30000,train loss is: 0.00035776339628439206\n",
      "test loss is 0.0007049245037736718\n",
      "Batch: 30100,train loss is: 0.0005808663366922446\n",
      "test loss is 0.0006945834036874269\n",
      "Batch: 30200,train loss is: 0.00037074583106196427\n",
      "test loss is 0.0006675049922034759\n",
      "Batch: 30300,train loss is: 0.0002744837825407249\n",
      "test loss is 0.0006522705132622316\n",
      "Batch: 30400,train loss is: 0.0003123221784438688\n",
      "test loss is 0.0006603978563248436\n",
      "Batch: 30500,train loss is: 0.0003139975691395593\n",
      "test loss is 0.0006952086034580624\n",
      "Batch: 30600,train loss is: 0.0002596400677243104\n",
      "test loss is 0.0006612986898292538\n",
      "Batch: 30700,train loss is: 0.0003729961113173221\n",
      "test loss is 0.0007420451098368461\n",
      "Batch: 30800,train loss is: 0.00025984146838921415\n",
      "test loss is 0.0007192812717646553\n",
      "Batch: 30900,train loss is: 0.00033206022833010365\n",
      "test loss is 0.0006630627589184857\n",
      "Batch: 31000,train loss is: 0.0004705348168734933\n",
      "test loss is 0.0006747417180036538\n",
      "Batch: 31100,train loss is: 0.00032061656464524226\n",
      "test loss is 0.0007489422724937635\n",
      "Batch: 31200,train loss is: 0.00045610515541150257\n",
      "test loss is 0.0006636319502957201\n",
      "Batch: 31300,train loss is: 0.0004405962562991293\n",
      "test loss is 0.0006620688211072654\n",
      "Batch: 31400,train loss is: 0.000734892610469486\n",
      "test loss is 0.000657461090857251\n",
      "Batch: 31500,train loss is: 0.0004477433801176721\n",
      "test loss is 0.0006677406749574147\n",
      "Batch: 31600,train loss is: 0.00040871963197297855\n",
      "test loss is 0.0006664685660586205\n",
      "Batch: 31700,train loss is: 0.0005567263741988029\n",
      "test loss is 0.0006872262557676085\n",
      "Batch: 31800,train loss is: 0.0012605790461730994\n",
      "test loss is 0.0006637594221566632\n",
      "Batch: 31900,train loss is: 0.0006938129071809816\n",
      "test loss is 0.000671300702236904\n",
      "Batch: 32000,train loss is: 0.0037080978446710363\n",
      "test loss is 0.0006689075586510261\n",
      "Batch: 32100,train loss is: 0.0003682250374148712\n",
      "test loss is 0.0006866809804382002\n",
      "Batch: 32200,train loss is: 0.00032191091658003503\n",
      "test loss is 0.0006660686874384215\n",
      "Batch: 32300,train loss is: 0.00034084208649139434\n",
      "test loss is 0.0006515493871324154\n",
      "Batch: 32400,train loss is: 0.00036233744631802004\n",
      "test loss is 0.0006478712350553042\n",
      "Batch: 32500,train loss is: 0.0008631373734386607\n",
      "test loss is 0.0006949918658687807\n",
      "Batch: 32600,train loss is: 0.0004666224546641962\n",
      "test loss is 0.00067695993880152\n",
      "Batch: 32700,train loss is: 0.0005180974929091971\n",
      "test loss is 0.0007056779317761794\n",
      "Batch: 32800,train loss is: 0.0004696537589237048\n",
      "test loss is 0.00070418518791303\n",
      "Batch: 32900,train loss is: 0.0006526779041831299\n",
      "test loss is 0.0006599818653546755\n",
      "Batch: 33000,train loss is: 0.000416027725572546\n",
      "test loss is 0.0006625928337686234\n",
      "Batch: 33100,train loss is: 0.0004205879679734946\n",
      "test loss is 0.0006724602385541719\n",
      "Batch: 33200,train loss is: 0.00043660134379331565\n",
      "test loss is 0.0007315292887911067\n",
      "Batch: 33300,train loss is: 0.000528535337922259\n",
      "test loss is 0.0006571405005833123\n",
      "Batch: 33400,train loss is: 0.0004725011733734796\n",
      "test loss is 0.0006679392267490584\n",
      "Batch: 33500,train loss is: 0.0003389546173968089\n",
      "test loss is 0.0006620938019644502\n",
      "Batch: 33600,train loss is: 0.0005444044109735321\n",
      "test loss is 0.0006568615568360632\n",
      "Batch: 33700,train loss is: 0.0005240392473630797\n",
      "test loss is 0.0006748652106805947\n",
      "Batch: 33800,train loss is: 0.0007330362645759196\n",
      "test loss is 0.0006562636308583676\n",
      "Batch: 33900,train loss is: 0.0006258005892754781\n",
      "test loss is 0.0006578034363935826\n",
      "-----------------------Epoch: 18----------------------------------\n",
      "Batch: 0,train loss is: 0.00029384503260114703\n",
      "test loss is 0.0006761035415106239\n",
      "Batch: 100,train loss is: 0.0011764165615600105\n",
      "test loss is 0.0006719656687628964\n",
      "Batch: 200,train loss is: 0.0003716813845085208\n",
      "test loss is 0.000687251134820868\n",
      "Batch: 300,train loss is: 0.0002818178287417354\n",
      "test loss is 0.0006636418162615101\n",
      "Batch: 400,train loss is: 0.00043812113419178407\n",
      "test loss is 0.0006488503780163555\n",
      "Batch: 500,train loss is: 0.00046198060687850575\n",
      "test loss is 0.0006766209138180092\n",
      "Batch: 600,train loss is: 0.00023141239421775958\n",
      "test loss is 0.0006932610494376441\n",
      "Batch: 700,train loss is: 0.0003711070735529799\n",
      "test loss is 0.0006535008434158211\n",
      "Batch: 800,train loss is: 0.0004156671297731605\n",
      "test loss is 0.0006584507956017313\n",
      "Batch: 900,train loss is: 0.0003232841906816271\n",
      "test loss is 0.0006634146612275688\n",
      "Batch: 1000,train loss is: 0.00043142591225466085\n",
      "test loss is 0.0006906766422314847\n",
      "Batch: 1100,train loss is: 0.0017631116807921403\n",
      "test loss is 0.0006521340631918935\n",
      "Batch: 1200,train loss is: 0.0002614326309050226\n",
      "test loss is 0.0006752891213299689\n",
      "Batch: 1300,train loss is: 0.0005228506109624882\n",
      "test loss is 0.0006580679113580377\n",
      "Batch: 1400,train loss is: 0.00041100153408167876\n",
      "test loss is 0.0006822256661915646\n",
      "Batch: 1500,train loss is: 0.0003771471656098457\n",
      "test loss is 0.0006908848696167814\n",
      "Batch: 1600,train loss is: 0.0006733890300613603\n",
      "test loss is 0.0007282152570092547\n",
      "Batch: 1700,train loss is: 0.0011587599479968736\n",
      "test loss is 0.0006508678935709615\n",
      "Batch: 1800,train loss is: 0.0003529000401134002\n",
      "test loss is 0.0006535436928805001\n",
      "Batch: 1900,train loss is: 0.0003969537138137301\n",
      "test loss is 0.00069406967869194\n",
      "Batch: 2000,train loss is: 0.0006679338837593042\n",
      "test loss is 0.0008023224441071411\n",
      "Batch: 2100,train loss is: 0.0006459501815956918\n",
      "test loss is 0.0008120363988535472\n",
      "Batch: 2200,train loss is: 0.0007045592833441944\n",
      "test loss is 0.0006655823092104536\n",
      "Batch: 2300,train loss is: 0.0004595482495175724\n",
      "test loss is 0.000680771703449254\n",
      "Batch: 2400,train loss is: 0.00043326045216685293\n",
      "test loss is 0.0006633392216683016\n",
      "Batch: 2500,train loss is: 0.0005345567343107044\n",
      "test loss is 0.0006548607460550523\n",
      "Batch: 2600,train loss is: 0.0004079889244296307\n",
      "test loss is 0.0006608150438532569\n",
      "Batch: 2700,train loss is: 0.0005624006268921223\n",
      "test loss is 0.0006424243241716738\n",
      "Batch: 2800,train loss is: 0.0004465977040454344\n",
      "test loss is 0.000670302161010558\n",
      "Batch: 2900,train loss is: 0.0004130557795208616\n",
      "test loss is 0.0006552669356539503\n",
      "Batch: 3000,train loss is: 0.00047310235111935466\n",
      "test loss is 0.0006534651254097608\n",
      "Batch: 3100,train loss is: 0.0005995348056545823\n",
      "test loss is 0.000693686659593491\n",
      "Batch: 3200,train loss is: 0.0010223093392395825\n",
      "test loss is 0.0006731121759660164\n",
      "Batch: 3300,train loss is: 0.0005672819635976535\n",
      "test loss is 0.0006668523490912393\n",
      "Batch: 3400,train loss is: 0.00036667777007494943\n",
      "test loss is 0.0006589590676078147\n",
      "Batch: 3500,train loss is: 0.00022514517500503078\n",
      "test loss is 0.0006531629966679816\n",
      "Batch: 3600,train loss is: 0.0005244122980471957\n",
      "test loss is 0.0007039916946107421\n",
      "Batch: 3700,train loss is: 0.00034458816019584146\n",
      "test loss is 0.0006644097548532598\n",
      "Batch: 3800,train loss is: 0.0004280377989265054\n",
      "test loss is 0.0006774513707088322\n",
      "Batch: 3900,train loss is: 0.0005710610895653198\n",
      "test loss is 0.0006640901635178714\n",
      "Batch: 4000,train loss is: 0.00041309312163671905\n",
      "test loss is 0.0006575667432064235\n",
      "Batch: 4100,train loss is: 0.000737215798808802\n",
      "test loss is 0.0006799267345741586\n",
      "Batch: 4200,train loss is: 0.0002599197354870807\n",
      "test loss is 0.0006815427427393483\n",
      "Batch: 4300,train loss is: 0.00041383434273193937\n",
      "test loss is 0.0006485055193791839\n",
      "Batch: 4400,train loss is: 0.000278678699070662\n",
      "test loss is 0.0006809069868676137\n",
      "Batch: 4500,train loss is: 0.0007341285115086579\n",
      "test loss is 0.0006774546325491862\n",
      "Batch: 4600,train loss is: 0.0005733152352411259\n",
      "test loss is 0.000668292668053386\n",
      "Batch: 4700,train loss is: 0.0005612326125923948\n",
      "test loss is 0.0006847346414096368\n",
      "Batch: 4800,train loss is: 0.0013367931047198538\n",
      "test loss is 0.0006508861506474526\n",
      "Batch: 4900,train loss is: 0.0004623805945292651\n",
      "test loss is 0.0006633942116751841\n",
      "Batch: 5000,train loss is: 0.0005410511060367253\n",
      "test loss is 0.0006768668047686993\n",
      "Batch: 5100,train loss is: 0.00044743855339269845\n",
      "test loss is 0.0007138904391910923\n",
      "Batch: 5200,train loss is: 0.0003005252255421242\n",
      "test loss is 0.0006753031276239686\n",
      "Batch: 5300,train loss is: 0.0004524269084434723\n",
      "test loss is 0.0006790952552326099\n",
      "Batch: 5400,train loss is: 0.0007056292563619633\n",
      "test loss is 0.0006528526593860338\n",
      "Batch: 5500,train loss is: 0.0004162158460331645\n",
      "test loss is 0.0006799952313445062\n",
      "Batch: 5600,train loss is: 0.0005549080788028824\n",
      "test loss is 0.0006608094386835899\n",
      "Batch: 5700,train loss is: 0.00047096587352549684\n",
      "test loss is 0.0006680584925580984\n",
      "Batch: 5800,train loss is: 0.00044275802462456163\n",
      "test loss is 0.0006602590564675508\n",
      "Batch: 5900,train loss is: 0.0033025801100676822\n",
      "test loss is 0.0006646619517441639\n",
      "Batch: 6000,train loss is: 0.0016578815882575468\n",
      "test loss is 0.0006792056290112755\n",
      "Batch: 6100,train loss is: 0.00040716783384121873\n",
      "test loss is 0.0006504103559752442\n",
      "Batch: 6200,train loss is: 0.0005426799717581829\n",
      "test loss is 0.0006618021575584957\n",
      "Batch: 6300,train loss is: 0.001043720621173904\n",
      "test loss is 0.0006764481681212586\n",
      "Batch: 6400,train loss is: 0.0005219458889929044\n",
      "test loss is 0.0007027376221290833\n",
      "Batch: 6500,train loss is: 0.00030236537371168933\n",
      "test loss is 0.0006637943828124628\n",
      "Batch: 6600,train loss is: 0.001952227680203214\n",
      "test loss is 0.0006538894908714597\n",
      "Batch: 6700,train loss is: 0.0003065553412704864\n",
      "test loss is 0.0006664184643598947\n",
      "Batch: 6800,train loss is: 0.0007051045098037097\n",
      "test loss is 0.0006589896694658993\n",
      "Batch: 6900,train loss is: 0.0006589575128763848\n",
      "test loss is 0.0006704875585587548\n",
      "Batch: 7000,train loss is: 0.0005513456838374075\n",
      "test loss is 0.0006771565912921128\n",
      "Batch: 7100,train loss is: 0.0010983074275742\n",
      "test loss is 0.0006670664249515665\n",
      "Batch: 7200,train loss is: 0.0004520893521645495\n",
      "test loss is 0.0006733647288700727\n",
      "Batch: 7300,train loss is: 0.000706653353804082\n",
      "test loss is 0.0006852454019591251\n",
      "Batch: 7400,train loss is: 0.0003527837674040947\n",
      "test loss is 0.0006585078254032217\n",
      "Batch: 7500,train loss is: 0.0006341957902007224\n",
      "test loss is 0.0006810511259229971\n",
      "Batch: 7600,train loss is: 0.00040582197221969153\n",
      "test loss is 0.0006438253928481404\n",
      "Batch: 7700,train loss is: 0.0008855616046045837\n",
      "test loss is 0.000669741196468552\n",
      "Batch: 7800,train loss is: 0.0002770506729296631\n",
      "test loss is 0.0006516217705364358\n",
      "Batch: 7900,train loss is: 0.000509523593414503\n",
      "test loss is 0.0006758860512821561\n",
      "Batch: 8000,train loss is: 0.0003791555297307301\n",
      "test loss is 0.0006927126070010246\n",
      "Batch: 8100,train loss is: 0.0005680578033515054\n",
      "test loss is 0.0006790852852963682\n",
      "Batch: 8200,train loss is: 0.0010681309797218213\n",
      "test loss is 0.0006753091511246404\n",
      "Batch: 8300,train loss is: 0.00032172080720401094\n",
      "test loss is 0.0006736088134685693\n",
      "Batch: 8400,train loss is: 0.0005515772016271328\n",
      "test loss is 0.0006628365659290366\n",
      "Batch: 8500,train loss is: 0.0005975229524635546\n",
      "test loss is 0.0006737020760692183\n",
      "Batch: 8600,train loss is: 0.0006627920518869122\n",
      "test loss is 0.0006806826103198016\n",
      "Batch: 8700,train loss is: 0.00044885330769530227\n",
      "test loss is 0.0007191704446472412\n",
      "Batch: 8800,train loss is: 0.0003646073901812966\n",
      "test loss is 0.000655851390232123\n",
      "Batch: 8900,train loss is: 0.0010340056917368533\n",
      "test loss is 0.0006542630046967242\n",
      "Batch: 9000,train loss is: 0.0002592429243354034\n",
      "test loss is 0.0006770853805506182\n",
      "Batch: 9100,train loss is: 0.00031242675082619785\n",
      "test loss is 0.0006533492468860146\n",
      "Batch: 9200,train loss is: 0.0007645176307282192\n",
      "test loss is 0.0007190429577510717\n",
      "Batch: 9300,train loss is: 0.0005316757687941599\n",
      "test loss is 0.0007041265765094576\n",
      "Batch: 9400,train loss is: 0.0004508473010423852\n",
      "test loss is 0.0006839515020488961\n",
      "Batch: 9500,train loss is: 0.0003583111231728056\n",
      "test loss is 0.0006880485866668369\n",
      "Batch: 9600,train loss is: 0.0003927453527133787\n",
      "test loss is 0.00064896310054364\n",
      "Batch: 9700,train loss is: 0.0003713447638005747\n",
      "test loss is 0.0006633365148162089\n",
      "Batch: 9800,train loss is: 0.0002554681178853232\n",
      "test loss is 0.0006602269069429328\n",
      "Batch: 9900,train loss is: 0.0004628245487162561\n",
      "test loss is 0.0006833947972127193\n",
      "Batch: 10000,train loss is: 0.00043597472368714157\n",
      "test loss is 0.0006475873803450748\n",
      "Batch: 10100,train loss is: 0.000937177661644526\n",
      "test loss is 0.0006694611984833991\n",
      "Batch: 10200,train loss is: 0.0016906267609137033\n",
      "test loss is 0.0007495034824482424\n",
      "Batch: 10300,train loss is: 0.0004205512200864221\n",
      "test loss is 0.0006608000414792983\n",
      "Batch: 10400,train loss is: 0.000271506226283731\n",
      "test loss is 0.0006569514604412207\n",
      "Batch: 10500,train loss is: 0.0007769880814629389\n",
      "test loss is 0.0006665870911356074\n",
      "Batch: 10600,train loss is: 0.0003061071098875043\n",
      "test loss is 0.0007183489268473292\n",
      "Batch: 10700,train loss is: 0.000591021427859298\n",
      "test loss is 0.0006557777467673801\n",
      "Batch: 10800,train loss is: 0.0006093523247148988\n",
      "test loss is 0.0006670037506971833\n",
      "Batch: 10900,train loss is: 0.0006976147128405842\n",
      "test loss is 0.0006625877415574401\n",
      "Batch: 11000,train loss is: 0.0007516287258646073\n",
      "test loss is 0.0006835162992381403\n",
      "Batch: 11100,train loss is: 0.0009214184482362413\n",
      "test loss is 0.0006504684070471748\n",
      "Batch: 11200,train loss is: 0.0006825655744305769\n",
      "test loss is 0.0007033355976893731\n",
      "Batch: 11300,train loss is: 0.00029475595058093467\n",
      "test loss is 0.0006704854676375105\n",
      "Batch: 11400,train loss is: 0.0007231356625662204\n",
      "test loss is 0.000660675500704148\n",
      "Batch: 11500,train loss is: 0.0004958458217861624\n",
      "test loss is 0.0006673576129578093\n",
      "Batch: 11600,train loss is: 0.001802372434902726\n",
      "test loss is 0.0006590768136008847\n",
      "Batch: 11700,train loss is: 0.000300692210568673\n",
      "test loss is 0.0006480970538062205\n",
      "Batch: 11800,train loss is: 0.000589763528959943\n",
      "test loss is 0.0006574396944909577\n",
      "Batch: 11900,train loss is: 0.00048799777027391436\n",
      "test loss is 0.000668091604662523\n",
      "Batch: 12000,train loss is: 0.0003038623234538881\n",
      "test loss is 0.0006595151844349242\n",
      "Batch: 12100,train loss is: 0.0015292067594769026\n",
      "test loss is 0.0006704567240495938\n",
      "Batch: 12200,train loss is: 0.0006278336865223412\n",
      "test loss is 0.0006874376480291407\n",
      "Batch: 12300,train loss is: 0.0004513252482791252\n",
      "test loss is 0.0006723789432978271\n",
      "Batch: 12400,train loss is: 0.00037690372226072924\n",
      "test loss is 0.0006603917992211032\n",
      "Batch: 12500,train loss is: 0.00048531107531775056\n",
      "test loss is 0.0006855505528724724\n",
      "Batch: 12600,train loss is: 0.0004962941969438937\n",
      "test loss is 0.0006743622044995675\n",
      "Batch: 12700,train loss is: 0.0003635746656620121\n",
      "test loss is 0.000653501863913465\n",
      "Batch: 12800,train loss is: 0.00036198074237503913\n",
      "test loss is 0.0006536552397822962\n",
      "Batch: 12900,train loss is: 0.0005813985441885511\n",
      "test loss is 0.0006682850689813144\n",
      "Batch: 13000,train loss is: 0.00020396520558528858\n",
      "test loss is 0.000655868018137476\n",
      "Batch: 13100,train loss is: 0.0020327428782374005\n",
      "test loss is 0.0006519374611994524\n",
      "Batch: 13200,train loss is: 0.0008398894969859192\n",
      "test loss is 0.0007846597656279737\n",
      "Batch: 13300,train loss is: 0.0006248563519234912\n",
      "test loss is 0.000666565024114731\n",
      "Batch: 13400,train loss is: 0.00035974670072573957\n",
      "test loss is 0.0006862905026319537\n",
      "Batch: 13500,train loss is: 0.0005550352448626673\n",
      "test loss is 0.0006844830649246825\n",
      "Batch: 13600,train loss is: 0.0005442731308396677\n",
      "test loss is 0.0006766047157545764\n",
      "Batch: 13700,train loss is: 0.0006518745804037227\n",
      "test loss is 0.0006480591890909924\n",
      "Batch: 13800,train loss is: 0.0004051041793994691\n",
      "test loss is 0.0006510861922200707\n",
      "Batch: 13900,train loss is: 0.0002925906843318445\n",
      "test loss is 0.0007202583891807701\n",
      "Batch: 14000,train loss is: 0.00036518246939525766\n",
      "test loss is 0.0006808556341786087\n",
      "Batch: 14100,train loss is: 0.0007200456588163692\n",
      "test loss is 0.0007000367076967562\n",
      "Batch: 14200,train loss is: 0.0005682781571600644\n",
      "test loss is 0.0006472245447923775\n",
      "Batch: 14300,train loss is: 0.00024062735607828918\n",
      "test loss is 0.0006537637913161689\n",
      "Batch: 14400,train loss is: 0.0012447993518242412\n",
      "test loss is 0.0006518296505706628\n",
      "Batch: 14500,train loss is: 0.0005985745648301436\n",
      "test loss is 0.0006532893341322237\n",
      "Batch: 14600,train loss is: 0.0008880662102326175\n",
      "test loss is 0.0006659373650651844\n",
      "Batch: 14700,train loss is: 0.0004631557636609251\n",
      "test loss is 0.0007156401475266966\n",
      "Batch: 14800,train loss is: 0.0007701056135007467\n",
      "test loss is 0.0006734693089460337\n",
      "Batch: 14900,train loss is: 0.0004391196397910611\n",
      "test loss is 0.0007074472666196741\n",
      "Batch: 15000,train loss is: 0.0006339363490668432\n",
      "test loss is 0.0006475768081380846\n",
      "Batch: 15100,train loss is: 0.0007057566836595384\n",
      "test loss is 0.0006788386812357577\n",
      "Batch: 15200,train loss is: 0.0008065220269640312\n",
      "test loss is 0.000663430049150353\n",
      "Batch: 15300,train loss is: 0.00034200208528181725\n",
      "test loss is 0.000655160088509377\n",
      "Batch: 15400,train loss is: 0.00045888415278220497\n",
      "test loss is 0.0006712067682499844\n",
      "Batch: 15500,train loss is: 0.0006724188557502785\n",
      "test loss is 0.0006488443603245121\n",
      "Batch: 15600,train loss is: 0.0008331511768195651\n",
      "test loss is 0.0006744627302416842\n",
      "Batch: 15700,train loss is: 0.0003841742666642101\n",
      "test loss is 0.0006559528767224085\n",
      "Batch: 15800,train loss is: 0.0006177236020790907\n",
      "test loss is 0.0006427989201379192\n",
      "Batch: 15900,train loss is: 0.0007951896792630623\n",
      "test loss is 0.0006646295637859456\n",
      "Batch: 16000,train loss is: 0.00036605866317651614\n",
      "test loss is 0.0006626068083760312\n",
      "Batch: 16100,train loss is: 0.0005357473587072524\n",
      "test loss is 0.0006578385337948701\n",
      "Batch: 16200,train loss is: 0.00037678381236350643\n",
      "test loss is 0.0006479915783148526\n",
      "Batch: 16300,train loss is: 0.0008996896746424048\n",
      "test loss is 0.0006864768465013677\n",
      "Batch: 16400,train loss is: 0.0003107360902294065\n",
      "test loss is 0.0006502097907182535\n",
      "Batch: 16500,train loss is: 0.0006737709817598964\n",
      "test loss is 0.0006723277766736071\n",
      "Batch: 16600,train loss is: 0.0003217285340521449\n",
      "test loss is 0.0006523685906266039\n",
      "Batch: 16700,train loss is: 0.0008218026945515435\n",
      "test loss is 0.0006652451292204326\n",
      "Batch: 16800,train loss is: 0.0005021032571921536\n",
      "test loss is 0.0006872079627601921\n",
      "Batch: 16900,train loss is: 0.0006983744129917695\n",
      "test loss is 0.0006645370041090637\n",
      "Batch: 17000,train loss is: 0.0005878886280377005\n",
      "test loss is 0.0006461739191713811\n",
      "Batch: 17100,train loss is: 0.0004980427920639645\n",
      "test loss is 0.0006576186308909941\n",
      "Batch: 17200,train loss is: 0.000443599708154881\n",
      "test loss is 0.0006617968716883945\n",
      "Batch: 17300,train loss is: 0.0007715552737726057\n",
      "test loss is 0.0006864830120440638\n",
      "Batch: 17400,train loss is: 0.000370729060800276\n",
      "test loss is 0.0006680080210458212\n",
      "Batch: 17500,train loss is: 0.0005611114126812882\n",
      "test loss is 0.0006618897218287375\n",
      "Batch: 17600,train loss is: 0.0004170652535919639\n",
      "test loss is 0.0006680191257275991\n",
      "Batch: 17700,train loss is: 0.0003355863147533343\n",
      "test loss is 0.0006947916671214686\n",
      "Batch: 17800,train loss is: 0.0014829650836210113\n",
      "test loss is 0.0006730141360884213\n",
      "Batch: 17900,train loss is: 0.0004750160766472264\n",
      "test loss is 0.0006913155107969688\n",
      "Batch: 18000,train loss is: 0.00027698059725993635\n",
      "test loss is 0.0006441548037457765\n",
      "Batch: 18100,train loss is: 0.00037480905401062115\n",
      "test loss is 0.0006533567050355192\n",
      "Batch: 18200,train loss is: 0.0005808138901980672\n",
      "test loss is 0.0006661678348606831\n",
      "Batch: 18300,train loss is: 0.0012694843452479374\n",
      "test loss is 0.0006639496671831986\n",
      "Batch: 18400,train loss is: 0.0003880825481732538\n",
      "test loss is 0.0006543274495144383\n",
      "Batch: 18500,train loss is: 0.0009007757395410569\n",
      "test loss is 0.0006636396246026315\n",
      "Batch: 18600,train loss is: 0.0004458323353823548\n",
      "test loss is 0.0006579544769913922\n",
      "Batch: 18700,train loss is: 0.0005624978048498812\n",
      "test loss is 0.0006762158343272427\n",
      "Batch: 18800,train loss is: 0.0005457748170757363\n",
      "test loss is 0.0006517917540213127\n",
      "Batch: 18900,train loss is: 0.000540031363263365\n",
      "test loss is 0.0006772728442822461\n",
      "Batch: 19000,train loss is: 0.0005648624799963805\n",
      "test loss is 0.0006559666996644375\n",
      "Batch: 19100,train loss is: 0.0003829600436137755\n",
      "test loss is 0.0006823177003431179\n",
      "Batch: 19200,train loss is: 0.00050495705172497\n",
      "test loss is 0.0006649192584871051\n",
      "Batch: 19300,train loss is: 0.0006087486348942598\n",
      "test loss is 0.0006452602082552178\n",
      "Batch: 19400,train loss is: 0.000265223398610067\n",
      "test loss is 0.0006511608316760606\n",
      "Batch: 19500,train loss is: 0.0005057734453963062\n",
      "test loss is 0.000688466905995808\n",
      "Batch: 19600,train loss is: 0.0003847045564924321\n",
      "test loss is 0.0006665963788424437\n",
      "Batch: 19700,train loss is: 0.0006888040312929775\n",
      "test loss is 0.0007359809228376631\n",
      "Batch: 19800,train loss is: 0.0005607670773345463\n",
      "test loss is 0.0006679552212601064\n",
      "Batch: 19900,train loss is: 0.00036281621544617166\n",
      "test loss is 0.0006792271667694675\n",
      "Batch: 20000,train loss is: 0.00066412515866491\n",
      "test loss is 0.0006563291020174382\n",
      "Batch: 20100,train loss is: 0.0005925550168961986\n",
      "test loss is 0.0006725659757298693\n",
      "Batch: 20200,train loss is: 0.0007357498410637567\n",
      "test loss is 0.0006771952623287859\n",
      "Batch: 20300,train loss is: 0.00028198147785252827\n",
      "test loss is 0.0006777591668717948\n",
      "Batch: 20400,train loss is: 0.0004744958154847516\n",
      "test loss is 0.0006862520355656745\n",
      "Batch: 20500,train loss is: 0.0006101818981574141\n",
      "test loss is 0.0006532699172337731\n",
      "Batch: 20600,train loss is: 0.0004786863735219099\n",
      "test loss is 0.0006883432026383937\n",
      "Batch: 20700,train loss is: 0.0007653770259891004\n",
      "test loss is 0.0006598514937020248\n",
      "Batch: 20800,train loss is: 0.0003241762615967714\n",
      "test loss is 0.0006922631934334236\n",
      "Batch: 20900,train loss is: 0.0005459334419328884\n",
      "test loss is 0.000706204766556935\n",
      "Batch: 21000,train loss is: 0.00021738294663466707\n",
      "test loss is 0.00064762060687621\n",
      "Batch: 21100,train loss is: 0.0006724910737797525\n",
      "test loss is 0.0006867875275067823\n",
      "Batch: 21200,train loss is: 0.00037475839880291576\n",
      "test loss is 0.0006926116091254531\n",
      "Batch: 21300,train loss is: 0.0003082518101289139\n",
      "test loss is 0.0006604477484978039\n",
      "Batch: 21400,train loss is: 0.0007741604521012391\n",
      "test loss is 0.0006542135782714729\n",
      "Batch: 21500,train loss is: 0.00023069433000630943\n",
      "test loss is 0.000672677096563777\n",
      "Batch: 21600,train loss is: 0.0007511428678544706\n",
      "test loss is 0.0006752823273466923\n",
      "Batch: 21700,train loss is: 0.0002838989244188876\n",
      "test loss is 0.0006485190141846203\n",
      "Batch: 21800,train loss is: 0.0008111909597178464\n",
      "test loss is 0.0007030061150486367\n",
      "Batch: 21900,train loss is: 0.0004128421658713513\n",
      "test loss is 0.0007390488305568486\n",
      "Batch: 22000,train loss is: 0.000658444345694479\n",
      "test loss is 0.0006551967845506272\n",
      "Batch: 22100,train loss is: 0.00036003166155266196\n",
      "test loss is 0.0006547540503335018\n",
      "Batch: 22200,train loss is: 0.00047724248928203926\n",
      "test loss is 0.0006622140593787719\n",
      "Batch: 22300,train loss is: 0.0005118276969553388\n",
      "test loss is 0.0006720320050111013\n",
      "Batch: 22400,train loss is: 0.00045229915076786073\n",
      "test loss is 0.0006722162129005733\n",
      "Batch: 22500,train loss is: 0.00048832502319085\n",
      "test loss is 0.0006472570627556684\n",
      "Batch: 22600,train loss is: 0.0009076631474601429\n",
      "test loss is 0.0006882633069389259\n",
      "Batch: 22700,train loss is: 0.00033076265961662365\n",
      "test loss is 0.0006501691485373482\n",
      "Batch: 22800,train loss is: 0.0004917904234774111\n",
      "test loss is 0.0006837804553697857\n",
      "Batch: 22900,train loss is: 0.0002958430373603549\n",
      "test loss is 0.0007354904377893176\n",
      "Batch: 23000,train loss is: 0.0005060108014365253\n",
      "test loss is 0.0006631024873552826\n",
      "Batch: 23100,train loss is: 0.0004279543989814891\n",
      "test loss is 0.0006433400748182787\n",
      "Batch: 23200,train loss is: 0.0002882975964012462\n",
      "test loss is 0.0006417045810210675\n",
      "Batch: 23300,train loss is: 0.0004366814066222877\n",
      "test loss is 0.0006502615397981406\n",
      "Batch: 23400,train loss is: 0.0005945533704044813\n",
      "test loss is 0.0006793279516747375\n",
      "Batch: 23500,train loss is: 0.001467030230817353\n",
      "test loss is 0.0006434629247024417\n",
      "Batch: 23600,train loss is: 0.0003093527459391932\n",
      "test loss is 0.0006553055466543346\n",
      "Batch: 23700,train loss is: 0.0009654899885491954\n",
      "test loss is 0.0006517000174740862\n",
      "Batch: 23800,train loss is: 0.0008100028786186035\n",
      "test loss is 0.000655642361129305\n",
      "Batch: 23900,train loss is: 0.0009263384671538106\n",
      "test loss is 0.0006726467623509013\n",
      "Batch: 24000,train loss is: 0.0007277190524372353\n",
      "test loss is 0.0007056625145323778\n",
      "Batch: 24100,train loss is: 0.0002889793254479958\n",
      "test loss is 0.000641791434783401\n",
      "Batch: 24200,train loss is: 0.0013463792754442904\n",
      "test loss is 0.0006676597853464555\n",
      "Batch: 24300,train loss is: 0.0005164822188170781\n",
      "test loss is 0.0007767245247991135\n",
      "Batch: 24400,train loss is: 0.00131286545320921\n",
      "test loss is 0.0006547060245405915\n",
      "Batch: 24500,train loss is: 0.0006791418447717019\n",
      "test loss is 0.0006422043429133399\n",
      "Batch: 24600,train loss is: 0.0003779855523616723\n",
      "test loss is 0.0006797075340035742\n",
      "Batch: 24700,train loss is: 0.0002781628696193842\n",
      "test loss is 0.0006612923949482531\n",
      "Batch: 24800,train loss is: 0.0006738493262916267\n",
      "test loss is 0.0006671461569163408\n",
      "Batch: 24900,train loss is: 0.00019360016400507522\n",
      "test loss is 0.0006545954447347154\n",
      "Batch: 25000,train loss is: 0.0005225535728305672\n",
      "test loss is 0.0006516353677642\n",
      "Batch: 25100,train loss is: 0.0004215847210008434\n",
      "test loss is 0.0006622692767849325\n",
      "Batch: 25200,train loss is: 0.0007270645218376465\n",
      "test loss is 0.000663978315876725\n",
      "Batch: 25300,train loss is: 0.0007812525741979433\n",
      "test loss is 0.0007110790104462344\n",
      "Batch: 25400,train loss is: 0.00041027189252762736\n",
      "test loss is 0.000654015960449395\n",
      "Batch: 25500,train loss is: 0.0005827524639992823\n",
      "test loss is 0.0006627471314356354\n",
      "Batch: 25600,train loss is: 0.0006158932045778201\n",
      "test loss is 0.0006609589085655069\n",
      "Batch: 25700,train loss is: 0.0005785617345876456\n",
      "test loss is 0.0006786110781464642\n",
      "Batch: 25800,train loss is: 0.0003403924510207844\n",
      "test loss is 0.000647509801236598\n",
      "Batch: 25900,train loss is: 0.00028025914747775916\n",
      "test loss is 0.0007048020424696284\n",
      "Batch: 26000,train loss is: 0.0014863658174297978\n",
      "test loss is 0.0007041824360440809\n",
      "Batch: 26100,train loss is: 0.0007680814578017767\n",
      "test loss is 0.0006464994539323539\n",
      "Batch: 26200,train loss is: 0.0006047910186445369\n",
      "test loss is 0.0006441856909204681\n",
      "Batch: 26300,train loss is: 0.0010048294782959715\n",
      "test loss is 0.0006638294793334266\n",
      "Batch: 26400,train loss is: 0.0005621105557647738\n",
      "test loss is 0.0006441626595465873\n",
      "Batch: 26500,train loss is: 0.0005535457088882338\n",
      "test loss is 0.0006872836154599286\n",
      "Batch: 26600,train loss is: 0.0012899527429794527\n",
      "test loss is 0.0007395152094181588\n",
      "Batch: 26700,train loss is: 0.0005055114901390851\n",
      "test loss is 0.000734271747534968\n",
      "Batch: 26800,train loss is: 0.0011410710300246848\n",
      "test loss is 0.0006703407221982619\n",
      "Batch: 26900,train loss is: 0.00045750563094650964\n",
      "test loss is 0.0006611739528110256\n",
      "Batch: 27000,train loss is: 0.0006330838337505596\n",
      "test loss is 0.0007649759117013639\n",
      "Batch: 27100,train loss is: 0.0006589556593799575\n",
      "test loss is 0.0007141065125275675\n",
      "Batch: 27200,train loss is: 0.0006312350507132173\n",
      "test loss is 0.0006877010485763368\n",
      "Batch: 27300,train loss is: 0.00044489992125998096\n",
      "test loss is 0.0006703187787206567\n",
      "Batch: 27400,train loss is: 0.0006332620815335554\n",
      "test loss is 0.0006612376915436518\n",
      "Batch: 27500,train loss is: 0.0004507114146880249\n",
      "test loss is 0.0007197487882012228\n",
      "Batch: 27600,train loss is: 0.0004896431941900669\n",
      "test loss is 0.0006814182881746368\n",
      "Batch: 27700,train loss is: 0.001522636100333551\n",
      "test loss is 0.000663529923906673\n",
      "Batch: 27800,train loss is: 0.000769627583896738\n",
      "test loss is 0.0006815640066488766\n",
      "Batch: 27900,train loss is: 0.0012380339304170426\n",
      "test loss is 0.0006525879463693203\n",
      "Batch: 28000,train loss is: 0.00038326940233532244\n",
      "test loss is 0.0006546957827518514\n",
      "Batch: 28100,train loss is: 0.000797669398713489\n",
      "test loss is 0.0006674295022341073\n",
      "Batch: 28200,train loss is: 0.0005450174473344916\n",
      "test loss is 0.0006398381935056685\n",
      "Batch: 28300,train loss is: 0.0016632079701920355\n",
      "test loss is 0.000669103692700751\n",
      "Batch: 28400,train loss is: 0.0005541783125380408\n",
      "test loss is 0.00064742159907733\n",
      "Batch: 28500,train loss is: 0.0013602178583873154\n",
      "test loss is 0.0006994734232234908\n",
      "Batch: 28600,train loss is: 0.0003407432528184454\n",
      "test loss is 0.0006608856756236002\n",
      "Batch: 28700,train loss is: 0.0005551387891677024\n",
      "test loss is 0.0006531301852569027\n",
      "Batch: 28800,train loss is: 0.001047625771958141\n",
      "test loss is 0.0006662235694001491\n",
      "Batch: 28900,train loss is: 0.00045145724065007734\n",
      "test loss is 0.0006445543906560241\n",
      "Batch: 29000,train loss is: 0.000482611017498454\n",
      "test loss is 0.0006507879741082808\n",
      "Batch: 29100,train loss is: 0.000486652046784165\n",
      "test loss is 0.0006562754449361923\n",
      "Batch: 29200,train loss is: 0.00041514305876688473\n",
      "test loss is 0.0006643534224481249\n",
      "Batch: 29300,train loss is: 0.0009322847443520496\n",
      "test loss is 0.0006500331339836766\n",
      "Batch: 29400,train loss is: 0.0006737233662300328\n",
      "test loss is 0.0007408778422225466\n",
      "Batch: 29500,train loss is: 0.0004860520104598545\n",
      "test loss is 0.0006868665142810213\n",
      "Batch: 29600,train loss is: 0.0005234982894368369\n",
      "test loss is 0.0006396021982960526\n",
      "Batch: 29700,train loss is: 0.00035062266329172393\n",
      "test loss is 0.0006607607226725745\n",
      "Batch: 29800,train loss is: 0.0007939425719027362\n",
      "test loss is 0.0006438712261725757\n",
      "Batch: 29900,train loss is: 0.0028104280418511764\n",
      "test loss is 0.0006498923056111471\n",
      "Batch: 30000,train loss is: 0.0003529240362225924\n",
      "test loss is 0.0006965868318019002\n",
      "Batch: 30100,train loss is: 0.000576983497182979\n",
      "test loss is 0.0006867194045337586\n",
      "Batch: 30200,train loss is: 0.0003660852023148425\n",
      "test loss is 0.0006610378602051452\n",
      "Batch: 30300,train loss is: 0.0002759825212146775\n",
      "test loss is 0.0006459852353766629\n",
      "Batch: 30400,train loss is: 0.0003065436792879014\n",
      "test loss is 0.000653867827503351\n",
      "Batch: 30500,train loss is: 0.0003110581831156951\n",
      "test loss is 0.0006900108426435267\n",
      "Batch: 30600,train loss is: 0.00026164433809533786\n",
      "test loss is 0.0006562434618638885\n",
      "Batch: 30700,train loss is: 0.00037561397890545145\n",
      "test loss is 0.0007323481413962262\n",
      "Batch: 30800,train loss is: 0.00026000893237154336\n",
      "test loss is 0.0007127806964761359\n",
      "Batch: 30900,train loss is: 0.00032489005783540285\n",
      "test loss is 0.0006566062978710326\n",
      "Batch: 31000,train loss is: 0.00047106016135145166\n",
      "test loss is 0.0006682728731881302\n",
      "Batch: 31100,train loss is: 0.00031091535634115086\n",
      "test loss is 0.0007428436717047501\n",
      "Batch: 31200,train loss is: 0.0004399669399420686\n",
      "test loss is 0.0006563229900456589\n",
      "Batch: 31300,train loss is: 0.00042734499203682684\n",
      "test loss is 0.0006563607770114227\n",
      "Batch: 31400,train loss is: 0.0007258829734376218\n",
      "test loss is 0.0006513914284916756\n",
      "Batch: 31500,train loss is: 0.00044045620135730256\n",
      "test loss is 0.0006633781541490576\n",
      "Batch: 31600,train loss is: 0.00039298170209576676\n",
      "test loss is 0.0006598778965863446\n",
      "Batch: 31700,train loss is: 0.0005292327123873613\n",
      "test loss is 0.0006784293413799489\n",
      "Batch: 31800,train loss is: 0.0012406415098610134\n",
      "test loss is 0.0006564208945382687\n",
      "Batch: 31900,train loss is: 0.0006739040801202496\n",
      "test loss is 0.0006637482759236863\n",
      "Batch: 32000,train loss is: 0.003634224234748913\n",
      "test loss is 0.0006623009285053144\n",
      "Batch: 32100,train loss is: 0.0003550317196389177\n",
      "test loss is 0.0006823363938663974\n",
      "Batch: 32200,train loss is: 0.0003165463972338324\n",
      "test loss is 0.0006583509698024211\n",
      "Batch: 32300,train loss is: 0.0003375838399034889\n",
      "test loss is 0.0006450215694531779\n",
      "Batch: 32400,train loss is: 0.00035628696259199253\n",
      "test loss is 0.0006407497429384599\n",
      "Batch: 32500,train loss is: 0.00084236317191859\n",
      "test loss is 0.0006870805761887905\n",
      "Batch: 32600,train loss is: 0.00046707141477519\n",
      "test loss is 0.0006711170287575364\n",
      "Batch: 32700,train loss is: 0.0005212177028518057\n",
      "test loss is 0.0007011089745086506\n",
      "Batch: 32800,train loss is: 0.0004806414178481034\n",
      "test loss is 0.0006972025582889397\n",
      "Batch: 32900,train loss is: 0.0006459010978963966\n",
      "test loss is 0.000655576489099192\n",
      "Batch: 33000,train loss is: 0.000402037062596193\n",
      "test loss is 0.0006571808301724279\n",
      "Batch: 33100,train loss is: 0.00041735258557962403\n",
      "test loss is 0.0006646341364392659\n",
      "Batch: 33200,train loss is: 0.0004409654864384429\n",
      "test loss is 0.0007250372128354373\n",
      "Batch: 33300,train loss is: 0.0005393538770864591\n",
      "test loss is 0.0006524363047844795\n",
      "Batch: 33400,train loss is: 0.00046635792892838195\n",
      "test loss is 0.0006615646937832541\n",
      "Batch: 33500,train loss is: 0.0003352223956526455\n",
      "test loss is 0.0006566155996257436\n",
      "Batch: 33600,train loss is: 0.0005407152416890125\n",
      "test loss is 0.0006504825099966185\n",
      "Batch: 33700,train loss is: 0.0005201264568183598\n",
      "test loss is 0.0006698949975855877\n",
      "Batch: 33800,train loss is: 0.0007197954294495334\n",
      "test loss is 0.0006502465044090906\n",
      "Batch: 33900,train loss is: 0.0006162543757328242\n",
      "test loss is 0.0006503261279057068\n",
      "-----------------------Epoch: 19----------------------------------\n",
      "Batch: 0,train loss is: 0.00029345962569341725\n",
      "test loss is 0.0006692716664375727\n",
      "Batch: 100,train loss is: 0.001138477005828676\n",
      "test loss is 0.000666531008687601\n",
      "Batch: 200,train loss is: 0.00036085112034290776\n",
      "test loss is 0.0006824581119154808\n",
      "Batch: 300,train loss is: 0.0002789060983136196\n",
      "test loss is 0.0006573734421234905\n",
      "Batch: 400,train loss is: 0.00043134646875806794\n",
      "test loss is 0.0006422618231776416\n",
      "Batch: 500,train loss is: 0.0004548864984576881\n",
      "test loss is 0.0006697657186331287\n",
      "Batch: 600,train loss is: 0.0002271690686780071\n",
      "test loss is 0.0006862631152150256\n",
      "Batch: 700,train loss is: 0.00037812223756953\n",
      "test loss is 0.000646873333623081\n",
      "Batch: 800,train loss is: 0.00040779904128627407\n",
      "test loss is 0.0006525993025300506\n",
      "Batch: 900,train loss is: 0.0003279552437672847\n",
      "test loss is 0.000657844442583693\n",
      "Batch: 1000,train loss is: 0.0004238323965150357\n",
      "test loss is 0.0006821986763577019\n",
      "Batch: 1100,train loss is: 0.00176540456781666\n",
      "test loss is 0.0006456108669222054\n",
      "Batch: 1200,train loss is: 0.0002519184953809683\n",
      "test loss is 0.000669950096019692\n",
      "Batch: 1300,train loss is: 0.0005206275896705171\n",
      "test loss is 0.0006519543656426857\n",
      "Batch: 1400,train loss is: 0.00040254945035107266\n",
      "test loss is 0.0006772949370486897\n",
      "Batch: 1500,train loss is: 0.0003547238436169293\n",
      "test loss is 0.0006859071059210115\n",
      "Batch: 1600,train loss is: 0.0006666214275644404\n",
      "test loss is 0.0007195860976269591\n",
      "Batch: 1700,train loss is: 0.0011595837383060086\n",
      "test loss is 0.0006453466166747738\n",
      "Batch: 1800,train loss is: 0.0003506454295376658\n",
      "test loss is 0.0006457148820263712\n",
      "Batch: 1900,train loss is: 0.0003864130616911015\n",
      "test loss is 0.0006872307886348837\n",
      "Batch: 2000,train loss is: 0.0006740798449963683\n",
      "test loss is 0.0007947648862694272\n",
      "Batch: 2100,train loss is: 0.0006433221499171036\n",
      "test loss is 0.0008068468916047375\n",
      "Batch: 2200,train loss is: 0.0006978539515720365\n",
      "test loss is 0.0006595678567931148\n",
      "Batch: 2300,train loss is: 0.0004441229444507866\n",
      "test loss is 0.0006724672231843438\n",
      "Batch: 2400,train loss is: 0.0004293480470420604\n",
      "test loss is 0.0006571177212676831\n",
      "Batch: 2500,train loss is: 0.0005134472707511604\n",
      "test loss is 0.000648335402113622\n",
      "Batch: 2600,train loss is: 0.000412971103295529\n",
      "test loss is 0.0006538411410719205\n",
      "Batch: 2700,train loss is: 0.0005453961057234263\n",
      "test loss is 0.0006353195985363376\n",
      "Batch: 2800,train loss is: 0.00043911088175225074\n",
      "test loss is 0.00066295907631593\n",
      "Batch: 2900,train loss is: 0.0004068286891348302\n",
      "test loss is 0.0006498542125068\n",
      "Batch: 3000,train loss is: 0.0004643581425619002\n",
      "test loss is 0.0006476802852209605\n",
      "Batch: 3100,train loss is: 0.0005849491683545667\n",
      "test loss is 0.0006852202750877026\n",
      "Batch: 3200,train loss is: 0.0010128656578984158\n",
      "test loss is 0.0006665284597951894\n",
      "Batch: 3300,train loss is: 0.0005651521747945584\n",
      "test loss is 0.0006585627845646319\n",
      "Batch: 3400,train loss is: 0.00035864956025719074\n",
      "test loss is 0.0006528432977067164\n",
      "Batch: 3500,train loss is: 0.00022517587558599567\n",
      "test loss is 0.0006461683999047505\n",
      "Batch: 3600,train loss is: 0.0005144591253670734\n",
      "test loss is 0.0007007752818948587\n",
      "Batch: 3700,train loss is: 0.0003422335423637629\n",
      "test loss is 0.0006596922704060648\n",
      "Batch: 3800,train loss is: 0.0004241053206375489\n",
      "test loss is 0.0006692636522088606\n",
      "Batch: 3900,train loss is: 0.0005677715032197001\n",
      "test loss is 0.0006571056100383172\n",
      "Batch: 4000,train loss is: 0.00041150690236831463\n",
      "test loss is 0.0006507610430115965\n",
      "Batch: 4100,train loss is: 0.0007346260214918176\n",
      "test loss is 0.0006727876665865831\n",
      "Batch: 4200,train loss is: 0.00025780364380068965\n",
      "test loss is 0.0006749430720328667\n",
      "Batch: 4300,train loss is: 0.0004107608995726014\n",
      "test loss is 0.0006413916256659826\n",
      "Batch: 4400,train loss is: 0.0002735887373919903\n",
      "test loss is 0.0006757205665477015\n",
      "Batch: 4500,train loss is: 0.0007125369464726416\n",
      "test loss is 0.0006700574149037685\n",
      "Batch: 4600,train loss is: 0.0005799576801620188\n",
      "test loss is 0.0006633560959888795\n",
      "Batch: 4700,train loss is: 0.0005591018448151303\n",
      "test loss is 0.0006761092289271505\n",
      "Batch: 4800,train loss is: 0.0013461871555731652\n",
      "test loss is 0.0006442581452581471\n",
      "Batch: 4900,train loss is: 0.00045349347427106455\n",
      "test loss is 0.0006569455376200963\n",
      "Batch: 5000,train loss is: 0.000538125690419227\n",
      "test loss is 0.0006703821546575362\n",
      "Batch: 5100,train loss is: 0.00044512700147239384\n",
      "test loss is 0.0007062231575489668\n",
      "Batch: 5200,train loss is: 0.00030403876976486894\n",
      "test loss is 0.0006699151676426371\n",
      "Batch: 5300,train loss is: 0.0004451859844351825\n",
      "test loss is 0.0006729837638466033\n",
      "Batch: 5400,train loss is: 0.0007029649025590409\n",
      "test loss is 0.0006471857912995779\n",
      "Batch: 5500,train loss is: 0.0004131160869729779\n",
      "test loss is 0.0006739194408965648\n",
      "Batch: 5600,train loss is: 0.0005606746117897851\n",
      "test loss is 0.0006519073481293098\n",
      "Batch: 5700,train loss is: 0.0004483413177107806\n",
      "test loss is 0.0006623616045048473\n",
      "Batch: 5800,train loss is: 0.0004361266310802855\n",
      "test loss is 0.0006538279393007187\n",
      "Batch: 5900,train loss is: 0.0032888640264956233\n",
      "test loss is 0.0006588966913923258\n",
      "Batch: 6000,train loss is: 0.0016867114510514065\n",
      "test loss is 0.0006764995172042621\n",
      "Batch: 6100,train loss is: 0.00040607792842704826\n",
      "test loss is 0.0006439813734507871\n",
      "Batch: 6200,train loss is: 0.0005396903052346958\n",
      "test loss is 0.0006546283473048033\n",
      "Batch: 6300,train loss is: 0.0010285730978900737\n",
      "test loss is 0.0006692830376048611\n",
      "Batch: 6400,train loss is: 0.0005243454378629265\n",
      "test loss is 0.0006966503960795771\n",
      "Batch: 6500,train loss is: 0.0003032804755307641\n",
      "test loss is 0.0006563233516664018\n",
      "Batch: 6600,train loss is: 0.001918400450305927\n",
      "test loss is 0.0006469216459330076\n",
      "Batch: 6700,train loss is: 0.000299623893062758\n",
      "test loss is 0.0006580694407133073\n",
      "Batch: 6800,train loss is: 0.000695672126610505\n",
      "test loss is 0.0006523012180669465\n",
      "Batch: 6900,train loss is: 0.0006459360210169343\n",
      "test loss is 0.0006650516205100998\n",
      "Batch: 7000,train loss is: 0.0005449065845261121\n",
      "test loss is 0.0006706404999103178\n",
      "Batch: 7100,train loss is: 0.001100402082076354\n",
      "test loss is 0.0006601329337797393\n",
      "Batch: 7200,train loss is: 0.00044999415368809963\n",
      "test loss is 0.0006661641103006406\n",
      "Batch: 7300,train loss is: 0.0006868015981173855\n",
      "test loss is 0.0006777377664930845\n",
      "Batch: 7400,train loss is: 0.00035198668050393677\n",
      "test loss is 0.0006522538621115032\n",
      "Batch: 7500,train loss is: 0.0006249226456317916\n",
      "test loss is 0.0006744970787135336\n",
      "Batch: 7600,train loss is: 0.0004084682014232222\n",
      "test loss is 0.0006374256110813506\n",
      "Batch: 7700,train loss is: 0.0008848161077325832\n",
      "test loss is 0.000664562168827192\n",
      "Batch: 7800,train loss is: 0.00027013408967137205\n",
      "test loss is 0.0006453844941755338\n",
      "Batch: 7900,train loss is: 0.00048405355998845136\n",
      "test loss is 0.0006685878601596099\n",
      "Batch: 8000,train loss is: 0.00037038748714942645\n",
      "test loss is 0.0006876543643807458\n",
      "Batch: 8100,train loss is: 0.0005682113618525319\n",
      "test loss is 0.000671609474253696\n",
      "Batch: 8200,train loss is: 0.0010631095659382754\n",
      "test loss is 0.000668090497075384\n",
      "Batch: 8300,train loss is: 0.00031298226025360004\n",
      "test loss is 0.0006660498426953754\n",
      "Batch: 8400,train loss is: 0.0005475719384706435\n",
      "test loss is 0.0006579158198928247\n",
      "Batch: 8500,train loss is: 0.0005778448164743012\n",
      "test loss is 0.000666685378032699\n",
      "Batch: 8600,train loss is: 0.0006595446927162927\n",
      "test loss is 0.0006739259363771812\n",
      "Batch: 8700,train loss is: 0.0004414955657583125\n",
      "test loss is 0.0007154228277794895\n",
      "Batch: 8800,train loss is: 0.0003563437387470673\n",
      "test loss is 0.0006497603608286082\n",
      "Batch: 8900,train loss is: 0.0010185095036160628\n",
      "test loss is 0.0006484974545831238\n",
      "Batch: 9000,train loss is: 0.00026138190331569295\n",
      "test loss is 0.0006724585779484795\n",
      "Batch: 9100,train loss is: 0.0003168914245365742\n",
      "test loss is 0.0006468895553411053\n",
      "Batch: 9200,train loss is: 0.0007526672351125925\n",
      "test loss is 0.0007084929462624688\n",
      "Batch: 9300,train loss is: 0.0005172410686021775\n",
      "test loss is 0.0006988399700133279\n",
      "Batch: 9400,train loss is: 0.0004529329072990927\n",
      "test loss is 0.0006782215931845728\n",
      "Batch: 9500,train loss is: 0.0003526768135186448\n",
      "test loss is 0.000681860524866801\n",
      "Batch: 9600,train loss is: 0.0003867483709357536\n",
      "test loss is 0.0006427723514580561\n",
      "Batch: 9700,train loss is: 0.00038065126332804917\n",
      "test loss is 0.0006567586127531579\n",
      "Batch: 9800,train loss is: 0.00024776216735290655\n",
      "test loss is 0.0006544167245898577\n",
      "Batch: 9900,train loss is: 0.0004545243325062985\n",
      "test loss is 0.000675604962375315\n",
      "Batch: 10000,train loss is: 0.00043029934072763863\n",
      "test loss is 0.0006418112182777548\n",
      "Batch: 10100,train loss is: 0.0009022718292895963\n",
      "test loss is 0.0006628831032611685\n",
      "Batch: 10200,train loss is: 0.0016908180416074926\n",
      "test loss is 0.0007405043857762044\n",
      "Batch: 10300,train loss is: 0.0004142343386662536\n",
      "test loss is 0.0006548442522026403\n",
      "Batch: 10400,train loss is: 0.0002661871794457864\n",
      "test loss is 0.0006506003715408374\n",
      "Batch: 10500,train loss is: 0.0007770084299298273\n",
      "test loss is 0.0006620158449817977\n",
      "Batch: 10600,train loss is: 0.0002983162429953397\n",
      "test loss is 0.0007117834788801136\n",
      "Batch: 10700,train loss is: 0.0005757559225815706\n",
      "test loss is 0.0006492554544483807\n",
      "Batch: 10800,train loss is: 0.0006202699353964956\n",
      "test loss is 0.0006602527576940529\n",
      "Batch: 10900,train loss is: 0.0006941058583977872\n",
      "test loss is 0.0006559229370521634\n",
      "Batch: 11000,train loss is: 0.0007478309824746572\n",
      "test loss is 0.00067704161658693\n",
      "Batch: 11100,train loss is: 0.0009069364242109512\n",
      "test loss is 0.0006443300976886779\n",
      "Batch: 11200,train loss is: 0.0006910535096770784\n",
      "test loss is 0.0006983030391638688\n",
      "Batch: 11300,train loss is: 0.00029000691831575443\n",
      "test loss is 0.0006643675622325126\n",
      "Batch: 11400,train loss is: 0.0007290511448424447\n",
      "test loss is 0.0006546902691714579\n",
      "Batch: 11500,train loss is: 0.00048047858617821285\n",
      "test loss is 0.0006596013587827513\n",
      "Batch: 11600,train loss is: 0.00177686045906167\n",
      "test loss is 0.0006518120503522126\n",
      "Batch: 11700,train loss is: 0.0002983518117338588\n",
      "test loss is 0.0006425003960582447\n",
      "Batch: 11800,train loss is: 0.0005794032684290379\n",
      "test loss is 0.0006522350100183783\n",
      "Batch: 11900,train loss is: 0.00047465717292054737\n",
      "test loss is 0.0006617910267549268\n",
      "Batch: 12000,train loss is: 0.0002996409544351692\n",
      "test loss is 0.0006532220032951907\n",
      "Batch: 12100,train loss is: 0.0015251341385415366\n",
      "test loss is 0.0006642128295607252\n",
      "Batch: 12200,train loss is: 0.0006231398705805043\n",
      "test loss is 0.0006796372272661999\n",
      "Batch: 12300,train loss is: 0.00044321104099583145\n",
      "test loss is 0.0006662403537733637\n",
      "Batch: 12400,train loss is: 0.00036280025507310054\n",
      "test loss is 0.0006545647527607326\n",
      "Batch: 12500,train loss is: 0.00047709550628322414\n",
      "test loss is 0.0006793571551866131\n",
      "Batch: 12600,train loss is: 0.0004930566774583446\n",
      "test loss is 0.0006674778825633934\n",
      "Batch: 12700,train loss is: 0.00035559820898738734\n",
      "test loss is 0.0006475626013977803\n",
      "Batch: 12800,train loss is: 0.0003541840055586202\n",
      "test loss is 0.0006468020496114586\n",
      "Batch: 12900,train loss is: 0.0005587545363007799\n",
      "test loss is 0.000662087095320171\n",
      "Batch: 13000,train loss is: 0.00020336575765420755\n",
      "test loss is 0.000650175930352951\n",
      "Batch: 13100,train loss is: 0.002011960287405045\n",
      "test loss is 0.0006452530459499152\n",
      "Batch: 13200,train loss is: 0.0008551720943856734\n",
      "test loss is 0.0007807129078937594\n",
      "Batch: 13300,train loss is: 0.0006075488352025866\n",
      "test loss is 0.000659372259623809\n",
      "Batch: 13400,train loss is: 0.0003545968541123406\n",
      "test loss is 0.000679035605675056\n",
      "Batch: 13500,train loss is: 0.0005439932441303686\n",
      "test loss is 0.0006757567449160165\n",
      "Batch: 13600,train loss is: 0.0005316960153502064\n",
      "test loss is 0.0006711690112264308\n",
      "Batch: 13700,train loss is: 0.000656546084425959\n",
      "test loss is 0.0006416659995943519\n",
      "Batch: 13800,train loss is: 0.0004073307386946957\n",
      "test loss is 0.0006447496012679353\n",
      "Batch: 13900,train loss is: 0.00028493735702866117\n",
      "test loss is 0.0007163747966862691\n",
      "Batch: 14000,train loss is: 0.00035862661944371666\n",
      "test loss is 0.0006739794786005971\n",
      "Batch: 14100,train loss is: 0.000708034219421597\n",
      "test loss is 0.0006912454168370498\n",
      "Batch: 14200,train loss is: 0.0005645517262863448\n",
      "test loss is 0.0006411803279869164\n",
      "Batch: 14300,train loss is: 0.0002408067116833586\n",
      "test loss is 0.0006461324406215526\n",
      "Batch: 14400,train loss is: 0.0012261127349862792\n",
      "test loss is 0.0006447616795787443\n",
      "Batch: 14500,train loss is: 0.0006126658905381408\n",
      "test loss is 0.0006457114874632026\n",
      "Batch: 14600,train loss is: 0.0008564590289930357\n",
      "test loss is 0.0006612986419471776\n",
      "Batch: 14700,train loss is: 0.00046648757590322644\n",
      "test loss is 0.0007124774819732594\n",
      "Batch: 14800,train loss is: 0.0007731822620359593\n",
      "test loss is 0.0006670850523854038\n",
      "Batch: 14900,train loss is: 0.00042318484097342757\n",
      "test loss is 0.0007024507435527547\n",
      "Batch: 15000,train loss is: 0.0006069509792250485\n",
      "test loss is 0.0006416362355653993\n",
      "Batch: 15100,train loss is: 0.0007133316812843425\n",
      "test loss is 0.000673034719101404\n",
      "Batch: 15200,train loss is: 0.0007914785839128885\n",
      "test loss is 0.0006566034674811918\n",
      "Batch: 15300,train loss is: 0.00033134071678281475\n",
      "test loss is 0.0006483107900623197\n",
      "Batch: 15400,train loss is: 0.00046030343647901706\n",
      "test loss is 0.0006670008971676944\n",
      "Batch: 15500,train loss is: 0.0006690589661376319\n",
      "test loss is 0.0006428047133642605\n",
      "Batch: 15600,train loss is: 0.0008137483868766111\n",
      "test loss is 0.0006650392147221941\n",
      "Batch: 15700,train loss is: 0.00037372646183336935\n",
      "test loss is 0.000650214534724509\n",
      "Batch: 15800,train loss is: 0.0006160817475454321\n",
      "test loss is 0.0006364400190967349\n",
      "Batch: 15900,train loss is: 0.0007844249806351466\n",
      "test loss is 0.0006582303171434695\n",
      "Batch: 16000,train loss is: 0.00036279246537112983\n",
      "test loss is 0.000654322322398495\n",
      "Batch: 16100,train loss is: 0.0005490766793462909\n",
      "test loss is 0.0006525466194087964\n",
      "Batch: 16200,train loss is: 0.0003678634929640778\n",
      "test loss is 0.0006412105044838589\n",
      "Batch: 16300,train loss is: 0.000892647735954206\n",
      "test loss is 0.0006778357929356358\n",
      "Batch: 16400,train loss is: 0.0003022853184790435\n",
      "test loss is 0.0006437146353862791\n",
      "Batch: 16500,train loss is: 0.0006776496128399097\n",
      "test loss is 0.0006656470036109462\n",
      "Batch: 16600,train loss is: 0.00032548571859364344\n",
      "test loss is 0.0006475245512961417\n",
      "Batch: 16700,train loss is: 0.000805852321906448\n",
      "test loss is 0.0006583576330099947\n",
      "Batch: 16800,train loss is: 0.0005017940923725008\n",
      "test loss is 0.0006827842887450871\n",
      "Batch: 16900,train loss is: 0.0006932444099869921\n",
      "test loss is 0.0006588304132453332\n",
      "Batch: 17000,train loss is: 0.0005870063321440819\n",
      "test loss is 0.0006395901993077553\n",
      "Batch: 17100,train loss is: 0.0004855151912339988\n",
      "test loss is 0.000650048790447243\n",
      "Batch: 17200,train loss is: 0.0004320080282472307\n",
      "test loss is 0.000655207538911277\n",
      "Batch: 17300,train loss is: 0.0007616384434195489\n",
      "test loss is 0.0006782459863098858\n",
      "Batch: 17400,train loss is: 0.00036651637012471684\n",
      "test loss is 0.0006618937393763561\n",
      "Batch: 17500,train loss is: 0.000562933009666774\n",
      "test loss is 0.0006559488953771773\n",
      "Batch: 17600,train loss is: 0.0004152900717265163\n",
      "test loss is 0.0006615368308565623\n",
      "Batch: 17700,train loss is: 0.0003309361216704836\n",
      "test loss is 0.0006887835785130187\n",
      "Batch: 17800,train loss is: 0.0015044473676052554\n",
      "test loss is 0.0006707548383638924\n",
      "Batch: 17900,train loss is: 0.00046097100750244584\n",
      "test loss is 0.0006833112495875261\n",
      "Batch: 18000,train loss is: 0.0002693440727028153\n",
      "test loss is 0.0006377008388423443\n",
      "Batch: 18100,train loss is: 0.0003694038763352833\n",
      "test loss is 0.0006475011525223234\n",
      "Batch: 18200,train loss is: 0.0005822645548149364\n",
      "test loss is 0.0006600731635536089\n",
      "Batch: 18300,train loss is: 0.001281040233389401\n",
      "test loss is 0.0006585994022787625\n",
      "Batch: 18400,train loss is: 0.0003894774497477037\n",
      "test loss is 0.0006483823655343191\n",
      "Batch: 18500,train loss is: 0.0008901323614603258\n",
      "test loss is 0.0006561934636852663\n",
      "Batch: 18600,train loss is: 0.0004429007658487359\n",
      "test loss is 0.0006518724098354911\n",
      "Batch: 18700,train loss is: 0.0005547728184850622\n",
      "test loss is 0.0006701409634508429\n",
      "Batch: 18800,train loss is: 0.0005496429831325875\n",
      "test loss is 0.0006452037693891815\n",
      "Batch: 18900,train loss is: 0.0005228522038649502\n",
      "test loss is 0.0006692226953768146\n",
      "Batch: 19000,train loss is: 0.0005627616275992938\n",
      "test loss is 0.0006497796200427278\n",
      "Batch: 19100,train loss is: 0.00038658029806183945\n",
      "test loss is 0.000675892367732207\n",
      "Batch: 19200,train loss is: 0.0005110405466805511\n",
      "test loss is 0.0006589427926554164\n",
      "Batch: 19300,train loss is: 0.0005757082646636662\n",
      "test loss is 0.0006393639266527011\n",
      "Batch: 19400,train loss is: 0.000261884547096833\n",
      "test loss is 0.000645011316691318\n",
      "Batch: 19500,train loss is: 0.0005030509768211536\n",
      "test loss is 0.0006829450062147117\n",
      "Batch: 19600,train loss is: 0.00038383674741044487\n",
      "test loss is 0.0006600444133078548\n",
      "Batch: 19700,train loss is: 0.0006793437940761593\n",
      "test loss is 0.0007290161626447904\n",
      "Batch: 19800,train loss is: 0.0005575511880375096\n",
      "test loss is 0.0006621605158596397\n",
      "Batch: 19900,train loss is: 0.00037544231304756555\n",
      "test loss is 0.000673328022328566\n",
      "Batch: 20000,train loss is: 0.0006693487892315358\n",
      "test loss is 0.0006500197707988256\n",
      "Batch: 20100,train loss is: 0.0005952877796392348\n",
      "test loss is 0.0006661677943074568\n",
      "Batch: 20200,train loss is: 0.0007163438098786698\n",
      "test loss is 0.0006704022596648756\n",
      "Batch: 20300,train loss is: 0.0002816850262715795\n",
      "test loss is 0.0006700217595659951\n",
      "Batch: 20400,train loss is: 0.0004692623707805005\n",
      "test loss is 0.0006768506462241437\n",
      "Batch: 20500,train loss is: 0.0006122423697910533\n",
      "test loss is 0.0006472138205774995\n",
      "Batch: 20600,train loss is: 0.0004894280902144637\n",
      "test loss is 0.0006821379273247954\n",
      "Batch: 20700,train loss is: 0.0007654264242549283\n",
      "test loss is 0.000651849684268519\n",
      "Batch: 20800,train loss is: 0.0003196024125980723\n",
      "test loss is 0.0006882707295406906\n",
      "Batch: 20900,train loss is: 0.0005463124516010912\n",
      "test loss is 0.0006989676200338996\n",
      "Batch: 21000,train loss is: 0.0002162581855709883\n",
      "test loss is 0.0006415095113662579\n",
      "Batch: 21100,train loss is: 0.0006690695643213973\n",
      "test loss is 0.0006830075759208797\n",
      "Batch: 21200,train loss is: 0.000371472480349262\n",
      "test loss is 0.0006858300554391254\n",
      "Batch: 21300,train loss is: 0.0003034037771768691\n",
      "test loss is 0.0006571012953650924\n",
      "Batch: 21400,train loss is: 0.000780497806281046\n",
      "test loss is 0.0006468159017752701\n",
      "Batch: 21500,train loss is: 0.0002331789582113772\n",
      "test loss is 0.000666801851678302\n",
      "Batch: 21600,train loss is: 0.0007352128799560767\n",
      "test loss is 0.0006689353425756424\n",
      "Batch: 21700,train loss is: 0.00027809018226414967\n",
      "test loss is 0.0006426207829628989\n",
      "Batch: 21800,train loss is: 0.000777124736720802\n",
      "test loss is 0.0006967652424963085\n",
      "Batch: 21900,train loss is: 0.0003968241371627444\n",
      "test loss is 0.0007304728135854091\n",
      "Batch: 22000,train loss is: 0.0006343334163002235\n",
      "test loss is 0.0006485600858639009\n",
      "Batch: 22100,train loss is: 0.0003541086602595879\n",
      "test loss is 0.0006475485670600776\n",
      "Batch: 22200,train loss is: 0.0004705089425054268\n",
      "test loss is 0.0006564555716697604\n",
      "Batch: 22300,train loss is: 0.0004980368955871895\n",
      "test loss is 0.0006682745562050967\n",
      "Batch: 22400,train loss is: 0.0004491502325233342\n",
      "test loss is 0.0006665291298550962\n",
      "Batch: 22500,train loss is: 0.0004908012609400969\n",
      "test loss is 0.0006409583677374531\n",
      "Batch: 22600,train loss is: 0.0008781087304149871\n",
      "test loss is 0.000682111959920388\n",
      "Batch: 22700,train loss is: 0.00032297263829075585\n",
      "test loss is 0.0006442385468300387\n",
      "Batch: 22800,train loss is: 0.0004840084222361458\n",
      "test loss is 0.0006761106325535953\n",
      "Batch: 22900,train loss is: 0.0002947064336163129\n",
      "test loss is 0.0007332984138641859\n",
      "Batch: 23000,train loss is: 0.0005170466710551219\n",
      "test loss is 0.0006569080543828228\n",
      "Batch: 23100,train loss is: 0.00042372590486064676\n",
      "test loss is 0.0006376417204912845\n",
      "Batch: 23200,train loss is: 0.00028866957303513586\n",
      "test loss is 0.0006350213934525575\n",
      "Batch: 23300,train loss is: 0.0004295882585343507\n",
      "test loss is 0.0006433607083602053\n",
      "Batch: 23400,train loss is: 0.0005856623955587967\n",
      "test loss is 0.0006719856838141792\n",
      "Batch: 23500,train loss is: 0.0014413260853962972\n",
      "test loss is 0.0006363763422200007\n",
      "Batch: 23600,train loss is: 0.0003108994325635816\n",
      "test loss is 0.0006503216629424252\n",
      "Batch: 23700,train loss is: 0.000982432503621397\n",
      "test loss is 0.0006452268503295814\n",
      "Batch: 23800,train loss is: 0.0008182616518811798\n",
      "test loss is 0.0006498126680273025\n",
      "Batch: 23900,train loss is: 0.0009404862317792246\n",
      "test loss is 0.000667422951007282\n",
      "Batch: 24000,train loss is: 0.0007466120622333686\n",
      "test loss is 0.0007020382613570011\n",
      "Batch: 24100,train loss is: 0.000281058715166968\n",
      "test loss is 0.0006367737064838343\n",
      "Batch: 24200,train loss is: 0.0013374683127097807\n",
      "test loss is 0.000662101180547792\n",
      "Batch: 24300,train loss is: 0.0005022026840657565\n",
      "test loss is 0.0007641157895994008\n",
      "Batch: 24400,train loss is: 0.001306256268405156\n",
      "test loss is 0.000648724334910806\n",
      "Batch: 24500,train loss is: 0.0006956878879002132\n",
      "test loss is 0.0006357894566993124\n",
      "Batch: 24600,train loss is: 0.0003701042932585215\n",
      "test loss is 0.0006731030354677422\n",
      "Batch: 24700,train loss is: 0.0002757513708227552\n",
      "test loss is 0.0006560493154436067\n",
      "Batch: 24800,train loss is: 0.0006639191966615542\n",
      "test loss is 0.000660790175769886\n",
      "Batch: 24900,train loss is: 0.0001911040746075838\n",
      "test loss is 0.0006473169858026977\n",
      "Batch: 25000,train loss is: 0.0005018811976750129\n",
      "test loss is 0.0006456797646732042\n",
      "Batch: 25100,train loss is: 0.0004054951787933636\n",
      "test loss is 0.0006553955688741777\n",
      "Batch: 25200,train loss is: 0.0007194029554861192\n",
      "test loss is 0.0006574903360243344\n",
      "Batch: 25300,train loss is: 0.0007757095034854589\n",
      "test loss is 0.000704636358572706\n",
      "Batch: 25400,train loss is: 0.0004111248024148202\n",
      "test loss is 0.0006477768103987555\n",
      "Batch: 25500,train loss is: 0.0005782933845162577\n",
      "test loss is 0.0006558020146006792\n",
      "Batch: 25600,train loss is: 0.0006098761842734958\n",
      "test loss is 0.000655397214445421\n",
      "Batch: 25700,train loss is: 0.0005912591270462314\n",
      "test loss is 0.0006693382732626448\n",
      "Batch: 25800,train loss is: 0.00033291951843343614\n",
      "test loss is 0.0006421672570152623\n",
      "Batch: 25900,train loss is: 0.0002754164333713923\n",
      "test loss is 0.0006989212730512242\n",
      "Batch: 26000,train loss is: 0.0014693336816325952\n",
      "test loss is 0.000696748856888809\n",
      "Batch: 26100,train loss is: 0.0007620268809986597\n",
      "test loss is 0.0006421423292791072\n",
      "Batch: 26200,train loss is: 0.0005982216069525167\n",
      "test loss is 0.000638155687045393\n",
      "Batch: 26300,train loss is: 0.0009888298140312737\n",
      "test loss is 0.0006564937233730101\n",
      "Batch: 26400,train loss is: 0.0005619641325926879\n",
      "test loss is 0.0006375723081736326\n",
      "Batch: 26500,train loss is: 0.0005715300379540137\n",
      "test loss is 0.0006811899332287346\n",
      "Batch: 26600,train loss is: 0.0012670359998520207\n",
      "test loss is 0.0007344655796788597\n",
      "Batch: 26700,train loss is: 0.0005076418341673333\n",
      "test loss is 0.0007266567871975745\n",
      "Batch: 26800,train loss is: 0.0011365868888396733\n",
      "test loss is 0.0006642887669377673\n",
      "Batch: 26900,train loss is: 0.0004593302160997984\n",
      "test loss is 0.0006554315372870422\n",
      "Batch: 27000,train loss is: 0.0006460097352806142\n",
      "test loss is 0.0007575768660639401\n",
      "Batch: 27100,train loss is: 0.0006653363991354002\n",
      "test loss is 0.0007096643369407747\n",
      "Batch: 27200,train loss is: 0.0006130642669182168\n",
      "test loss is 0.0006784442761934817\n",
      "Batch: 27300,train loss is: 0.00043663396282455545\n",
      "test loss is 0.000663777834230688\n",
      "Batch: 27400,train loss is: 0.0006108566379853178\n",
      "test loss is 0.0006543442037388396\n",
      "Batch: 27500,train loss is: 0.0004424713340148795\n",
      "test loss is 0.0007119335133593427\n",
      "Batch: 27600,train loss is: 0.0004772848513453401\n",
      "test loss is 0.0006743158011163548\n",
      "Batch: 27700,train loss is: 0.0015256036072414415\n",
      "test loss is 0.0006566844446086358\n",
      "Batch: 27800,train loss is: 0.0007753578100689786\n",
      "test loss is 0.0006738137503196109\n",
      "Batch: 27900,train loss is: 0.0012086229651852049\n",
      "test loss is 0.0006461433095901464\n",
      "Batch: 28000,train loss is: 0.00038405422530190625\n",
      "test loss is 0.0006487495184265199\n",
      "Batch: 28100,train loss is: 0.0007828505396894007\n",
      "test loss is 0.0006606957493167703\n",
      "Batch: 28200,train loss is: 0.0005444533419119116\n",
      "test loss is 0.0006350710533902144\n",
      "Batch: 28300,train loss is: 0.001692162035404803\n",
      "test loss is 0.0006617643057400928\n",
      "Batch: 28400,train loss is: 0.0005621988213756317\n",
      "test loss is 0.0006417400115560633\n",
      "Batch: 28500,train loss is: 0.0013268596039942258\n",
      "test loss is 0.0006955401481541155\n",
      "Batch: 28600,train loss is: 0.0003421974333447182\n",
      "test loss is 0.0006547891575447331\n",
      "Batch: 28700,train loss is: 0.0005494566028882687\n",
      "test loss is 0.0006462348710074894\n",
      "Batch: 28800,train loss is: 0.0010174406266564085\n",
      "test loss is 0.0006604556014412594\n",
      "Batch: 28900,train loss is: 0.00045609166304488467\n",
      "test loss is 0.000638047349302064\n",
      "Batch: 29000,train loss is: 0.00047980090277088876\n",
      "test loss is 0.0006435167033748722\n",
      "Batch: 29100,train loss is: 0.0004802756574842663\n",
      "test loss is 0.0006510421418476442\n",
      "Batch: 29200,train loss is: 0.0004178519219001094\n",
      "test loss is 0.0006587956680619456\n",
      "Batch: 29300,train loss is: 0.0009062790357767046\n",
      "test loss is 0.0006418967039904811\n",
      "Batch: 29400,train loss is: 0.0006562368485082768\n",
      "test loss is 0.0007393971794064329\n",
      "Batch: 29500,train loss is: 0.0004991177593063323\n",
      "test loss is 0.00068365416021183\n",
      "Batch: 29600,train loss is: 0.0005351534133706488\n",
      "test loss is 0.0006332393773400272\n",
      "Batch: 29700,train loss is: 0.00035139511140414107\n",
      "test loss is 0.000654001893684696\n",
      "Batch: 29800,train loss is: 0.0007928699253959812\n",
      "test loss is 0.0006384031556451263\n",
      "Batch: 29900,train loss is: 0.0027649047105941507\n",
      "test loss is 0.0006439277516904067\n",
      "Batch: 30000,train loss is: 0.0003560761233095778\n",
      "test loss is 0.0006874083985946734\n",
      "Batch: 30100,train loss is: 0.0005620770228619124\n",
      "test loss is 0.0006789118215471869\n",
      "Batch: 30200,train loss is: 0.00035885356020943506\n",
      "test loss is 0.0006557136957532868\n",
      "Batch: 30300,train loss is: 0.0002771115130467296\n",
      "test loss is 0.0006398526068253412\n",
      "Batch: 30400,train loss is: 0.00030116020483825624\n",
      "test loss is 0.0006479198368929894\n",
      "Batch: 30500,train loss is: 0.0003105902324083112\n",
      "test loss is 0.0006845132623232036\n",
      "Batch: 30600,train loss is: 0.00025795629508978013\n",
      "test loss is 0.0006502022326108719\n",
      "Batch: 30700,train loss is: 0.0003800610093709312\n",
      "test loss is 0.0007211842872143231\n",
      "Batch: 30800,train loss is: 0.00026082027953056795\n",
      "test loss is 0.0007055992383584058\n",
      "Batch: 30900,train loss is: 0.00031950831958562525\n",
      "test loss is 0.0006506639300829548\n",
      "Batch: 31000,train loss is: 0.0004699661500174554\n",
      "test loss is 0.0006614750745845022\n",
      "Batch: 31100,train loss is: 0.00030403204100083357\n",
      "test loss is 0.0007371118059806816\n",
      "Batch: 31200,train loss is: 0.00041890537978366937\n",
      "test loss is 0.0006504833759983068\n",
      "Batch: 31300,train loss is: 0.0004251740460135182\n",
      "test loss is 0.0006492851502832218\n",
      "Batch: 31400,train loss is: 0.0007149668797099366\n",
      "test loss is 0.0006445652744404744\n",
      "Batch: 31500,train loss is: 0.0004307169856257307\n",
      "test loss is 0.0006570479374905722\n",
      "Batch: 31600,train loss is: 0.00038027636422534143\n",
      "test loss is 0.000653047297784822\n",
      "Batch: 31700,train loss is: 0.0005061579399514011\n",
      "test loss is 0.0006703273588854906\n",
      "Batch: 31800,train loss is: 0.001208801062597456\n",
      "test loss is 0.0006495038277663653\n",
      "Batch: 31900,train loss is: 0.000656494205012173\n",
      "test loss is 0.0006547604601515515\n",
      "Batch: 32000,train loss is: 0.00355170787794534\n",
      "test loss is 0.000656105140003198\n",
      "Batch: 32100,train loss is: 0.0003402631542880526\n",
      "test loss is 0.0006784414826607647\n",
      "Batch: 32200,train loss is: 0.0003107268304625184\n",
      "test loss is 0.0006502281031772209\n",
      "Batch: 32300,train loss is: 0.0003360341130901886\n",
      "test loss is 0.0006381974329632547\n",
      "Batch: 32400,train loss is: 0.00035208861360509677\n",
      "test loss is 0.0006342774883974755\n",
      "Batch: 32500,train loss is: 0.0008294364000247496\n",
      "test loss is 0.0006798452240275732\n",
      "Batch: 32600,train loss is: 0.00046461380505819963\n",
      "test loss is 0.0006649771213534725\n",
      "Batch: 32700,train loss is: 0.0005196114336851542\n",
      "test loss is 0.0006960173236226314\n",
      "Batch: 32800,train loss is: 0.0004905097641170766\n",
      "test loss is 0.0006910780595585674\n",
      "Batch: 32900,train loss is: 0.0006373810383087098\n",
      "test loss is 0.0006521369755993955\n",
      "Batch: 33000,train loss is: 0.00039469897114067755\n",
      "test loss is 0.0006512075882165915\n",
      "Batch: 33100,train loss is: 0.0004128378336814019\n",
      "test loss is 0.0006561972752346189\n",
      "Batch: 33200,train loss is: 0.000441436971505688\n",
      "test loss is 0.0007181942035231139\n",
      "Batch: 33300,train loss is: 0.0005554439899067631\n",
      "test loss is 0.0006469229743870756\n",
      "Batch: 33400,train loss is: 0.0004585738264564296\n",
      "test loss is 0.000654499398791197\n",
      "Batch: 33500,train loss is: 0.0003276881923867027\n",
      "test loss is 0.0006525969948199011\n",
      "Batch: 33600,train loss is: 0.0005396263689673296\n",
      "test loss is 0.0006440296973020757\n",
      "Batch: 33700,train loss is: 0.0005156279316392281\n",
      "test loss is 0.00066323350405368\n",
      "Batch: 33800,train loss is: 0.0007051485748362009\n",
      "test loss is 0.000644160672358751\n",
      "Batch: 33900,train loss is: 0.0006059348878188055\n",
      "test loss is 0.0006438234126940524\n",
      "-----------------------Epoch: 20----------------------------------\n",
      "Batch: 0,train loss is: 0.0002995205213804774\n",
      "test loss is 0.0006625263502278604\n",
      "Batch: 100,train loss is: 0.001100369702208885\n",
      "test loss is 0.0006618204237337338\n",
      "Batch: 200,train loss is: 0.0003436898745957735\n",
      "test loss is 0.0006774779542516551\n",
      "Batch: 300,train loss is: 0.0002769450570323319\n",
      "test loss is 0.0006513016932262837\n",
      "Batch: 400,train loss is: 0.00042237916301763576\n",
      "test loss is 0.0006352842169210866\n",
      "Batch: 500,train loss is: 0.00044872881008003784\n",
      "test loss is 0.0006621215858407345\n",
      "Batch: 600,train loss is: 0.00022457964464864808\n",
      "test loss is 0.0006795713590195862\n",
      "Batch: 700,train loss is: 0.0003821929364719624\n",
      "test loss is 0.0006401504840807266\n",
      "Batch: 800,train loss is: 0.00039878759218484364\n",
      "test loss is 0.0006463176923970488\n",
      "Batch: 900,train loss is: 0.000335696148774543\n",
      "test loss is 0.0006523616037852868\n",
      "Batch: 1000,train loss is: 0.00041269334964897975\n",
      "test loss is 0.0006734733785491413\n",
      "Batch: 1100,train loss is: 0.0017586291240012887\n",
      "test loss is 0.0006394995238793472\n",
      "Batch: 1200,train loss is: 0.00024224544130007185\n",
      "test loss is 0.0006655404884547918\n",
      "Batch: 1300,train loss is: 0.0005158626559229148\n",
      "test loss is 0.0006463410171632013\n",
      "Batch: 1400,train loss is: 0.00040116333087687625\n",
      "test loss is 0.0006728377796739205\n",
      "Batch: 1500,train loss is: 0.000330781858847955\n",
      "test loss is 0.0006803857357954753\n",
      "Batch: 1600,train loss is: 0.0006646694279039663\n",
      "test loss is 0.0007100029092620897\n",
      "Batch: 1700,train loss is: 0.001146220501847468\n",
      "test loss is 0.000639421014415058\n",
      "Batch: 1800,train loss is: 0.00034610327883125247\n",
      "test loss is 0.0006384930945976824\n",
      "Batch: 1900,train loss is: 0.00037925316151068424\n",
      "test loss is 0.0006824564253125914\n",
      "Batch: 2000,train loss is: 0.0006740169056475774\n",
      "test loss is 0.0007886949563461665\n",
      "Batch: 2100,train loss is: 0.0006429948673629954\n",
      "test loss is 0.0007991154070886433\n",
      "Batch: 2200,train loss is: 0.0006983595798241263\n",
      "test loss is 0.000653859715423644\n",
      "Batch: 2300,train loss is: 0.00043269353133617357\n",
      "test loss is 0.0006646129058443822\n",
      "Batch: 2400,train loss is: 0.00042486730513964114\n",
      "test loss is 0.0006513260536642983\n",
      "Batch: 2500,train loss is: 0.0004934313922088491\n",
      "test loss is 0.0006420557367496486\n",
      "Batch: 2600,train loss is: 0.00041928375946661715\n",
      "test loss is 0.0006469932358859629\n",
      "Batch: 2700,train loss is: 0.0005256217946657173\n",
      "test loss is 0.0006286114500999374\n",
      "Batch: 2800,train loss is: 0.0004296667463963337\n",
      "test loss is 0.0006555267204599014\n",
      "Batch: 2900,train loss is: 0.0004032113097999517\n",
      "test loss is 0.0006445597163853551\n",
      "Batch: 3000,train loss is: 0.00045834336735742475\n",
      "test loss is 0.000642174667726905\n",
      "Batch: 3100,train loss is: 0.0005654622763349114\n",
      "test loss is 0.0006760051964767125\n",
      "Batch: 3200,train loss is: 0.0009964997885872023\n",
      "test loss is 0.0006604410814222245\n",
      "Batch: 3300,train loss is: 0.0005651416108894565\n",
      "test loss is 0.0006513622129772552\n",
      "Batch: 3400,train loss is: 0.0003492762294842595\n",
      "test loss is 0.0006471057935453935\n",
      "Batch: 3500,train loss is: 0.0002248522465426901\n",
      "test loss is 0.0006395299174711531\n",
      "Batch: 3600,train loss is: 0.0004926138872238083\n",
      "test loss is 0.0006966198823639627\n",
      "Batch: 3700,train loss is: 0.0003343862944601327\n",
      "test loss is 0.00065535529247983\n",
      "Batch: 3800,train loss is: 0.0004166053055324759\n",
      "test loss is 0.0006611237279848772\n",
      "Batch: 3900,train loss is: 0.0005646824381487335\n",
      "test loss is 0.0006498251123590334\n",
      "Batch: 4000,train loss is: 0.000413271236139907\n",
      "test loss is 0.0006437157915349036\n",
      "Batch: 4100,train loss is: 0.0007308076555714693\n",
      "test loss is 0.0006665993330765354\n",
      "Batch: 4200,train loss is: 0.0002558785240189312\n",
      "test loss is 0.0006691491550399987\n",
      "Batch: 4300,train loss is: 0.0004119024731201433\n",
      "test loss is 0.0006347988378738322\n",
      "Batch: 4400,train loss is: 0.00026670234696691745\n",
      "test loss is 0.0006699452131097794\n",
      "Batch: 4500,train loss is: 0.0006951209873888087\n",
      "test loss is 0.000663446671992808\n",
      "Batch: 4600,train loss is: 0.0005890413911866936\n",
      "test loss is 0.0006586181996836741\n",
      "Batch: 4700,train loss is: 0.0005578310778872369\n",
      "test loss is 0.0006689873668672854\n",
      "Batch: 4800,train loss is: 0.0013580322047746494\n",
      "test loss is 0.0006371705575971957\n",
      "Batch: 4900,train loss is: 0.0004426731158445825\n",
      "test loss is 0.0006512453380094026\n",
      "Batch: 5000,train loss is: 0.0005403938591465025\n",
      "test loss is 0.0006630935191762788\n",
      "Batch: 5100,train loss is: 0.00044230156834953305\n",
      "test loss is 0.0006990641259264425\n",
      "Batch: 5200,train loss is: 0.00031718809992872517\n",
      "test loss is 0.0006638584010479281\n",
      "Batch: 5300,train loss is: 0.00043368520004517417\n",
      "test loss is 0.0006663824071077242\n",
      "Batch: 5400,train loss is: 0.0006993388307068863\n",
      "test loss is 0.0006416344631400717\n",
      "Batch: 5500,train loss is: 0.0004152953669342469\n",
      "test loss is 0.0006668933506068152\n",
      "Batch: 5600,train loss is: 0.0005572855777039597\n",
      "test loss is 0.0006444000167308825\n",
      "Batch: 5700,train loss is: 0.00043192971297264924\n",
      "test loss is 0.0006571890684131521\n",
      "Batch: 5800,train loss is: 0.0004297092230292362\n",
      "test loss is 0.0006473419491987583\n",
      "Batch: 5900,train loss is: 0.003236609362151724\n",
      "test loss is 0.0006530045238428323\n",
      "Batch: 6000,train loss is: 0.0017011357393493774\n",
      "test loss is 0.0006743702094129102\n",
      "Batch: 6100,train loss is: 0.00040655070562371754\n",
      "test loss is 0.0006382783151869529\n",
      "Batch: 6200,train loss is: 0.0005369831219353126\n",
      "test loss is 0.0006473508741168881\n",
      "Batch: 6300,train loss is: 0.0010252128587526462\n",
      "test loss is 0.0006630817918547395\n",
      "Batch: 6400,train loss is: 0.0005223664005960828\n",
      "test loss is 0.0006897061965892808\n",
      "Batch: 6500,train loss is: 0.00030457023058937663\n",
      "test loss is 0.0006496661050660145\n",
      "Batch: 6600,train loss is: 0.0018909143289057757\n",
      "test loss is 0.0006404725719309455\n",
      "Batch: 6700,train loss is: 0.0002974245354253158\n",
      "test loss is 0.0006501523496566872\n",
      "Batch: 6800,train loss is: 0.0006918331099949788\n",
      "test loss is 0.00064559191552147\n",
      "Batch: 6900,train loss is: 0.0006284938913629384\n",
      "test loss is 0.0006596281773427457\n",
      "Batch: 7000,train loss is: 0.0005352541660005756\n",
      "test loss is 0.0006637951876820316\n",
      "Batch: 7100,train loss is: 0.0011063267012487862\n",
      "test loss is 0.0006533587558362375\n",
      "Batch: 7200,train loss is: 0.0004526133535485428\n",
      "test loss is 0.0006581792290933537\n",
      "Batch: 7300,train loss is: 0.0006713009613274831\n",
      "test loss is 0.000671073293270001\n",
      "Batch: 7400,train loss is: 0.0003490468106074711\n",
      "test loss is 0.0006462417872315729\n",
      "Batch: 7500,train loss is: 0.0006176072028362722\n",
      "test loss is 0.0006673029603627922\n",
      "Batch: 7600,train loss is: 0.00040867697835587127\n",
      "test loss is 0.0006310204402475289\n",
      "Batch: 7700,train loss is: 0.0008830656915625917\n",
      "test loss is 0.0006604061408400889\n",
      "Batch: 7800,train loss is: 0.0002657140318166485\n",
      "test loss is 0.0006392954596518315\n",
      "Batch: 7900,train loss is: 0.0004648961495499403\n",
      "test loss is 0.0006613701394881916\n",
      "Batch: 8000,train loss is: 0.00036378552825620163\n",
      "test loss is 0.000682585130540356\n",
      "Batch: 8100,train loss is: 0.0005649064897752584\n",
      "test loss is 0.000664565487636168\n",
      "Batch: 8200,train loss is: 0.0010733010012036117\n",
      "test loss is 0.0006631402192632062\n",
      "Batch: 8300,train loss is: 0.00030056616153723047\n",
      "test loss is 0.0006592742522433463\n",
      "Batch: 8400,train loss is: 0.0005425272388877106\n",
      "test loss is 0.0006531827246153223\n",
      "Batch: 8500,train loss is: 0.0005627715623444642\n",
      "test loss is 0.0006601965199410502\n",
      "Batch: 8600,train loss is: 0.0006453754474244542\n",
      "test loss is 0.0006665701766627404\n",
      "Batch: 8700,train loss is: 0.000439295606385811\n",
      "test loss is 0.0007104907557331199\n",
      "Batch: 8800,train loss is: 0.0003506035375971389\n",
      "test loss is 0.00064388947131541\n",
      "Batch: 8900,train loss is: 0.0010072873268134107\n",
      "test loss is 0.000641941371068624\n",
      "Batch: 9000,train loss is: 0.0002612678831577226\n",
      "test loss is 0.00066746997936652\n",
      "Batch: 9100,train loss is: 0.00031910800480060124\n",
      "test loss is 0.0006410579615364155\n",
      "Batch: 9200,train loss is: 0.0007371797188919807\n",
      "test loss is 0.0006965971955946487\n",
      "Batch: 9300,train loss is: 0.0005030193702451266\n",
      "test loss is 0.0006926899730095758\n",
      "Batch: 9400,train loss is: 0.0004550030478467555\n",
      "test loss is 0.0006733603390126289\n",
      "Batch: 9500,train loss is: 0.00034871187888449375\n",
      "test loss is 0.0006744818503626207\n",
      "Batch: 9600,train loss is: 0.0003826759791719775\n",
      "test loss is 0.000636634285854487\n",
      "Batch: 9700,train loss is: 0.0003912480405316935\n",
      "test loss is 0.0006496772674813796\n",
      "Batch: 9800,train loss is: 0.0002438326904752372\n",
      "test loss is 0.0006492402217938141\n",
      "Batch: 9900,train loss is: 0.0004455483050803519\n",
      "test loss is 0.0006675399948554901\n",
      "Batch: 10000,train loss is: 0.00041848749683117727\n",
      "test loss is 0.0006358313737291127\n",
      "Batch: 10100,train loss is: 0.0008718284886128878\n",
      "test loss is 0.0006558234510937539\n",
      "Batch: 10200,train loss is: 0.0016693179433458028\n",
      "test loss is 0.0007319844360096769\n",
      "Batch: 10300,train loss is: 0.0004058049414737569\n",
      "test loss is 0.0006487060714645533\n",
      "Batch: 10400,train loss is: 0.0002625394559570108\n",
      "test loss is 0.0006438822418237249\n",
      "Batch: 10500,train loss is: 0.0007836273304402015\n",
      "test loss is 0.0006566416402767564\n",
      "Batch: 10600,train loss is: 0.00028783212215779383\n",
      "test loss is 0.0007044456039375124\n",
      "Batch: 10700,train loss is: 0.0005657100259581588\n",
      "test loss is 0.0006425702309124311\n",
      "Batch: 10800,train loss is: 0.0006263394836021569\n",
      "test loss is 0.0006536332038778874\n",
      "Batch: 10900,train loss is: 0.0006808727699880877\n",
      "test loss is 0.00064970645817984\n",
      "Batch: 11000,train loss is: 0.0007363929054879498\n",
      "test loss is 0.0006704155966962997\n",
      "Batch: 11100,train loss is: 0.0008924534169982893\n",
      "test loss is 0.0006381915186741542\n",
      "Batch: 11200,train loss is: 0.0006935551686374997\n",
      "test loss is 0.0006937907675988032\n",
      "Batch: 11300,train loss is: 0.00028751987919641844\n",
      "test loss is 0.0006596751889725185\n",
      "Batch: 11400,train loss is: 0.0007343385838541066\n",
      "test loss is 0.0006489495521176437\n",
      "Batch: 11500,train loss is: 0.0004646533154304483\n",
      "test loss is 0.0006523907335664263\n",
      "Batch: 11600,train loss is: 0.0017673086197438\n",
      "test loss is 0.0006451525638459033\n",
      "Batch: 11700,train loss is: 0.00030302294625029666\n",
      "test loss is 0.0006373193537408169\n",
      "Batch: 11800,train loss is: 0.0005660880277503444\n",
      "test loss is 0.0006474291990368023\n",
      "Batch: 11900,train loss is: 0.00046330410629940547\n",
      "test loss is 0.0006564175879789905\n",
      "Batch: 12000,train loss is: 0.0002991251959089152\n",
      "test loss is 0.000647964301590047\n",
      "Batch: 12100,train loss is: 0.0015441162655328176\n",
      "test loss is 0.0006592915122662547\n",
      "Batch: 12200,train loss is: 0.0006089632695921743\n",
      "test loss is 0.0006715248499560027\n",
      "Batch: 12300,train loss is: 0.0004400815980846875\n",
      "test loss is 0.0006605835751000735\n",
      "Batch: 12400,train loss is: 0.0003535117899853997\n",
      "test loss is 0.0006502002517759297\n",
      "Batch: 12500,train loss is: 0.0004639007928973729\n",
      "test loss is 0.0006729715089302554\n",
      "Batch: 12600,train loss is: 0.000492168658138625\n",
      "test loss is 0.0006610607843657749\n",
      "Batch: 12700,train loss is: 0.0003471076661130901\n",
      "test loss is 0.0006420847821789179\n",
      "Batch: 12800,train loss is: 0.00035008752091162694\n",
      "test loss is 0.000641168941275409\n",
      "Batch: 12900,train loss is: 0.0005430609478607045\n",
      "test loss is 0.0006556523111230957\n",
      "Batch: 13000,train loss is: 0.00020476348367297155\n",
      "test loss is 0.0006445237218733504\n",
      "Batch: 13100,train loss is: 0.0019826412228676118\n",
      "test loss is 0.0006382656284550461\n",
      "Batch: 13200,train loss is: 0.0008749523986967078\n",
      "test loss is 0.0007750661261710445\n",
      "Batch: 13300,train loss is: 0.0005772654257824261\n",
      "test loss is 0.000653146643790751\n",
      "Batch: 13400,train loss is: 0.00035348237817685226\n",
      "test loss is 0.0006726809257046727\n",
      "Batch: 13500,train loss is: 0.0005348331333289348\n",
      "test loss is 0.0006673270396711316\n",
      "Batch: 13600,train loss is: 0.0005182661254533112\n",
      "test loss is 0.0006664351945970266\n",
      "Batch: 13700,train loss is: 0.0006508996020142458\n",
      "test loss is 0.0006348170070357645\n",
      "Batch: 13800,train loss is: 0.00039763441615789456\n",
      "test loss is 0.0006379445392010528\n",
      "Batch: 13900,train loss is: 0.0002732341788228464\n",
      "test loss is 0.0007103390351303226\n",
      "Batch: 14000,train loss is: 0.0003460903368448099\n",
      "test loss is 0.0006665319882035876\n",
      "Batch: 14100,train loss is: 0.0006982514208507453\n",
      "test loss is 0.0006834958934227676\n",
      "Batch: 14200,train loss is: 0.0005602319892184916\n",
      "test loss is 0.0006354098529398412\n",
      "Batch: 14300,train loss is: 0.00024054573738096783\n",
      "test loss is 0.0006390490350104344\n",
      "Batch: 14400,train loss is: 0.0012130554470207691\n",
      "test loss is 0.0006380381175159615\n",
      "Batch: 14500,train loss is: 0.0006202865337189141\n",
      "test loss is 0.0006395117236297198\n",
      "Batch: 14600,train loss is: 0.0008239008715116281\n",
      "test loss is 0.0006566226041889958\n",
      "Batch: 14700,train loss is: 0.0004598354290470775\n",
      "test loss is 0.000708481128172164\n",
      "Batch: 14800,train loss is: 0.0007730451963910084\n",
      "test loss is 0.0006616619877698554\n",
      "Batch: 14900,train loss is: 0.0004135359144636299\n",
      "test loss is 0.0006983661696724375\n",
      "Batch: 15000,train loss is: 0.0005842681491503704\n",
      "test loss is 0.0006363960113406263\n",
      "Batch: 15100,train loss is: 0.000726702870815933\n",
      "test loss is 0.0006671698732786105\n",
      "Batch: 15200,train loss is: 0.0007745899439187786\n",
      "test loss is 0.0006506289497884539\n",
      "Batch: 15300,train loss is: 0.00032562048422513047\n",
      "test loss is 0.0006417605009965024\n",
      "Batch: 15400,train loss is: 0.0004586313842064442\n",
      "test loss is 0.000662775590316329\n",
      "Batch: 15500,train loss is: 0.0006645040907231216\n",
      "test loss is 0.0006372720599022911\n",
      "Batch: 15600,train loss is: 0.0008060784824128277\n",
      "test loss is 0.0006573826764928226\n",
      "Batch: 15700,train loss is: 0.00036536982045923624\n",
      "test loss is 0.0006445763831822895\n",
      "Batch: 15800,train loss is: 0.0006164440858296438\n",
      "test loss is 0.0006302720755765218\n",
      "Batch: 15900,train loss is: 0.0007908490513404475\n",
      "test loss is 0.0006521614980132425\n",
      "Batch: 16000,train loss is: 0.00035962116589092206\n",
      "test loss is 0.000646498627717169\n",
      "Batch: 16100,train loss is: 0.0005662558304219801\n",
      "test loss is 0.0006461945813220768\n",
      "Batch: 16200,train loss is: 0.00036583228147753135\n",
      "test loss is 0.0006348499371936738\n",
      "Batch: 16300,train loss is: 0.0008868607738609715\n",
      "test loss is 0.0006686265132561726\n",
      "Batch: 16400,train loss is: 0.00029569159228094786\n",
      "test loss is 0.00063738033189069\n",
      "Batch: 16500,train loss is: 0.0006767101588176816\n",
      "test loss is 0.0006591862373293999\n",
      "Batch: 16600,train loss is: 0.0003299514757937064\n",
      "test loss is 0.0006413274233436786\n",
      "Batch: 16700,train loss is: 0.000800742930068676\n",
      "test loss is 0.0006509694753413166\n",
      "Batch: 16800,train loss is: 0.0005018571712183639\n",
      "test loss is 0.0006757403744199028\n",
      "Batch: 16900,train loss is: 0.0006977144644008948\n",
      "test loss is 0.0006532394484035044\n",
      "Batch: 17000,train loss is: 0.0005867281803310009\n",
      "test loss is 0.0006333792714185617\n",
      "Batch: 17100,train loss is: 0.0004668669606681063\n",
      "test loss is 0.0006423446409958861\n",
      "Batch: 17200,train loss is: 0.00041910914114633125\n",
      "test loss is 0.0006491307214180741\n",
      "Batch: 17300,train loss is: 0.0007541764609876267\n",
      "test loss is 0.0006686628683439872\n",
      "Batch: 17400,train loss is: 0.0003625952738844107\n",
      "test loss is 0.000658680960723804\n",
      "Batch: 17500,train loss is: 0.0005696036232259157\n",
      "test loss is 0.0006505641545561806\n",
      "Batch: 17600,train loss is: 0.0004173145912343633\n",
      "test loss is 0.0006575701423023392\n",
      "Batch: 17700,train loss is: 0.0003239748398147993\n",
      "test loss is 0.0006855866303495987\n",
      "Batch: 17800,train loss is: 0.0015303630501534845\n",
      "test loss is 0.0006693021994233001\n",
      "Batch: 17900,train loss is: 0.00045121261954288725\n",
      "test loss is 0.0006743664966751861\n",
      "Batch: 18000,train loss is: 0.0002651343578045733\n",
      "test loss is 0.0006318758213568374\n",
      "Batch: 18100,train loss is: 0.0003606780525072784\n",
      "test loss is 0.0006415840070842598\n",
      "Batch: 18200,train loss is: 0.0005802893238520164\n",
      "test loss is 0.0006533251326936647\n",
      "Batch: 18300,train loss is: 0.0012964565491100602\n",
      "test loss is 0.000653175149590767\n",
      "Batch: 18400,train loss is: 0.0003961917718241593\n",
      "test loss is 0.000643059771168567\n",
      "Batch: 18500,train loss is: 0.0008765307204326295\n",
      "test loss is 0.0006496137291357805\n",
      "Batch: 18600,train loss is: 0.0004380421705039152\n",
      "test loss is 0.000646099337223703\n",
      "Batch: 18700,train loss is: 0.0005493035571306709\n",
      "test loss is 0.0006629665910566595\n",
      "Batch: 18800,train loss is: 0.0005445513692733411\n",
      "test loss is 0.0006388818087527498\n",
      "Batch: 18900,train loss is: 0.0005085042199664823\n",
      "test loss is 0.0006608283402975712\n",
      "Batch: 19000,train loss is: 0.0005513997081538838\n",
      "test loss is 0.0006434260417620451\n",
      "Batch: 19100,train loss is: 0.00038325316730834384\n",
      "test loss is 0.0006698026043405725\n",
      "Batch: 19200,train loss is: 0.0005131735068183227\n",
      "test loss is 0.0006526048665372884\n",
      "Batch: 19300,train loss is: 0.0005316724069987001\n",
      "test loss is 0.0006335303572781855\n",
      "Batch: 19400,train loss is: 0.0002596679326295103\n",
      "test loss is 0.0006399357371221951\n",
      "Batch: 19500,train loss is: 0.0004941695617714627\n",
      "test loss is 0.0006769270839243051\n",
      "Batch: 19600,train loss is: 0.0003795034911915867\n",
      "test loss is 0.0006525462476245819\n",
      "Batch: 19700,train loss is: 0.000663223747717237\n",
      "test loss is 0.0007229648895198254\n",
      "Batch: 19800,train loss is: 0.000556451786556039\n",
      "test loss is 0.0006565909899362339\n",
      "Batch: 19900,train loss is: 0.0003837792041506638\n",
      "test loss is 0.0006672959131726456\n",
      "Batch: 20000,train loss is: 0.000684723669476586\n",
      "test loss is 0.0006436102537243045\n",
      "Batch: 20100,train loss is: 0.0005842106277339766\n",
      "test loss is 0.0006597209926623937\n",
      "Batch: 20200,train loss is: 0.0007072730956262446\n",
      "test loss is 0.0006633646979038829\n",
      "Batch: 20300,train loss is: 0.0002830546498324544\n",
      "test loss is 0.0006628979977208737\n",
      "Batch: 20400,train loss is: 0.00047149185868766605\n",
      "test loss is 0.0006683020852170322\n",
      "Batch: 20500,train loss is: 0.0006183321833071672\n",
      "test loss is 0.0006410970434760512\n",
      "Batch: 20600,train loss is: 0.0004958807725589314\n",
      "test loss is 0.0006743771142876526\n",
      "Batch: 20700,train loss is: 0.0007824385653641115\n",
      "test loss is 0.0006445714336796863\n",
      "Batch: 20800,train loss is: 0.00032081803105471664\n",
      "test loss is 0.0006838390107988244\n",
      "Batch: 20900,train loss is: 0.0005437801789074991\n",
      "test loss is 0.0006917454389892964\n",
      "Batch: 21000,train loss is: 0.0002172501344774961\n",
      "test loss is 0.0006347086146192992\n",
      "Batch: 21100,train loss is: 0.000664697355319223\n",
      "test loss is 0.0006785200042368533\n",
      "Batch: 21200,train loss is: 0.00036874657670125775\n",
      "test loss is 0.0006798362009602012\n",
      "Batch: 21300,train loss is: 0.0002991418802788264\n",
      "test loss is 0.0006542879488883697\n",
      "Batch: 21400,train loss is: 0.0007870795241604599\n",
      "test loss is 0.0006394590289506815\n",
      "Batch: 21500,train loss is: 0.0002378849580040358\n",
      "test loss is 0.0006618825146779607\n",
      "Batch: 21600,train loss is: 0.0007150769323740928\n",
      "test loss is 0.000660951651005614\n",
      "Batch: 21700,train loss is: 0.0002727272709126461\n",
      "test loss is 0.0006360681820333608\n",
      "Batch: 21800,train loss is: 0.0007486970517342279\n",
      "test loss is 0.0006907113236760663\n",
      "Batch: 21900,train loss is: 0.0003782046527365211\n",
      "test loss is 0.000720721729080982\n",
      "Batch: 22000,train loss is: 0.0006404934989504452\n",
      "test loss is 0.0006415100791221905\n",
      "Batch: 22100,train loss is: 0.00034746268914591735\n",
      "test loss is 0.0006405664080970066\n",
      "Batch: 22200,train loss is: 0.00046540700140383374\n",
      "test loss is 0.0006513893740943488\n",
      "Batch: 22300,train loss is: 0.0004812263700158327\n",
      "test loss is 0.0006645923284963945\n",
      "Batch: 22400,train loss is: 0.0004498646416670517\n",
      "test loss is 0.0006601466368988451\n",
      "Batch: 22500,train loss is: 0.0004919649048378339\n",
      "test loss is 0.0006343492227256693\n",
      "Batch: 22600,train loss is: 0.000844986268750594\n",
      "test loss is 0.0006771597036964305\n",
      "Batch: 22700,train loss is: 0.0003145665945078503\n",
      "test loss is 0.0006380389421348824\n",
      "Batch: 22800,train loss is: 0.0004808888222233926\n",
      "test loss is 0.0006688575977852414\n",
      "Batch: 22900,train loss is: 0.0002934090447100348\n",
      "test loss is 0.0007272023403098746\n",
      "Batch: 23000,train loss is: 0.0005329773172495888\n",
      "test loss is 0.0006504984719546608\n",
      "Batch: 23100,train loss is: 0.00041710009876635144\n",
      "test loss is 0.0006321000702816414\n",
      "Batch: 23200,train loss is: 0.00028761062466718207\n",
      "test loss is 0.0006285979417912648\n",
      "Batch: 23300,train loss is: 0.00042568734586826204\n",
      "test loss is 0.0006363578572635381\n",
      "Batch: 23400,train loss is: 0.0005801679568128372\n",
      "test loss is 0.0006643299527032089\n",
      "Batch: 23500,train loss is: 0.0014124559642571728\n",
      "test loss is 0.0006293652872526747\n",
      "Batch: 23600,train loss is: 0.00031193222875646495\n",
      "test loss is 0.0006445370815781771\n",
      "Batch: 23700,train loss is: 0.0010103169692968065\n",
      "test loss is 0.0006390847822018964\n",
      "Batch: 23800,train loss is: 0.0008201958904304794\n",
      "test loss is 0.0006436913057457831\n",
      "Batch: 23900,train loss is: 0.0009518441311827689\n",
      "test loss is 0.0006626746188288739\n",
      "Batch: 24000,train loss is: 0.0007648924210768139\n",
      "test loss is 0.0006956570411685381\n",
      "Batch: 24100,train loss is: 0.00027481945088276975\n",
      "test loss is 0.0006318860236215719\n",
      "Batch: 24200,train loss is: 0.0013270912214771874\n",
      "test loss is 0.0006557792954616542\n",
      "Batch: 24300,train loss is: 0.0004911942113370962\n",
      "test loss is 0.0007536869155547148\n",
      "Batch: 24400,train loss is: 0.0012809197350891867\n",
      "test loss is 0.0006429476767740512\n",
      "Batch: 24500,train loss is: 0.0007061155862699283\n",
      "test loss is 0.0006288371872157298\n",
      "Batch: 24600,train loss is: 0.0003640379908884519\n",
      "test loss is 0.0006685527038615768\n",
      "Batch: 24700,train loss is: 0.00027332245702361296\n",
      "test loss is 0.0006510219638071526\n",
      "Batch: 24800,train loss is: 0.0006467545705472952\n",
      "test loss is 0.0006541723112698765\n",
      "Batch: 24900,train loss is: 0.000195623404653528\n",
      "test loss is 0.0006421220725541072\n",
      "Batch: 25000,train loss is: 0.00047994294744953877\n",
      "test loss is 0.0006398216073831172\n",
      "Batch: 25100,train loss is: 0.0003888890767613445\n",
      "test loss is 0.0006488301891539583\n",
      "Batch: 25200,train loss is: 0.0007160468159591512\n",
      "test loss is 0.00065113810591534\n",
      "Batch: 25300,train loss is: 0.0007761938973965675\n",
      "test loss is 0.0006981206506542286\n",
      "Batch: 25400,train loss is: 0.00041129674486639986\n",
      "test loss is 0.0006417399293367016\n",
      "Batch: 25500,train loss is: 0.000574202655029855\n",
      "test loss is 0.000649278324945275\n",
      "Batch: 25600,train loss is: 0.0006026554186780414\n",
      "test loss is 0.0006502391157524286\n",
      "Batch: 25700,train loss is: 0.0005952994255694911\n",
      "test loss is 0.000661097647476346\n",
      "Batch: 25800,train loss is: 0.0003238058234091499\n",
      "test loss is 0.0006370241856665719\n",
      "Batch: 25900,train loss is: 0.00027087051912432235\n",
      "test loss is 0.0006936940441765509\n",
      "Batch: 26000,train loss is: 0.0014578194005192015\n",
      "test loss is 0.0006892977613597695\n",
      "Batch: 26100,train loss is: 0.000749999463553224\n",
      "test loss is 0.0006361836795486794\n",
      "Batch: 26200,train loss is: 0.000587538500040979\n",
      "test loss is 0.0006323182625701822\n",
      "Batch: 26300,train loss is: 0.0009770826038153194\n",
      "test loss is 0.0006497170040733301\n",
      "Batch: 26400,train loss is: 0.0005561047583011752\n",
      "test loss is 0.0006312830674319653\n",
      "Batch: 26500,train loss is: 0.000588359656660567\n",
      "test loss is 0.0006782612291346406\n",
      "Batch: 26600,train loss is: 0.0012505068480178764\n",
      "test loss is 0.0007303559312562286\n",
      "Batch: 26700,train loss is: 0.0005088713875200325\n",
      "test loss is 0.0007177143750223814\n",
      "Batch: 26800,train loss is: 0.001133804886030962\n",
      "test loss is 0.0006588251715279077\n",
      "Batch: 26900,train loss is: 0.00046466853435820406\n",
      "test loss is 0.0006489959064779079\n",
      "Batch: 27000,train loss is: 0.0006434146735942113\n",
      "test loss is 0.0007512019207894032\n",
      "Batch: 27100,train loss is: 0.0006776965629116115\n",
      "test loss is 0.0007072392778566773\n",
      "Batch: 27200,train loss is: 0.000598675572445962\n",
      "test loss is 0.0006678161147011518\n",
      "Batch: 27300,train loss is: 0.00043795694667600883\n",
      "test loss is 0.0006580118689831087\n",
      "Batch: 27400,train loss is: 0.0005974023193322163\n",
      "test loss is 0.0006486344902118924\n",
      "Batch: 27500,train loss is: 0.00043704688139478795\n",
      "test loss is 0.0007043367252922721\n",
      "Batch: 27600,train loss is: 0.0004666778353409586\n",
      "test loss is 0.000667852516335817\n",
      "Batch: 27700,train loss is: 0.0015326919851838447\n",
      "test loss is 0.0006497583655752457\n",
      "Batch: 27800,train loss is: 0.0007716068091587198\n",
      "test loss is 0.0006656728974074791\n",
      "Batch: 27900,train loss is: 0.0011941071266529254\n",
      "test loss is 0.0006392447456606148\n",
      "Batch: 28000,train loss is: 0.0003836921054341254\n",
      "test loss is 0.0006433025076803037\n",
      "Batch: 28100,train loss is: 0.0007759507766918769\n",
      "test loss is 0.0006532530817466439\n",
      "Batch: 28200,train loss is: 0.0005391083355362694\n",
      "test loss is 0.0006292928899788916\n",
      "Batch: 28300,train loss is: 0.001693993580059501\n",
      "test loss is 0.0006541720133319456\n",
      "Batch: 28400,train loss is: 0.0005681740644312256\n",
      "test loss is 0.0006358339975335262\n",
      "Batch: 28500,train loss is: 0.0013088444407210365\n",
      "test loss is 0.0006911662086624741\n",
      "Batch: 28600,train loss is: 0.00034364865031496854\n",
      "test loss is 0.0006483637715559372\n",
      "Batch: 28700,train loss is: 0.0005466024042938878\n",
      "test loss is 0.0006394106984013154\n",
      "Batch: 28800,train loss is: 0.0009978235171238315\n",
      "test loss is 0.0006549379844893404\n",
      "Batch: 28900,train loss is: 0.0004539182783952563\n",
      "test loss is 0.0006316891066956437\n",
      "Batch: 29000,train loss is: 0.00047671683924556185\n",
      "test loss is 0.0006360443595937623\n",
      "Batch: 29100,train loss is: 0.00047443560361172803\n",
      "test loss is 0.0006447394461271727\n",
      "Batch: 29200,train loss is: 0.00042059219867941826\n",
      "test loss is 0.0006502635146782352\n",
      "Batch: 29300,train loss is: 0.0009004612908839783\n",
      "test loss is 0.0006373843275401086\n",
      "Batch: 29400,train loss is: 0.0006217649091080625\n",
      "test loss is 0.0007361757701791478\n",
      "Batch: 29500,train loss is: 0.000510621573450121\n",
      "test loss is 0.0006798607909843454\n",
      "Batch: 29600,train loss is: 0.0005518819484012871\n",
      "test loss is 0.0006272533484480501\n",
      "Batch: 29700,train loss is: 0.0003505455362721941\n",
      "test loss is 0.0006473017714575583\n",
      "Batch: 29800,train loss is: 0.0007989593416443611\n",
      "test loss is 0.0006334828382479615\n",
      "Batch: 29900,train loss is: 0.0027335080577747623\n",
      "test loss is 0.000639173220941699\n",
      "Batch: 30000,train loss is: 0.0003616177313627846\n",
      "test loss is 0.0006794285796944535\n",
      "Batch: 30100,train loss is: 0.0005429123488015313\n",
      "test loss is 0.0006714560779510186\n",
      "Batch: 30200,train loss is: 0.0003506000453557943\n",
      "test loss is 0.0006487473524065293\n",
      "Batch: 30300,train loss is: 0.00027441487124748654\n",
      "test loss is 0.0006333352436597579\n",
      "Batch: 30400,train loss is: 0.0003017406381093587\n",
      "test loss is 0.0006411138810104899\n",
      "Batch: 30500,train loss is: 0.0003084108461879318\n",
      "test loss is 0.0006785822624997917\n",
      "Batch: 30600,train loss is: 0.00024828192689523417\n",
      "test loss is 0.0006441324177659266\n",
      "Batch: 30700,train loss is: 0.0003975591059588501\n",
      "test loss is 0.0007129661177708991\n",
      "Batch: 30800,train loss is: 0.0002627337656120811\n",
      "test loss is 0.000698775759220072\n",
      "Batch: 30900,train loss is: 0.00031316290503387095\n",
      "test loss is 0.000644263547332615\n",
      "Batch: 31000,train loss is: 0.00046744470404321047\n",
      "test loss is 0.0006544930841163526\n",
      "Batch: 31100,train loss is: 0.00030013805277077785\n",
      "test loss is 0.0007318187662859623\n",
      "Batch: 31200,train loss is: 0.00040355026738939174\n",
      "test loss is 0.0006444814128814102\n",
      "Batch: 31300,train loss is: 0.00043078446342363055\n",
      "test loss is 0.0006427908338354719\n",
      "Batch: 31400,train loss is: 0.0006963775065132081\n",
      "test loss is 0.0006377185215448508\n",
      "Batch: 31500,train loss is: 0.00042742365082875457\n",
      "test loss is 0.0006530924043185237\n",
      "Batch: 31600,train loss is: 0.0003748487719952467\n",
      "test loss is 0.0006463298935762337\n",
      "Batch: 31700,train loss is: 0.00048272994728518464\n",
      "test loss is 0.0006628004524438869\n",
      "Batch: 31800,train loss is: 0.0011798558766428619\n",
      "test loss is 0.0006430865102051954\n",
      "Batch: 31900,train loss is: 0.00064209093612447\n",
      "test loss is 0.0006483334207812053\n",
      "Batch: 32000,train loss is: 0.0034853370627984086\n",
      "test loss is 0.0006494536591298782\n",
      "Batch: 32100,train loss is: 0.00032343561714522496\n",
      "test loss is 0.0006741878139982821\n",
      "Batch: 32200,train loss is: 0.00030464512043962803\n",
      "test loss is 0.0006428959557296298\n",
      "Batch: 32300,train loss is: 0.0003360434833556194\n",
      "test loss is 0.0006324424588940643\n",
      "Batch: 32400,train loss is: 0.0003465940324181477\n",
      "test loss is 0.0006284601396202471\n",
      "Batch: 32500,train loss is: 0.0008137678495591303\n",
      "test loss is 0.0006715868444580783\n",
      "Batch: 32600,train loss is: 0.00046244316778158376\n",
      "test loss is 0.0006581078299895097\n",
      "Batch: 32700,train loss is: 0.0005114380306192052\n",
      "test loss is 0.0006898805977769874\n",
      "Batch: 32800,train loss is: 0.0004974077400865243\n",
      "test loss is 0.0006839453583659385\n",
      "Batch: 32900,train loss is: 0.0006345571119496556\n",
      "test loss is 0.0006484486893650537\n",
      "Batch: 33000,train loss is: 0.0003882436108385487\n",
      "test loss is 0.0006466304205593752\n",
      "Batch: 33100,train loss is: 0.00040913295038744744\n",
      "test loss is 0.0006488751258890243\n",
      "Batch: 33200,train loss is: 0.0004426670085802028\n",
      "test loss is 0.0007127867462532884\n",
      "Batch: 33300,train loss is: 0.0005680840052744775\n",
      "test loss is 0.000641471843739377\n",
      "Batch: 33400,train loss is: 0.0004477362530477887\n",
      "test loss is 0.0006475136742703352\n",
      "Batch: 33500,train loss is: 0.0003227538896019108\n",
      "test loss is 0.0006479406850488518\n",
      "Batch: 33600,train loss is: 0.0005325582318420647\n",
      "test loss is 0.0006379232268767335\n",
      "Batch: 33700,train loss is: 0.0005100216543362511\n",
      "test loss is 0.0006579731859442714\n",
      "Batch: 33800,train loss is: 0.0006980374354931166\n",
      "test loss is 0.0006387232100543656\n",
      "Batch: 33900,train loss is: 0.0005950782962202055\n",
      "test loss is 0.0006374467087767809\n",
      "-----------------------Epoch: 21----------------------------------\n",
      "Batch: 0,train loss is: 0.0003124018063276961\n",
      "test loss is 0.0006546437653964535\n",
      "Batch: 100,train loss is: 0.0010751885210842054\n",
      "test loss is 0.0006577537394528893\n",
      "Batch: 200,train loss is: 0.00032960385164574176\n",
      "test loss is 0.000672649987467417\n",
      "Batch: 300,train loss is: 0.000277639205919075\n",
      "test loss is 0.0006442363649757923\n",
      "Batch: 400,train loss is: 0.0004213753225071681\n",
      "test loss is 0.0006285761230700314\n",
      "Batch: 500,train loss is: 0.00044093411542095505\n",
      "test loss is 0.0006548757287805578\n",
      "Batch: 600,train loss is: 0.0002203226765390335\n",
      "test loss is 0.0006727862525004681\n",
      "Batch: 700,train loss is: 0.0003897049017661812\n",
      "test loss is 0.0006339588205020524\n",
      "Batch: 800,train loss is: 0.0003937324952132422\n",
      "test loss is 0.0006410355699761005\n",
      "Batch: 900,train loss is: 0.00035029953163764145\n",
      "test loss is 0.0006480432996150713\n",
      "Batch: 1000,train loss is: 0.0003986784930571688\n",
      "test loss is 0.00066484350576749\n",
      "Batch: 1100,train loss is: 0.0017680527683823844\n",
      "test loss is 0.000633319668033468\n",
      "Batch: 1200,train loss is: 0.00023562567581933465\n",
      "test loss is 0.0006609470182085223\n",
      "Batch: 1300,train loss is: 0.0005133080106692736\n",
      "test loss is 0.0006401424348400093\n",
      "Batch: 1400,train loss is: 0.00039496600999331045\n",
      "test loss is 0.0006672188787223628\n",
      "Batch: 1500,train loss is: 0.00030485896653393175\n",
      "test loss is 0.0006723907499525632\n",
      "Batch: 1600,train loss is: 0.0006595210867749525\n",
      "test loss is 0.0006988939102144562\n",
      "Batch: 1700,train loss is: 0.0011585409874020445\n",
      "test loss is 0.0006338092563225844\n",
      "Batch: 1800,train loss is: 0.0003443141750732395\n",
      "test loss is 0.0006306545529379554\n",
      "Batch: 1900,train loss is: 0.0003753460698518845\n",
      "test loss is 0.0006790302468291223\n",
      "Batch: 2000,train loss is: 0.0006796533635138754\n",
      "test loss is 0.0007789199680966452\n",
      "Batch: 2100,train loss is: 0.0006417597223352386\n",
      "test loss is 0.0007915198598953506\n",
      "Batch: 2200,train loss is: 0.0006987146034867412\n",
      "test loss is 0.0006492049769957055\n",
      "Batch: 2300,train loss is: 0.00041628760304696734\n",
      "test loss is 0.0006572775179215651\n",
      "Batch: 2400,train loss is: 0.00042167059152033816\n",
      "test loss is 0.0006464058449903\n",
      "Batch: 2500,train loss is: 0.0004705394304470709\n",
      "test loss is 0.0006366978008008171\n",
      "Batch: 2600,train loss is: 0.0004265183839383386\n",
      "test loss is 0.0006406897863202788\n",
      "Batch: 2700,train loss is: 0.0005056564191830644\n",
      "test loss is 0.0006217303199069965\n",
      "Batch: 2800,train loss is: 0.00042089217675150815\n",
      "test loss is 0.0006485963935105594\n",
      "Batch: 2900,train loss is: 0.00039527086193411607\n",
      "test loss is 0.0006391231294347135\n",
      "Batch: 3000,train loss is: 0.00045376122546287\n",
      "test loss is 0.0006372617004045283\n",
      "Batch: 3100,train loss is: 0.0005497833487058755\n",
      "test loss is 0.0006663789359658689\n",
      "Batch: 3200,train loss is: 0.0009718228573331836\n",
      "test loss is 0.0006544277560060898\n",
      "Batch: 3300,train loss is: 0.0005675951689949626\n",
      "test loss is 0.0006436917281683941\n",
      "Batch: 3400,train loss is: 0.0003503426939012523\n",
      "test loss is 0.0006412370525717192\n",
      "Batch: 3500,train loss is: 0.00022595907027537872\n",
      "test loss is 0.0006321617674003914\n",
      "Batch: 3600,train loss is: 0.0004628155053064986\n",
      "test loss is 0.0006899442118044816\n",
      "Batch: 3700,train loss is: 0.0003255234911214183\n",
      "test loss is 0.0006516435065195213\n",
      "Batch: 3800,train loss is: 0.000414838783352938\n",
      "test loss is 0.0006536151748077907\n",
      "Batch: 3900,train loss is: 0.0005639772875350293\n",
      "test loss is 0.0006425051455114535\n",
      "Batch: 4000,train loss is: 0.0004099779933725792\n",
      "test loss is 0.0006365894801253531\n",
      "Batch: 4100,train loss is: 0.0007284108833627591\n",
      "test loss is 0.000661654485771547\n",
      "Batch: 4200,train loss is: 0.0002513005866374269\n",
      "test loss is 0.0006626150303346117\n",
      "Batch: 4300,train loss is: 0.00041073549245408935\n",
      "test loss is 0.0006289818385603425\n",
      "Batch: 4400,train loss is: 0.00025259802603518533\n",
      "test loss is 0.0006633444493845346\n",
      "Batch: 4500,train loss is: 0.0006615547376055592\n",
      "test loss is 0.000656128156538944\n",
      "Batch: 4600,train loss is: 0.0005984303554242991\n",
      "test loss is 0.0006537469120705902\n",
      "Batch: 4700,train loss is: 0.0005569432114672926\n",
      "test loss is 0.000661885734686764\n",
      "Batch: 4800,train loss is: 0.0013798673095859794\n",
      "test loss is 0.0006310463993246789\n",
      "Batch: 4900,train loss is: 0.0004352997202041804\n",
      "test loss is 0.0006459199323751158\n",
      "Batch: 5000,train loss is: 0.0005424215192408644\n",
      "test loss is 0.0006569614779229395\n",
      "Batch: 5100,train loss is: 0.00044218995761868043\n",
      "test loss is 0.0006928928914633499\n",
      "Batch: 5200,train loss is: 0.0003294006605596112\n",
      "test loss is 0.0006576175732743188\n",
      "Batch: 5300,train loss is: 0.0004242007917634243\n",
      "test loss is 0.0006605094917476087\n",
      "Batch: 5400,train loss is: 0.0007021350393585546\n",
      "test loss is 0.0006367060796100983\n",
      "Batch: 5500,train loss is: 0.00041825843878496113\n",
      "test loss is 0.0006600735804782484\n",
      "Batch: 5600,train loss is: 0.0005526488938600239\n",
      "test loss is 0.0006362860332567812\n",
      "Batch: 5700,train loss is: 0.0004155115054522651\n",
      "test loss is 0.000649900325879906\n",
      "Batch: 5800,train loss is: 0.00042516296110604103\n",
      "test loss is 0.000641534819782568\n",
      "Batch: 5900,train loss is: 0.0031868459917918043\n",
      "test loss is 0.0006471016730063874\n",
      "Batch: 6000,train loss is: 0.001711259576010132\n",
      "test loss is 0.0006703463015995204\n",
      "Batch: 6100,train loss is: 0.0004086781028978527\n",
      "test loss is 0.000633252384618331\n",
      "Batch: 6200,train loss is: 0.0005380626287057728\n",
      "test loss is 0.0006400364067510821\n",
      "Batch: 6300,train loss is: 0.0010008897193838344\n",
      "test loss is 0.0006560697856019417\n",
      "Batch: 6400,train loss is: 0.0005192530973723904\n",
      "test loss is 0.0006818902239472724\n",
      "Batch: 6500,train loss is: 0.0003061203574729349\n",
      "test loss is 0.0006429855745734916\n",
      "Batch: 6600,train loss is: 0.00185907472687072\n",
      "test loss is 0.0006345662498798433\n",
      "Batch: 6700,train loss is: 0.00029110069398251883\n",
      "test loss is 0.0006417265185687037\n",
      "Batch: 6800,train loss is: 0.000691177610668889\n",
      "test loss is 0.0006377582674200903\n",
      "Batch: 6900,train loss is: 0.0006088732487168202\n",
      "test loss is 0.0006537160402683078\n",
      "Batch: 7000,train loss is: 0.0005278123078311276\n",
      "test loss is 0.000656986634699518\n",
      "Batch: 7100,train loss is: 0.0011043288104541208\n",
      "test loss is 0.0006471496378869312\n",
      "Batch: 7200,train loss is: 0.00045058446322007744\n",
      "test loss is 0.0006500269821274511\n",
      "Batch: 7300,train loss is: 0.0006556241353334536\n",
      "test loss is 0.0006626716877942583\n",
      "Batch: 7400,train loss is: 0.0003465955331402004\n",
      "test loss is 0.0006402633289214155\n",
      "Batch: 7500,train loss is: 0.0006106787065680502\n",
      "test loss is 0.0006598534347647151\n",
      "Batch: 7600,train loss is: 0.0004076305152301332\n",
      "test loss is 0.0006247539870183265\n",
      "Batch: 7700,train loss is: 0.0008696920663198691\n",
      "test loss is 0.0006548610719719996\n",
      "Batch: 7800,train loss is: 0.0002588115462344644\n",
      "test loss is 0.0006330277858423585\n",
      "Batch: 7900,train loss is: 0.00043791981760787376\n",
      "test loss is 0.0006539956490097008\n",
      "Batch: 8000,train loss is: 0.0003525870925177989\n",
      "test loss is 0.0006758922784710102\n",
      "Batch: 8100,train loss is: 0.0005626200987374708\n",
      "test loss is 0.0006573767636287368\n",
      "Batch: 8200,train loss is: 0.0010820199314324006\n",
      "test loss is 0.0006565761980712701\n",
      "Batch: 8300,train loss is: 0.0002865988327166023\n",
      "test loss is 0.0006525532638038083\n",
      "Batch: 8400,train loss is: 0.0005395835754123805\n",
      "test loss is 0.0006502744686563411\n",
      "Batch: 8500,train loss is: 0.0005533634741595114\n",
      "test loss is 0.0006533788622469095\n",
      "Batch: 8600,train loss is: 0.0006445393299282673\n",
      "test loss is 0.0006615136998356594\n",
      "Batch: 8700,train loss is: 0.0004411900591839823\n",
      "test loss is 0.0007043975182045551\n",
      "Batch: 8800,train loss is: 0.00034716100846342415\n",
      "test loss is 0.0006378342659219865\n",
      "Batch: 8900,train loss is: 0.0010062837699640393\n",
      "test loss is 0.0006362807708999201\n",
      "Batch: 9000,train loss is: 0.0002602289654071513\n",
      "test loss is 0.0006632433976305422\n",
      "Batch: 9100,train loss is: 0.00031598984756520826\n",
      "test loss is 0.000635369969145723\n",
      "Batch: 9200,train loss is: 0.0007198503130126182\n",
      "test loss is 0.0006849525777025327\n",
      "Batch: 9300,train loss is: 0.0004988062211706234\n",
      "test loss is 0.0006866262734737161\n",
      "Batch: 9400,train loss is: 0.00045488913381128355\n",
      "test loss is 0.000670167428695562\n",
      "Batch: 9500,train loss is: 0.0003412625440085484\n",
      "test loss is 0.0006666521304508062\n",
      "Batch: 9600,train loss is: 0.000381060947367355\n",
      "test loss is 0.0006304746843221277\n",
      "Batch: 9700,train loss is: 0.0003936680622875523\n",
      "test loss is 0.0006423724576186515\n",
      "Batch: 9800,train loss is: 0.00024008381726164162\n",
      "test loss is 0.0006443495778215147\n",
      "Batch: 9900,train loss is: 0.0004397679237155177\n",
      "test loss is 0.0006592900793974047\n",
      "Batch: 10000,train loss is: 0.00040842674517954853\n",
      "test loss is 0.0006301208764655078\n",
      "Batch: 10100,train loss is: 0.00083668136408571\n",
      "test loss is 0.0006488962432820073\n",
      "Batch: 10200,train loss is: 0.001649542306076162\n",
      "test loss is 0.0007223794118563793\n",
      "Batch: 10300,train loss is: 0.0003992622692251601\n",
      "test loss is 0.0006428739093002616\n",
      "Batch: 10400,train loss is: 0.00025823213503304226\n",
      "test loss is 0.0006373904385381128\n",
      "Batch: 10500,train loss is: 0.0007759519149611225\n",
      "test loss is 0.0006511669059721487\n",
      "Batch: 10600,train loss is: 0.0002767547036443897\n",
      "test loss is 0.0006951188124646228\n",
      "Batch: 10700,train loss is: 0.0005575131180002686\n",
      "test loss is 0.000636496366830412\n",
      "Batch: 10800,train loss is: 0.0006301169105992721\n",
      "test loss is 0.0006473824826304178\n",
      "Batch: 10900,train loss is: 0.0006641071035412727\n",
      "test loss is 0.0006444688562754312\n",
      "Batch: 11000,train loss is: 0.0007197824786902375\n",
      "test loss is 0.0006643080133010144\n",
      "Batch: 11100,train loss is: 0.0008890963023761864\n",
      "test loss is 0.0006320117984185251\n",
      "Batch: 11200,train loss is: 0.0006886390014582441\n",
      "test loss is 0.000690086617318739\n",
      "Batch: 11300,train loss is: 0.0002853641829075148\n",
      "test loss is 0.0006540988850329274\n",
      "Batch: 11400,train loss is: 0.0007400537509908161\n",
      "test loss is 0.0006439918029650807\n",
      "Batch: 11500,train loss is: 0.0004496277288517782\n",
      "test loss is 0.0006451263755957248\n",
      "Batch: 11600,train loss is: 0.0017687641847477478\n",
      "test loss is 0.0006385203032517404\n",
      "Batch: 11700,train loss is: 0.00030620050762223306\n",
      "test loss is 0.0006318670928122735\n",
      "Batch: 11800,train loss is: 0.0005439003899239081\n",
      "test loss is 0.0006424076615884524\n",
      "Batch: 11900,train loss is: 0.0004542355615361628\n",
      "test loss is 0.0006525229581123996\n",
      "Batch: 12000,train loss is: 0.00029965556696287716\n",
      "test loss is 0.000642976213091141\n",
      "Batch: 12100,train loss is: 0.001552236686250383\n",
      "test loss is 0.0006530477136269589\n",
      "Batch: 12200,train loss is: 0.0005980895963971808\n",
      "test loss is 0.000663410882487996\n",
      "Batch: 12300,train loss is: 0.00043839120338117073\n",
      "test loss is 0.0006544783452196934\n",
      "Batch: 12400,train loss is: 0.00034183992292074035\n",
      "test loss is 0.0006456689817080132\n",
      "Batch: 12500,train loss is: 0.0004582862419020653\n",
      "test loss is 0.0006667719308293151\n",
      "Batch: 12600,train loss is: 0.0004892384242925707\n",
      "test loss is 0.0006544714432107439\n",
      "Batch: 12700,train loss is: 0.0003438702608522683\n",
      "test loss is 0.0006375398940939285\n",
      "Batch: 12800,train loss is: 0.000348126334807901\n",
      "test loss is 0.0006355566538109102\n",
      "Batch: 12900,train loss is: 0.0005307260158361886\n",
      "test loss is 0.0006469282158735114\n",
      "Batch: 13000,train loss is: 0.00020245771415300539\n",
      "test loss is 0.0006384609961345349\n",
      "Batch: 13100,train loss is: 0.0019457927402344454\n",
      "test loss is 0.0006312036458297237\n",
      "Batch: 13200,train loss is: 0.000910891568929344\n",
      "test loss is 0.000772033549712018\n",
      "Batch: 13300,train loss is: 0.0005380063653656856\n",
      "test loss is 0.0006464953830264406\n",
      "Batch: 13400,train loss is: 0.00035177056881066226\n",
      "test loss is 0.0006648647743332139\n",
      "Batch: 13500,train loss is: 0.0005154340801077451\n",
      "test loss is 0.0006592896966761199\n",
      "Batch: 13600,train loss is: 0.0004995637196719828\n",
      "test loss is 0.0006617498325216077\n",
      "Batch: 13700,train loss is: 0.0006446746340126693\n",
      "test loss is 0.0006279527102803384\n",
      "Batch: 13800,train loss is: 0.00037725836176058183\n",
      "test loss is 0.0006317077540529319\n",
      "Batch: 13900,train loss is: 0.00026520545331947855\n",
      "test loss is 0.0007052189223448287\n",
      "Batch: 14000,train loss is: 0.00033876475185106656\n",
      "test loss is 0.0006589685384350195\n",
      "Batch: 14100,train loss is: 0.0006814997474317105\n",
      "test loss is 0.0006741652936182219\n",
      "Batch: 14200,train loss is: 0.000555984928717381\n",
      "test loss is 0.0006297248408280779\n",
      "Batch: 14300,train loss is: 0.000240807975163729\n",
      "test loss is 0.000632977642490595\n",
      "Batch: 14400,train loss is: 0.0011906816602428518\n",
      "test loss is 0.0006312214006612762\n",
      "Batch: 14500,train loss is: 0.0006284160726177825\n",
      "test loss is 0.0006329047460853057\n",
      "Batch: 14600,train loss is: 0.0007916108677435264\n",
      "test loss is 0.0006519930896405342\n",
      "Batch: 14700,train loss is: 0.00047416270196656927\n",
      "test loss is 0.0007089613018717733\n",
      "Batch: 14800,train loss is: 0.0007781169160593921\n",
      "test loss is 0.0006575563156926654\n",
      "Batch: 14900,train loss is: 0.0004141306405901016\n",
      "test loss is 0.0006942643357446741\n",
      "Batch: 15000,train loss is: 0.0005586264604631712\n",
      "test loss is 0.000630330884281277\n",
      "Batch: 15100,train loss is: 0.0007364228380973742\n",
      "test loss is 0.0006617396952250321\n",
      "Batch: 15200,train loss is: 0.0007650268912247491\n",
      "test loss is 0.0006440413737973923\n",
      "Batch: 15300,train loss is: 0.00031730910451776174\n",
      "test loss is 0.0006347463941971477\n",
      "Batch: 15400,train loss is: 0.0004614051214203534\n",
      "test loss is 0.0006595251041177058\n",
      "Batch: 15500,train loss is: 0.0006607054636068509\n",
      "test loss is 0.0006315242475124296\n",
      "Batch: 15600,train loss is: 0.0007916142900023553\n",
      "test loss is 0.000648771947845861\n",
      "Batch: 15700,train loss is: 0.0003554595317885024\n",
      "test loss is 0.0006398385951309652\n",
      "Batch: 15800,train loss is: 0.0006077811012716177\n",
      "test loss is 0.0006238535944328781\n",
      "Batch: 15900,train loss is: 0.0007884105043166752\n",
      "test loss is 0.0006449312528712299\n",
      "Batch: 16000,train loss is: 0.0003583662399430627\n",
      "test loss is 0.0006390464985247306\n",
      "Batch: 16100,train loss is: 0.000580512007483589\n",
      "test loss is 0.0006408618422637572\n",
      "Batch: 16200,train loss is: 0.000360993636875322\n",
      "test loss is 0.0006286837028118443\n",
      "Batch: 16300,train loss is: 0.0008801850385970765\n",
      "test loss is 0.0006601204033797071\n",
      "Batch: 16400,train loss is: 0.00029058277752788957\n",
      "test loss is 0.00063150026049951\n",
      "Batch: 16500,train loss is: 0.000672831011406538\n",
      "test loss is 0.0006525344240622474\n",
      "Batch: 16600,train loss is: 0.0003252826260445243\n",
      "test loss is 0.0006359218204218886\n",
      "Batch: 16700,train loss is: 0.0007959384034221778\n",
      "test loss is 0.0006444243236573553\n",
      "Batch: 16800,train loss is: 0.0004994629863903771\n",
      "test loss is 0.0006691774537214165\n",
      "Batch: 16900,train loss is: 0.000703612823411678\n",
      "test loss is 0.0006487288567040601\n",
      "Batch: 17000,train loss is: 0.000584863078826432\n",
      "test loss is 0.0006265970982641997\n",
      "Batch: 17100,train loss is: 0.00044721219001600185\n",
      "test loss is 0.0006341966451135666\n",
      "Batch: 17200,train loss is: 0.000407945536879536\n",
      "test loss is 0.0006431200284192528\n",
      "Batch: 17300,train loss is: 0.0007403758939477229\n",
      "test loss is 0.0006581175096077338\n",
      "Batch: 17400,train loss is: 0.0003603527775257457\n",
      "test loss is 0.0006532005569142308\n",
      "Batch: 17500,train loss is: 0.0005827571310060113\n",
      "test loss is 0.0006449023339630939\n",
      "Batch: 17600,train loss is: 0.0004185256204809281\n",
      "test loss is 0.0006519443887106834\n",
      "Batch: 17700,train loss is: 0.0003193741010576794\n",
      "test loss is 0.00067948663909914\n",
      "Batch: 17800,train loss is: 0.0015538667539649411\n",
      "test loss is 0.0006680172246912843\n",
      "Batch: 17900,train loss is: 0.00044001979202634066\n",
      "test loss is 0.000665876299545127\n",
      "Batch: 18000,train loss is: 0.0002628033523246006\n",
      "test loss is 0.0006261024885599795\n",
      "Batch: 18100,train loss is: 0.0003541867281658691\n",
      "test loss is 0.0006357434823848789\n",
      "Batch: 18200,train loss is: 0.0005838711153810932\n",
      "test loss is 0.0006457129757116844\n",
      "Batch: 18300,train loss is: 0.001311528657147142\n",
      "test loss is 0.0006473585885866534\n",
      "Batch: 18400,train loss is: 0.0004011070093681363\n",
      "test loss is 0.0006378332005371024\n",
      "Batch: 18500,train loss is: 0.0008520678815527827\n",
      "test loss is 0.0006417161315065792\n",
      "Batch: 18600,train loss is: 0.000438488313541765\n",
      "test loss is 0.0006394493893540773\n",
      "Batch: 18700,train loss is: 0.0005497939240589336\n",
      "test loss is 0.0006556890704410562\n",
      "Batch: 18800,train loss is: 0.000539842700911255\n",
      "test loss is 0.0006324350153902847\n",
      "Batch: 18900,train loss is: 0.000498376647743727\n",
      "test loss is 0.0006520278740284463\n",
      "Batch: 19000,train loss is: 0.0005411555909580053\n",
      "test loss is 0.0006366849503114056\n",
      "Batch: 19100,train loss is: 0.0003759476461662907\n",
      "test loss is 0.0006627869825143657\n",
      "Batch: 19200,train loss is: 0.0005170770565389761\n",
      "test loss is 0.0006461361716543115\n",
      "Batch: 19300,train loss is: 0.000483568261384455\n",
      "test loss is 0.000627696131716428\n",
      "Batch: 19400,train loss is: 0.0002564993699964451\n",
      "test loss is 0.000634095332158663\n",
      "Batch: 19500,train loss is: 0.0004890661084883994\n",
      "test loss is 0.0006706513825806725\n",
      "Batch: 19600,train loss is: 0.0003751667519281357\n",
      "test loss is 0.000645361146987887\n",
      "Batch: 19700,train loss is: 0.0006420364726990457\n",
      "test loss is 0.0007141489114713894\n",
      "Batch: 19800,train loss is: 0.0005595658159967543\n",
      "test loss is 0.0006506438955003051\n",
      "Batch: 19900,train loss is: 0.0003882140090293834\n",
      "test loss is 0.0006610554836814491\n",
      "Batch: 20000,train loss is: 0.0007039157943467122\n",
      "test loss is 0.0006365433850641921\n",
      "Batch: 20100,train loss is: 0.0005709967250927008\n",
      "test loss is 0.0006532613726585274\n",
      "Batch: 20200,train loss is: 0.0006923764325577231\n",
      "test loss is 0.0006551830305340277\n",
      "Batch: 20300,train loss is: 0.00028932582128765827\n",
      "test loss is 0.0006562150281521443\n",
      "Batch: 20400,train loss is: 0.00047709539259287534\n",
      "test loss is 0.00065961166359135\n",
      "Batch: 20500,train loss is: 0.0006276489061289184\n",
      "test loss is 0.0006344211874071883\n",
      "Batch: 20600,train loss is: 0.0004976643161899263\n",
      "test loss is 0.0006667330729380045\n",
      "Batch: 20700,train loss is: 0.0007917870874323051\n",
      "test loss is 0.0006380916821263433\n",
      "Batch: 20800,train loss is: 0.00033212523621430583\n",
      "test loss is 0.0006788856663487307\n",
      "Batch: 20900,train loss is: 0.0005393851350578391\n",
      "test loss is 0.0006831113414774017\n",
      "Batch: 21000,train loss is: 0.00021664202515339683\n",
      "test loss is 0.0006279293988395768\n",
      "Batch: 21100,train loss is: 0.0006648521451051115\n",
      "test loss is 0.0006746915360794922\n",
      "Batch: 21200,train loss is: 0.0003686282727946391\n",
      "test loss is 0.0006723268068077722\n",
      "Batch: 21300,train loss is: 0.00029715565809129696\n",
      "test loss is 0.0006512772216487223\n",
      "Batch: 21400,train loss is: 0.0007990286210994204\n",
      "test loss is 0.0006324604752489939\n",
      "Batch: 21500,train loss is: 0.00023762734181939957\n",
      "test loss is 0.0006570931265291715\n",
      "Batch: 21600,train loss is: 0.0006851972824994929\n",
      "test loss is 0.0006521370084277907\n",
      "Batch: 21700,train loss is: 0.00027352245829464627\n",
      "test loss is 0.0006294629074084739\n",
      "Batch: 21800,train loss is: 0.0007156110172882496\n",
      "test loss is 0.0006843482833271395\n",
      "Batch: 21900,train loss is: 0.00035768116902195696\n",
      "test loss is 0.0007090725050872391\n",
      "Batch: 22000,train loss is: 0.0006459780879675673\n",
      "test loss is 0.000635166812854041\n",
      "Batch: 22100,train loss is: 0.0003458499932931863\n",
      "test loss is 0.0006332379489139738\n",
      "Batch: 22200,train loss is: 0.00045645289645952856\n",
      "test loss is 0.0006458736209949775\n",
      "Batch: 22300,train loss is: 0.0004719266310066363\n",
      "test loss is 0.0006600115599914383\n",
      "Batch: 22400,train loss is: 0.0004455037434967522\n",
      "test loss is 0.0006532618172725979\n",
      "Batch: 22500,train loss is: 0.0004962200353556691\n",
      "test loss is 0.0006275556040479969\n",
      "Batch: 22600,train loss is: 0.0008068524398954953\n",
      "test loss is 0.0006719891722403383\n",
      "Batch: 22700,train loss is: 0.0003078200240418202\n",
      "test loss is 0.0006324166989663308\n",
      "Batch: 22800,train loss is: 0.00047778863197856186\n",
      "test loss is 0.0006607355951059996\n",
      "Batch: 22900,train loss is: 0.0002937058296149496\n",
      "test loss is 0.0007211141914712741\n",
      "Batch: 23000,train loss is: 0.0005452727583350754\n",
      "test loss is 0.0006441907713922343\n",
      "Batch: 23100,train loss is: 0.0004131563973603813\n",
      "test loss is 0.0006269439969144574\n",
      "Batch: 23200,train loss is: 0.00028935631056771694\n",
      "test loss is 0.0006221497783365618\n",
      "Batch: 23300,train loss is: 0.00042168434184319825\n",
      "test loss is 0.0006291214681656314\n",
      "Batch: 23400,train loss is: 0.0005699583638085151\n",
      "test loss is 0.0006586749211433725\n",
      "Batch: 23500,train loss is: 0.0014018801640287915\n",
      "test loss is 0.00062200348589393\n",
      "Batch: 23600,train loss is: 0.0003121114428377945\n",
      "test loss is 0.0006379811946079318\n",
      "Batch: 23700,train loss is: 0.0010344306510057504\n",
      "test loss is 0.0006325283170218988\n",
      "Batch: 23800,train loss is: 0.000812847163465279\n",
      "test loss is 0.0006372545505081664\n",
      "Batch: 23900,train loss is: 0.0009624684711575269\n",
      "test loss is 0.0006579629272541659\n",
      "Batch: 24000,train loss is: 0.0007542691677243426\n",
      "test loss is 0.0006894069574775465\n",
      "Batch: 24100,train loss is: 0.000267178346115147\n",
      "test loss is 0.0006278532682286067\n",
      "Batch: 24200,train loss is: 0.0013030086584536703\n",
      "test loss is 0.0006502607629397605\n",
      "Batch: 24300,train loss is: 0.00047849080360340375\n",
      "test loss is 0.0007425435364042527\n",
      "Batch: 24400,train loss is: 0.001254423622902665\n",
      "test loss is 0.000637648315715366\n",
      "Batch: 24500,train loss is: 0.0007219085547817291\n",
      "test loss is 0.0006222024722288268\n",
      "Batch: 24600,train loss is: 0.00035550358117872116\n",
      "test loss is 0.0006607259394876673\n",
      "Batch: 24700,train loss is: 0.000268910393823818\n",
      "test loss is 0.0006450038883623499\n",
      "Batch: 24800,train loss is: 0.0006417746587027976\n",
      "test loss is 0.0006460273688642696\n",
      "Batch: 24900,train loss is: 0.00019489415362266752\n",
      "test loss is 0.0006354362096143155\n",
      "Batch: 25000,train loss is: 0.00046049318810308326\n",
      "test loss is 0.0006342629424760017\n",
      "Batch: 25100,train loss is: 0.000374278766504239\n",
      "test loss is 0.0006432089360334942\n",
      "Batch: 25200,train loss is: 0.0007084566207567238\n",
      "test loss is 0.0006447790603737523\n",
      "Batch: 25300,train loss is: 0.0007795997465453566\n",
      "test loss is 0.0006913743618145173\n",
      "Batch: 25400,train loss is: 0.00040904618689243435\n",
      "test loss is 0.0006354691284983679\n",
      "Batch: 25500,train loss is: 0.0005694476069845615\n",
      "test loss is 0.0006434383140620962\n",
      "Batch: 25600,train loss is: 0.0005991187734186997\n",
      "test loss is 0.0006452563854807067\n",
      "Batch: 25700,train loss is: 0.0005967716425752166\n",
      "test loss is 0.0006532653831494532\n",
      "Batch: 25800,train loss is: 0.0003172376991327171\n",
      "test loss is 0.0006321519612806568\n",
      "Batch: 25900,train loss is: 0.0002701980549371179\n",
      "test loss is 0.0006872807175202263\n",
      "Batch: 26000,train loss is: 0.0014377536359700554\n",
      "test loss is 0.000682852033636215\n",
      "Batch: 26100,train loss is: 0.0007354072818748664\n",
      "test loss is 0.0006306088676515385\n",
      "Batch: 26200,train loss is: 0.0005818538286162313\n",
      "test loss is 0.0006261778277007741\n",
      "Batch: 26300,train loss is: 0.0009677192198573805\n",
      "test loss is 0.0006430944204372461\n",
      "Batch: 26400,train loss is: 0.0005523781226255614\n",
      "test loss is 0.0006250120554143289\n",
      "Batch: 26500,train loss is: 0.0006078313940030729\n",
      "test loss is 0.0006743647973359124\n",
      "Batch: 26600,train loss is: 0.0012333611723670437\n",
      "test loss is 0.0007257686780726244\n",
      "Batch: 26700,train loss is: 0.0005143860968346699\n",
      "test loss is 0.0007091543694098913\n",
      "Batch: 26800,train loss is: 0.0011407474867056152\n",
      "test loss is 0.0006530447359846682\n",
      "Batch: 26900,train loss is: 0.0004685367032771693\n",
      "test loss is 0.0006420602258488201\n",
      "Batch: 27000,train loss is: 0.000645979086725551\n",
      "test loss is 0.0007438209723554434\n",
      "Batch: 27100,train loss is: 0.0006921729507379997\n",
      "test loss is 0.0007052872186444923\n",
      "Batch: 27200,train loss is: 0.0005860869787336654\n",
      "test loss is 0.0006575891561680079\n",
      "Batch: 27300,train loss is: 0.0004416898430626459\n",
      "test loss is 0.0006518139190618008\n",
      "Batch: 27400,train loss is: 0.0005925332489887202\n",
      "test loss is 0.000643138313001523\n",
      "Batch: 27500,train loss is: 0.00044113191174479245\n",
      "test loss is 0.0006964918480496143\n",
      "Batch: 27600,train loss is: 0.0004631921398985376\n",
      "test loss is 0.000661794045998323\n",
      "Batch: 27700,train loss is: 0.0015355535390794547\n",
      "test loss is 0.0006436970204627616\n",
      "Batch: 27800,train loss is: 0.0007742863143752626\n",
      "test loss is 0.0006564666448562041\n",
      "Batch: 27900,train loss is: 0.0011689544822011788\n",
      "test loss is 0.0006318899008240106\n",
      "Batch: 28000,train loss is: 0.00038243978793671245\n",
      "test loss is 0.0006372347072852119\n",
      "Batch: 28100,train loss is: 0.0007670407685140085\n",
      "test loss is 0.0006457394980070164\n",
      "Batch: 28200,train loss is: 0.000522942519257208\n",
      "test loss is 0.0006235171429472764\n",
      "Batch: 28300,train loss is: 0.001688094809043042\n",
      "test loss is 0.0006461982238020037\n",
      "Batch: 28400,train loss is: 0.000577259546414404\n",
      "test loss is 0.0006297798050122008\n",
      "Batch: 28500,train loss is: 0.0012947513419941914\n",
      "test loss is 0.0006851262335768812\n",
      "Batch: 28600,train loss is: 0.00034772504889456126\n",
      "test loss is 0.0006425509482030972\n",
      "Batch: 28700,train loss is: 0.0005430448411995378\n",
      "test loss is 0.0006320764377021899\n",
      "Batch: 28800,train loss is: 0.000990663605774118\n",
      "test loss is 0.0006492675334015132\n",
      "Batch: 28900,train loss is: 0.00045600994264831005\n",
      "test loss is 0.0006251894825551395\n",
      "Batch: 29000,train loss is: 0.0004765647301740283\n",
      "test loss is 0.0006291547069881499\n",
      "Batch: 29100,train loss is: 0.0004704983835343877\n",
      "test loss is 0.0006395755227931002\n",
      "Batch: 29200,train loss is: 0.0004243351962989984\n",
      "test loss is 0.0006427972542593335\n",
      "Batch: 29300,train loss is: 0.0008890759188023664\n",
      "test loss is 0.0006311246347040673\n",
      "Batch: 29400,train loss is: 0.0006055701723774343\n",
      "test loss is 0.0007368734967620796\n",
      "Batch: 29500,train loss is: 0.0005196272678112718\n",
      "test loss is 0.0006751337376208551\n",
      "Batch: 29600,train loss is: 0.0005688333427403418\n",
      "test loss is 0.0006213958048356012\n",
      "Batch: 29700,train loss is: 0.0003473960842561935\n",
      "test loss is 0.0006410955621301575\n",
      "Batch: 29800,train loss is: 0.0007932448609435375\n",
      "test loss is 0.0006293195382051918\n",
      "Batch: 29900,train loss is: 0.002710077798076969\n",
      "test loss is 0.0006334434449800377\n",
      "Batch: 30000,train loss is: 0.0003658691037405913\n",
      "test loss is 0.0006709324373348191\n",
      "Batch: 30100,train loss is: 0.0005247559851757005\n",
      "test loss is 0.0006631769103824415\n",
      "Batch: 30200,train loss is: 0.0003429271884213064\n",
      "test loss is 0.0006427419059727886\n",
      "Batch: 30300,train loss is: 0.0002706199017011434\n",
      "test loss is 0.0006271210003956447\n",
      "Batch: 30400,train loss is: 0.0002994749854741665\n",
      "test loss is 0.0006357065836772084\n",
      "Batch: 30500,train loss is: 0.0003065481771458713\n",
      "test loss is 0.0006719680835818097\n",
      "Batch: 30600,train loss is: 0.0002422976114368155\n",
      "test loss is 0.0006391289136893347\n",
      "Batch: 30700,train loss is: 0.00040447802915642375\n",
      "test loss is 0.0007052502401252771\n",
      "Batch: 30800,train loss is: 0.0002651392546631071\n",
      "test loss is 0.0006916984824406119\n",
      "Batch: 30900,train loss is: 0.0003051508196693116\n",
      "test loss is 0.0006382386784032409\n",
      "Batch: 31000,train loss is: 0.0004650099475046374\n",
      "test loss is 0.0006479848359902465\n",
      "Batch: 31100,train loss is: 0.00029787815517579903\n",
      "test loss is 0.0007254300042969016\n",
      "Batch: 31200,train loss is: 0.0003907074620641321\n",
      "test loss is 0.0006384299896131951\n",
      "Batch: 31300,train loss is: 0.00044423767567233606\n",
      "test loss is 0.0006363255341563094\n",
      "Batch: 31400,train loss is: 0.0006826751128205878\n",
      "test loss is 0.0006310677032181598\n",
      "Batch: 31500,train loss is: 0.00042281291707947915\n",
      "test loss is 0.0006480100346405298\n",
      "Batch: 31600,train loss is: 0.000372518586101124\n",
      "test loss is 0.000639406192701885\n",
      "Batch: 31700,train loss is: 0.0004700015132415948\n",
      "test loss is 0.0006576197001901964\n",
      "Batch: 31800,train loss is: 0.0011517863333300998\n",
      "test loss is 0.0006363357362226468\n",
      "Batch: 31900,train loss is: 0.0006326880617284701\n",
      "test loss is 0.0006417463552630676\n",
      "Batch: 32000,train loss is: 0.0034113524592963807\n",
      "test loss is 0.0006438675007983282\n",
      "Batch: 32100,train loss is: 0.000306517759728795\n",
      "test loss is 0.0006705490343025329\n",
      "Batch: 32200,train loss is: 0.00029767412898044085\n",
      "test loss is 0.0006357320356431203\n",
      "Batch: 32300,train loss is: 0.0003360959916316862\n",
      "test loss is 0.0006262941283089213\n",
      "Batch: 32400,train loss is: 0.00034205285938472324\n",
      "test loss is 0.0006224490594325473\n",
      "Batch: 32500,train loss is: 0.0008001693526909801\n",
      "test loss is 0.0006631870767792617\n",
      "Batch: 32600,train loss is: 0.00046361020678040024\n",
      "test loss is 0.000651686207049885\n",
      "Batch: 32700,train loss is: 0.000500060870937197\n",
      "test loss is 0.0006834580710116478\n",
      "Batch: 32800,train loss is: 0.0004961996710820838\n",
      "test loss is 0.000677177770661007\n",
      "Batch: 32900,train loss is: 0.000627274481074056\n",
      "test loss is 0.0006444515264212653\n",
      "Batch: 33000,train loss is: 0.0003805477551478984\n",
      "test loss is 0.0006408905948313402\n",
      "Batch: 33100,train loss is: 0.00040223609700767976\n",
      "test loss is 0.0006421688485201804\n",
      "Batch: 33200,train loss is: 0.0004440068982154568\n",
      "test loss is 0.0007103718412016605\n",
      "Batch: 33300,train loss is: 0.0005792727666983621\n",
      "test loss is 0.0006362330110964253\n",
      "Batch: 33400,train loss is: 0.0004377075102484159\n",
      "test loss is 0.0006401449423892137\n",
      "Batch: 33500,train loss is: 0.0003205960640499706\n",
      "test loss is 0.0006433215833485467\n",
      "Batch: 33600,train loss is: 0.000530987102121253\n",
      "test loss is 0.0006318150464370486\n",
      "Batch: 33700,train loss is: 0.0005047561312527282\n",
      "test loss is 0.0006522770771587377\n",
      "Batch: 33800,train loss is: 0.0006891102263214446\n",
      "test loss is 0.0006339198587458109\n",
      "Batch: 33900,train loss is: 0.0005861137416129685\n",
      "test loss is 0.000631033532783144\n",
      "-----------------------Epoch: 22----------------------------------\n",
      "Batch: 0,train loss is: 0.0003249026201234773\n",
      "test loss is 0.0006473415589452092\n",
      "Batch: 100,train loss is: 0.0010470803075543378\n",
      "test loss is 0.0006530920313616259\n",
      "Batch: 200,train loss is: 0.00031686716096796983\n",
      "test loss is 0.0006667234391496078\n",
      "Batch: 300,train loss is: 0.00027967847404313016\n",
      "test loss is 0.0006382964364840148\n",
      "Batch: 400,train loss is: 0.0004144055562399455\n",
      "test loss is 0.0006220869510113494\n",
      "Batch: 500,train loss is: 0.00043704692081821206\n",
      "test loss is 0.0006477162604333828\n",
      "Batch: 600,train loss is: 0.00021802880894395\n",
      "test loss is 0.000665210672759372\n",
      "Batch: 700,train loss is: 0.0003983710592523266\n",
      "test loss is 0.0006297858280691971\n",
      "Batch: 800,train loss is: 0.0003916411421253523\n",
      "test loss is 0.0006360700555948999\n",
      "Batch: 900,train loss is: 0.0003692334354504762\n",
      "test loss is 0.0006437409259061604\n",
      "Batch: 1000,train loss is: 0.0003868155769009547\n",
      "test loss is 0.0006562624714332396\n",
      "Batch: 1100,train loss is: 0.001790016278423955\n",
      "test loss is 0.0006274099489695662\n",
      "Batch: 1200,train loss is: 0.00023232606332775516\n",
      "test loss is 0.0006559007664817491\n",
      "Batch: 1300,train loss is: 0.0005097704248363422\n",
      "test loss is 0.0006347431512271874\n",
      "Batch: 1400,train loss is: 0.0003883691391906094\n",
      "test loss is 0.0006621115674556481\n",
      "Batch: 1500,train loss is: 0.0002835883305414814\n",
      "test loss is 0.0006654855302249317\n",
      "Batch: 1600,train loss is: 0.00064926396173441\n",
      "test loss is 0.0006886974654068902\n",
      "Batch: 1700,train loss is: 0.001155210952821234\n",
      "test loss is 0.0006273809157843577\n",
      "Batch: 1800,train loss is: 0.000346915134922384\n",
      "test loss is 0.0006239199445151841\n",
      "Batch: 1900,train loss is: 0.000370672398356296\n",
      "test loss is 0.0006759402944012768\n",
      "Batch: 2000,train loss is: 0.0007212161642868767\n",
      "test loss is 0.0007706605614828937\n",
      "Batch: 2100,train loss is: 0.0006376761704632974\n",
      "test loss is 0.0007796920257765048\n",
      "Batch: 2200,train loss is: 0.0007032282839442777\n",
      "test loss is 0.0006448762104614603\n",
      "Batch: 2300,train loss is: 0.00040235728672190603\n",
      "test loss is 0.0006497073763219914\n",
      "Batch: 2400,train loss is: 0.0004208860849814659\n",
      "test loss is 0.0006413902065125008\n",
      "Batch: 2500,train loss is: 0.0004494527221360619\n",
      "test loss is 0.000631264250571683\n",
      "Batch: 2600,train loss is: 0.0004297502339613462\n",
      "test loss is 0.000634197755296483\n",
      "Batch: 2700,train loss is: 0.0004837176326166267\n",
      "test loss is 0.0006153247115608399\n",
      "Batch: 2800,train loss is: 0.0004118277459581121\n",
      "test loss is 0.0006414882773497349\n",
      "Batch: 2900,train loss is: 0.0003918466532981454\n",
      "test loss is 0.0006346591299127916\n",
      "Batch: 3000,train loss is: 0.00044751274367027543\n",
      "test loss is 0.0006313424145920065\n",
      "Batch: 3100,train loss is: 0.0005358821083872495\n",
      "test loss is 0.0006581665937156626\n",
      "Batch: 3200,train loss is: 0.0009550545848497134\n",
      "test loss is 0.0006476372201063462\n",
      "Batch: 3300,train loss is: 0.000568391476687135\n",
      "test loss is 0.0006363190589213102\n",
      "Batch: 3400,train loss is: 0.0003439965510252274\n",
      "test loss is 0.000635150947061622\n",
      "Batch: 3500,train loss is: 0.00022938881612039297\n",
      "test loss is 0.0006262320598584319\n",
      "Batch: 3600,train loss is: 0.0004404597250711077\n",
      "test loss is 0.0006865461351350901\n",
      "Batch: 3700,train loss is: 0.0003205235189161327\n",
      "test loss is 0.0006481911457371245\n",
      "Batch: 3800,train loss is: 0.00041027121732317267\n",
      "test loss is 0.0006467389174686733\n",
      "Batch: 3900,train loss is: 0.0005600456475899516\n",
      "test loss is 0.0006360839338086693\n",
      "Batch: 4000,train loss is: 0.0004058591969969964\n",
      "test loss is 0.0006298413132913187\n",
      "Batch: 4100,train loss is: 0.0007225943151198319\n",
      "test loss is 0.0006569567632312825\n",
      "Batch: 4200,train loss is: 0.0002503443674253322\n",
      "test loss is 0.0006571265783287707\n",
      "Batch: 4300,train loss is: 0.00041768500432254536\n",
      "test loss is 0.0006225593296992365\n",
      "Batch: 4400,train loss is: 0.00023961504464638404\n",
      "test loss is 0.0006568683300285044\n",
      "Batch: 4500,train loss is: 0.0006336871648470133\n",
      "test loss is 0.000649739289695151\n",
      "Batch: 4600,train loss is: 0.0006080325146656243\n",
      "test loss is 0.0006498902733719482\n",
      "Batch: 4700,train loss is: 0.0005552326216887681\n",
      "test loss is 0.0006552032350299303\n",
      "Batch: 4800,train loss is: 0.0014032890678951455\n",
      "test loss is 0.0006253513001684972\n",
      "Batch: 4900,train loss is: 0.00042929134793527905\n",
      "test loss is 0.0006402046207099084\n",
      "Batch: 5000,train loss is: 0.0005438775235078243\n",
      "test loss is 0.0006506963848533381\n",
      "Batch: 5100,train loss is: 0.00044662872891465077\n",
      "test loss is 0.0006861522256841735\n",
      "Batch: 5200,train loss is: 0.0003422787020008529\n",
      "test loss is 0.0006509891355433473\n",
      "Batch: 5300,train loss is: 0.0004166208817464665\n",
      "test loss is 0.0006551734756398661\n",
      "Batch: 5400,train loss is: 0.0007112614449876271\n",
      "test loss is 0.0006320924155359816\n",
      "Batch: 5500,train loss is: 0.00041914993817623224\n",
      "test loss is 0.0006530848767460869\n",
      "Batch: 5600,train loss is: 0.0005511463116203541\n",
      "test loss is 0.0006287417009622142\n",
      "Batch: 5700,train loss is: 0.00040278471197194976\n",
      "test loss is 0.0006436327194756194\n",
      "Batch: 5800,train loss is: 0.0004193589122971799\n",
      "test loss is 0.0006362775234044037\n",
      "Batch: 5900,train loss is: 0.0031214332703163867\n",
      "test loss is 0.0006408666162037311\n",
      "Batch: 6000,train loss is: 0.0017286669315236829\n",
      "test loss is 0.0006662264105823575\n",
      "Batch: 6100,train loss is: 0.0004015385773323438\n",
      "test loss is 0.0006282887474213524\n",
      "Batch: 6200,train loss is: 0.0005369827258117691\n",
      "test loss is 0.0006330557338039037\n",
      "Batch: 6300,train loss is: 0.0009776886501034468\n",
      "test loss is 0.0006484574168373248\n",
      "Batch: 6400,train loss is: 0.000511798492160337\n",
      "test loss is 0.0006739242775889381\n",
      "Batch: 6500,train loss is: 0.00031249252529400994\n",
      "test loss is 0.0006354009059002999\n",
      "Batch: 6600,train loss is: 0.0018351188767316278\n",
      "test loss is 0.0006285550803420847\n",
      "Batch: 6700,train loss is: 0.00028609231499295945\n",
      "test loss is 0.0006336054431639996\n",
      "Batch: 6800,train loss is: 0.0006926873155144996\n",
      "test loss is 0.0006298396342126129\n",
      "Batch: 6900,train loss is: 0.000586364008372008\n",
      "test loss is 0.0006483356458601679\n",
      "Batch: 7000,train loss is: 0.0005219400250458293\n",
      "test loss is 0.0006511853333382822\n",
      "Batch: 7100,train loss is: 0.001093588973190899\n",
      "test loss is 0.0006425653471784187\n",
      "Batch: 7200,train loss is: 0.00044725469510435364\n",
      "test loss is 0.0006430774457359674\n",
      "Batch: 7300,train loss is: 0.0006333951120562849\n",
      "test loss is 0.0006565274910030911\n",
      "Batch: 7400,train loss is: 0.00034471064450537173\n",
      "test loss is 0.0006347764629064821\n",
      "Batch: 7500,train loss is: 0.0006113863081253164\n",
      "test loss is 0.0006543599858493893\n",
      "Batch: 7600,train loss is: 0.00040545991133069336\n",
      "test loss is 0.0006186491076486075\n",
      "Batch: 7700,train loss is: 0.0008622498797226105\n",
      "test loss is 0.0006494488714615916\n",
      "Batch: 7800,train loss is: 0.00025305351362387205\n",
      "test loss is 0.0006264903709641784\n",
      "Batch: 7900,train loss is: 0.0004194910639425066\n",
      "test loss is 0.000646864851034845\n",
      "Batch: 8000,train loss is: 0.00034083769157121135\n",
      "test loss is 0.0006700876200193616\n",
      "Batch: 8100,train loss is: 0.000562728460420192\n",
      "test loss is 0.0006515007745411208\n",
      "Batch: 8200,train loss is: 0.0010913752243248578\n",
      "test loss is 0.0006525765192673537\n",
      "Batch: 8300,train loss is: 0.00027614811028039055\n",
      "test loss is 0.0006462133246595294\n",
      "Batch: 8400,train loss is: 0.0005354830046244012\n",
      "test loss is 0.0006470408015140918\n",
      "Batch: 8500,train loss is: 0.0005396353222348304\n",
      "test loss is 0.0006468582689151353\n",
      "Batch: 8600,train loss is: 0.0006321380749828557\n",
      "test loss is 0.0006541132901314919\n",
      "Batch: 8700,train loss is: 0.00044081425496548496\n",
      "test loss is 0.0006977356375477954\n",
      "Batch: 8800,train loss is: 0.0003405115908262511\n",
      "test loss is 0.0006320213144112246\n",
      "Batch: 8900,train loss is: 0.0010080731424837951\n",
      "test loss is 0.0006309420595324925\n",
      "Batch: 9000,train loss is: 0.0002607049660935584\n",
      "test loss is 0.0006580063860147473\n",
      "Batch: 9100,train loss is: 0.0003087696146359892\n",
      "test loss is 0.0006300345234865528\n",
      "Batch: 9200,train loss is: 0.0007024179571332259\n",
      "test loss is 0.00067619276019905\n",
      "Batch: 9300,train loss is: 0.0004937458100733824\n",
      "test loss is 0.0006795103634620265\n",
      "Batch: 9400,train loss is: 0.0004495737194146915\n",
      "test loss is 0.0006656307996979933\n",
      "Batch: 9500,train loss is: 0.0003350083914290246\n",
      "test loss is 0.0006596696379136138\n",
      "Batch: 9600,train loss is: 0.0003787781130212988\n",
      "test loss is 0.0006245461642081328\n",
      "Batch: 9700,train loss is: 0.00040531649406514315\n",
      "test loss is 0.000635522236246035\n",
      "Batch: 9800,train loss is: 0.0002320309092467461\n",
      "test loss is 0.00063917494915148\n",
      "Batch: 9900,train loss is: 0.00043329906896447904\n",
      "test loss is 0.0006512663970132476\n",
      "Batch: 10000,train loss is: 0.00039884748848767243\n",
      "test loss is 0.0006248371035989165\n",
      "Batch: 10100,train loss is: 0.0008252728366822927\n",
      "test loss is 0.0006430099962916648\n",
      "Batch: 10200,train loss is: 0.0016362439881246317\n",
      "test loss is 0.0007144499129532797\n",
      "Batch: 10300,train loss is: 0.00039157661503227704\n",
      "test loss is 0.0006370868587934796\n",
      "Batch: 10400,train loss is: 0.0002565008425640568\n",
      "test loss is 0.0006310477302554396\n",
      "Batch: 10500,train loss is: 0.0007683366335226795\n",
      "test loss is 0.0006450296136150517\n",
      "Batch: 10600,train loss is: 0.00026367298819847634\n",
      "test loss is 0.0006861587799826793\n",
      "Batch: 10700,train loss is: 0.000554626315635835\n",
      "test loss is 0.0006303480488243481\n",
      "Batch: 10800,train loss is: 0.0006359305042947497\n",
      "test loss is 0.0006406404790676646\n",
      "Batch: 10900,train loss is: 0.0006455755913235036\n",
      "test loss is 0.000638839136627997\n",
      "Batch: 11000,train loss is: 0.0007053659235995606\n",
      "test loss is 0.0006588248725321012\n",
      "Batch: 11100,train loss is: 0.0008737678129593767\n",
      "test loss is 0.0006260672899947949\n",
      "Batch: 11200,train loss is: 0.0006816928235616398\n",
      "test loss is 0.0006851025332721423\n",
      "Batch: 11300,train loss is: 0.0002814053744294055\n",
      "test loss is 0.000649237848184883\n",
      "Batch: 11400,train loss is: 0.0007397519123177289\n",
      "test loss is 0.0006387250215336095\n",
      "Batch: 11500,train loss is: 0.0004330816673846554\n",
      "test loss is 0.0006387761743571463\n",
      "Batch: 11600,train loss is: 0.001773155125748168\n",
      "test loss is 0.0006315938186725082\n",
      "Batch: 11700,train loss is: 0.000309714929785271\n",
      "test loss is 0.0006261165251183125\n",
      "Batch: 11800,train loss is: 0.0005239927891715346\n",
      "test loss is 0.0006372005152548351\n",
      "Batch: 11900,train loss is: 0.00043868088369795264\n",
      "test loss is 0.0006477700802629066\n",
      "Batch: 12000,train loss is: 0.00030385893011770064\n",
      "test loss is 0.0006373453211592215\n",
      "Batch: 12100,train loss is: 0.0015666766515485303\n",
      "test loss is 0.0006481818164941399\n",
      "Batch: 12200,train loss is: 0.0005883215518548991\n",
      "test loss is 0.0006550545179574138\n",
      "Batch: 12300,train loss is: 0.0004390187728323721\n",
      "test loss is 0.0006488503650852755\n",
      "Batch: 12400,train loss is: 0.00032374457477438813\n",
      "test loss is 0.0006414716125407799\n",
      "Batch: 12500,train loss is: 0.0004623658896327893\n",
      "test loss is 0.0006610101224207207\n",
      "Batch: 12600,train loss is: 0.00048090993621802076\n",
      "test loss is 0.0006469205223821163\n",
      "Batch: 12700,train loss is: 0.00033820049358158607\n",
      "test loss is 0.000632904669276586\n",
      "Batch: 12800,train loss is: 0.00034816846561547916\n",
      "test loss is 0.0006299061839106515\n",
      "Batch: 12900,train loss is: 0.0005185658626066371\n",
      "test loss is 0.0006401559058101877\n",
      "Batch: 13000,train loss is: 0.0002050418174506353\n",
      "test loss is 0.0006326195056131521\n",
      "Batch: 13100,train loss is: 0.0018953311300689908\n",
      "test loss is 0.0006247602170109787\n",
      "Batch: 13200,train loss is: 0.0009459520537040307\n",
      "test loss is 0.0007683309808023017\n",
      "Batch: 13300,train loss is: 0.0005007711917408372\n",
      "test loss is 0.0006406670086725565\n",
      "Batch: 13400,train loss is: 0.00035195558650136114\n",
      "test loss is 0.0006575016506983444\n",
      "Batch: 13500,train loss is: 0.0005010693591789724\n",
      "test loss is 0.0006512595395178777\n",
      "Batch: 13600,train loss is: 0.0004812074444819694\n",
      "test loss is 0.0006555747822108914\n",
      "Batch: 13700,train loss is: 0.0006374290001098049\n",
      "test loss is 0.0006216659859325845\n",
      "Batch: 13800,train loss is: 0.0003607051058755743\n",
      "test loss is 0.000625894073933715\n",
      "Batch: 13900,train loss is: 0.0002570678086118151\n",
      "test loss is 0.0007011700716689596\n",
      "Batch: 14000,train loss is: 0.0003380887281836486\n",
      "test loss is 0.0006508282560968415\n",
      "Batch: 14100,train loss is: 0.0006679820143088258\n",
      "test loss is 0.0006668836719789736\n",
      "Batch: 14200,train loss is: 0.0005510394931595913\n",
      "test loss is 0.0006238935009268294\n",
      "Batch: 14300,train loss is: 0.00024113253333506363\n",
      "test loss is 0.0006266810286618212\n",
      "Batch: 14400,train loss is: 0.001161370552080497\n",
      "test loss is 0.0006246905511225824\n",
      "Batch: 14500,train loss is: 0.0006312756982564802\n",
      "test loss is 0.0006261698729633675\n",
      "Batch: 14600,train loss is: 0.0007569294317202358\n",
      "test loss is 0.0006470129818331287\n",
      "Batch: 14700,train loss is: 0.0004776961016114505\n",
      "test loss is 0.0007055584750039071\n",
      "Batch: 14800,train loss is: 0.0007742044529194865\n",
      "test loss is 0.0006535263357882852\n",
      "Batch: 14900,train loss is: 0.0004119974695352536\n",
      "test loss is 0.0006895322666286745\n",
      "Batch: 15000,train loss is: 0.0005401636009138757\n",
      "test loss is 0.0006244637828765792\n",
      "Batch: 15100,train loss is: 0.0007466496171744326\n",
      "test loss is 0.0006567149533249473\n",
      "Batch: 15200,train loss is: 0.0007638181497144863\n",
      "test loss is 0.0006374090307412341\n",
      "Batch: 15300,train loss is: 0.00031288719557048133\n",
      "test loss is 0.0006281662187828818\n",
      "Batch: 15400,train loss is: 0.0004597310338091638\n",
      "test loss is 0.0006548261773784197\n",
      "Batch: 15500,train loss is: 0.0006486883231608131\n",
      "test loss is 0.0006255595751381473\n",
      "Batch: 15600,train loss is: 0.0007829227224132339\n",
      "test loss is 0.0006405831072149493\n",
      "Batch: 15700,train loss is: 0.0003465641546412197\n",
      "test loss is 0.0006348488002109107\n",
      "Batch: 15800,train loss is: 0.0005925036111992861\n",
      "test loss is 0.0006176061497120196\n",
      "Batch: 15900,train loss is: 0.0007858520411792423\n",
      "test loss is 0.0006377656117503424\n",
      "Batch: 16000,train loss is: 0.00035219623647522264\n",
      "test loss is 0.0006313779781221479\n",
      "Batch: 16100,train loss is: 0.0005886079296983769\n",
      "test loss is 0.0006362489653013667\n",
      "Batch: 16200,train loss is: 0.0003581040155831145\n",
      "test loss is 0.0006227465959858845\n",
      "Batch: 16300,train loss is: 0.0008697439296892414\n",
      "test loss is 0.0006507816765385585\n",
      "Batch: 16400,train loss is: 0.00028782306458814477\n",
      "test loss is 0.0006253814654415247\n",
      "Batch: 16500,train loss is: 0.0006663314971212434\n",
      "test loss is 0.0006449103472543705\n",
      "Batch: 16600,train loss is: 0.0003133184129284504\n",
      "test loss is 0.0006316525863527577\n",
      "Batch: 16700,train loss is: 0.0007858052810818635\n",
      "test loss is 0.000638028528352573\n",
      "Batch: 16800,train loss is: 0.0004967602798366288\n",
      "test loss is 0.0006635563246629448\n",
      "Batch: 16900,train loss is: 0.0007072709735926072\n",
      "test loss is 0.0006437580428270048\n",
      "Batch: 17000,train loss is: 0.0005863539612921422\n",
      "test loss is 0.0006204189540199649\n",
      "Batch: 17100,train loss is: 0.0004345786549138806\n",
      "test loss is 0.0006271073847363873\n",
      "Batch: 17200,train loss is: 0.0004010452918016449\n",
      "test loss is 0.0006373967226441522\n",
      "Batch: 17300,train loss is: 0.0007304455187942458\n",
      "test loss is 0.0006502588392867494\n",
      "Batch: 17400,train loss is: 0.0003571313704907854\n",
      "test loss is 0.0006498603509127827\n",
      "Batch: 17500,train loss is: 0.0005919969262392448\n",
      "test loss is 0.0006392114030734235\n",
      "Batch: 17600,train loss is: 0.00041809676334514026\n",
      "test loss is 0.0006462686193241876\n",
      "Batch: 17700,train loss is: 0.00031669754991427715\n",
      "test loss is 0.0006734095157253507\n",
      "Batch: 17800,train loss is: 0.0015720654363062592\n",
      "test loss is 0.0006675873986485096\n",
      "Batch: 17900,train loss is: 0.00043116005112818484\n",
      "test loss is 0.0006589078005682406\n",
      "Batch: 18000,train loss is: 0.00025986091516702625\n",
      "test loss is 0.0006204591764578352\n",
      "Batch: 18100,train loss is: 0.0003542317290217103\n",
      "test loss is 0.0006301582239508077\n",
      "Batch: 18200,train loss is: 0.0005850471222063808\n",
      "test loss is 0.0006390656539233161\n",
      "Batch: 18300,train loss is: 0.001308072017829801\n",
      "test loss is 0.0006416201697511927\n",
      "Batch: 18400,train loss is: 0.00040058144302239444\n",
      "test loss is 0.0006331409792911171\n",
      "Batch: 18500,train loss is: 0.0008440569610434483\n",
      "test loss is 0.0006354942570926515\n",
      "Batch: 18600,train loss is: 0.00043730652139140183\n",
      "test loss is 0.0006335323275416106\n",
      "Batch: 18700,train loss is: 0.000548762086055623\n",
      "test loss is 0.0006486195686367802\n",
      "Batch: 18800,train loss is: 0.0005366589489305976\n",
      "test loss is 0.0006270192456642761\n",
      "Batch: 18900,train loss is: 0.0004910506248195577\n",
      "test loss is 0.0006437673260184492\n",
      "Batch: 19000,train loss is: 0.0005252685711483161\n",
      "test loss is 0.0006306059241863959\n",
      "Batch: 19100,train loss is: 0.0003717694738024812\n",
      "test loss is 0.0006550894582845174\n",
      "Batch: 19200,train loss is: 0.0005138019958182448\n",
      "test loss is 0.0006392089402281682\n",
      "Batch: 19300,train loss is: 0.00045397912349078723\n",
      "test loss is 0.0006221861212863098\n",
      "Batch: 19400,train loss is: 0.0002568591787655346\n",
      "test loss is 0.0006285512581726464\n",
      "Batch: 19500,train loss is: 0.00048483414620502385\n",
      "test loss is 0.0006663753194166445\n",
      "Batch: 19600,train loss is: 0.00037405393379230694\n",
      "test loss is 0.0006388059546376633\n",
      "Batch: 19700,train loss is: 0.0006314589897637314\n",
      "test loss is 0.0007065543987752527\n",
      "Batch: 19800,train loss is: 0.0005599745413769326\n",
      "test loss is 0.000645327318914284\n",
      "Batch: 19900,train loss is: 0.000390277683958111\n",
      "test loss is 0.0006544257650793382\n",
      "Batch: 20000,train loss is: 0.0007118266547226723\n",
      "test loss is 0.0006295736585225836\n",
      "Batch: 20100,train loss is: 0.0005713061652189397\n",
      "test loss is 0.000648749397667009\n",
      "Batch: 20200,train loss is: 0.0006778054497089643\n",
      "test loss is 0.0006480363004515396\n",
      "Batch: 20300,train loss is: 0.0002928910138284638\n",
      "test loss is 0.0006512730964103841\n",
      "Batch: 20400,train loss is: 0.00048202037181813774\n",
      "test loss is 0.000651493721573349\n",
      "Batch: 20500,train loss is: 0.0006235622205135032\n",
      "test loss is 0.0006282311595276623\n",
      "Batch: 20600,train loss is: 0.0004912564195807775\n",
      "test loss is 0.0006596783822508371\n",
      "Batch: 20700,train loss is: 0.0007935347623901636\n",
      "test loss is 0.0006314391044902587\n",
      "Batch: 20800,train loss is: 0.0003372310714649764\n",
      "test loss is 0.0006749601459011986\n",
      "Batch: 20900,train loss is: 0.0005350988890636732\n",
      "test loss is 0.0006774485049013116\n",
      "Batch: 21000,train loss is: 0.00021751700564777437\n",
      "test loss is 0.0006218820486389259\n",
      "Batch: 21100,train loss is: 0.000660219110782878\n",
      "test loss is 0.000670932613941814\n",
      "Batch: 21200,train loss is: 0.0003685105269216789\n",
      "test loss is 0.0006672046892558369\n",
      "Batch: 21300,train loss is: 0.0002950347966749597\n",
      "test loss is 0.0006491064195739279\n",
      "Batch: 21400,train loss is: 0.0007995890585292815\n",
      "test loss is 0.0006259030066532674\n",
      "Batch: 21500,train loss is: 0.00023771393578822785\n",
      "test loss is 0.0006516982210422161\n",
      "Batch: 21600,train loss is: 0.0006470154788847128\n",
      "test loss is 0.0006451419856954503\n",
      "Batch: 21700,train loss is: 0.00027162004586741114\n",
      "test loss is 0.0006234267051825562\n",
      "Batch: 21800,train loss is: 0.0007082617065564905\n",
      "test loss is 0.0006757534523557394\n",
      "Batch: 21900,train loss is: 0.00034114793296327676\n",
      "test loss is 0.0006994908086851813\n",
      "Batch: 22000,train loss is: 0.0006421858682507829\n",
      "test loss is 0.0006291452221199026\n",
      "Batch: 22100,train loss is: 0.00034747818046635797\n",
      "test loss is 0.0006274575727383937\n",
      "Batch: 22200,train loss is: 0.0004458580770187195\n",
      "test loss is 0.000640490878141352\n",
      "Batch: 22300,train loss is: 0.000464809540981518\n",
      "test loss is 0.0006557332703510531\n",
      "Batch: 22400,train loss is: 0.00044033393942127726\n",
      "test loss is 0.0006469958269602713\n",
      "Batch: 22500,train loss is: 0.0004962842454402787\n",
      "test loss is 0.0006211301500607544\n",
      "Batch: 22600,train loss is: 0.0007806078963837588\n",
      "test loss is 0.0006650972738796075\n",
      "Batch: 22700,train loss is: 0.0002996838900216835\n",
      "test loss is 0.0006273234929928327\n",
      "Batch: 22800,train loss is: 0.0004758497275858605\n",
      "test loss is 0.0006542226181007599\n",
      "Batch: 22900,train loss is: 0.0002956569658331303\n",
      "test loss is 0.0007130298116903884\n",
      "Batch: 23000,train loss is: 0.0005607179090546691\n",
      "test loss is 0.0006379976581817741\n",
      "Batch: 23100,train loss is: 0.0004080778087640412\n",
      "test loss is 0.0006223279520560861\n",
      "Batch: 23200,train loss is: 0.00029038281079053497\n",
      "test loss is 0.000616631796830774\n",
      "Batch: 23300,train loss is: 0.00041905801542381423\n",
      "test loss is 0.000622228552478892\n",
      "Batch: 23400,train loss is: 0.0005586075076590798\n",
      "test loss is 0.000652908344660157\n",
      "Batch: 23500,train loss is: 0.0013691963630826439\n",
      "test loss is 0.0006152628676315775\n",
      "Batch: 23600,train loss is: 0.000308849191992923\n",
      "test loss is 0.000631948649913913\n",
      "Batch: 23700,train loss is: 0.0010629788427847031\n",
      "test loss is 0.0006258166187926105\n",
      "Batch: 23800,train loss is: 0.0008104813867864205\n",
      "test loss is 0.0006314356446001191\n",
      "Batch: 23900,train loss is: 0.000977750145321978\n",
      "test loss is 0.0006535278310851776\n",
      "Batch: 24000,train loss is: 0.0007478060125799487\n",
      "test loss is 0.0006855220468294325\n",
      "Batch: 24100,train loss is: 0.00026125507465108794\n",
      "test loss is 0.000623951700098874\n",
      "Batch: 24200,train loss is: 0.0012815997517035115\n",
      "test loss is 0.0006457012023188956\n",
      "Batch: 24300,train loss is: 0.00046731995108700587\n",
      "test loss is 0.0007295516054404109\n",
      "Batch: 24400,train loss is: 0.0012139886193144218\n",
      "test loss is 0.0006324548168574395\n",
      "Batch: 24500,train loss is: 0.0007302837730934132\n",
      "test loss is 0.000615957490760047\n",
      "Batch: 24600,train loss is: 0.00035367007838890696\n",
      "test loss is 0.0006553248875826903\n",
      "Batch: 24700,train loss is: 0.0002655500009683383\n",
      "test loss is 0.0006400738237959794\n",
      "Batch: 24800,train loss is: 0.0006343124384723705\n",
      "test loss is 0.0006388406579460856\n",
      "Batch: 24900,train loss is: 0.00019180072725143835\n",
      "test loss is 0.0006281355513810954\n",
      "Batch: 25000,train loss is: 0.00044446960044429\n",
      "test loss is 0.0006287110486507104\n",
      "Batch: 25100,train loss is: 0.00035911694672233863\n",
      "test loss is 0.0006373042022719665\n",
      "Batch: 25200,train loss is: 0.0006946302314747784\n",
      "test loss is 0.0006381425926851801\n",
      "Batch: 25300,train loss is: 0.0007949922023280553\n",
      "test loss is 0.0006848304372551567\n",
      "Batch: 25400,train loss is: 0.00041005491715760553\n",
      "test loss is 0.0006297108276419125\n",
      "Batch: 25500,train loss is: 0.0005596194186897925\n",
      "test loss is 0.0006375782065045768\n",
      "Batch: 25600,train loss is: 0.0005916919192717415\n",
      "test loss is 0.000639935746084962\n",
      "Batch: 25700,train loss is: 0.0005890078151782892\n",
      "test loss is 0.0006462549265142952\n",
      "Batch: 25800,train loss is: 0.0003159069791045057\n",
      "test loss is 0.0006277542986468899\n",
      "Batch: 25900,train loss is: 0.000266353492936658\n",
      "test loss is 0.0006826610451110232\n",
      "Batch: 26000,train loss is: 0.0014215971702234724\n",
      "test loss is 0.0006763584874203772\n",
      "Batch: 26100,train loss is: 0.0007240882462911281\n",
      "test loss is 0.0006261717418797156\n",
      "Batch: 26200,train loss is: 0.0005773677199500517\n",
      "test loss is 0.0006207714180332384\n",
      "Batch: 26300,train loss is: 0.0009543577017596476\n",
      "test loss is 0.0006370095840280255\n",
      "Batch: 26400,train loss is: 0.0005461010551809306\n",
      "test loss is 0.0006190914274878565\n",
      "Batch: 26500,train loss is: 0.0006218715862096\n",
      "test loss is 0.00067059066493809\n",
      "Batch: 26600,train loss is: 0.001233500924125358\n",
      "test loss is 0.0007242433835919163\n",
      "Batch: 26700,train loss is: 0.0005295396856479075\n",
      "test loss is 0.0007001771068787086\n",
      "Batch: 26800,train loss is: 0.00114786240458203\n",
      "test loss is 0.0006479568774454992\n",
      "Batch: 26900,train loss is: 0.00046892536735009514\n",
      "test loss is 0.0006359418287659213\n",
      "Batch: 27000,train loss is: 0.0006493376099045671\n",
      "test loss is 0.0007360679176849479\n",
      "Batch: 27100,train loss is: 0.0007108540983272618\n",
      "test loss is 0.0007036618800236197\n",
      "Batch: 27200,train loss is: 0.0005832013448725839\n",
      "test loss is 0.0006490883678084734\n",
      "Batch: 27300,train loss is: 0.00043832284962947024\n",
      "test loss is 0.0006465893311393665\n",
      "Batch: 27400,train loss is: 0.0005918728494271289\n",
      "test loss is 0.0006387054086531749\n",
      "Batch: 27500,train loss is: 0.0004483997882095583\n",
      "test loss is 0.0006912150242176458\n",
      "Batch: 27600,train loss is: 0.0004564825550831604\n",
      "test loss is 0.0006543162356723254\n",
      "Batch: 27700,train loss is: 0.0015361079374677676\n",
      "test loss is 0.0006377872487858691\n",
      "Batch: 27800,train loss is: 0.0007741614008040769\n",
      "test loss is 0.00064748804961829\n",
      "Batch: 27900,train loss is: 0.0011672470424665521\n",
      "test loss is 0.0006249691170239035\n",
      "Batch: 28000,train loss is: 0.0003829729893024785\n",
      "test loss is 0.000631600223557805\n",
      "Batch: 28100,train loss is: 0.0007424810933713458\n",
      "test loss is 0.0006380859898916018\n",
      "Batch: 28200,train loss is: 0.0005137213717346823\n",
      "test loss is 0.0006184426905468665\n",
      "Batch: 28300,train loss is: 0.0016825874349803264\n",
      "test loss is 0.0006403259561249817\n",
      "Batch: 28400,train loss is: 0.0005843554538600487\n",
      "test loss is 0.0006240135065445795\n",
      "Batch: 28500,train loss is: 0.001273747167180718\n",
      "test loss is 0.0006795105118985865\n",
      "Batch: 28600,train loss is: 0.00035090248931270304\n",
      "test loss is 0.0006370917906083413\n",
      "Batch: 28700,train loss is: 0.0005428029210563251\n",
      "test loss is 0.0006255416198784301\n",
      "Batch: 28800,train loss is: 0.000980730862916365\n",
      "test loss is 0.0006432172642622171\n",
      "Batch: 28900,train loss is: 0.0004557183021135394\n",
      "test loss is 0.0006190625770772533\n",
      "Batch: 29000,train loss is: 0.00047689483082068526\n",
      "test loss is 0.0006224912131344085\n",
      "Batch: 29100,train loss is: 0.00046565840253844743\n",
      "test loss is 0.0006334767930382209\n",
      "Batch: 29200,train loss is: 0.0004258126188254442\n",
      "test loss is 0.0006374941261862543\n",
      "Batch: 29300,train loss is: 0.0008766210196225417\n",
      "test loss is 0.0006254780167214168\n",
      "Batch: 29400,train loss is: 0.0005989098604011278\n",
      "test loss is 0.0007354301711246759\n",
      "Batch: 29500,train loss is: 0.000528315197042423\n",
      "test loss is 0.0006694214396036465\n",
      "Batch: 29600,train loss is: 0.0005804169661341757\n",
      "test loss is 0.0006160492709863454\n",
      "Batch: 29700,train loss is: 0.0003450955751515117\n",
      "test loss is 0.0006349456534753974\n",
      "Batch: 29800,train loss is: 0.0008002793645397376\n",
      "test loss is 0.0006248264615140349\n",
      "Batch: 29900,train loss is: 0.002671273395205342\n",
      "test loss is 0.0006280382042500513\n",
      "Batch: 30000,train loss is: 0.0003706803679120719\n",
      "test loss is 0.0006635904997593014\n",
      "Batch: 30100,train loss is: 0.0005166532794824138\n",
      "test loss is 0.0006550892782649978\n",
      "Batch: 30200,train loss is: 0.00033541356647211006\n",
      "test loss is 0.0006358567327564116\n",
      "Batch: 30300,train loss is: 0.0002677543617446058\n",
      "test loss is 0.0006213456639104923\n",
      "Batch: 30400,train loss is: 0.00029660341341500665\n",
      "test loss is 0.0006302211306510844\n",
      "Batch: 30500,train loss is: 0.0003054890321669841\n",
      "test loss is 0.0006668387937572406\n",
      "Batch: 30600,train loss is: 0.00023723705480350855\n",
      "test loss is 0.0006337462197657473\n",
      "Batch: 30700,train loss is: 0.0004149928192395208\n",
      "test loss is 0.0006964934241937607\n",
      "Batch: 30800,train loss is: 0.0002660494498021321\n",
      "test loss is 0.0006837096042438529\n",
      "Batch: 30900,train loss is: 0.00030026554534625565\n",
      "test loss is 0.000632602286976868\n",
      "Batch: 31000,train loss is: 0.00046205658991856405\n",
      "test loss is 0.0006427678666509975\n",
      "Batch: 31100,train loss is: 0.00029587494175750917\n",
      "test loss is 0.0007206185296848923\n",
      "Batch: 31200,train loss is: 0.0003758992559452799\n",
      "test loss is 0.0006324155278672055\n",
      "Batch: 31300,train loss is: 0.00045588833425516456\n",
      "test loss is 0.0006296462712232281\n",
      "Batch: 31400,train loss is: 0.0006746850924080643\n",
      "test loss is 0.0006251218558981648\n",
      "Batch: 31500,train loss is: 0.0004181680473088331\n",
      "test loss is 0.000640624626236743\n",
      "Batch: 31600,train loss is: 0.0003717847105359433\n",
      "test loss is 0.0006326372533563797\n",
      "Batch: 31700,train loss is: 0.00045399029093150223\n",
      "test loss is 0.0006523995637107064\n",
      "Batch: 31800,train loss is: 0.0011304319109033006\n",
      "test loss is 0.0006309100195315904\n",
      "Batch: 31900,train loss is: 0.0006191434587461942\n",
      "test loss is 0.0006351450119110399\n",
      "Batch: 32000,train loss is: 0.003334392609241481\n",
      "test loss is 0.0006388976181908904\n",
      "Batch: 32100,train loss is: 0.00029363127320862476\n",
      "test loss is 0.0006673038086223663\n",
      "Batch: 32200,train loss is: 0.0002913635927697016\n",
      "test loss is 0.0006290845340579975\n",
      "Batch: 32300,train loss is: 0.00033556630149947586\n",
      "test loss is 0.0006211365881830993\n",
      "Batch: 32400,train loss is: 0.00033609181880041463\n",
      "test loss is 0.0006178274516461744\n",
      "Batch: 32500,train loss is: 0.000789252850290412\n",
      "test loss is 0.0006566628854490641\n",
      "Batch: 32600,train loss is: 0.000461587793518884\n",
      "test loss is 0.0006454736165633394\n",
      "Batch: 32700,train loss is: 0.0004919528310789136\n",
      "test loss is 0.0006769131418990085\n",
      "Batch: 32800,train loss is: 0.0004999139213008874\n",
      "test loss is 0.000670885274331784\n",
      "Batch: 32900,train loss is: 0.0006208494506597886\n",
      "test loss is 0.0006402791687805958\n",
      "Batch: 33000,train loss is: 0.00037583263406682806\n",
      "test loss is 0.0006340877023649951\n",
      "Batch: 33100,train loss is: 0.00039866180731813907\n",
      "test loss is 0.000635778095400857\n",
      "Batch: 33200,train loss is: 0.000439484539286917\n",
      "test loss is 0.0007032925017490431\n",
      "Batch: 33300,train loss is: 0.0005821788958300281\n",
      "test loss is 0.0006315406059708406\n",
      "Batch: 33400,train loss is: 0.00042813610634271406\n",
      "test loss is 0.0006337344968415457\n",
      "Batch: 33500,train loss is: 0.00031651126225741707\n",
      "test loss is 0.0006386571460956988\n",
      "Batch: 33600,train loss is: 0.0005179961022529548\n",
      "test loss is 0.0006259320614954857\n",
      "Batch: 33700,train loss is: 0.000490773154117195\n",
      "test loss is 0.0006470418292075056\n",
      "Batch: 33800,train loss is: 0.0006782048258910676\n",
      "test loss is 0.0006292939467160587\n",
      "Batch: 33900,train loss is: 0.0005750297935120913\n",
      "test loss is 0.000624618471724556\n",
      "-----------------------Epoch: 23----------------------------------\n",
      "Batch: 0,train loss is: 0.00033563298189755983\n",
      "test loss is 0.0006409116401252912\n",
      "Batch: 100,train loss is: 0.001015772392929284\n",
      "test loss is 0.000648291260506911\n",
      "Batch: 200,train loss is: 0.00030487534889540575\n",
      "test loss is 0.0006607264017263565\n",
      "Batch: 300,train loss is: 0.0002868303884099843\n",
      "test loss is 0.000632874356885956\n",
      "Batch: 400,train loss is: 0.0004131194352760796\n",
      "test loss is 0.0006165175840465092\n",
      "Batch: 500,train loss is: 0.000435625706299307\n",
      "test loss is 0.0006409435997368962\n",
      "Batch: 600,train loss is: 0.00021936178768718133\n",
      "test loss is 0.0006593318787374187\n",
      "Batch: 700,train loss is: 0.0004054722954440494\n",
      "test loss is 0.0006242614325254264\n",
      "Batch: 800,train loss is: 0.0003875214845102784\n",
      "test loss is 0.0006318531965676561\n",
      "Batch: 900,train loss is: 0.00038900391722660084\n",
      "test loss is 0.0006393218714772755\n",
      "Batch: 1000,train loss is: 0.00037850184416695227\n",
      "test loss is 0.0006495361511819958\n",
      "Batch: 1100,train loss is: 0.0018118234072970114\n",
      "test loss is 0.0006223283830120685\n",
      "Batch: 1200,train loss is: 0.00023354613184627874\n",
      "test loss is 0.0006502033113345942\n",
      "Batch: 1300,train loss is: 0.0005067638855524915\n",
      "test loss is 0.0006286276405999602\n",
      "Batch: 1400,train loss is: 0.0003828318310518776\n",
      "test loss is 0.0006572239424912548\n",
      "Batch: 1500,train loss is: 0.0002754286639413737\n",
      "test loss is 0.0006593056063267344\n",
      "Batch: 1600,train loss is: 0.0006322092865627064\n",
      "test loss is 0.0006797374012517183\n",
      "Batch: 1700,train loss is: 0.0011683183461990777\n",
      "test loss is 0.0006221751498232558\n",
      "Batch: 1800,train loss is: 0.00034640270948878277\n",
      "test loss is 0.0006171765319372934\n",
      "Batch: 1900,train loss is: 0.00036650324392082897\n",
      "test loss is 0.000673258440785191\n",
      "Batch: 2000,train loss is: 0.0007553780832013559\n",
      "test loss is 0.0007601249408249924\n",
      "Batch: 2100,train loss is: 0.0006463351754342324\n",
      "test loss is 0.00077101976012506\n",
      "Batch: 2200,train loss is: 0.0006956775762971187\n",
      "test loss is 0.0006414426434658809\n",
      "Batch: 2300,train loss is: 0.00039542562589196285\n",
      "test loss is 0.0006427391384689402\n",
      "Batch: 2400,train loss is: 0.0004175418560034649\n",
      "test loss is 0.0006362181515339346\n",
      "Batch: 2500,train loss is: 0.00042650439918525735\n",
      "test loss is 0.0006255121391302969\n",
      "Batch: 2600,train loss is: 0.000436171973015465\n",
      "test loss is 0.0006281256492592339\n",
      "Batch: 2700,train loss is: 0.00046944944662238197\n",
      "test loss is 0.0006094113041972167\n",
      "Batch: 2800,train loss is: 0.00040297183974378115\n",
      "test loss is 0.0006364224164808778\n",
      "Batch: 2900,train loss is: 0.0003911080376513394\n",
      "test loss is 0.0006307873688128346\n",
      "Batch: 3000,train loss is: 0.00044167343858898085\n",
      "test loss is 0.0006269489737869107\n",
      "Batch: 3100,train loss is: 0.0005315647883764055\n",
      "test loss is 0.0006518836324662952\n",
      "Batch: 3200,train loss is: 0.0009351840402088271\n",
      "test loss is 0.0006416313895458268\n",
      "Batch: 3300,train loss is: 0.0005665822093033737\n",
      "test loss is 0.0006295252488386402\n",
      "Batch: 3400,train loss is: 0.00034352087930694355\n",
      "test loss is 0.000629774652259914\n",
      "Batch: 3500,train loss is: 0.0002302533656982155\n",
      "test loss is 0.0006206833593959474\n",
      "Batch: 3600,train loss is: 0.000425972313855634\n",
      "test loss is 0.0006842717065644802\n",
      "Batch: 3700,train loss is: 0.0003154231214698094\n",
      "test loss is 0.0006435959748384918\n",
      "Batch: 3800,train loss is: 0.0004071446147184836\n",
      "test loss is 0.0006406720871844506\n",
      "Batch: 3900,train loss is: 0.0005626893495723679\n",
      "test loss is 0.0006292585606149174\n",
      "Batch: 4000,train loss is: 0.0003972667155966796\n",
      "test loss is 0.000623385266631911\n",
      "Batch: 4100,train loss is: 0.0007126787661018785\n",
      "test loss is 0.0006529844671121097\n",
      "Batch: 4200,train loss is: 0.00025082727202704484\n",
      "test loss is 0.0006509661309234061\n",
      "Batch: 4300,train loss is: 0.0004229954145187238\n",
      "test loss is 0.0006164886635255838\n",
      "Batch: 4400,train loss is: 0.0002324775987917741\n",
      "test loss is 0.0006509296808594036\n",
      "Batch: 4500,train loss is: 0.0005967171356404584\n",
      "test loss is 0.0006444450358230238\n",
      "Batch: 4600,train loss is: 0.0006097620870132225\n",
      "test loss is 0.0006450921514700523\n",
      "Batch: 4700,train loss is: 0.0005454762471918582\n",
      "test loss is 0.0006474910548659524\n",
      "Batch: 4800,train loss is: 0.00140927309781381\n",
      "test loss is 0.0006197612955714071\n",
      "Batch: 4900,train loss is: 0.0004235803073084264\n",
      "test loss is 0.0006345670045866297\n",
      "Batch: 5000,train loss is: 0.0005459526111420492\n",
      "test loss is 0.0006451432357223879\n",
      "Batch: 5100,train loss is: 0.0004515717744696917\n",
      "test loss is 0.0006802889784746338\n",
      "Batch: 5200,train loss is: 0.00035455961234069644\n",
      "test loss is 0.0006443866820988715\n",
      "Batch: 5300,train loss is: 0.0004086481329031837\n",
      "test loss is 0.0006495677695984133\n",
      "Batch: 5400,train loss is: 0.0007182839741830251\n",
      "test loss is 0.0006274264265692808\n",
      "Batch: 5500,train loss is: 0.0004203015907630219\n",
      "test loss is 0.0006465157239859924\n",
      "Batch: 5600,train loss is: 0.000550175317461389\n",
      "test loss is 0.0006232418021969592\n",
      "Batch: 5700,train loss is: 0.00038823345558097185\n",
      "test loss is 0.0006378730512558845\n",
      "Batch: 5800,train loss is: 0.00041809058452827363\n",
      "test loss is 0.0006308968068015964\n",
      "Batch: 5900,train loss is: 0.0030720397111123343\n",
      "test loss is 0.0006344333203925831\n",
      "Batch: 6000,train loss is: 0.0017357034935502112\n",
      "test loss is 0.0006634529311452139\n",
      "Batch: 6100,train loss is: 0.00039450916163697743\n",
      "test loss is 0.0006233233969074254\n",
      "Batch: 6200,train loss is: 0.0005368990538775476\n",
      "test loss is 0.0006260069976392788\n",
      "Batch: 6300,train loss is: 0.0009575193072112953\n",
      "test loss is 0.000640242106316137\n",
      "Batch: 6400,train loss is: 0.0005010520678471865\n",
      "test loss is 0.0006653634073885197\n",
      "Batch: 6500,train loss is: 0.0003167491654854189\n",
      "test loss is 0.0006288197053436478\n",
      "Batch: 6600,train loss is: 0.0018213453894614052\n",
      "test loss is 0.0006228747446407392\n",
      "Batch: 6700,train loss is: 0.00027928656137344275\n",
      "test loss is 0.0006265090449599256\n",
      "Batch: 6800,train loss is: 0.0006929342952191998\n",
      "test loss is 0.0006234556401919428\n",
      "Batch: 6900,train loss is: 0.0005779651588384688\n",
      "test loss is 0.0006440103959927675\n",
      "Batch: 7000,train loss is: 0.0005216375048690719\n",
      "test loss is 0.0006465059639939946\n",
      "Batch: 7100,train loss is: 0.001080324270114995\n",
      "test loss is 0.0006366286941782966\n",
      "Batch: 7200,train loss is: 0.0004380224815684754\n",
      "test loss is 0.0006373322861153289\n",
      "Batch: 7300,train loss is: 0.0006086366231984489\n",
      "test loss is 0.0006496098449863943\n",
      "Batch: 7400,train loss is: 0.00033847758991906055\n",
      "test loss is 0.0006290901444303201\n",
      "Batch: 7500,train loss is: 0.0006093932766348152\n",
      "test loss is 0.000648386958192054\n",
      "Batch: 7600,train loss is: 0.0004069602345059222\n",
      "test loss is 0.0006131709824612006\n",
      "Batch: 7700,train loss is: 0.0008365853532193859\n",
      "test loss is 0.0006440929395955719\n",
      "Batch: 7800,train loss is: 0.0002502115472301509\n",
      "test loss is 0.0006204612459609903\n",
      "Batch: 7900,train loss is: 0.00040355209648095864\n",
      "test loss is 0.000640055619325276\n",
      "Batch: 8000,train loss is: 0.0003320115858024321\n",
      "test loss is 0.0006664675376774099\n",
      "Batch: 8100,train loss is: 0.0005621636048253585\n",
      "test loss is 0.0006457583172356224\n",
      "Batch: 8200,train loss is: 0.0011071018937971627\n",
      "test loss is 0.0006485326494605669\n",
      "Batch: 8300,train loss is: 0.0002712297389553476\n",
      "test loss is 0.0006404873263985114\n",
      "Batch: 8400,train loss is: 0.0005332973223682626\n",
      "test loss is 0.0006451468153140192\n",
      "Batch: 8500,train loss is: 0.000528279329419102\n",
      "test loss is 0.0006400781195659846\n",
      "Batch: 8600,train loss is: 0.000620066268500315\n",
      "test loss is 0.0006487418633572749\n",
      "Batch: 8700,train loss is: 0.0004426285766953124\n",
      "test loss is 0.0006917769575151511\n",
      "Batch: 8800,train loss is: 0.0003421402047143199\n",
      "test loss is 0.0006267799733032988\n",
      "Batch: 8900,train loss is: 0.0010058333223226725\n",
      "test loss is 0.0006253761730235824\n",
      "Batch: 9000,train loss is: 0.0002584532273579897\n",
      "test loss is 0.0006525046193934688\n",
      "Batch: 9100,train loss is: 0.00030516441878806924\n",
      "test loss is 0.0006250510624439642\n",
      "Batch: 9200,train loss is: 0.0006940392621929062\n",
      "test loss is 0.0006694549034276448\n",
      "Batch: 9300,train loss is: 0.00048700211794701733\n",
      "test loss is 0.00067279507174321\n",
      "Batch: 9400,train loss is: 0.0004379874087929962\n",
      "test loss is 0.0006599820121055183\n",
      "Batch: 9500,train loss is: 0.0003289658380083517\n",
      "test loss is 0.0006529166341427966\n",
      "Batch: 9600,train loss is: 0.0003748948788801574\n",
      "test loss is 0.0006192323833535557\n",
      "Batch: 9700,train loss is: 0.00040494547525181786\n",
      "test loss is 0.0006292994585028529\n",
      "Batch: 9800,train loss is: 0.0002241370581538655\n",
      "test loss is 0.0006342303778800775\n",
      "Batch: 9900,train loss is: 0.0004265902769554399\n",
      "test loss is 0.00064426956764878\n",
      "Batch: 10000,train loss is: 0.00038688783883774426\n",
      "test loss is 0.0006202732181804064\n",
      "Batch: 10100,train loss is: 0.0008261330886030672\n",
      "test loss is 0.0006373696032489932\n",
      "Batch: 10200,train loss is: 0.001644041161685865\n",
      "test loss is 0.0007072681574384552\n",
      "Batch: 10300,train loss is: 0.0003843449469390818\n",
      "test loss is 0.0006312541234437355\n",
      "Batch: 10400,train loss is: 0.0002544473737701022\n",
      "test loss is 0.0006249516732454425\n",
      "Batch: 10500,train loss is: 0.0007527505252617576\n",
      "test loss is 0.0006386733605490125\n",
      "Batch: 10600,train loss is: 0.0002522610953667263\n",
      "test loss is 0.0006763023598660189\n",
      "Batch: 10700,train loss is: 0.0005485822287072704\n",
      "test loss is 0.0006245155548362993\n",
      "Batch: 10800,train loss is: 0.0006381127929399363\n",
      "test loss is 0.0006342295117696085\n",
      "Batch: 10900,train loss is: 0.0006327177665387987\n",
      "test loss is 0.0006332079447310716\n",
      "Batch: 11000,train loss is: 0.0006981929794406529\n",
      "test loss is 0.0006532468696622606\n",
      "Batch: 11100,train loss is: 0.0008638072779009632\n",
      "test loss is 0.0006198533013018146\n",
      "Batch: 11200,train loss is: 0.0006759018469663407\n",
      "test loss is 0.0006796789448683476\n",
      "Batch: 11300,train loss is: 0.0002773917143840429\n",
      "test loss is 0.0006444515424054258\n",
      "Batch: 11400,train loss is: 0.000742904559662871\n",
      "test loss is 0.000633802571554554\n",
      "Batch: 11500,train loss is: 0.00041634589460067645\n",
      "test loss is 0.0006328170651290243\n",
      "Batch: 11600,train loss is: 0.0017702006356198165\n",
      "test loss is 0.0006254139369220769\n",
      "Batch: 11700,train loss is: 0.0003103635874548998\n",
      "test loss is 0.0006210357551036361\n",
      "Batch: 11800,train loss is: 0.000516672857713877\n",
      "test loss is 0.0006327862612715812\n",
      "Batch: 11900,train loss is: 0.000427636491613372\n",
      "test loss is 0.0006437131753854149\n",
      "Batch: 12000,train loss is: 0.0003073717041446581\n",
      "test loss is 0.000632017777866258\n",
      "Batch: 12100,train loss is: 0.0015952282189713623\n",
      "test loss is 0.0006429612073330497\n",
      "Batch: 12200,train loss is: 0.000578444479127851\n",
      "test loss is 0.0006471061335442307\n",
      "Batch: 12300,train loss is: 0.0004336000846743071\n",
      "test loss is 0.0006428476639094581\n",
      "Batch: 12400,train loss is: 0.0003042384268420963\n",
      "test loss is 0.0006366430986641801\n",
      "Batch: 12500,train loss is: 0.00045933863487379703\n",
      "test loss is 0.0006549152907238215\n",
      "Batch: 12600,train loss is: 0.0004689069698554527\n",
      "test loss is 0.0006391919297605507\n",
      "Batch: 12700,train loss is: 0.00033169918447200833\n",
      "test loss is 0.0006279868633536639\n",
      "Batch: 12800,train loss is: 0.0003453571318809245\n",
      "test loss is 0.0006242085380248068\n",
      "Batch: 12900,train loss is: 0.0005105436621924209\n",
      "test loss is 0.0006334603994531223\n",
      "Batch: 13000,train loss is: 0.00020595655365171148\n",
      "test loss is 0.0006275261659462444\n",
      "Batch: 13100,train loss is: 0.001847994625521703\n",
      "test loss is 0.0006183638758948392\n",
      "Batch: 13200,train loss is: 0.0009655649317196916\n",
      "test loss is 0.0007648727137352767\n",
      "Batch: 13300,train loss is: 0.0004728067989969092\n",
      "test loss is 0.0006320887114641893\n",
      "Batch: 13400,train loss is: 0.00035175166025624256\n",
      "test loss is 0.0006513924016228554\n",
      "Batch: 13500,train loss is: 0.0004850031557891848\n",
      "test loss is 0.0006433881574181983\n",
      "Batch: 13600,train loss is: 0.0004617357776131829\n",
      "test loss is 0.0006500848967880471\n",
      "Batch: 13700,train loss is: 0.0006316924759958866\n",
      "test loss is 0.0006164419086155617\n",
      "Batch: 13800,train loss is: 0.0003508042858867251\n",
      "test loss is 0.0006206546981112747\n",
      "Batch: 13900,train loss is: 0.0002459610996864752\n",
      "test loss is 0.0006950183834096578\n",
      "Batch: 14000,train loss is: 0.00033748816040254996\n",
      "test loss is 0.0006449383283883076\n",
      "Batch: 14100,train loss is: 0.0006660602233910893\n",
      "test loss is 0.0006603793378277395\n",
      "Batch: 14200,train loss is: 0.0005426782744333889\n",
      "test loss is 0.0006190834394792492\n",
      "Batch: 14300,train loss is: 0.00024425153085920074\n",
      "test loss is 0.0006210322459903816\n",
      "Batch: 14400,train loss is: 0.0011380842778603865\n",
      "test loss is 0.0006186097600694572\n",
      "Batch: 14500,train loss is: 0.0006369287713644695\n",
      "test loss is 0.0006201193313994965\n",
      "Batch: 14600,train loss is: 0.0007242823844437626\n",
      "test loss is 0.0006428397761182136\n",
      "Batch: 14700,train loss is: 0.0004767522267736183\n",
      "test loss is 0.0007022900398590107\n",
      "Batch: 14800,train loss is: 0.000771166134320706\n",
      "test loss is 0.0006478043946502406\n",
      "Batch: 14900,train loss is: 0.0004070958520790683\n",
      "test loss is 0.0006860199700031007\n",
      "Batch: 15000,train loss is: 0.0005312795242158455\n",
      "test loss is 0.0006191907854893137\n",
      "Batch: 15100,train loss is: 0.000748864428267131\n",
      "test loss is 0.0006513396528130208\n",
      "Batch: 15200,train loss is: 0.0007568791774066142\n",
      "test loss is 0.0006316635004245418\n",
      "Batch: 15300,train loss is: 0.0003093167716073235\n",
      "test loss is 0.0006221152884047065\n",
      "Batch: 15400,train loss is: 0.0004615926567530982\n",
      "test loss is 0.0006491185043848998\n",
      "Batch: 15500,train loss is: 0.00064104410350238\n",
      "test loss is 0.0006200704675938923\n",
      "Batch: 15600,train loss is: 0.0007737432354680419\n",
      "test loss is 0.0006335517956453059\n",
      "Batch: 15700,train loss is: 0.00033958475592455264\n",
      "test loss is 0.0006294932585688197\n",
      "Batch: 15800,train loss is: 0.0005820911811160608\n",
      "test loss is 0.0006122574759773118\n",
      "Batch: 15900,train loss is: 0.0007792401444083641\n",
      "test loss is 0.0006310692412663928\n",
      "Batch: 16000,train loss is: 0.0003442376315220641\n",
      "test loss is 0.0006247303825735765\n",
      "Batch: 16100,train loss is: 0.0005939622167016883\n",
      "test loss is 0.0006321219260109441\n",
      "Batch: 16200,train loss is: 0.0003549380775360568\n",
      "test loss is 0.0006172955929534933\n",
      "Batch: 16300,train loss is: 0.0008581501849929271\n",
      "test loss is 0.0006436932553976344\n",
      "Batch: 16400,train loss is: 0.00028650986407028283\n",
      "test loss is 0.0006197099697544068\n",
      "Batch: 16500,train loss is: 0.0006621721150701388\n",
      "test loss is 0.0006376541754345569\n",
      "Batch: 16600,train loss is: 0.00030739768364511285\n",
      "test loss is 0.0006272322074860822\n",
      "Batch: 16700,train loss is: 0.0007786537920874694\n",
      "test loss is 0.0006316874744360628\n",
      "Batch: 16800,train loss is: 0.0004917137221272814\n",
      "test loss is 0.000657265401524214\n",
      "Batch: 16900,train loss is: 0.0007112628675172469\n",
      "test loss is 0.0006402687388623943\n",
      "Batch: 17000,train loss is: 0.0005871687686118944\n",
      "test loss is 0.0006144891237716521\n",
      "Batch: 17100,train loss is: 0.0004260769425697322\n",
      "test loss is 0.0006205950762811342\n",
      "Batch: 17200,train loss is: 0.0003912779998487766\n",
      "test loss is 0.0006323511494156574\n",
      "Batch: 17300,train loss is: 0.0007176296296630035\n",
      "test loss is 0.0006429617337661398\n",
      "Batch: 17400,train loss is: 0.0003589118005457883\n",
      "test loss is 0.0006474945144656937\n",
      "Batch: 17500,train loss is: 0.0005994755920438179\n",
      "test loss is 0.0006343553689095768\n",
      "Batch: 17600,train loss is: 0.0004154791939374981\n",
      "test loss is 0.0006401227643613863\n",
      "Batch: 17700,train loss is: 0.0003142292036262422\n",
      "test loss is 0.000667714762388415\n",
      "Batch: 17800,train loss is: 0.001584525497011282\n",
      "test loss is 0.0006661642162205188\n",
      "Batch: 17900,train loss is: 0.0004232316606596106\n",
      "test loss is 0.0006527496903582766\n",
      "Batch: 18000,train loss is: 0.000255681882369657\n",
      "test loss is 0.0006155114381754128\n",
      "Batch: 18100,train loss is: 0.0003568052478946138\n",
      "test loss is 0.0006253838383950157\n",
      "Batch: 18200,train loss is: 0.0005822232449629231\n",
      "test loss is 0.000631487467068668\n",
      "Batch: 18300,train loss is: 0.0012928246831101192\n",
      "test loss is 0.0006360612166384556\n",
      "Batch: 18400,train loss is: 0.0003946524889793909\n",
      "test loss is 0.000628025780906592\n",
      "Batch: 18500,train loss is: 0.0008311182294998403\n",
      "test loss is 0.0006297966224439667\n",
      "Batch: 18600,train loss is: 0.0004332604329956661\n",
      "test loss is 0.0006275328360730273\n",
      "Batch: 18700,train loss is: 0.0005472816102894586\n",
      "test loss is 0.000642001173735077\n",
      "Batch: 18800,train loss is: 0.0005269472156080506\n",
      "test loss is 0.0006214891585901266\n",
      "Batch: 18900,train loss is: 0.0004871096107693992\n",
      "test loss is 0.0006366459829246133\n",
      "Batch: 19000,train loss is: 0.000509732038911712\n",
      "test loss is 0.0006251400031192772\n",
      "Batch: 19100,train loss is: 0.00036933265057245295\n",
      "test loss is 0.000648876648607399\n",
      "Batch: 19200,train loss is: 0.0005041027813379584\n",
      "test loss is 0.0006325888189630746\n",
      "Batch: 19300,train loss is: 0.0004371781653979848\n",
      "test loss is 0.0006169184228323428\n",
      "Batch: 19400,train loss is: 0.0002569677907526701\n",
      "test loss is 0.0006229972434880971\n",
      "Batch: 19500,train loss is: 0.00047922262676371223\n",
      "test loss is 0.0006601328784406503\n",
      "Batch: 19600,train loss is: 0.00037091850135913403\n",
      "test loss is 0.000632538947341435\n",
      "Batch: 19700,train loss is: 0.0006162503669274503\n",
      "test loss is 0.0006994346513888451\n",
      "Batch: 19800,train loss is: 0.00056357682936901\n",
      "test loss is 0.0006398586300903325\n",
      "Batch: 19900,train loss is: 0.00039397384384053504\n",
      "test loss is 0.0006482551213226303\n",
      "Batch: 20000,train loss is: 0.0007148946283526251\n",
      "test loss is 0.0006230195287204018\n",
      "Batch: 20100,train loss is: 0.0005769876186324316\n",
      "test loss is 0.0006430714919230141\n",
      "Batch: 20200,train loss is: 0.0006689491567660642\n",
      "test loss is 0.0006401627743894955\n",
      "Batch: 20300,train loss is: 0.0002961774276309891\n",
      "test loss is 0.0006457601236549571\n",
      "Batch: 20400,train loss is: 0.000480831434512934\n",
      "test loss is 0.0006433174091059997\n",
      "Batch: 20500,train loss is: 0.00061874609729413\n",
      "test loss is 0.0006225907907550817\n",
      "Batch: 20600,train loss is: 0.0004899278036751227\n",
      "test loss is 0.0006536571583089777\n",
      "Batch: 20700,train loss is: 0.0007942082115952811\n",
      "test loss is 0.000625578706160504\n",
      "Batch: 20800,train loss is: 0.0003399702367154381\n",
      "test loss is 0.0006705202708335228\n",
      "Batch: 20900,train loss is: 0.0005262342522903516\n",
      "test loss is 0.0006709387031203847\n",
      "Batch: 21000,train loss is: 0.00021757515844018422\n",
      "test loss is 0.0006157372151074797\n",
      "Batch: 21100,train loss is: 0.0006579458924675141\n",
      "test loss is 0.0006671827493914277\n",
      "Batch: 21200,train loss is: 0.0003682227097461722\n",
      "test loss is 0.0006619427533436824\n",
      "Batch: 21300,train loss is: 0.00029146688816542325\n",
      "test loss is 0.0006458569745656388\n",
      "Batch: 21400,train loss is: 0.0008056204104875616\n",
      "test loss is 0.0006204290147562578\n",
      "Batch: 21500,train loss is: 0.0002372403712732513\n",
      "test loss is 0.0006459928675706825\n",
      "Batch: 21600,train loss is: 0.000633834685106886\n",
      "test loss is 0.0006380629639162867\n",
      "Batch: 21700,train loss is: 0.0002742235194130955\n",
      "test loss is 0.0006182996032842852\n",
      "Batch: 21800,train loss is: 0.0006778625609923987\n",
      "test loss is 0.000668558497087186\n",
      "Batch: 21900,train loss is: 0.00032591270667467926\n",
      "test loss is 0.0006885836087446484\n",
      "Batch: 22000,train loss is: 0.0006457371601000411\n",
      "test loss is 0.0006228070950891968\n",
      "Batch: 22100,train loss is: 0.0003492650326334723\n",
      "test loss is 0.0006220937350933761\n",
      "Batch: 22200,train loss is: 0.0004354572920959258\n",
      "test loss is 0.0006345917109778639\n",
      "Batch: 22300,train loss is: 0.00045592094691989824\n",
      "test loss is 0.0006500579130532886\n",
      "Batch: 22400,train loss is: 0.0004372968582052872\n",
      "test loss is 0.0006410588445141681\n",
      "Batch: 22500,train loss is: 0.0004974184831612157\n",
      "test loss is 0.0006149529998200736\n",
      "Batch: 22600,train loss is: 0.000772870307679722\n",
      "test loss is 0.000659010997145888\n",
      "Batch: 22700,train loss is: 0.0002932065659641845\n",
      "test loss is 0.0006222445204467817\n",
      "Batch: 22800,train loss is: 0.0004750417201257828\n",
      "test loss is 0.0006469794396917328\n",
      "Batch: 22900,train loss is: 0.0002930797620498692\n",
      "test loss is 0.0007069427766051737\n",
      "Batch: 23000,train loss is: 0.000571694459263412\n",
      "test loss is 0.0006313903483804519\n",
      "Batch: 23100,train loss is: 0.0004031760095361571\n",
      "test loss is 0.0006181327906770364\n",
      "Batch: 23200,train loss is: 0.0002893911917965893\n",
      "test loss is 0.0006112962100274281\n",
      "Batch: 23300,train loss is: 0.00041724325125541933\n",
      "test loss is 0.0006155253527058847\n",
      "Batch: 23400,train loss is: 0.0005469047985676368\n",
      "test loss is 0.0006486755202300839\n",
      "Batch: 23500,train loss is: 0.0013595671054140728\n",
      "test loss is 0.0006089629189121589\n",
      "Batch: 23600,train loss is: 0.00030455007074713136\n",
      "test loss is 0.0006272100463373083\n",
      "Batch: 23700,train loss is: 0.0010785826267042523\n",
      "test loss is 0.0006202726873113506\n",
      "Batch: 23800,train loss is: 0.0008088790932146907\n",
      "test loss is 0.0006266268053353369\n",
      "Batch: 23900,train loss is: 0.0009912052444718336\n",
      "test loss is 0.0006479564223065761\n",
      "Batch: 24000,train loss is: 0.0007263035683753642\n",
      "test loss is 0.0006793303205673327\n",
      "Batch: 24100,train loss is: 0.00025800924457521025\n",
      "test loss is 0.0006196492210237939\n",
      "Batch: 24200,train loss is: 0.0012561105547362378\n",
      "test loss is 0.0006408721157092834\n",
      "Batch: 24300,train loss is: 0.000460259111888063\n",
      "test loss is 0.0007205739666177096\n",
      "Batch: 24400,train loss is: 0.0011732652644815093\n",
      "test loss is 0.0006270219574343099\n",
      "Batch: 24500,train loss is: 0.0007381485169628394\n",
      "test loss is 0.0006107564921830614\n",
      "Batch: 24600,train loss is: 0.00034945653109671337\n",
      "test loss is 0.0006495876840264382\n",
      "Batch: 24700,train loss is: 0.00026380728046927434\n",
      "test loss is 0.0006351956886648632\n",
      "Batch: 24800,train loss is: 0.0006264964639403402\n",
      "test loss is 0.0006325047102848986\n",
      "Batch: 24900,train loss is: 0.0001911068743272554\n",
      "test loss is 0.0006220155247366497\n",
      "Batch: 25000,train loss is: 0.00043347873055021684\n",
      "test loss is 0.0006234041066785845\n",
      "Batch: 25100,train loss is: 0.00034466599889384624\n",
      "test loss is 0.0006319564773360208\n",
      "Batch: 25200,train loss is: 0.0006973282582179952\n",
      "test loss is 0.0006322442270462268\n",
      "Batch: 25300,train loss is: 0.00079772043181026\n",
      "test loss is 0.0006783786869156595\n",
      "Batch: 25400,train loss is: 0.00040805727214205236\n",
      "test loss is 0.000623978796095529\n",
      "Batch: 25500,train loss is: 0.0005517414970202794\n",
      "test loss is 0.0006323506146600834\n",
      "Batch: 25600,train loss is: 0.000580269647483917\n",
      "test loss is 0.0006341462000716998\n",
      "Batch: 25700,train loss is: 0.0005783178821414333\n",
      "test loss is 0.0006394718549337513\n",
      "Batch: 25800,train loss is: 0.0003139728461035881\n",
      "test loss is 0.0006235833895881554\n",
      "Batch: 25900,train loss is: 0.00026281786107750226\n",
      "test loss is 0.0006772233352358004\n",
      "Batch: 26000,train loss is: 0.001401937884566859\n",
      "test loss is 0.0006693497264686469\n",
      "Batch: 26100,train loss is: 0.0007169842245605464\n",
      "test loss is 0.0006210835585694528\n",
      "Batch: 26200,train loss is: 0.0005761713750369338\n",
      "test loss is 0.0006152380404000654\n",
      "Batch: 26300,train loss is: 0.0009465118713802907\n",
      "test loss is 0.000630694476033937\n",
      "Batch: 26400,train loss is: 0.0005381400719077056\n",
      "test loss is 0.0006134860726372272\n",
      "Batch: 26500,train loss is: 0.000628450737722942\n",
      "test loss is 0.0006684362990328011\n",
      "Batch: 26600,train loss is: 0.0012073122569236895\n",
      "test loss is 0.0007210883626633859\n",
      "Batch: 26700,train loss is: 0.0005382900681028899\n",
      "test loss is 0.0006928703889981616\n",
      "Batch: 26800,train loss is: 0.0011522694663043647\n",
      "test loss is 0.0006422013832671119\n",
      "Batch: 26900,train loss is: 0.00046785937181211386\n",
      "test loss is 0.0006297809717503011\n",
      "Batch: 27000,train loss is: 0.000651638891764645\n",
      "test loss is 0.0007271482576875394\n",
      "Batch: 27100,train loss is: 0.0007182508269804675\n",
      "test loss is 0.0007028895494836265\n",
      "Batch: 27200,train loss is: 0.0005794337279778368\n",
      "test loss is 0.000641265107094187\n",
      "Batch: 27300,train loss is: 0.00043763126228881264\n",
      "test loss is 0.0006412855097450437\n",
      "Batch: 27400,train loss is: 0.0005866224307124841\n",
      "test loss is 0.0006344842718543378\n",
      "Batch: 27500,train loss is: 0.00044515488659473406\n",
      "test loss is 0.0006845372138645927\n",
      "Batch: 27600,train loss is: 0.0004555102751716923\n",
      "test loss is 0.0006497991711046822\n",
      "Batch: 27700,train loss is: 0.0015289044289689348\n",
      "test loss is 0.0006325717630629631\n",
      "Batch: 27800,train loss is: 0.0007760249968871202\n",
      "test loss is 0.0006408686806018781\n",
      "Batch: 27900,train loss is: 0.001145779196941237\n",
      "test loss is 0.0006183737867878321\n",
      "Batch: 28000,train loss is: 0.0003832523387361383\n",
      "test loss is 0.0006258654717873729\n",
      "Batch: 28100,train loss is: 0.0007231412528335735\n",
      "test loss is 0.0006318996769396082\n",
      "Batch: 28200,train loss is: 0.0005130630821032512\n",
      "test loss is 0.0006133463927603423\n",
      "Batch: 28300,train loss is: 0.001667804535159316\n",
      "test loss is 0.0006342506228685714\n",
      "Batch: 28400,train loss is: 0.0005910345448148765\n",
      "test loss is 0.0006183637365771678\n",
      "Batch: 28500,train loss is: 0.0012495203424644967\n",
      "test loss is 0.0006737791987052123\n",
      "Batch: 28600,train loss is: 0.0003552920459409674\n",
      "test loss is 0.0006322122795649322\n",
      "Batch: 28700,train loss is: 0.0005437580526933023\n",
      "test loss is 0.0006199436452734229\n",
      "Batch: 28800,train loss is: 0.0009894462966038434\n",
      "test loss is 0.0006367110858086192\n",
      "Batch: 28900,train loss is: 0.00045600067672220293\n",
      "test loss is 0.0006134506680760115\n",
      "Batch: 29000,train loss is: 0.00047998362378315253\n",
      "test loss is 0.0006165105867058355\n",
      "Batch: 29100,train loss is: 0.0004539987032352961\n",
      "test loss is 0.0006284124342619748\n",
      "Batch: 29200,train loss is: 0.0004302268512303215\n",
      "test loss is 0.0006324646618548825\n",
      "Batch: 29300,train loss is: 0.0008585190413002971\n",
      "test loss is 0.0006199339765299287\n",
      "Batch: 29400,train loss is: 0.0005904047717756278\n",
      "test loss is 0.0007321829518773445\n",
      "Batch: 29500,train loss is: 0.0005379421016888435\n",
      "test loss is 0.0006635398685958651\n",
      "Batch: 29600,train loss is: 0.0005920634841226492\n",
      "test loss is 0.0006111561375133825\n",
      "Batch: 29700,train loss is: 0.0003420696137507864\n",
      "test loss is 0.0006285087571997957\n",
      "Batch: 29800,train loss is: 0.000810789166449953\n",
      "test loss is 0.0006218242161807725\n",
      "Batch: 29900,train loss is: 0.002620609521599967\n",
      "test loss is 0.0006223830215129976\n",
      "Batch: 30000,train loss is: 0.00037130238737437374\n",
      "test loss is 0.0006569210141166047\n",
      "Batch: 30100,train loss is: 0.0005146229539135253\n",
      "test loss is 0.0006482160977406173\n",
      "Batch: 30200,train loss is: 0.00033191891464788223\n",
      "test loss is 0.0006302001320180389\n",
      "Batch: 30300,train loss is: 0.0002657870699218931\n",
      "test loss is 0.0006157339315711623\n",
      "Batch: 30400,train loss is: 0.00029154589155966126\n",
      "test loss is 0.0006242609376219733\n",
      "Batch: 30500,train loss is: 0.00030480652124428306\n",
      "test loss is 0.0006617285422527581\n",
      "Batch: 30600,train loss is: 0.00023510897582906375\n",
      "test loss is 0.0006301821598797293\n",
      "Batch: 30700,train loss is: 0.0004142510811658414\n",
      "test loss is 0.0006882084218453298\n",
      "Batch: 30800,train loss is: 0.0002652330496820496\n",
      "test loss is 0.0006757809338144526\n",
      "Batch: 30900,train loss is: 0.00029628935141491603\n",
      "test loss is 0.0006271219324560496\n",
      "Batch: 31000,train loss is: 0.00046091182797493334\n",
      "test loss is 0.0006375828223382987\n",
      "Batch: 31100,train loss is: 0.0002938562550338907\n",
      "test loss is 0.0007151676860561712\n",
      "Batch: 31200,train loss is: 0.0003618941407250715\n",
      "test loss is 0.0006268104701399624\n",
      "Batch: 31300,train loss is: 0.000455633279511704\n",
      "test loss is 0.0006240478174711568\n",
      "Batch: 31400,train loss is: 0.000669810702378782\n",
      "test loss is 0.0006194409630210868\n",
      "Batch: 31500,train loss is: 0.0004153261211209787\n",
      "test loss is 0.0006362258111099058\n",
      "Batch: 31600,train loss is: 0.0003661768264037974\n",
      "test loss is 0.000626860705236012\n",
      "Batch: 31700,train loss is: 0.0004355575608093234\n",
      "test loss is 0.0006474289130325326\n",
      "Batch: 31800,train loss is: 0.0011119217631154153\n",
      "test loss is 0.0006252990053464987\n",
      "Batch: 31900,train loss is: 0.0006106645270749975\n",
      "test loss is 0.0006288218647472271\n",
      "Batch: 32000,train loss is: 0.0032785512807184734\n",
      "test loss is 0.0006345199719119204\n",
      "Batch: 32100,train loss is: 0.00028530363316040495\n",
      "test loss is 0.0006669522159294892\n",
      "Batch: 32200,train loss is: 0.00028666318690372773\n",
      "test loss is 0.0006237536762476833\n",
      "Batch: 32300,train loss is: 0.0003361728576219154\n",
      "test loss is 0.0006164795842429897\n",
      "Batch: 32400,train loss is: 0.0003333010886206297\n",
      "test loss is 0.0006127664150170596\n",
      "Batch: 32500,train loss is: 0.0007867313474316468\n",
      "test loss is 0.0006505580764464742\n",
      "Batch: 32600,train loss is: 0.0004625409239414585\n",
      "test loss is 0.0006392293031707163\n",
      "Batch: 32700,train loss is: 0.0004799741761249478\n",
      "test loss is 0.0006710028413577802\n",
      "Batch: 32800,train loss is: 0.0004979332699556704\n",
      "test loss is 0.0006637112569020076\n",
      "Batch: 32900,train loss is: 0.000612968130194615\n",
      "test loss is 0.0006360358282173569\n",
      "Batch: 33000,train loss is: 0.00036740760139291056\n",
      "test loss is 0.0006289216973439868\n",
      "Batch: 33100,train loss is: 0.0003940633562328467\n",
      "test loss is 0.0006305972604724013\n",
      "Batch: 33200,train loss is: 0.0004309606168075892\n",
      "test loss is 0.0006954072986911753\n",
      "Batch: 33300,train loss is: 0.0005817752485410022\n",
      "test loss is 0.0006273241238559359\n",
      "Batch: 33400,train loss is: 0.0004233159269097413\n",
      "test loss is 0.000628071026646392\n",
      "Batch: 33500,train loss is: 0.0003128756605743755\n",
      "test loss is 0.0006339129745712588\n",
      "Batch: 33600,train loss is: 0.0005009389444171653\n",
      "test loss is 0.0006206127274194043\n",
      "Batch: 33700,train loss is: 0.00048017761473723825\n",
      "test loss is 0.0006418140825098756\n",
      "Batch: 33800,train loss is: 0.0006667997374470266\n",
      "test loss is 0.0006244265914912258\n",
      "Batch: 33900,train loss is: 0.0005592306616903697\n",
      "test loss is 0.0006179775797762607\n",
      "-----------------------Epoch: 24----------------------------------\n",
      "Batch: 0,train loss is: 0.0003510471727563965\n",
      "test loss is 0.0006349211870541152\n",
      "Batch: 100,train loss is: 0.0009832141971035406\n",
      "test loss is 0.0006423254705567803\n",
      "Batch: 200,train loss is: 0.0002932409483268379\n",
      "test loss is 0.0006557601208987766\n",
      "Batch: 300,train loss is: 0.0002935079538813574\n",
      "test loss is 0.0006276763335713384\n",
      "Batch: 400,train loss is: 0.0004079890615660654\n",
      "test loss is 0.0006113850353531241\n",
      "Batch: 500,train loss is: 0.00043444431190775616\n",
      "test loss is 0.0006344193642119248\n",
      "Batch: 600,train loss is: 0.00022244714542196415\n",
      "test loss is 0.0006527957024311394\n",
      "Batch: 700,train loss is: 0.00041150003612710573\n",
      "test loss is 0.0006191433953484514\n",
      "Batch: 800,train loss is: 0.00038510756318062435\n",
      "test loss is 0.000628360794606027\n",
      "Batch: 900,train loss is: 0.00039883558984270056\n",
      "test loss is 0.000633977882111045\n",
      "Batch: 1000,train loss is: 0.00036816307218806197\n",
      "test loss is 0.0006422303138616962\n",
      "Batch: 1100,train loss is: 0.001838669904600335\n",
      "test loss is 0.0006171860689933696\n",
      "Batch: 1200,train loss is: 0.00023150904440493694\n",
      "test loss is 0.0006465482708005253\n",
      "Batch: 1300,train loss is: 0.0005003375006530096\n",
      "test loss is 0.0006229128897003521\n",
      "Batch: 1400,train loss is: 0.0003810331330927712\n",
      "test loss is 0.0006526193051922178\n",
      "Batch: 1500,train loss is: 0.00026912169543164623\n",
      "test loss is 0.00065329535171056\n",
      "Batch: 1600,train loss is: 0.0006186038932607639\n",
      "test loss is 0.0006717258612207351\n",
      "Batch: 1700,train loss is: 0.0011624369718340158\n",
      "test loss is 0.000617508899278189\n",
      "Batch: 1800,train loss is: 0.0003454380146581047\n",
      "test loss is 0.0006111832988752186\n",
      "Batch: 1900,train loss is: 0.0003613386127169525\n",
      "test loss is 0.0006709212844482512\n",
      "Batch: 2000,train loss is: 0.0007744796090781494\n",
      "test loss is 0.0007498931037429707\n",
      "Batch: 2100,train loss is: 0.0006529128533679238\n",
      "test loss is 0.0007629617145790581\n",
      "Batch: 2200,train loss is: 0.000677416318079724\n",
      "test loss is 0.0006367852695953439\n",
      "Batch: 2300,train loss is: 0.0003874393541966742\n",
      "test loss is 0.0006363534706984011\n",
      "Batch: 2400,train loss is: 0.0004154962667012895\n",
      "test loss is 0.0006309136359636453\n",
      "Batch: 2500,train loss is: 0.00040733666616733067\n",
      "test loss is 0.0006202606962725254\n",
      "Batch: 2600,train loss is: 0.0004400239444759163\n",
      "test loss is 0.0006218390512601872\n",
      "Batch: 2700,train loss is: 0.0004527019550983834\n",
      "test loss is 0.0006038256862323257\n",
      "Batch: 2800,train loss is: 0.0003964802800950209\n",
      "test loss is 0.0006306135729134623\n",
      "Batch: 2900,train loss is: 0.0003896880641947467\n",
      "test loss is 0.000627204502861473\n",
      "Batch: 3000,train loss is: 0.0004351132665627595\n",
      "test loss is 0.0006222833785285149\n",
      "Batch: 3100,train loss is: 0.0005208833275638479\n",
      "test loss is 0.0006456833879469265\n",
      "Batch: 3200,train loss is: 0.0009275899121991569\n",
      "test loss is 0.000635880768102201\n",
      "Batch: 3300,train loss is: 0.0005661619565724331\n",
      "test loss is 0.0006235997267074488\n",
      "Batch: 3400,train loss is: 0.000343912928649592\n",
      "test loss is 0.0006246002185311702\n",
      "Batch: 3500,train loss is: 0.00023027974303065158\n",
      "test loss is 0.0006155950998165681\n",
      "Batch: 3600,train loss is: 0.00041058859106552485\n",
      "test loss is 0.0006816145650660625\n",
      "Batch: 3700,train loss is: 0.0003165590455955318\n",
      "test loss is 0.0006391785015409218\n",
      "Batch: 3800,train loss is: 0.00040286028932450484\n",
      "test loss is 0.0006352252392304137\n",
      "Batch: 3900,train loss is: 0.0005663036692466794\n",
      "test loss is 0.0006231802959967954\n",
      "Batch: 4000,train loss is: 0.00039229098179065236\n",
      "test loss is 0.0006175853514462128\n",
      "Batch: 4100,train loss is: 0.0007168361404897821\n",
      "test loss is 0.0006503604630774189\n",
      "Batch: 4200,train loss is: 0.00025377607361694635\n",
      "test loss is 0.0006454905086394924\n",
      "Batch: 4300,train loss is: 0.0004200670561994846\n",
      "test loss is 0.0006114592718623385\n",
      "Batch: 4400,train loss is: 0.00022819492201075838\n",
      "test loss is 0.000645392873521585\n",
      "Batch: 4500,train loss is: 0.0005660356327102873\n",
      "test loss is 0.0006396643423727503\n",
      "Batch: 4600,train loss is: 0.0006140216414236524\n",
      "test loss is 0.0006401806980387251\n",
      "Batch: 4700,train loss is: 0.0005392087661230463\n",
      "test loss is 0.0006417720895503213\n",
      "Batch: 4800,train loss is: 0.0014127135387327551\n",
      "test loss is 0.0006139076158947751\n",
      "Batch: 4900,train loss is: 0.00041970313800654077\n",
      "test loss is 0.0006303034156112617\n",
      "Batch: 5000,train loss is: 0.0005447477296712344\n",
      "test loss is 0.0006393588289694787\n",
      "Batch: 5100,train loss is: 0.0004550618320919355\n",
      "test loss is 0.0006758752691378771\n",
      "Batch: 5200,train loss is: 0.00035930673608345563\n",
      "test loss is 0.0006375687183828248\n",
      "Batch: 5300,train loss is: 0.0004031414916343913\n",
      "test loss is 0.0006453798433762505\n",
      "Batch: 5400,train loss is: 0.0007191374755192071\n",
      "test loss is 0.0006225795037759364\n",
      "Batch: 5500,train loss is: 0.0004179273217578255\n",
      "test loss is 0.0006406723657346443\n",
      "Batch: 5600,train loss is: 0.000547051561552784\n",
      "test loss is 0.0006169586897984974\n",
      "Batch: 5700,train loss is: 0.00037982122339709034\n",
      "test loss is 0.0006317384476695478\n",
      "Batch: 5800,train loss is: 0.0004160610707525308\n",
      "test loss is 0.0006255945232712528\n",
      "Batch: 5900,train loss is: 0.00304305077070699\n",
      "test loss is 0.000628711458616586\n",
      "Batch: 6000,train loss is: 0.0017124787556309236\n",
      "test loss is 0.0006593022872164612\n",
      "Batch: 6100,train loss is: 0.00038662277926208627\n",
      "test loss is 0.0006188658407289428\n",
      "Batch: 6200,train loss is: 0.0005412182410281408\n",
      "test loss is 0.0006205482480765292\n",
      "Batch: 6300,train loss is: 0.0009406572515206469\n",
      "test loss is 0.000633972373642837\n",
      "Batch: 6400,train loss is: 0.0004905807816799661\n",
      "test loss is 0.0006577263064972553\n",
      "Batch: 6500,train loss is: 0.00031863135950587835\n",
      "test loss is 0.0006233083834522686\n",
      "Batch: 6600,train loss is: 0.0018207646942848218\n",
      "test loss is 0.0006180151590208065\n",
      "Batch: 6700,train loss is: 0.0002723495508199909\n",
      "test loss is 0.0006202960888012461\n",
      "Batch: 6800,train loss is: 0.0006952747021738081\n",
      "test loss is 0.000617292328943388\n",
      "Batch: 6900,train loss is: 0.0005656301209664579\n",
      "test loss is 0.0006386192525720711\n",
      "Batch: 7000,train loss is: 0.0005178667283240967\n",
      "test loss is 0.0006409356279667429\n",
      "Batch: 7100,train loss is: 0.001076199320112219\n",
      "test loss is 0.0006314890095597728\n",
      "Batch: 7200,train loss is: 0.00043237594430900803\n",
      "test loss is 0.0006314705164229344\n",
      "Batch: 7300,train loss is: 0.0005878961125010384\n",
      "test loss is 0.0006441335841356657\n",
      "Batch: 7400,train loss is: 0.0003360129843593103\n",
      "test loss is 0.0006238102195174387\n",
      "Batch: 7500,train loss is: 0.000602408058481241\n",
      "test loss is 0.0006436929406339006\n",
      "Batch: 7600,train loss is: 0.00040777584968066917\n",
      "test loss is 0.0006076532780451554\n",
      "Batch: 7700,train loss is: 0.0008228412536611357\n",
      "test loss is 0.0006391378557874135\n",
      "Batch: 7800,train loss is: 0.00024930743703487567\n",
      "test loss is 0.000614454419407881\n",
      "Batch: 7900,train loss is: 0.0003881117306203864\n",
      "test loss is 0.0006330757646703833\n",
      "Batch: 8000,train loss is: 0.000327113745232277\n",
      "test loss is 0.0006627609164372887\n",
      "Batch: 8100,train loss is: 0.000560423401951484\n",
      "test loss is 0.0006399301463487354\n",
      "Batch: 8200,train loss is: 0.0011134717826596563\n",
      "test loss is 0.0006437674639213224\n",
      "Batch: 8300,train loss is: 0.0002673107701362161\n",
      "test loss is 0.0006349974913012204\n",
      "Batch: 8400,train loss is: 0.0005286535978940156\n",
      "test loss is 0.0006422212454377621\n",
      "Batch: 8500,train loss is: 0.0005212558223902404\n",
      "test loss is 0.0006344608386568785\n",
      "Batch: 8600,train loss is: 0.0006188459021007635\n",
      "test loss is 0.0006435464024732054\n",
      "Batch: 8700,train loss is: 0.0004404805549860707\n",
      "test loss is 0.0006870574864478532\n",
      "Batch: 8800,train loss is: 0.00034270548930519674\n",
      "test loss is 0.0006213902280166597\n",
      "Batch: 8900,train loss is: 0.0010029358678489996\n",
      "test loss is 0.0006205431346669874\n",
      "Batch: 9000,train loss is: 0.00025871237133808964\n",
      "test loss is 0.0006477765519662089\n",
      "Batch: 9100,train loss is: 0.00030037176840787895\n",
      "test loss is 0.0006205547888277213\n",
      "Batch: 9200,train loss is: 0.0006730733647233416\n",
      "test loss is 0.0006628869260896557\n",
      "Batch: 9300,train loss is: 0.00048258429604915767\n",
      "test loss is 0.0006669909186460688\n",
      "Batch: 9400,train loss is: 0.00043111765046760736\n",
      "test loss is 0.0006550897620096933\n",
      "Batch: 9500,train loss is: 0.00032147547120416655\n",
      "test loss is 0.0006463687280536956\n",
      "Batch: 9600,train loss is: 0.00036830562445920134\n",
      "test loss is 0.0006140498155309007\n",
      "Batch: 9700,train loss is: 0.0004053480429792792\n",
      "test loss is 0.0006231198846914583\n",
      "Batch: 9800,train loss is: 0.0002198716060259948\n",
      "test loss is 0.0006284760744752327\n",
      "Batch: 9900,train loss is: 0.000420892802029401\n",
      "test loss is 0.0006375649703322646\n",
      "Batch: 10000,train loss is: 0.00037494778543084166\n",
      "test loss is 0.0006156583427775605\n",
      "Batch: 10100,train loss is: 0.0008069198655265882\n",
      "test loss is 0.0006320498946987131\n",
      "Batch: 10200,train loss is: 0.0016416932231534957\n",
      "test loss is 0.0007011380903012388\n",
      "Batch: 10300,train loss is: 0.00037999963972202726\n",
      "test loss is 0.0006254630393075492\n",
      "Batch: 10400,train loss is: 0.0002548421280131942\n",
      "test loss is 0.0006193836281796937\n",
      "Batch: 10500,train loss is: 0.0007428142679597544\n",
      "test loss is 0.0006323732966999423\n",
      "Batch: 10600,train loss is: 0.00024520110953281306\n",
      "test loss is 0.0006680116289432093\n",
      "Batch: 10700,train loss is: 0.000540589429694454\n",
      "test loss is 0.0006191214634172523\n",
      "Batch: 10800,train loss is: 0.0006514055146299265\n",
      "test loss is 0.0006287945556184613\n",
      "Batch: 10900,train loss is: 0.0006219290359031781\n",
      "test loss is 0.0006281648604661076\n",
      "Batch: 11000,train loss is: 0.0006958096090521145\n",
      "test loss is 0.0006481465224917141\n",
      "Batch: 11100,train loss is: 0.0008497086008298094\n",
      "test loss is 0.0006140259786071518\n",
      "Batch: 11200,train loss is: 0.0006664444075017667\n",
      "test loss is 0.0006733862909464237\n",
      "Batch: 11300,train loss is: 0.0002743605709773571\n",
      "test loss is 0.0006396664947002717\n",
      "Batch: 11400,train loss is: 0.0007487556457201705\n",
      "test loss is 0.0006286356160603357\n",
      "Batch: 11500,train loss is: 0.0004127702883096896\n",
      "test loss is 0.0006267324026047901\n",
      "Batch: 11600,train loss is: 0.0017563597782301195\n",
      "test loss is 0.0006194819315807832\n",
      "Batch: 11700,train loss is: 0.00031458838872392645\n",
      "test loss is 0.0006158848994913685\n",
      "Batch: 11800,train loss is: 0.0005073754133031682\n",
      "test loss is 0.0006286273228763414\n",
      "Batch: 11900,train loss is: 0.00041649721833945404\n",
      "test loss is 0.0006390956288430988\n",
      "Batch: 12000,train loss is: 0.00030897661286018386\n",
      "test loss is 0.0006269489904019488\n",
      "Batch: 12100,train loss is: 0.00158299414671045\n",
      "test loss is 0.0006376636433646683\n",
      "Batch: 12200,train loss is: 0.0005736123439603783\n",
      "test loss is 0.0006405494570072788\n",
      "Batch: 12300,train loss is: 0.00043340387815624744\n",
      "test loss is 0.0006377123103188926\n",
      "Batch: 12400,train loss is: 0.0002900464852066719\n",
      "test loss is 0.000632691864312779\n",
      "Batch: 12500,train loss is: 0.0004588418063256487\n",
      "test loss is 0.0006490206015081261\n",
      "Batch: 12600,train loss is: 0.0004612525090930538\n",
      "test loss is 0.0006334354847276458\n",
      "Batch: 12700,train loss is: 0.00032691062951572305\n",
      "test loss is 0.0006221799293478192\n",
      "Batch: 12800,train loss is: 0.0003401060215636072\n",
      "test loss is 0.0006187632763293489\n",
      "Batch: 12900,train loss is: 0.0005005671506385002\n",
      "test loss is 0.0006283277371275564\n",
      "Batch: 13000,train loss is: 0.00020882522702134657\n",
      "test loss is 0.0006226054143802248\n",
      "Batch: 13100,train loss is: 0.0018084606871414906\n",
      "test loss is 0.0006126703212042071\n",
      "Batch: 13200,train loss is: 0.000988488125964159\n",
      "test loss is 0.0007606113035349994\n",
      "Batch: 13300,train loss is: 0.0004550580633305854\n",
      "test loss is 0.0006266879871852935\n",
      "Batch: 13400,train loss is: 0.000354683138119184\n",
      "test loss is 0.00064483186438288\n",
      "Batch: 13500,train loss is: 0.0004765275099674485\n",
      "test loss is 0.0006358440884421553\n",
      "Batch: 13600,train loss is: 0.00044032109829494274\n",
      "test loss is 0.0006458918633917099\n",
      "Batch: 13700,train loss is: 0.0006227721600179957\n",
      "test loss is 0.000610862884719713\n",
      "Batch: 13800,train loss is: 0.0003411153548680778\n",
      "test loss is 0.0006159923171438842\n",
      "Batch: 13900,train loss is: 0.00023594083742337486\n",
      "test loss is 0.0006872224188275546\n",
      "Batch: 14000,train loss is: 0.00033862225618733216\n",
      "test loss is 0.0006379157352423969\n",
      "Batch: 14100,train loss is: 0.0006652586571480845\n",
      "test loss is 0.0006550126344637055\n",
      "Batch: 14200,train loss is: 0.000535680961241909\n",
      "test loss is 0.0006143859389342006\n",
      "Batch: 14300,train loss is: 0.0002460589212218094\n",
      "test loss is 0.0006146083332588374\n",
      "Batch: 14400,train loss is: 0.0011172727513057193\n",
      "test loss is 0.0006127581025612708\n",
      "Batch: 14500,train loss is: 0.0006401466394293938\n",
      "test loss is 0.0006144812057342118\n",
      "Batch: 14600,train loss is: 0.0007019095536257745\n",
      "test loss is 0.0006380545345653182\n",
      "Batch: 14700,train loss is: 0.00047673628521477623\n",
      "test loss is 0.0006984996576516501\n",
      "Batch: 14800,train loss is: 0.0007696547540937573\n",
      "test loss is 0.000643990269265808\n",
      "Batch: 14900,train loss is: 0.00039955708583112745\n",
      "test loss is 0.0006807753860596984\n",
      "Batch: 15000,train loss is: 0.0005202304344973012\n",
      "test loss is 0.0006143841996957667\n",
      "Batch: 15100,train loss is: 0.0007518380302849761\n",
      "test loss is 0.0006468943573388876\n",
      "Batch: 15200,train loss is: 0.0007676918819290792\n",
      "test loss is 0.000626005380877676\n",
      "Batch: 15300,train loss is: 0.00030335080139532556\n",
      "test loss is 0.0006160594095852876\n",
      "Batch: 15400,train loss is: 0.00046070958760882\n",
      "test loss is 0.0006439024573264775\n",
      "Batch: 15500,train loss is: 0.0006330540229750478\n",
      "test loss is 0.0006152135105257579\n",
      "Batch: 15600,train loss is: 0.000770188843622372\n",
      "test loss is 0.0006264779495074387\n",
      "Batch: 15700,train loss is: 0.00033287222932870274\n",
      "test loss is 0.0006246042443665567\n",
      "Batch: 15800,train loss is: 0.000567614643413515\n",
      "test loss is 0.0006071330587468352\n",
      "Batch: 15900,train loss is: 0.0007749280665905657\n",
      "test loss is 0.0006242769733766167\n",
      "Batch: 16000,train loss is: 0.00033872184173601054\n",
      "test loss is 0.0006184111211451\n",
      "Batch: 16100,train loss is: 0.000592634327744182\n",
      "test loss is 0.0006282424694142722\n",
      "Batch: 16200,train loss is: 0.00035671783333674265\n",
      "test loss is 0.0006119918205297744\n",
      "Batch: 16300,train loss is: 0.0008495099602198246\n",
      "test loss is 0.0006382548026664613\n",
      "Batch: 16400,train loss is: 0.0002840554965777289\n",
      "test loss is 0.000614886785494189\n",
      "Batch: 16500,train loss is: 0.0006502735828509041\n",
      "test loss is 0.0006295580837738441\n",
      "Batch: 16600,train loss is: 0.0003039474054171767\n",
      "test loss is 0.0006231607064413944\n",
      "Batch: 16700,train loss is: 0.0007698545516848207\n",
      "test loss is 0.0006255889150376364\n",
      "Batch: 16800,train loss is: 0.00048675126062996244\n",
      "test loss is 0.0006518660835343546\n",
      "Batch: 16900,train loss is: 0.0007090393918900547\n",
      "test loss is 0.0006363723997094458\n",
      "Batch: 17000,train loss is: 0.0005879085455123719\n",
      "test loss is 0.0006086393533851872\n",
      "Batch: 17100,train loss is: 0.0004146652976620587\n",
      "test loss is 0.0006147888580857732\n",
      "Batch: 17200,train loss is: 0.00038189349630128887\n",
      "test loss is 0.0006275534364658321\n",
      "Batch: 17300,train loss is: 0.0007033249167780411\n",
      "test loss is 0.0006359814172760868\n",
      "Batch: 17400,train loss is: 0.00035692296142551767\n",
      "test loss is 0.0006448257815365151\n",
      "Batch: 17500,train loss is: 0.000603093123917023\n",
      "test loss is 0.0006294101305317045\n",
      "Batch: 17600,train loss is: 0.0004132554484337971\n",
      "test loss is 0.0006342179662414069\n",
      "Batch: 17700,train loss is: 0.00031427702496475716\n",
      "test loss is 0.0006614194598547093\n",
      "Batch: 17800,train loss is: 0.0016325629066986785\n",
      "test loss is 0.0006636197761949451\n",
      "Batch: 17900,train loss is: 0.0004109700185572581\n",
      "test loss is 0.0006457096468843723\n",
      "Batch: 18000,train loss is: 0.0002509524683546148\n",
      "test loss is 0.0006108834645952945\n",
      "Batch: 18100,train loss is: 0.00036593266351047174\n",
      "test loss is 0.0006205305540665901\n",
      "Batch: 18200,train loss is: 0.0005837568975641804\n",
      "test loss is 0.0006246197721166929\n",
      "Batch: 18300,train loss is: 0.0012748122528618683\n",
      "test loss is 0.0006306826362834334\n",
      "Batch: 18400,train loss is: 0.00039065532105751196\n",
      "test loss is 0.000623862521229111\n",
      "Batch: 18500,train loss is: 0.000812806398740562\n",
      "test loss is 0.0006249022919809048\n",
      "Batch: 18600,train loss is: 0.00043060982936319117\n",
      "test loss is 0.0006222887497433834\n",
      "Batch: 18700,train loss is: 0.0005466600728611259\n",
      "test loss is 0.0006355738305032348\n",
      "Batch: 18800,train loss is: 0.0005228527693715945\n",
      "test loss is 0.0006163844586986456\n",
      "Batch: 18900,train loss is: 0.00048419631380120664\n",
      "test loss is 0.0006300191153589448\n",
      "Batch: 19000,train loss is: 0.0004986337038968543\n",
      "test loss is 0.0006201298919428012\n",
      "Batch: 19100,train loss is: 0.00036513260673341573\n",
      "test loss is 0.0006421717865178582\n",
      "Batch: 19200,train loss is: 0.0004930331552125503\n",
      "test loss is 0.0006264710522720064\n",
      "Batch: 19300,train loss is: 0.000422163811120152\n",
      "test loss is 0.0006118126656765595\n",
      "Batch: 19400,train loss is: 0.0002569676754908381\n",
      "test loss is 0.0006178492677197022\n",
      "Batch: 19500,train loss is: 0.0004771777799068238\n",
      "test loss is 0.0006536170598027642\n",
      "Batch: 19600,train loss is: 0.00036486322657993113\n",
      "test loss is 0.0006262576466849155\n",
      "Batch: 19700,train loss is: 0.0006006188415533845\n",
      "test loss is 0.0006904497855202479\n",
      "Batch: 19800,train loss is: 0.0005675047232137014\n",
      "test loss is 0.0006351898643671535\n",
      "Batch: 19900,train loss is: 0.00039434844138255506\n",
      "test loss is 0.0006434691875719036\n",
      "Batch: 20000,train loss is: 0.0007198656929466642\n",
      "test loss is 0.000616886930645208\n",
      "Batch: 20100,train loss is: 0.0005788851123474355\n",
      "test loss is 0.0006373345939978158\n",
      "Batch: 20200,train loss is: 0.0006509911158910268\n",
      "test loss is 0.0006343941179303237\n",
      "Batch: 20300,train loss is: 0.00029765945810297675\n",
      "test loss is 0.0006414544728688599\n",
      "Batch: 20400,train loss is: 0.00048284888877146297\n",
      "test loss is 0.0006367519822637865\n",
      "Batch: 20500,train loss is: 0.0006152440617752631\n",
      "test loss is 0.0006171433469047816\n",
      "Batch: 20600,train loss is: 0.0004932548916688387\n",
      "test loss is 0.0006471174943389247\n",
      "Batch: 20700,train loss is: 0.0007791960435647637\n",
      "test loss is 0.0006199048568422933\n",
      "Batch: 20800,train loss is: 0.00034042308361300803\n",
      "test loss is 0.0006662826901359876\n",
      "Batch: 20900,train loss is: 0.0005176015807876304\n",
      "test loss is 0.0006647556168943447\n",
      "Batch: 21000,train loss is: 0.00021755691517160437\n",
      "test loss is 0.000609871704490578\n",
      "Batch: 21100,train loss is: 0.0006593212902452287\n",
      "test loss is 0.0006640127959453489\n",
      "Batch: 21200,train loss is: 0.00036405083513610137\n",
      "test loss is 0.0006590448754982185\n",
      "Batch: 21300,train loss is: 0.0002912456965909801\n",
      "test loss is 0.0006432038230656885\n",
      "Batch: 21400,train loss is: 0.0008054449877016219\n",
      "test loss is 0.0006154019164730091\n",
      "Batch: 21500,train loss is: 0.0002401821974381017\n",
      "test loss is 0.0006398063721517908\n",
      "Batch: 21600,train loss is: 0.0006168595252733514\n",
      "test loss is 0.0006306537172457177\n",
      "Batch: 21700,train loss is: 0.0002724409608637052\n",
      "test loss is 0.0006130008812364301\n",
      "Batch: 21800,train loss is: 0.0006712449985461071\n",
      "test loss is 0.0006606033895469662\n",
      "Batch: 21900,train loss is: 0.0003131246672405847\n",
      "test loss is 0.000680966154341248\n",
      "Batch: 22000,train loss is: 0.0006419759771507746\n",
      "test loss is 0.000617326195749726\n",
      "Batch: 22100,train loss is: 0.0003506632973801674\n",
      "test loss is 0.000616733582657429\n",
      "Batch: 22200,train loss is: 0.00042374753438102455\n",
      "test loss is 0.000629361282263708\n",
      "Batch: 22300,train loss is: 0.0004471056410686578\n",
      "test loss is 0.0006457066949266975\n",
      "Batch: 22400,train loss is: 0.0004340303392580617\n",
      "test loss is 0.0006355521959682938\n",
      "Batch: 22500,train loss is: 0.0004957272901846512\n",
      "test loss is 0.0006086755363532842\n",
      "Batch: 22600,train loss is: 0.0007706779596614634\n",
      "test loss is 0.0006536364485784979\n",
      "Batch: 22700,train loss is: 0.00028624795375637374\n",
      "test loss is 0.0006178744397215878\n",
      "Batch: 22800,train loss is: 0.0004733389039832641\n",
      "test loss is 0.0006406806635227717\n",
      "Batch: 22900,train loss is: 0.0002895380444211986\n",
      "test loss is 0.0007010232812172786\n",
      "Batch: 23000,train loss is: 0.000588233148852083\n",
      "test loss is 0.0006261603308270266\n",
      "Batch: 23100,train loss is: 0.00039641571268827\n",
      "test loss is 0.0006146639270883602\n",
      "Batch: 23200,train loss is: 0.0002906892847986237\n",
      "test loss is 0.0006063839022810138\n",
      "Batch: 23300,train loss is: 0.0004120898355674639\n",
      "test loss is 0.0006098681815197566\n",
      "Batch: 23400,train loss is: 0.0005410588567744721\n",
      "test loss is 0.0006445780435971231\n",
      "Batch: 23500,train loss is: 0.001350125046158707\n",
      "test loss is 0.0006033263688306794\n",
      "Batch: 23600,train loss is: 0.0003002990797260912\n",
      "test loss is 0.0006217714377376572\n",
      "Batch: 23700,train loss is: 0.0010917076962853327\n",
      "test loss is 0.0006145140621260779\n",
      "Batch: 23800,train loss is: 0.000803286124646937\n",
      "test loss is 0.0006212765999807658\n",
      "Batch: 23900,train loss is: 0.0010003674304155277\n",
      "test loss is 0.0006434193819511131\n",
      "Batch: 24000,train loss is: 0.0007178973981315096\n",
      "test loss is 0.0006741222767881265\n",
      "Batch: 24100,train loss is: 0.0002562544730549819\n",
      "test loss is 0.0006157160637733987\n",
      "Batch: 24200,train loss is: 0.0012365175731102857\n",
      "test loss is 0.0006359509839677829\n",
      "Batch: 24300,train loss is: 0.00045463105470706276\n",
      "test loss is 0.0007136664220365898\n",
      "Batch: 24400,train loss is: 0.0011484187779017775\n",
      "test loss is 0.0006222925708459073\n",
      "Batch: 24500,train loss is: 0.0007428981530932056\n",
      "test loss is 0.0006059652691189519\n",
      "Batch: 24600,train loss is: 0.0003496460748867331\n",
      "test loss is 0.0006451577456140234\n",
      "Batch: 24700,train loss is: 0.00026194135366054325\n",
      "test loss is 0.000630594348634278\n",
      "Batch: 24800,train loss is: 0.0006242370232215795\n",
      "test loss is 0.0006254962106599208\n",
      "Batch: 24900,train loss is: 0.000191729230722133\n",
      "test loss is 0.000616252912933251\n",
      "Batch: 25000,train loss is: 0.0004208234837247023\n",
      "test loss is 0.0006185129646614142\n",
      "Batch: 25100,train loss is: 0.0003290715655701238\n",
      "test loss is 0.0006260935161233235\n",
      "Batch: 25200,train loss is: 0.0006872792945222174\n",
      "test loss is 0.0006269156149990596\n",
      "Batch: 25300,train loss is: 0.0008006548592785774\n",
      "test loss is 0.0006726934464345513\n",
      "Batch: 25400,train loss is: 0.000406133539705655\n",
      "test loss is 0.0006187006291257981\n",
      "Batch: 25500,train loss is: 0.0005443610741613495\n",
      "test loss is 0.0006271244819464486\n",
      "Batch: 25600,train loss is: 0.0005746448591526546\n",
      "test loss is 0.0006293519672828252\n",
      "Batch: 25700,train loss is: 0.0005667660759430305\n",
      "test loss is 0.0006332244501433515\n",
      "Batch: 25800,train loss is: 0.000314941639398636\n",
      "test loss is 0.0006196143931125105\n",
      "Batch: 25900,train loss is: 0.0002594526138392194\n",
      "test loss is 0.0006713571128522687\n",
      "Batch: 26000,train loss is: 0.0013796182207948893\n",
      "test loss is 0.00066250467688518\n",
      "Batch: 26100,train loss is: 0.0007072817416587842\n",
      "test loss is 0.0006161674374173824\n",
      "Batch: 26200,train loss is: 0.0005718056234287722\n",
      "test loss is 0.0006100541811367305\n",
      "Batch: 26300,train loss is: 0.0009356728133333482\n",
      "test loss is 0.0006243865681313495\n",
      "Batch: 26400,train loss is: 0.000533813563499703\n",
      "test loss is 0.0006080231623203795\n",
      "Batch: 26500,train loss is: 0.0006313106647194783\n",
      "test loss is 0.0006663179200358839\n",
      "Batch: 26600,train loss is: 0.0011900742065561903\n",
      "test loss is 0.0007186877219034462\n",
      "Batch: 26700,train loss is: 0.0005478752650947649\n",
      "test loss is 0.0006836182028271335\n",
      "Batch: 26800,train loss is: 0.0011547425560560901\n",
      "test loss is 0.0006374564964167821\n",
      "Batch: 26900,train loss is: 0.000469766362699854\n",
      "test loss is 0.0006239508958387352\n",
      "Batch: 27000,train loss is: 0.000656991332674849\n",
      "test loss is 0.0007206190272806588\n",
      "Batch: 27100,train loss is: 0.0007195324162643599\n",
      "test loss is 0.00070046438541351\n",
      "Batch: 27200,train loss is: 0.0005724539298012735\n",
      "test loss is 0.0006327063172306543\n",
      "Batch: 27300,train loss is: 0.0004343746331603664\n",
      "test loss is 0.0006359300688414742\n",
      "Batch: 27400,train loss is: 0.0005845031253482183\n",
      "test loss is 0.0006296729194712934\n",
      "Batch: 27500,train loss is: 0.0004313747118969713\n",
      "test loss is 0.0006777887862058042\n",
      "Batch: 27600,train loss is: 0.0004525450389383586\n",
      "test loss is 0.000644466183798611\n",
      "Batch: 27700,train loss is: 0.001528067769537293\n",
      "test loss is 0.0006264177318265714\n",
      "Batch: 27800,train loss is: 0.000778800913211113\n",
      "test loss is 0.0006330691202822612\n",
      "Batch: 27900,train loss is: 0.0011153269022101855\n",
      "test loss is 0.0006123849861929681\n",
      "Batch: 28000,train loss is: 0.0003809559684338537\n",
      "test loss is 0.0006201130165689516\n",
      "Batch: 28100,train loss is: 0.000698032419102302\n",
      "test loss is 0.000625384986067057\n",
      "Batch: 28200,train loss is: 0.0005068371329288361\n",
      "test loss is 0.0006093052134508846\n",
      "Batch: 28300,train loss is: 0.0016636823277783994\n",
      "test loss is 0.0006287027919999629\n",
      "Batch: 28400,train loss is: 0.0005885007056288205\n",
      "test loss is 0.0006131555875751467\n",
      "Batch: 28500,train loss is: 0.001244762789881615\n",
      "test loss is 0.0006696085910698186\n",
      "Batch: 28600,train loss is: 0.00036106487879266426\n",
      "test loss is 0.0006270967027230958\n",
      "Batch: 28700,train loss is: 0.0005435486173216743\n",
      "test loss is 0.0006147696607701947\n",
      "Batch: 28800,train loss is: 0.0009902385904331326\n",
      "test loss is 0.0006313895793151177\n",
      "Batch: 28900,train loss is: 0.0004567955565848261\n",
      "test loss is 0.0006084139761751503\n",
      "Batch: 29000,train loss is: 0.0004816033880022157\n",
      "test loss is 0.000610844747180928\n",
      "Batch: 29100,train loss is: 0.0004471025971393866\n",
      "test loss is 0.0006233843254725802\n",
      "Batch: 29200,train loss is: 0.0004353887043624141\n",
      "test loss is 0.0006277705214308446\n",
      "Batch: 29300,train loss is: 0.0008519476754794994\n",
      "test loss is 0.0006144516047583985\n",
      "Batch: 29400,train loss is: 0.0005845938342106452\n",
      "test loss is 0.0007264148575357449\n",
      "Batch: 29500,train loss is: 0.0005450817379901868\n",
      "test loss is 0.0006584098150436811\n",
      "Batch: 29600,train loss is: 0.000598810672746655\n",
      "test loss is 0.0006063904461694838\n",
      "Batch: 29700,train loss is: 0.0003410520709252621\n",
      "test loss is 0.0006223481580516466\n",
      "Batch: 29800,train loss is: 0.0008163552757387733\n",
      "test loss is 0.0006174298118027984\n",
      "Batch: 29900,train loss is: 0.0025642068129834395\n",
      "test loss is 0.0006181622411003443\n",
      "Batch: 30000,train loss is: 0.0003693435804942939\n",
      "test loss is 0.0006509429166629933\n",
      "Batch: 30100,train loss is: 0.00052341884456112\n",
      "test loss is 0.000641968810821156\n",
      "Batch: 30200,train loss is: 0.00032875976141483545\n",
      "test loss is 0.0006242126718819364\n",
      "Batch: 30300,train loss is: 0.000266980660351872\n",
      "test loss is 0.0006106222614414886\n",
      "Batch: 30400,train loss is: 0.0002884098701483428\n",
      "test loss is 0.0006187545817464747\n",
      "Batch: 30500,train loss is: 0.00030187596838626804\n",
      "test loss is 0.0006570746671770491\n",
      "Batch: 30600,train loss is: 0.00023416841102659916\n",
      "test loss is 0.000625658223289655\n",
      "Batch: 30700,train loss is: 0.0004183864325899327\n",
      "test loss is 0.0006806373781916667\n",
      "Batch: 30800,train loss is: 0.00026527520280133376\n",
      "test loss is 0.0006690572688276012\n",
      "Batch: 30900,train loss is: 0.0002927751163274837\n",
      "test loss is 0.0006215627910179377\n",
      "Batch: 31000,train loss is: 0.0004556198604812168\n",
      "test loss is 0.0006321638567881301\n",
      "Batch: 31100,train loss is: 0.000292121126602545\n",
      "test loss is 0.0007118431490710106\n",
      "Batch: 31200,train loss is: 0.0003460237118812578\n",
      "test loss is 0.0006215984383413408\n",
      "Batch: 31300,train loss is: 0.00045555049770016373\n",
      "test loss is 0.0006187515810465222\n",
      "Batch: 31400,train loss is: 0.0006635189911580896\n",
      "test loss is 0.0006140346572783249\n",
      "Batch: 31500,train loss is: 0.0004146757961647594\n",
      "test loss is 0.0006324911147798591\n",
      "Batch: 31600,train loss is: 0.0003603234933221693\n",
      "test loss is 0.0006213353507954109\n",
      "Batch: 31700,train loss is: 0.0004149068549070809\n",
      "test loss is 0.0006425175862526018\n",
      "Batch: 31800,train loss is: 0.0010826859769003084\n",
      "test loss is 0.0006208790740897121\n",
      "Batch: 31900,train loss is: 0.0006032412599602705\n",
      "test loss is 0.0006230312103477313\n",
      "Batch: 32000,train loss is: 0.0032378766814554214\n",
      "test loss is 0.0006302385740465956\n",
      "Batch: 32100,train loss is: 0.0002788460704173705\n",
      "test loss is 0.0006647677927818916\n",
      "Batch: 32200,train loss is: 0.0002802950147647036\n",
      "test loss is 0.0006182076495647435\n",
      "Batch: 32300,train loss is: 0.0003325496418717354\n",
      "test loss is 0.0006117391798013462\n",
      "Batch: 32400,train loss is: 0.00033078310784593305\n",
      "test loss is 0.0006082884445799786\n",
      "Batch: 32500,train loss is: 0.0007831340040268436\n",
      "test loss is 0.0006450406148513945\n",
      "Batch: 32600,train loss is: 0.00046453462853634895\n",
      "test loss is 0.0006340529450939006\n",
      "Batch: 32700,train loss is: 0.0004712152845823483\n",
      "test loss is 0.0006654896916586413\n",
      "Batch: 32800,train loss is: 0.0004959204620434295\n",
      "test loss is 0.0006572959747362456\n",
      "Batch: 32900,train loss is: 0.0006070434990648131\n",
      "test loss is 0.0006320610354277819\n",
      "Batch: 33000,train loss is: 0.00036248558779273073\n",
      "test loss is 0.0006229015867771705\n",
      "Batch: 33100,train loss is: 0.0003859358476587522\n",
      "test loss is 0.0006246541679644854\n",
      "Batch: 33200,train loss is: 0.00042575066525905983\n",
      "test loss is 0.0006897079467315006\n",
      "Batch: 33300,train loss is: 0.0005771236518149948\n",
      "test loss is 0.0006231191524222479\n",
      "Batch: 33400,train loss is: 0.0004186875779760728\n",
      "test loss is 0.0006221617970609188\n",
      "Batch: 33500,train loss is: 0.0003124548739515731\n",
      "test loss is 0.0006281950793979145\n",
      "Batch: 33600,train loss is: 0.00048086757887593574\n",
      "test loss is 0.0006155201301686939\n",
      "Batch: 33700,train loss is: 0.0004690602712218217\n",
      "test loss is 0.0006367318565142201\n",
      "Batch: 33800,train loss is: 0.0006572036149200981\n",
      "test loss is 0.0006202886816914205\n",
      "Batch: 33900,train loss is: 0.0005456549801925624\n",
      "test loss is 0.000611617930780669\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for param_group in optim_Adam.param_groups:\n",
    "    param_group['lr'] = 0.0001\n",
    "\n",
    "train_model(loss_MSE,optim_Adam,model,data_loader,train_data,test_data,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'../restruct_data_results/rflatBergomi_pointwise88_4-layer.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "initial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
