{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "f = gzip.GzipFile(r\"../../Data/rBergomiTrainSet.txt.gz\", \"r\")\n",
    "dat=np.load(f)\n",
    "xx=dat[:,:4]\n",
    "yy=dat[:,4:]\n",
    "strikes=np.array([0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4,1.5 ])\n",
    "maturities=np.array([0.1,0.3,0.6,0.9,1.2,1.5,1.8,2.0 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    xx, yy, test_size=0.15, random_state=42)\n",
    "\n",
    "def append_and_expand(a,x,y):\n",
    "    # use choose and where !\n",
    "    n = len(x)*len(y)\n",
    "    a_index = np.arange(len(a))%n\n",
    "    \n",
    "    \n",
    "    x_index = a_index//len(y)\n",
    "    y_index = a_index%len(y)\n",
    "    \n",
    "    x_added = np.choose(x_index,x.reshape(-1,1)).reshape(-1,1)\n",
    "    y_added = np.choose(y_index,y.reshape(-1,1)).reshape(-1,1)\n",
    "    \n",
    "    return np.hstack([a,x_added,y_added])\n",
    "\n",
    "y_train,y_test = y_train.reshape(-1,8,11),y_test.reshape(-1,8,11)\n",
    "x_train,x_test = np.repeat(x_train, 8*11,axis=0),np.repeat(x_test, 8*11,axis=0)\n",
    "\n",
    "x_train,x_test=append_and_expand(x_train,maturities,strikes),append_and_expand(x_test,maturities,strikes)\n",
    "y_train,y_test = y_train.reshape(-1).reshape(-1,1), y_test.reshape(-1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "scale_y=  StandardScaler()\n",
    "\n",
    "def ytransform(y_train,y_test):\n",
    "    return [scale_y.fit_transform(y_train),scale_y.transform(y_test)]\n",
    "\n",
    "def yinversetransform(y):\n",
    "    return scale_y.inverse_transform(y)\n",
    "\n",
    "# Upper and lower bounds used in the training set\n",
    "ub=np.array([0.16,4,-0.1,0.5,2.0,1.5])\n",
    "lb=np.array([0.01,0.3,-0.95,0.025,0.1,0.5])\n",
    "\n",
    "def myscale(x):\n",
    "    return (x - (ub+lb)*0.5)*2/(ub-lb)\n",
    "def myinverse(x):\n",
    "    return x*(ub-lb)*0.5+(ub+lb)*0.5\n",
    "\n",
    "x_train_transform = myscale(x_train)\n",
    "x_test_transform = myscale(x_test)\n",
    "[y_train_transform,y_test_transform] = ytransform(y_train,y_test)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"device is {device}\")\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(x_train_transform).to(device=device),\n",
    "                                               torch.from_numpy(y_train_transform).to(device=device))\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.from_numpy(x_test_transform).to(device=device),\n",
    "                                              torch.from_numpy(y_test_transform).to(device=device))\n",
    "\n",
    "\n",
    "train_data = (torch.from_numpy(x_train_transform).to(device=device),torch.from_numpy(y_train_transform).to(device=device))\n",
    "test_data = (torch.from_numpy(x_test_transform).to(device=device),torch.from_numpy(y_test_transform).to(device=device))\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(train_dataset,batch_size =32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')  # Add the parent directory to the Python path\n",
    "\n",
    "from torch_NN.nn import KAN\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# hyperparas = {'input_dim':6,'hidden_dim':32,'hidden_nums':2,'output_dim':1,'block_layer_nums':2}\n",
    "\n",
    "layers_list_for_KAN = [x_train.shape[-1],32,1]\n",
    "\n",
    "model = KAN(layers_hidden=layers_list_for_KAN).to(device=device,dtype=torch.float64)\n",
    "loss_MSE = nn.MSELoss()\n",
    "optim_Adam = torch.optim.Adam(model.parameters(),lr= 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Epoch: 1----------------------------------\n",
      "Batch: 0%, train loss is: 0.6596936951828097\n",
      "Batch: 1%, train loss is: 0.011802204690760317\n",
      "Batch: 2%, train loss is: 0.0042651602446630195\n",
      "Batch: 3%, train loss is: 0.005274703318624546\n",
      "Batch: 4%, train loss is: 0.004619155679744886\n",
      "Batch: 5%, train loss is: 0.0016648206648426497\n",
      "Batch: 6%, train loss is: 0.0007955268166093207\n",
      "Batch: 7%, train loss is: 0.0010250921326635447\n",
      "Batch: 8%, train loss is: 0.0011711440730029056\n",
      "Batch: 9%, train loss is: 0.0017959549103175052\n",
      "Batch: 10%, train loss is: 0.0009429514919153238\n",
      "Batch: 11%, train loss is: 0.0011772051014358482\n",
      "Batch: 12%, train loss is: 0.0011066854163776319\n",
      "Batch: 13%, train loss is: 0.0007214779968843658\n",
      "Batch: 14%, train loss is: 0.0006821473209847559\n",
      "Batch: 15%, train loss is: 0.0009320434284458842\n",
      "Batch: 16%, train loss is: 0.0005209919907180281\n",
      "Batch: 17%, train loss is: 0.0004279553306613944\n",
      "Batch: 18%, train loss is: 0.0006699368598777972\n",
      "Batch: 19%, train loss is: 0.0008596890530093728\n",
      "Batch: 20%, train loss is: 0.0005246456195586288\n",
      "Batch: 21%, train loss is: 0.0006330719600775481\n",
      "Batch: 22%, train loss is: 0.00024904456010582145\n",
      "Batch: 23%, train loss is: 0.0005008971013128049\n",
      "Batch: 24%, train loss is: 0.001070989918095869\n",
      "Batch: 25%, train loss is: 0.0008406440162846534\n",
      "Batch: 26%, train loss is: 0.0005564301092863194\n",
      "Batch: 27%, train loss is: 0.0007483608547267634\n",
      "Batch: 28%, train loss is: 0.0010843001347189145\n",
      "Batch: 29%, train loss is: 0.0005843963018503117\n",
      "Batch: 30%, train loss is: 0.0005484431130431892\n",
      "Batch: 31%, train loss is: 0.0005162580562688692\n",
      "Batch: 32%, train loss is: 0.00041806071417584964\n",
      "Batch: 33%, train loss is: 0.0006626426246354597\n",
      "Batch: 34%, train loss is: 0.0005665823840510858\n",
      "Batch: 35%, train loss is: 0.0005513516055887821\n",
      "Batch: 36%, train loss is: 0.0002960075436293562\n",
      "Batch: 37%, train loss is: 0.00041088325899541996\n",
      "Batch: 38%, train loss is: 0.0005509269631087905\n",
      "Batch: 39%, train loss is: 0.0005286334294985402\n",
      "Batch: 40%, train loss is: 0.0005136095123204847\n",
      "Batch: 41%, train loss is: 0.00038701619819429833\n",
      "Batch: 42%, train loss is: 0.000501523969334333\n",
      "Batch: 43%, train loss is: 0.00023547361353065815\n",
      "Batch: 44%, train loss is: 0.0007901277015770078\n",
      "Batch: 45%, train loss is: 0.0003552388920850711\n",
      "Batch: 46%, train loss is: 0.0006481729620712559\n",
      "Batch: 47%, train loss is: 0.0008846223048803099\n",
      "Batch: 48%, train loss is: 0.0004180920264757345\n",
      "Batch: 49%, train loss is: 0.00021066375840949567\n",
      "Batch: 50%, train loss is: 0.00021820326584173354\n",
      "Batch: 51%, train loss is: 0.0013323943691572928\n",
      "Batch: 52%, train loss is: 0.0006016716913023726\n",
      "Batch: 53%, train loss is: 0.0004798826108581488\n",
      "Batch: 54%, train loss is: 0.00034098753308183237\n",
      "Batch: 55%, train loss is: 0.0006801216327036574\n",
      "Batch: 56%, train loss is: 0.0003756765836925894\n",
      "Batch: 57%, train loss is: 0.0004013646571097896\n",
      "Batch: 58%, train loss is: 0.00044541455142568895\n",
      "Batch: 59%, train loss is: 0.0004510770935972611\n",
      "Batch: 60%, train loss is: 0.0005635548999412465\n",
      "Batch: 61%, train loss is: 0.0008279225174925494\n",
      "Batch: 62%, train loss is: 0.0002499509361372291\n",
      "Batch: 63%, train loss is: 0.0002730912698678083\n",
      "Batch: 64%, train loss is: 0.00035441793815371215\n",
      "Batch: 65%, train loss is: 0.0004348420427027145\n",
      "Batch: 66%, train loss is: 0.00034476856221948447\n",
      "Batch: 67%, train loss is: 0.00042233755106427473\n",
      "Batch: 68%, train loss is: 0.0006375204622125284\n",
      "Batch: 69%, train loss is: 0.0003333156640915035\n",
      "Batch: 70%, train loss is: 0.00014370596775784318\n",
      "Batch: 71%, train loss is: 0.0004973048745939108\n",
      "Batch: 72%, train loss is: 0.0006927500421067833\n",
      "Batch: 73%, train loss is: 0.0002512783258140205\n",
      "Batch: 74%, train loss is: 0.0005346431232930902\n",
      "Batch: 75%, train loss is: 0.0008970064329510595\n",
      "Batch: 76%, train loss is: 0.000282229878436424\n",
      "Batch: 77%, train loss is: 0.00037247459787110345\n",
      "Batch: 78%, train loss is: 0.00021136692346135247\n",
      "Batch: 79%, train loss is: 0.0006379875531209789\n",
      "Batch: 80%, train loss is: 0.0002339090363390007\n",
      "Batch: 81%, train loss is: 0.00035365858835909237\n",
      "Batch: 82%, train loss is: 0.00017364233891926223\n",
      "Batch: 83%, train loss is: 0.00013440427992825798\n",
      "Batch: 84%, train loss is: 0.00034306613456228167\n",
      "Batch: 85%, train loss is: 0.0007069875331455523\n",
      "Batch: 86%, train loss is: 0.00024557251171936415\n",
      "Batch: 87%, train loss is: 0.0002979875023009144\n",
      "Batch: 88%, train loss is: 0.0001211444409862963\n",
      "Batch: 89%, train loss is: 0.0002535081227769772\n",
      "Batch: 90%, train loss is: 0.0003362579861800307\n",
      "Batch: 91%, train loss is: 0.0001175878847338782\n",
      "Batch: 92%, train loss is: 0.0006318265169833083\n",
      "Batch: 93%, train loss is: 0.00040361925526050433\n",
      "Batch: 94%, train loss is: 0.0002355744654403248\n",
      "Batch: 95%, train loss is: 0.0003398618829589309\n",
      "Batch: 96%, train loss is: 0.0004810801697739371\n",
      "Batch: 97%, train loss is: 0.0008456036592469354\n",
      "Batch: 98%, train loss is: 0.0002716234091188365\n",
      "Batch: 99%, train loss is: 0.00046308063630717285\n",
      "-----------------------Epoch: 2----------------------------------\n",
      "Batch: 0%, train loss is: 0.0008779727988332725\n",
      "Batch: 1%, train loss is: 0.00041983498576384133\n",
      "Batch: 2%, train loss is: 0.0010333692448792804\n",
      "Batch: 3%, train loss is: 0.00023249576186683485\n",
      "Batch: 4%, train loss is: 0.0003849069493903648\n",
      "Batch: 5%, train loss is: 0.0002797852527162565\n",
      "Batch: 6%, train loss is: 0.0005737570484983343\n",
      "Batch: 7%, train loss is: 0.0003306140925904834\n",
      "Batch: 8%, train loss is: 0.0005438424718457666\n",
      "Batch: 9%, train loss is: 0.00017368031492406356\n",
      "Batch: 10%, train loss is: 0.00034324821162136647\n",
      "Batch: 11%, train loss is: 0.0004517147055486849\n",
      "Batch: 12%, train loss is: 0.0003483638540933745\n",
      "Batch: 13%, train loss is: 0.00019311072774720416\n",
      "Batch: 14%, train loss is: 0.00035625304332889414\n",
      "Batch: 15%, train loss is: 0.00023090450785777784\n",
      "Batch: 16%, train loss is: 0.0004293732146864708\n",
      "Batch: 17%, train loss is: 0.008340072021674437\n",
      "Batch: 18%, train loss is: 0.0003067923537547338\n",
      "Batch: 19%, train loss is: 0.00020629372652691412\n",
      "Batch: 20%, train loss is: 0.00015212751168117286\n",
      "Batch: 21%, train loss is: 0.00028749376565217967\n",
      "Batch: 22%, train loss is: 0.00017480612844652664\n",
      "Batch: 23%, train loss is: 0.0010175821663823388\n",
      "Batch: 24%, train loss is: 0.005253000983646093\n",
      "Batch: 25%, train loss is: 0.0005354476920222098\n",
      "Batch: 26%, train loss is: 0.00023515395592229975\n",
      "Batch: 27%, train loss is: 0.00018934284176421294\n",
      "Batch: 28%, train loss is: 0.0005359600310497115\n",
      "Batch: 29%, train loss is: 0.0003493788544059699\n",
      "Batch: 30%, train loss is: 0.0004408045738947959\n",
      "Batch: 31%, train loss is: 0.000338259441508252\n",
      "Batch: 32%, train loss is: 0.00026964713813810054\n",
      "Batch: 33%, train loss is: 0.0005251193032112847\n",
      "Batch: 34%, train loss is: 0.0005369803750875924\n",
      "Batch: 35%, train loss is: 0.00019018845493278577\n",
      "Batch: 36%, train loss is: 0.0009702580337419319\n",
      "Batch: 37%, train loss is: 0.0003955716412293035\n",
      "Batch: 38%, train loss is: 0.0003732229864762752\n",
      "Batch: 39%, train loss is: 0.000312399333800453\n",
      "Batch: 40%, train loss is: 0.0004231839183584885\n",
      "Batch: 41%, train loss is: 0.0002979920025926538\n",
      "Batch: 42%, train loss is: 0.00036032341475993783\n",
      "Batch: 43%, train loss is: 0.0005895842607724132\n",
      "Batch: 44%, train loss is: 0.0003213137103270988\n",
      "Batch: 45%, train loss is: 0.0003914056502412936\n",
      "Batch: 46%, train loss is: 0.0005875012310673092\n",
      "Batch: 47%, train loss is: 0.0003829088190431043\n",
      "Batch: 48%, train loss is: 0.00019030138411001228\n",
      "Batch: 49%, train loss is: 0.00026594825321947155\n",
      "Batch: 50%, train loss is: 0.0002797390907154188\n",
      "Batch: 51%, train loss is: 0.00013156268791683428\n",
      "Batch: 52%, train loss is: 0.0005804873660485831\n",
      "Batch: 53%, train loss is: 0.0003819888489329121\n",
      "Batch: 54%, train loss is: 8.896866726892049e-05\n",
      "Batch: 55%, train loss is: 0.0004464053053339243\n",
      "Batch: 56%, train loss is: 0.0007826852638035172\n",
      "Batch: 57%, train loss is: 0.00030739873622316074\n",
      "Batch: 58%, train loss is: 0.00026654843275565723\n",
      "Batch: 59%, train loss is: 0.00022958033974496267\n",
      "Batch: 60%, train loss is: 0.00040925161088021806\n",
      "Batch: 61%, train loss is: 0.0004271388588002157\n",
      "Batch: 62%, train loss is: 0.0004174873950622489\n",
      "Batch: 63%, train loss is: 0.0003068314595071873\n",
      "Batch: 64%, train loss is: 0.00025424811037411674\n",
      "Batch: 65%, train loss is: 0.00012777506830210787\n",
      "Batch: 66%, train loss is: 0.0005012369992778635\n",
      "Batch: 67%, train loss is: 0.0002619490203974144\n",
      "Batch: 68%, train loss is: 0.00023068044540633365\n",
      "Batch: 69%, train loss is: 0.002047503521089758\n",
      "Batch: 70%, train loss is: 0.0004000595327893288\n",
      "Batch: 71%, train loss is: 0.00018708743144881977\n",
      "Batch: 72%, train loss is: 0.00043634490680070437\n",
      "Batch: 73%, train loss is: 0.0005174540280208705\n",
      "Batch: 74%, train loss is: 0.0008494090327657957\n",
      "Batch: 75%, train loss is: 0.00035357544536630527\n",
      "Batch: 76%, train loss is: 0.00029406926693199695\n",
      "Batch: 77%, train loss is: 0.00030356512459263184\n",
      "Batch: 78%, train loss is: 0.00017872121526354507\n",
      "Batch: 79%, train loss is: 0.00035422191005831695\n",
      "Batch: 80%, train loss is: 0.00014543048785303775\n",
      "Batch: 81%, train loss is: 0.00022304408324489268\n",
      "Batch: 82%, train loss is: 0.0005291750376138327\n",
      "Batch: 83%, train loss is: 0.000348467220979236\n",
      "Batch: 84%, train loss is: 0.00040398682319868907\n",
      "Batch: 85%, train loss is: 0.00035014850393740027\n",
      "Batch: 86%, train loss is: 0.00020052767299490477\n",
      "Batch: 87%, train loss is: 0.0003068453719337885\n",
      "Batch: 88%, train loss is: 0.0003328793918743163\n",
      "Batch: 89%, train loss is: 0.0012042962701283764\n",
      "Batch: 90%, train loss is: 0.00048231309693365604\n",
      "Batch: 91%, train loss is: 0.00019536631497982208\n",
      "Batch: 92%, train loss is: 0.0002738117584181096\n",
      "Batch: 93%, train loss is: 0.0003655338840459455\n",
      "Batch: 94%, train loss is: 0.0001181858409367827\n",
      "Batch: 95%, train loss is: 0.00026978475322970703\n",
      "Batch: 96%, train loss is: 0.00027392254291287916\n",
      "Batch: 97%, train loss is: 0.00040500411570296006\n",
      "Batch: 98%, train loss is: 0.00028194090268991723\n",
      "Batch: 99%, train loss is: 0.0003890593599238391\n",
      "-----------------------Epoch: 3----------------------------------\n",
      "Batch: 0%, train loss is: 0.000276843053092682\n",
      "Batch: 1%, train loss is: 0.0002380581865180146\n",
      "Batch: 2%, train loss is: 0.00031353148775511877\n",
      "Batch: 3%, train loss is: 0.00015329279759126633\n",
      "Batch: 4%, train loss is: 0.00017066065736190614\n",
      "Batch: 5%, train loss is: 0.00020995276316902365\n",
      "Batch: 6%, train loss is: 0.00023269698917083778\n",
      "Batch: 7%, train loss is: 0.00031023956224199344\n",
      "Batch: 8%, train loss is: 0.0003043166928155704\n",
      "Batch: 9%, train loss is: 0.00018932302155680525\n",
      "Batch: 10%, train loss is: 0.00029179019438467627\n",
      "Batch: 11%, train loss is: 0.00035432162791127354\n",
      "Batch: 12%, train loss is: 0.00016602086786369097\n",
      "Batch: 13%, train loss is: 0.00022898475167990967\n",
      "Batch: 14%, train loss is: 0.0003970263655250965\n",
      "Batch: 15%, train loss is: 0.00026836646896057607\n",
      "Batch: 16%, train loss is: 0.00023360958409677607\n",
      "Batch: 17%, train loss is: 0.0005290272380659854\n",
      "Batch: 18%, train loss is: 9.803898300220619e-05\n",
      "Batch: 19%, train loss is: 0.00035938592683522894\n",
      "Batch: 20%, train loss is: 0.0002784210986653562\n",
      "Batch: 21%, train loss is: 0.00040405073154502723\n",
      "Batch: 22%, train loss is: 0.00028443929978798376\n",
      "Batch: 23%, train loss is: 0.0001889656262933623\n",
      "Batch: 24%, train loss is: 0.00032980642726869247\n",
      "Batch: 25%, train loss is: 0.0002761472144356985\n",
      "Batch: 26%, train loss is: 0.0004792953813534738\n",
      "Batch: 27%, train loss is: 0.00019177535100006103\n",
      "Batch: 28%, train loss is: 0.0002842862365436256\n",
      "Batch: 29%, train loss is: 0.00025172297890669714\n",
      "Batch: 30%, train loss is: 0.0002757455051402679\n",
      "Batch: 31%, train loss is: 0.0006431424819556611\n",
      "Batch: 32%, train loss is: 0.00044754945369466446\n",
      "Batch: 33%, train loss is: 0.0013574304282222007\n",
      "Batch: 34%, train loss is: 0.003961015172369631\n",
      "Batch: 35%, train loss is: 0.00020986582130661898\n",
      "Batch: 36%, train loss is: 0.00019642432415838525\n",
      "Batch: 37%, train loss is: 0.0003707719582406002\n",
      "Batch: 38%, train loss is: 0.00017307595450393495\n",
      "Batch: 39%, train loss is: 0.0002059506677707708\n",
      "Batch: 40%, train loss is: 0.0002770701968650978\n",
      "Batch: 41%, train loss is: 0.0004024213386104581\n",
      "Batch: 42%, train loss is: 0.0004320581164712616\n",
      "Batch: 43%, train loss is: 0.00036522870272012666\n",
      "Batch: 44%, train loss is: 0.00027389320617856585\n",
      "Batch: 45%, train loss is: 0.0003591248315765121\n",
      "Batch: 46%, train loss is: 0.00028818228389179646\n",
      "Batch: 47%, train loss is: 0.0003870476707266376\n",
      "Batch: 48%, train loss is: 0.0005951765399326552\n",
      "Batch: 49%, train loss is: 0.0004143435833850282\n",
      "Batch: 50%, train loss is: 0.0002448437002373982\n",
      "Batch: 51%, train loss is: 0.0002419486975261019\n",
      "Batch: 52%, train loss is: 0.00017056128791949435\n",
      "Batch: 53%, train loss is: 0.0034707475997720226\n",
      "Batch: 54%, train loss is: 0.0006681089142691458\n",
      "Batch: 55%, train loss is: 0.00014140259272689144\n",
      "Batch: 56%, train loss is: 0.0006546165102610048\n",
      "Batch: 57%, train loss is: 0.0002328329580261786\n",
      "Batch: 58%, train loss is: 0.00020420869025705564\n",
      "Batch: 59%, train loss is: 0.0005956372528277709\n",
      "Batch: 60%, train loss is: 0.00015607645047460377\n",
      "Batch: 61%, train loss is: 0.0005052873541505599\n",
      "Batch: 62%, train loss is: 0.0001411731037460981\n",
      "Batch: 63%, train loss is: 0.00011161981957984353\n",
      "Batch: 64%, train loss is: 0.0002461170966330641\n",
      "Batch: 65%, train loss is: 0.0003036816271909069\n",
      "Batch: 66%, train loss is: 0.000619795381658533\n",
      "Batch: 67%, train loss is: 0.00023671323353962833\n",
      "Batch: 68%, train loss is: 0.0001818550461351483\n",
      "Batch: 69%, train loss is: 0.00033847027366436233\n",
      "Batch: 70%, train loss is: 0.0005925493717997253\n",
      "Batch: 71%, train loss is: 0.00022924672806012457\n",
      "Batch: 72%, train loss is: 0.00023059089317068848\n",
      "Batch: 73%, train loss is: 0.00028793593387066376\n",
      "Batch: 74%, train loss is: 0.0003882734196757029\n",
      "Batch: 75%, train loss is: 0.00021326635014787672\n",
      "Batch: 76%, train loss is: 0.00014428522147448122\n",
      "Batch: 77%, train loss is: 0.00026845295559271506\n",
      "Batch: 78%, train loss is: 0.0003699264291392812\n",
      "Batch: 79%, train loss is: 0.00019428643476088165\n",
      "Batch: 80%, train loss is: 0.00019942361791440235\n",
      "Batch: 81%, train loss is: 0.00026324776264489644\n",
      "Batch: 82%, train loss is: 0.00022937770964830313\n",
      "Batch: 83%, train loss is: 0.0001464553252224297\n",
      "Batch: 84%, train loss is: 0.0003877601898185085\n",
      "Batch: 85%, train loss is: 0.00020616505719031004\n",
      "Batch: 86%, train loss is: 0.00025522940815097324\n",
      "Batch: 87%, train loss is: 0.0001513873343531195\n",
      "Batch: 88%, train loss is: 0.0007443560649194102\n",
      "Batch: 89%, train loss is: 0.002931294670595009\n",
      "Batch: 90%, train loss is: 0.00024212979889995948\n",
      "Batch: 91%, train loss is: 8.672545377726732e-05\n",
      "Batch: 92%, train loss is: 0.0001936908143033719\n",
      "Batch: 93%, train loss is: 0.0003283311196201526\n",
      "Batch: 94%, train loss is: 0.0003802802055428311\n",
      "Batch: 95%, train loss is: 0.00016678962475385914\n",
      "Batch: 96%, train loss is: 0.00045608459000449637\n",
      "Batch: 97%, train loss is: 0.00030542167880866754\n",
      "Batch: 98%, train loss is: 0.00030329105113327667\n",
      "Batch: 99%, train loss is: 0.0001201729144672891\n"
     ]
    }
   ],
   "source": [
    "from torch_NN.train import train_model\n",
    "\n",
    "train_model(loss_MSE,optim_Adam,model,data_loader,train_data,test_data,3,test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Epoch: 1----------------------------------\n",
      "Batch: 0%, train loss is: 0.00016478871484257616\n",
      "Batch: 1%, train loss is: 0.00018496078120853942\n",
      "Batch: 2%, train loss is: 0.00011160217828781228\n",
      "Batch: 3%, train loss is: 0.00014543215718357602\n",
      "Batch: 4%, train loss is: 0.00010805820341015608\n",
      "Batch: 5%, train loss is: 8.634524504346583e-05\n",
      "Batch: 6%, train loss is: 0.00012615718561986893\n",
      "Batch: 7%, train loss is: 0.0004957112561547954\n",
      "Batch: 8%, train loss is: 7.078870265320839e-05\n",
      "Batch: 9%, train loss is: 0.00012311680388680957\n",
      "Batch: 10%, train loss is: 0.0002547191653876898\n",
      "Batch: 11%, train loss is: 0.00029670150444178426\n",
      "Batch: 12%, train loss is: 9.329084571421873e-05\n",
      "Batch: 13%, train loss is: 0.0001222879176347357\n",
      "Batch: 14%, train loss is: 0.00010786899171185985\n",
      "Batch: 15%, train loss is: 8.241233606670295e-05\n",
      "Batch: 16%, train loss is: 0.0001617947973773173\n",
      "Batch: 17%, train loss is: 0.0002716651833331634\n",
      "Batch: 18%, train loss is: 6.661243300234794e-05\n",
      "Batch: 19%, train loss is: 0.00021960541483006052\n",
      "Batch: 20%, train loss is: 0.00014124138025925237\n",
      "Batch: 21%, train loss is: 8.243257638455848e-05\n",
      "Batch: 22%, train loss is: 0.00018494923402456026\n",
      "Batch: 23%, train loss is: 0.00019190024407379788\n",
      "Batch: 24%, train loss is: 0.00010696836792616911\n",
      "Batch: 25%, train loss is: 0.000347509059641955\n",
      "Batch: 26%, train loss is: 0.0001291832673630681\n",
      "Batch: 27%, train loss is: 0.0011557100894215408\n",
      "Batch: 28%, train loss is: 0.00015625372184681783\n",
      "Batch: 29%, train loss is: 0.00013520492053227913\n",
      "Batch: 30%, train loss is: 0.00026794859289844524\n",
      "Batch: 31%, train loss is: 0.00011276105229491749\n",
      "Batch: 32%, train loss is: 0.0003549139593908923\n",
      "Batch: 33%, train loss is: 0.00010419415947993848\n",
      "Batch: 34%, train loss is: 0.00011827031373916107\n",
      "Batch: 35%, train loss is: 5.356330376963216e-05\n",
      "Batch: 36%, train loss is: 0.0001258716381108692\n",
      "Batch: 37%, train loss is: 0.000299677393353442\n",
      "Batch: 38%, train loss is: 0.0001320356918360673\n",
      "Batch: 39%, train loss is: 0.00012609895938761644\n",
      "Batch: 40%, train loss is: 0.00015392242006814235\n",
      "Batch: 41%, train loss is: 0.00013050808031667474\n",
      "Batch: 42%, train loss is: 8.044154291354371e-05\n",
      "Batch: 43%, train loss is: 5.40762497067511e-05\n",
      "Batch: 44%, train loss is: 0.00018742224079627763\n",
      "Batch: 45%, train loss is: 8.131411179269735e-05\n",
      "Batch: 46%, train loss is: 0.00010103719319835739\n",
      "Batch: 47%, train loss is: 7.352341403735359e-05\n",
      "Batch: 48%, train loss is: 0.000855118983959064\n",
      "Batch: 49%, train loss is: 7.71603534250337e-05\n",
      "Batch: 50%, train loss is: 7.5961053173047e-05\n",
      "Batch: 51%, train loss is: 0.00021930266717506828\n",
      "Batch: 52%, train loss is: 9.435931208636566e-05\n",
      "Batch: 53%, train loss is: 0.0001509534432657622\n",
      "Batch: 54%, train loss is: 0.00020277851154245485\n",
      "Batch: 55%, train loss is: 0.00010498597293217726\n",
      "Batch: 56%, train loss is: 0.00035816808925028714\n",
      "Batch: 57%, train loss is: 0.00020740136717669027\n",
      "Batch: 58%, train loss is: 0.0014958498043764377\n",
      "Batch: 59%, train loss is: 0.0003139552900953957\n",
      "Batch: 60%, train loss is: 0.0001483771370328787\n",
      "Batch: 61%, train loss is: 9.759509067179027e-05\n",
      "Batch: 62%, train loss is: 0.0005408887580695694\n",
      "Batch: 63%, train loss is: 0.00012985252670330355\n",
      "Batch: 64%, train loss is: 0.00010806304362979078\n",
      "Batch: 65%, train loss is: 0.00019267432785627706\n",
      "Batch: 66%, train loss is: 0.00011236052429914386\n",
      "Batch: 67%, train loss is: 9.146813941877852e-05\n",
      "Batch: 68%, train loss is: 6.334684323112181e-05\n",
      "Batch: 69%, train loss is: 0.00017161959630057893\n",
      "Batch: 70%, train loss is: 8.623610357013046e-05\n",
      "Batch: 71%, train loss is: 0.00010316138225940029\n",
      "Batch: 72%, train loss is: 0.0005436145461600436\n",
      "Batch: 73%, train loss is: 9.230320239551265e-05\n",
      "Batch: 74%, train loss is: 9.310081834087159e-05\n",
      "Batch: 75%, train loss is: 0.00015284150010176983\n",
      "Batch: 76%, train loss is: 0.000109382467091304\n",
      "Batch: 77%, train loss is: 3.3783533368040684e-05\n",
      "Batch: 78%, train loss is: 0.00020003067499899872\n",
      "Batch: 79%, train loss is: 0.00018763976062905377\n",
      "Batch: 80%, train loss is: 0.00024207466949520607\n",
      "Batch: 81%, train loss is: 0.00013031822957336055\n",
      "Batch: 82%, train loss is: 0.0001592253152244707\n",
      "Batch: 83%, train loss is: 0.00027179654932645047\n",
      "Batch: 84%, train loss is: 8.192277204677049e-05\n",
      "Batch: 85%, train loss is: 0.00015713917677511388\n",
      "Batch: 86%, train loss is: 0.00019714345430481593\n",
      "Batch: 87%, train loss is: 0.00014389226418806404\n",
      "Batch: 88%, train loss is: 9.01927617538692e-05\n",
      "Batch: 89%, train loss is: 6.16066454251689e-05\n",
      "Batch: 90%, train loss is: 0.00013600899370029413\n",
      "Batch: 91%, train loss is: 6.844858273022786e-05\n",
      "Batch: 92%, train loss is: 0.0001009870195751424\n",
      "Batch: 93%, train loss is: 0.00011433679009691713\n",
      "Batch: 94%, train loss is: 0.00012741246807806722\n",
      "Batch: 95%, train loss is: 6.379349729364889e-05\n",
      "Batch: 96%, train loss is: 6.719709753438186e-05\n",
      "Batch: 97%, train loss is: 0.00024107191896174197\n",
      "Batch: 98%, train loss is: 0.000166490661473796\n",
      "Batch: 99%, train loss is: 0.0001879833401269523\n",
      "-----------------------Epoch: 2----------------------------------\n",
      "Batch: 0%, train loss is: 0.0001336005692711344\n",
      "Batch: 1%, train loss is: 8.00962632770674e-05\n",
      "Batch: 2%, train loss is: 0.0005444448994965962\n",
      "Batch: 3%, train loss is: 0.00016802363885362435\n",
      "Batch: 4%, train loss is: 0.00010888764083438193\n",
      "Batch: 5%, train loss is: 0.0003520547671636993\n",
      "Batch: 6%, train loss is: 0.00012375151738809644\n",
      "Batch: 7%, train loss is: 6.649025681621819e-05\n",
      "Batch: 8%, train loss is: 8.833567435344547e-05\n",
      "Batch: 9%, train loss is: 6.873213593784868e-05\n",
      "Batch: 10%, train loss is: 0.0001640479335096466\n",
      "Batch: 11%, train loss is: 0.00025757927014856765\n",
      "Batch: 12%, train loss is: 0.0001437683286442273\n",
      "Batch: 13%, train loss is: 3.3936185429958235e-05\n",
      "Batch: 14%, train loss is: 0.0005956086211004622\n",
      "Batch: 15%, train loss is: 0.00011542029340427835\n",
      "Batch: 16%, train loss is: 6.933178908338113e-05\n",
      "Batch: 17%, train loss is: 0.00013028083678616389\n",
      "Batch: 18%, train loss is: 0.00015395844780957576\n",
      "Batch: 19%, train loss is: 0.0003052358386262707\n",
      "Batch: 20%, train loss is: 0.00025422427061384643\n",
      "Batch: 21%, train loss is: 0.00011699950320780395\n",
      "Batch: 22%, train loss is: 0.0002168854732097948\n",
      "Batch: 23%, train loss is: 9.623416210857372e-05\n",
      "Batch: 24%, train loss is: 0.00014153257448034625\n",
      "Batch: 25%, train loss is: 6.657196606596886e-05\n",
      "Batch: 26%, train loss is: 6.272889933725933e-05\n",
      "Batch: 27%, train loss is: 0.00031885767392620664\n",
      "Batch: 28%, train loss is: 7.169725447309756e-05\n",
      "Batch: 29%, train loss is: 0.00021069075736545047\n",
      "Batch: 30%, train loss is: 0.00016117640634221852\n",
      "Batch: 31%, train loss is: 0.0001281425286401724\n",
      "Batch: 32%, train loss is: 0.00015654079549644732\n",
      "Batch: 33%, train loss is: 0.00021140795926921245\n",
      "Batch: 34%, train loss is: 0.000172953323028892\n",
      "Batch: 35%, train loss is: 0.00015216414516138225\n",
      "Batch: 36%, train loss is: 8.815320129696672e-05\n",
      "Batch: 37%, train loss is: 0.0002266493905968749\n",
      "Batch: 38%, train loss is: 0.0003201271594414471\n",
      "Batch: 39%, train loss is: 0.00015670794956185906\n",
      "Batch: 40%, train loss is: 9.142820149319562e-05\n",
      "Batch: 41%, train loss is: 0.00013773533112442014\n",
      "Batch: 42%, train loss is: 9.752621450723293e-05\n",
      "Batch: 43%, train loss is: 0.00011986976851248335\n",
      "Batch: 44%, train loss is: 0.00013001087321822376\n",
      "Batch: 45%, train loss is: 5.4772146839309073e-05\n",
      "Batch: 46%, train loss is: 0.00015781796663992352\n",
      "Batch: 47%, train loss is: 0.00014785385963273374\n",
      "Batch: 48%, train loss is: 7.495266393938833e-05\n",
      "Batch: 49%, train loss is: 9.083212060447266e-05\n",
      "Batch: 50%, train loss is: 9.17227842086739e-05\n",
      "Batch: 51%, train loss is: 0.0001361091733074736\n",
      "Batch: 52%, train loss is: 0.00014441668697812355\n",
      "Batch: 53%, train loss is: 0.00011527229932760357\n",
      "Batch: 54%, train loss is: 0.00017201786425258682\n",
      "Batch: 55%, train loss is: 0.000927032390134223\n",
      "Batch: 56%, train loss is: 0.00012382625366964584\n",
      "Batch: 57%, train loss is: 0.0001612733062999984\n",
      "Batch: 58%, train loss is: 0.00011443978239027195\n",
      "Batch: 59%, train loss is: 6.454718537719678e-05\n",
      "Batch: 60%, train loss is: 8.527570783700881e-05\n",
      "Batch: 61%, train loss is: 9.372547118607501e-05\n",
      "Batch: 62%, train loss is: 0.0001511844576493435\n",
      "Batch: 63%, train loss is: 0.00023924915913450028\n",
      "Batch: 64%, train loss is: 0.0001854463020849024\n",
      "Batch: 65%, train loss is: 0.00011709344829352437\n",
      "Batch: 66%, train loss is: 0.0001213940796016453\n",
      "Batch: 67%, train loss is: 0.00016416055015634585\n",
      "Batch: 68%, train loss is: 0.00013111288286965825\n",
      "Batch: 69%, train loss is: 0.00031372196186558603\n",
      "Batch: 70%, train loss is: 0.00012730411669913686\n",
      "Batch: 71%, train loss is: 0.00011195291355399253\n",
      "Batch: 72%, train loss is: 0.00014035133510862662\n",
      "Batch: 73%, train loss is: 8.078972740050345e-05\n",
      "Batch: 74%, train loss is: 0.00010403640279335136\n",
      "Batch: 75%, train loss is: 7.191105302071868e-05\n",
      "Batch: 76%, train loss is: 0.0001344630406147223\n",
      "Batch: 77%, train loss is: 8.322058643375673e-05\n",
      "Batch: 78%, train loss is: 0.0002695144830984673\n",
      "Batch: 79%, train loss is: 0.00010209408639616082\n",
      "Batch: 80%, train loss is: 0.0001395998353644204\n",
      "Batch: 81%, train loss is: 0.0001261794557391416\n",
      "Batch: 82%, train loss is: 0.00017332018974326431\n",
      "Batch: 83%, train loss is: 0.00010343578252269587\n",
      "Batch: 84%, train loss is: 0.0003044897514081707\n",
      "Batch: 85%, train loss is: 0.0005260300468369037\n",
      "Batch: 86%, train loss is: 0.0001240824217220964\n",
      "Batch: 87%, train loss is: 0.00020538086220093523\n",
      "Batch: 88%, train loss is: 6.635923852847076e-05\n",
      "Batch: 89%, train loss is: 0.0003177252329202374\n",
      "Batch: 90%, train loss is: 7.472391049461095e-05\n",
      "Batch: 91%, train loss is: 0.00011431134707399621\n",
      "Batch: 92%, train loss is: 8.264202581683569e-05\n",
      "Batch: 93%, train loss is: 9.245616178133449e-05\n",
      "Batch: 94%, train loss is: 0.0001134720162202526\n",
      "Batch: 95%, train loss is: 0.0003920650481133306\n",
      "Batch: 96%, train loss is: 0.00014777903978305403\n",
      "Batch: 97%, train loss is: 0.00012421608305935954\n",
      "Batch: 98%, train loss is: 9.557637339397299e-05\n",
      "Batch: 99%, train loss is: 0.00018800259803116037\n",
      "-----------------------Epoch: 3----------------------------------\n",
      "Batch: 0%, train loss is: 0.0003006189222646639\n",
      "Batch: 1%, train loss is: 6.318739341778959e-05\n",
      "Batch: 2%, train loss is: 6.662617076342104e-05\n",
      "Batch: 3%, train loss is: 0.00030769601335627826\n",
      "Batch: 4%, train loss is: 9.078113269758888e-05\n",
      "Batch: 5%, train loss is: 8.054442299940771e-05\n",
      "Batch: 6%, train loss is: 0.00010942495739101499\n",
      "Batch: 7%, train loss is: 9.528487464928704e-05\n",
      "Batch: 8%, train loss is: 0.00014316110714358818\n",
      "Batch: 9%, train loss is: 0.0007727930115878161\n",
      "Batch: 10%, train loss is: 0.00023390691251836584\n",
      "Batch: 11%, train loss is: 0.0002628471089533007\n",
      "Batch: 12%, train loss is: 9.773181081785413e-05\n",
      "Batch: 13%, train loss is: 9.73669026325264e-05\n",
      "Batch: 14%, train loss is: 9.108488287583807e-05\n",
      "Batch: 15%, train loss is: 0.0002370281299394101\n",
      "Batch: 16%, train loss is: 0.00016384262035066893\n",
      "Batch: 17%, train loss is: 0.00010585621131234662\n",
      "Batch: 18%, train loss is: 6.19280679751636e-05\n",
      "Batch: 19%, train loss is: 7.663956909320962e-05\n",
      "Batch: 20%, train loss is: 9.75153267893817e-05\n",
      "Batch: 21%, train loss is: 0.00013077900849868738\n",
      "Batch: 22%, train loss is: 8.924640358291339e-05\n",
      "Batch: 23%, train loss is: 0.00016061856753822173\n",
      "Batch: 24%, train loss is: 0.00036452271735131874\n",
      "Batch: 25%, train loss is: 5.6094181411959004e-05\n",
      "Batch: 26%, train loss is: 0.00023938767335046282\n",
      "Batch: 27%, train loss is: 0.00011896969099859552\n",
      "Batch: 28%, train loss is: 6.390378314428953e-05\n",
      "Batch: 29%, train loss is: 0.00014821418608438865\n",
      "Batch: 30%, train loss is: 0.003332080288992137\n",
      "Batch: 31%, train loss is: 0.00021267862432521517\n",
      "Batch: 32%, train loss is: 0.00011499768364996026\n",
      "Batch: 33%, train loss is: 0.0003178421112598174\n",
      "Batch: 34%, train loss is: 8.94885342178422e-05\n",
      "Batch: 35%, train loss is: 0.000100703878910766\n",
      "Batch: 36%, train loss is: 0.00014563367257029234\n",
      "Batch: 37%, train loss is: 0.0007777772634321742\n",
      "Batch: 38%, train loss is: 0.0001505931083687791\n",
      "Batch: 39%, train loss is: 9.006853406048768e-05\n",
      "Batch: 40%, train loss is: 0.0001425659497454779\n",
      "Batch: 41%, train loss is: 0.00011509977323326303\n",
      "Batch: 42%, train loss is: 0.00013719794630234178\n",
      "Batch: 43%, train loss is: 0.0001527371150351121\n",
      "Batch: 44%, train loss is: 0.00017459191246420387\n",
      "Batch: 45%, train loss is: 0.00016987533683278106\n",
      "Batch: 46%, train loss is: 0.00020760567839354238\n",
      "Batch: 47%, train loss is: 0.00022675899498443794\n",
      "Batch: 48%, train loss is: 8.042900329500088e-05\n",
      "Batch: 49%, train loss is: 0.0001176548520586512\n",
      "Batch: 50%, train loss is: 0.0003668898458974915\n",
      "Batch: 51%, train loss is: 0.00015634578325250887\n",
      "Batch: 52%, train loss is: 0.00015488039305476737\n",
      "Batch: 53%, train loss is: 0.00027668427890712886\n",
      "Batch: 54%, train loss is: 3.289499207612805e-05\n",
      "Batch: 55%, train loss is: 9.429815592390184e-05\n",
      "Batch: 56%, train loss is: 0.00022534954764886624\n",
      "Batch: 57%, train loss is: 0.00010741953062857866\n",
      "Batch: 58%, train loss is: 9.565152491794094e-05\n",
      "Batch: 59%, train loss is: 8.561367288837441e-05\n",
      "Batch: 60%, train loss is: 7.418501310029027e-05\n",
      "Batch: 61%, train loss is: 0.00017159902499175162\n",
      "Batch: 62%, train loss is: 5.575631189192835e-05\n",
      "Batch: 63%, train loss is: 0.00023761783716877362\n",
      "Batch: 64%, train loss is: 0.0008653348938238176\n",
      "Batch: 65%, train loss is: 0.0013872218013923515\n",
      "Batch: 66%, train loss is: 9.10760099306512e-05\n",
      "Batch: 67%, train loss is: 0.00018461051911714598\n",
      "Batch: 68%, train loss is: 6.803840128989955e-05\n",
      "Batch: 69%, train loss is: 0.00029868024073855524\n",
      "Batch: 70%, train loss is: 0.00012563711724493273\n",
      "Batch: 71%, train loss is: 0.00033881189594663395\n",
      "Batch: 72%, train loss is: 0.00010036352326436076\n",
      "Batch: 73%, train loss is: 7.145806390605256e-05\n",
      "Batch: 74%, train loss is: 8.092081917633407e-05\n",
      "Batch: 75%, train loss is: 7.830725135180971e-05\n",
      "Batch: 76%, train loss is: 0.00014657463435383144\n",
      "Batch: 77%, train loss is: 0.00048087821388315485\n",
      "Batch: 78%, train loss is: 0.00022964398584060592\n",
      "Batch: 79%, train loss is: 0.00012534925686445917\n",
      "Batch: 80%, train loss is: 0.00010898603643405445\n",
      "Batch: 81%, train loss is: 9.845653918349938e-05\n",
      "Batch: 82%, train loss is: 0.00022464061319155303\n",
      "Batch: 83%, train loss is: 0.0008753627866650957\n",
      "Batch: 84%, train loss is: 0.00017605831641766522\n",
      "Batch: 85%, train loss is: 0.001094016911708708\n",
      "Batch: 86%, train loss is: 0.0001321841583828795\n",
      "Batch: 87%, train loss is: 0.00026699301713070927\n",
      "Batch: 88%, train loss is: 0.0008260386902075886\n",
      "Batch: 89%, train loss is: 0.00012069230934761354\n",
      "Batch: 90%, train loss is: 0.0001534236234092369\n",
      "Batch: 91%, train loss is: 4.897364476318774e-05\n",
      "Batch: 92%, train loss is: 4.501946419311592e-05\n",
      "Batch: 93%, train loss is: 9.282804130974389e-05\n",
      "Batch: 94%, train loss is: 7.138780901932031e-05\n",
      "Batch: 95%, train loss is: 4.572682429880353e-05\n",
      "Batch: 96%, train loss is: 0.00013298896729301853\n",
      "Batch: 97%, train loss is: 0.00011704999350295819\n",
      "Batch: 98%, train loss is: 0.0001496257364968697\n",
      "Batch: 99%, train loss is: 0.000506543258219855\n",
      "-----------------------Epoch: 4----------------------------------\n",
      "Batch: 0%, train loss is: 0.00010021778370383721\n",
      "Batch: 1%, train loss is: 0.0001529940441596574\n",
      "Batch: 2%, train loss is: 0.00040567065163495217\n",
      "Batch: 3%, train loss is: 0.0001151948269389796\n",
      "Batch: 4%, train loss is: 0.00014744822231516454\n",
      "Batch: 5%, train loss is: 0.00026369769818573353\n",
      "Batch: 6%, train loss is: 0.00029646923862435297\n",
      "Batch: 7%, train loss is: 0.00011623091511883056\n",
      "Batch: 8%, train loss is: 0.000707735577129785\n",
      "Batch: 9%, train loss is: 7.436076427739892e-05\n",
      "Batch: 10%, train loss is: 8.952589435320146e-05\n",
      "Batch: 11%, train loss is: 0.00012418350635872496\n",
      "Batch: 12%, train loss is: 6.933914993485023e-05\n",
      "Batch: 13%, train loss is: 0.00011736583816985183\n",
      "Batch: 14%, train loss is: 0.00016957241335577452\n",
      "Batch: 15%, train loss is: 0.00020876744349166486\n",
      "Batch: 16%, train loss is: 6.326908233755406e-05\n",
      "Batch: 17%, train loss is: 0.0001704042527209856\n",
      "Batch: 18%, train loss is: 0.000141652853878046\n",
      "Batch: 19%, train loss is: 6.0933716952284136e-05\n",
      "Batch: 20%, train loss is: 0.000282032237515924\n",
      "Batch: 21%, train loss is: 0.00060976810434229\n",
      "Batch: 22%, train loss is: 5.641490690337585e-05\n",
      "Batch: 23%, train loss is: 0.0003470376315312446\n",
      "Batch: 24%, train loss is: 0.00024187184648009528\n",
      "Batch: 25%, train loss is: 6.519270210921888e-05\n",
      "Batch: 26%, train loss is: 8.73649682587398e-05\n",
      "Batch: 27%, train loss is: 6.479331009794938e-05\n",
      "Batch: 28%, train loss is: 0.00024025420566534613\n",
      "Batch: 29%, train loss is: 0.00024072404263369052\n",
      "Batch: 30%, train loss is: 0.00020501671956132549\n",
      "Batch: 31%, train loss is: 0.0002520257148310315\n",
      "Batch: 32%, train loss is: 9.173262456117163e-05\n",
      "Batch: 33%, train loss is: 9.25342782654818e-05\n",
      "Batch: 34%, train loss is: 0.0002570753713537927\n",
      "Batch: 35%, train loss is: 8.854769085253668e-05\n",
      "Batch: 36%, train loss is: 7.733711359767385e-05\n",
      "Batch: 37%, train loss is: 9.003031708764515e-05\n",
      "Batch: 38%, train loss is: 0.0001507238813821535\n",
      "Batch: 39%, train loss is: 0.0008926278793560907\n",
      "Batch: 40%, train loss is: 8.342130528840732e-05\n",
      "Batch: 41%, train loss is: 0.0004294810526947649\n",
      "Batch: 42%, train loss is: 9.484171645037243e-05\n",
      "Batch: 43%, train loss is: 0.00013305559085710785\n",
      "Batch: 44%, train loss is: 0.0002368618698476988\n",
      "Batch: 45%, train loss is: 0.0004022537067349141\n",
      "Batch: 46%, train loss is: 0.00011386450816172416\n",
      "Batch: 47%, train loss is: 9.054616465786542e-05\n",
      "Batch: 48%, train loss is: 0.0001600401008184627\n",
      "Batch: 49%, train loss is: 0.00011035627805611344\n",
      "Batch: 50%, train loss is: 0.00046543211853436597\n",
      "Batch: 51%, train loss is: 9.70904428783533e-05\n",
      "Batch: 52%, train loss is: 0.000296146749172788\n",
      "Batch: 53%, train loss is: 0.0001086113328561199\n",
      "Batch: 54%, train loss is: 0.00010159746570459155\n",
      "Batch: 55%, train loss is: 0.0003102386997040368\n",
      "Batch: 56%, train loss is: 0.0002243255988182686\n",
      "Batch: 57%, train loss is: 0.00010307545363242099\n",
      "Batch: 58%, train loss is: 5.215686391222769e-05\n",
      "Batch: 59%, train loss is: 0.00022667427785898198\n",
      "Batch: 60%, train loss is: 0.0001795474738765725\n",
      "Batch: 61%, train loss is: 0.00029133064590078524\n",
      "Batch: 62%, train loss is: 0.00014607265053670742\n",
      "Batch: 63%, train loss is: 0.00021261736719780798\n",
      "Batch: 64%, train loss is: 0.00011576274561095153\n",
      "Batch: 65%, train loss is: 0.00012154670328167962\n",
      "Batch: 66%, train loss is: 0.0004367063836419593\n",
      "Batch: 67%, train loss is: 8.651702662291084e-05\n",
      "Batch: 68%, train loss is: 0.0001235566118656162\n",
      "Batch: 69%, train loss is: 0.00011363469385413546\n",
      "Batch: 70%, train loss is: 9.77462382374325e-05\n",
      "Batch: 71%, train loss is: 0.00017647365269240172\n",
      "Batch: 72%, train loss is: 0.00013959663520856654\n",
      "Batch: 73%, train loss is: 0.0001213761005169313\n",
      "Batch: 74%, train loss is: 8.663795596753926e-05\n",
      "Batch: 75%, train loss is: 0.00012146737465569614\n",
      "Batch: 76%, train loss is: 0.00017098852090802012\n",
      "Batch: 77%, train loss is: 0.00012192526323801469\n",
      "Batch: 78%, train loss is: 0.00015563597980801147\n",
      "Batch: 79%, train loss is: 0.00012016305835650147\n",
      "Batch: 80%, train loss is: 0.00015220352580306613\n",
      "Batch: 81%, train loss is: 8.108356500929621e-05\n",
      "Batch: 82%, train loss is: 0.00011957335918357918\n",
      "Batch: 83%, train loss is: 0.00015781797679663775\n",
      "Batch: 84%, train loss is: 7.999970031606691e-05\n",
      "Batch: 85%, train loss is: 0.00011459896644580674\n",
      "Batch: 86%, train loss is: 0.00013742236490724976\n",
      "Batch: 87%, train loss is: 0.00012138086760806079\n",
      "Batch: 88%, train loss is: 6.721111629955689e-05\n",
      "Batch: 89%, train loss is: 8.711822997975086e-05\n",
      "Batch: 90%, train loss is: 0.00029641907225717347\n",
      "Batch: 91%, train loss is: 6.298003864893426e-05\n",
      "Batch: 92%, train loss is: 0.00022845346869696198\n",
      "Batch: 93%, train loss is: 7.658160712627673e-05\n",
      "Batch: 94%, train loss is: 0.00030768399484914095\n",
      "Batch: 95%, train loss is: 0.00014231272832564905\n",
      "Batch: 96%, train loss is: 6.532918124039282e-05\n",
      "Batch: 97%, train loss is: 0.00012707309752374576\n",
      "Batch: 98%, train loss is: 0.00018631651578493693\n",
      "Batch: 99%, train loss is: 0.00014983009484604236\n",
      "-----------------------Epoch: 5----------------------------------\n",
      "Batch: 0%, train loss is: 0.0001336372701247837\n",
      "Batch: 1%, train loss is: 6.495013567265831e-05\n",
      "Batch: 2%, train loss is: 0.0001331045088370595\n",
      "Batch: 3%, train loss is: 0.00033237806039761967\n",
      "Batch: 4%, train loss is: 6.128246975747467e-05\n",
      "Batch: 5%, train loss is: 0.0002297137911572205\n",
      "Batch: 6%, train loss is: 0.00014970198748748671\n",
      "Batch: 7%, train loss is: 0.0001319168430996915\n",
      "Batch: 8%, train loss is: 0.0003554547112363728\n",
      "Batch: 9%, train loss is: 0.00010258479768881494\n",
      "Batch: 10%, train loss is: 0.0001423353642003197\n",
      "Batch: 11%, train loss is: 0.0005593249911767541\n",
      "Batch: 12%, train loss is: 9.433732101967581e-05\n",
      "Batch: 13%, train loss is: 7.643592233050252e-05\n",
      "Batch: 14%, train loss is: 0.0002084091026559193\n",
      "Batch: 15%, train loss is: 0.0002368769712399613\n",
      "Batch: 16%, train loss is: 6.917723976316244e-05\n",
      "Batch: 17%, train loss is: 6.424345937001756e-05\n",
      "Batch: 18%, train loss is: 0.00012813217350720067\n",
      "Batch: 19%, train loss is: 0.00011968778396180798\n",
      "Batch: 20%, train loss is: 9.119898966862657e-05\n",
      "Batch: 21%, train loss is: 0.0001428227077730707\n",
      "Batch: 22%, train loss is: 0.00010419783444594772\n",
      "Batch: 23%, train loss is: 0.00025979197166067075\n",
      "Batch: 24%, train loss is: 6.907600253255927e-05\n",
      "Batch: 25%, train loss is: 0.000167192254623274\n",
      "Batch: 26%, train loss is: 0.0002801141985669886\n",
      "Batch: 27%, train loss is: 0.00018183406409686954\n",
      "Batch: 28%, train loss is: 0.00010393345467789613\n",
      "Batch: 29%, train loss is: 0.00013226579852082396\n",
      "Batch: 30%, train loss is: 0.0003067982736942148\n",
      "Batch: 31%, train loss is: 0.0001259671155697902\n",
      "Batch: 32%, train loss is: 8.388457833865976e-05\n",
      "Batch: 33%, train loss is: 6.236306364864442e-05\n",
      "Batch: 34%, train loss is: 4.192906830756734e-05\n",
      "Batch: 35%, train loss is: 0.0001531918209650437\n",
      "Batch: 36%, train loss is: 0.00012773612979054334\n",
      "Batch: 37%, train loss is: 0.0001799027082774189\n",
      "Batch: 38%, train loss is: 8.810910894705245e-05\n",
      "Batch: 39%, train loss is: 0.00010697474202249043\n",
      "Batch: 40%, train loss is: 7.821674064657994e-05\n",
      "Batch: 41%, train loss is: 6.512943629239614e-05\n",
      "Batch: 42%, train loss is: 8.667080864786954e-05\n",
      "Batch: 43%, train loss is: 0.00014834966369936997\n",
      "Batch: 44%, train loss is: 7.546889711133363e-05\n",
      "Batch: 45%, train loss is: 0.000139895584354085\n",
      "Batch: 46%, train loss is: 0.00019929876136462213\n",
      "Batch: 47%, train loss is: 0.0001271006941243853\n",
      "Batch: 48%, train loss is: 0.00011212195011477301\n",
      "Batch: 49%, train loss is: 0.00010071462538404925\n",
      "Batch: 50%, train loss is: 0.00012175193358222587\n",
      "Batch: 51%, train loss is: 0.00023152651745883903\n",
      "Batch: 52%, train loss is: 0.00012821503015871476\n",
      "Batch: 53%, train loss is: 9.409754125542311e-05\n",
      "Batch: 54%, train loss is: 0.00017409827400345168\n",
      "Batch: 55%, train loss is: 0.00010937258731063582\n",
      "Batch: 56%, train loss is: 9.327641775412286e-05\n",
      "Batch: 57%, train loss is: 9.046934698044026e-05\n",
      "Batch: 58%, train loss is: 0.0004356829499250665\n",
      "Batch: 59%, train loss is: 0.00012271688675207393\n",
      "Batch: 60%, train loss is: 9.309417424403038e-05\n",
      "Batch: 61%, train loss is: 0.0002481051759149734\n",
      "Batch: 62%, train loss is: 7.043189402327681e-05\n",
      "Batch: 63%, train loss is: 0.00013742294982859875\n",
      "Batch: 64%, train loss is: 0.00010450097449989592\n",
      "Batch: 65%, train loss is: 9.993300104740845e-05\n",
      "Batch: 66%, train loss is: 0.0001548595276812409\n",
      "Batch: 67%, train loss is: 0.00010981491067379111\n",
      "Batch: 68%, train loss is: 9.824162589038037e-05\n",
      "Batch: 69%, train loss is: 0.00012649063714318347\n",
      "Batch: 70%, train loss is: 4.316663460788469e-05\n",
      "Batch: 71%, train loss is: 7.609558912940345e-05\n",
      "Batch: 72%, train loss is: 0.00020004223496573596\n",
      "Batch: 73%, train loss is: 0.00011759308266810092\n",
      "Batch: 74%, train loss is: 0.00019997834327609195\n",
      "Batch: 75%, train loss is: 0.00015593375304999408\n",
      "Batch: 76%, train loss is: 9.594666143266368e-05\n",
      "Batch: 77%, train loss is: 0.00016009960499605618\n",
      "Batch: 78%, train loss is: 7.834855371475051e-05\n",
      "Batch: 79%, train loss is: 0.0004406798142473973\n",
      "Batch: 80%, train loss is: 0.0002502817431596625\n",
      "Batch: 81%, train loss is: 0.0005290185244599165\n",
      "Batch: 82%, train loss is: 0.0007499292041554713\n",
      "Batch: 83%, train loss is: 6.379757361059901e-05\n",
      "Batch: 84%, train loss is: 0.0003983965121923538\n",
      "Batch: 85%, train loss is: 0.00011185116107167943\n",
      "Batch: 86%, train loss is: 0.00043635127486109864\n",
      "Batch: 87%, train loss is: 0.00010848234564202357\n",
      "Batch: 88%, train loss is: 0.00013484868977548366\n",
      "Batch: 89%, train loss is: 0.00041290995808071143\n",
      "Batch: 90%, train loss is: 5.33390037774982e-05\n",
      "Batch: 91%, train loss is: 0.00012574139017285118\n",
      "Batch: 92%, train loss is: 7.708233959619122e-05\n",
      "Batch: 93%, train loss is: 0.0002466520366455995\n",
      "Batch: 94%, train loss is: 0.00010168782574529638\n",
      "Batch: 95%, train loss is: 8.142581643974929e-05\n",
      "Batch: 96%, train loss is: 0.00010856861020714672\n",
      "Batch: 97%, train loss is: 0.00011902652082151362\n",
      "Batch: 98%, train loss is: 6.340687956419353e-05\n",
      "Batch: 99%, train loss is: 0.00016610947061780683\n",
      "-----------------------Epoch: 6----------------------------------\n",
      "Batch: 0%, train loss is: 0.0002064212880520077\n",
      "Batch: 1%, train loss is: 7.773036507598176e-05\n",
      "Batch: 2%, train loss is: 6.704107182341792e-05\n",
      "Batch: 3%, train loss is: 0.00010241556957739136\n",
      "Batch: 4%, train loss is: 0.0004516327476718124\n",
      "Batch: 5%, train loss is: 0.00020568434788162583\n",
      "Batch: 6%, train loss is: 8.241057647884217e-05\n",
      "Batch: 7%, train loss is: 6.928152778454543e-05\n",
      "Batch: 8%, train loss is: 0.0003117635937744193\n",
      "Batch: 9%, train loss is: 0.0002579189738094684\n",
      "Batch: 10%, train loss is: 0.00011607276626203167\n",
      "Batch: 11%, train loss is: 0.0001466802049108615\n",
      "Batch: 12%, train loss is: 0.00017830061384712482\n",
      "Batch: 13%, train loss is: 0.00011565544842295657\n",
      "Batch: 14%, train loss is: 9.712402158336063e-05\n",
      "Batch: 15%, train loss is: 7.63179735149455e-05\n",
      "Batch: 16%, train loss is: 0.0001087975188239638\n",
      "Batch: 17%, train loss is: 0.00016120276988995137\n",
      "Batch: 18%, train loss is: 0.0004010775219971891\n",
      "Batch: 19%, train loss is: 0.0001786300552946336\n",
      "Batch: 20%, train loss is: 0.00016335268167700783\n",
      "Batch: 21%, train loss is: 0.00033779493169206853\n",
      "Batch: 22%, train loss is: 0.00013174678440795506\n",
      "Batch: 23%, train loss is: 8.965389419363211e-05\n",
      "Batch: 24%, train loss is: 0.00017838296521817726\n",
      "Batch: 25%, train loss is: 0.00014316963078936346\n",
      "Batch: 26%, train loss is: 0.00012631809003920368\n",
      "Batch: 27%, train loss is: 0.00030487470508502277\n",
      "Batch: 28%, train loss is: 0.0002336001280786483\n",
      "Batch: 29%, train loss is: 9.696886883165061e-05\n",
      "Batch: 30%, train loss is: 0.000101255835060916\n",
      "Batch: 31%, train loss is: 4.87576666331917e-05\n",
      "Batch: 32%, train loss is: 0.0003415322856533212\n",
      "Batch: 33%, train loss is: 0.00010673877851359034\n",
      "Batch: 34%, train loss is: 5.6235106709990256e-05\n",
      "Batch: 35%, train loss is: 0.00012872857262528344\n",
      "Batch: 36%, train loss is: 8.707310898317474e-05\n",
      "Batch: 37%, train loss is: 0.001709235378382909\n",
      "Batch: 38%, train loss is: 0.00030069909418261107\n",
      "Batch: 39%, train loss is: 5.192137267826923e-05\n",
      "Batch: 40%, train loss is: 0.00017367841913108005\n",
      "Batch: 41%, train loss is: 7.455426940585234e-05\n",
      "Batch: 42%, train loss is: 9.542174396356075e-05\n",
      "Batch: 43%, train loss is: 0.00013110365072717926\n",
      "Batch: 44%, train loss is: 0.00038307938202728213\n",
      "Batch: 45%, train loss is: 0.00015220674849137023\n",
      "Batch: 46%, train loss is: 8.158629519144586e-05\n",
      "Batch: 47%, train loss is: 0.00015343559246698188\n",
      "Batch: 48%, train loss is: 8.027172431298865e-05\n",
      "Batch: 49%, train loss is: 0.00019410585730708662\n",
      "Batch: 50%, train loss is: 0.00022592517950771612\n",
      "Batch: 51%, train loss is: 0.000144823156270628\n",
      "Batch: 52%, train loss is: 9.644509461769127e-05\n",
      "Batch: 53%, train loss is: 0.00015329121589173765\n",
      "Batch: 54%, train loss is: 0.0005266199723847568\n",
      "Batch: 55%, train loss is: 0.00012627410677192258\n",
      "Batch: 56%, train loss is: 0.00011377102853820875\n",
      "Batch: 57%, train loss is: 6.740373821845306e-05\n",
      "Batch: 58%, train loss is: 0.00011152963877783055\n",
      "Batch: 59%, train loss is: 7.558892686742069e-05\n",
      "Batch: 60%, train loss is: 0.00015519264006573513\n",
      "Batch: 61%, train loss is: 7.331423584759847e-05\n",
      "Batch: 62%, train loss is: 5.066442094213792e-05\n",
      "Batch: 63%, train loss is: 0.00013321752183208452\n",
      "Batch: 64%, train loss is: 8.091663420953684e-05\n",
      "Batch: 65%, train loss is: 9.114605864337887e-05\n",
      "Batch: 66%, train loss is: 6.444549653702013e-05\n",
      "Batch: 67%, train loss is: 7.413717704308204e-05\n",
      "Batch: 68%, train loss is: 0.00030466618716711903\n",
      "Batch: 69%, train loss is: 0.00013534970596932854\n",
      "Batch: 70%, train loss is: 0.00014224096264480704\n",
      "Batch: 71%, train loss is: 0.0002857077245166056\n",
      "Batch: 72%, train loss is: 0.00015055197227331586\n",
      "Batch: 73%, train loss is: 0.0001029693746175119\n",
      "Batch: 74%, train loss is: 0.0001085879382024346\n",
      "Batch: 75%, train loss is: 0.00013441197411635195\n",
      "Batch: 76%, train loss is: 0.00012491089191447033\n",
      "Batch: 77%, train loss is: 0.0002481251099392881\n",
      "Batch: 78%, train loss is: 0.00013564544481509724\n",
      "Batch: 79%, train loss is: 0.00014645199743462158\n",
      "Batch: 80%, train loss is: 0.00012580616245960968\n",
      "Batch: 81%, train loss is: 0.00020431022513880452\n",
      "Batch: 82%, train loss is: 0.0002228192598665646\n",
      "Batch: 83%, train loss is: 9.039745257915821e-05\n",
      "Batch: 84%, train loss is: 0.00016895222446470926\n",
      "Batch: 85%, train loss is: 0.0002664128695327725\n",
      "Batch: 86%, train loss is: 8.135130980306145e-05\n",
      "Batch: 87%, train loss is: 9.014614437827538e-05\n",
      "Batch: 88%, train loss is: 0.0007931248860018047\n",
      "Batch: 89%, train loss is: 0.0001561279605706611\n",
      "Batch: 90%, train loss is: 0.00016406378651490699\n",
      "Batch: 91%, train loss is: 0.0001965191587393969\n",
      "Batch: 92%, train loss is: 0.00011245317968477657\n",
      "Batch: 93%, train loss is: 0.00016141405590236956\n",
      "Batch: 94%, train loss is: 0.005330636856614439\n",
      "Batch: 95%, train loss is: 0.00010611866303029365\n",
      "Batch: 96%, train loss is: 0.00013260446983676585\n",
      "Batch: 97%, train loss is: 0.00015712693660858615\n",
      "Batch: 98%, train loss is: 0.00015010845960218282\n",
      "Batch: 99%, train loss is: 5.875068659950596e-05\n",
      "-----------------------Epoch: 7----------------------------------\n",
      "Batch: 0%, train loss is: 0.00011691634220430157\n",
      "Batch: 1%, train loss is: 0.0001844234018517153\n",
      "Batch: 2%, train loss is: 0.0001334494189896944\n",
      "Batch: 3%, train loss is: 8.342387229209952e-05\n",
      "Batch: 4%, train loss is: 0.0001527181052960972\n",
      "Batch: 5%, train loss is: 6.457979916580373e-05\n",
      "Batch: 6%, train loss is: 8.109514804935256e-05\n",
      "Batch: 7%, train loss is: 0.00012053373417792273\n",
      "Batch: 8%, train loss is: 0.0006373057208417178\n",
      "Batch: 9%, train loss is: 0.0003941901610616515\n",
      "Batch: 10%, train loss is: 0.00032261260594350187\n",
      "Batch: 11%, train loss is: 0.00010326762756588286\n",
      "Batch: 12%, train loss is: 0.00018724472620524913\n",
      "Batch: 13%, train loss is: 0.000182524706648551\n",
      "Batch: 14%, train loss is: 0.00015377298054921907\n",
      "Batch: 15%, train loss is: 8.24736334092979e-05\n",
      "Batch: 16%, train loss is: 0.00016625728366646386\n",
      "Batch: 17%, train loss is: 0.00018995179620934322\n",
      "Batch: 18%, train loss is: 9.688298897882616e-05\n",
      "Batch: 19%, train loss is: 0.00028462687864422195\n",
      "Batch: 20%, train loss is: 6.577707844606103e-05\n",
      "Batch: 21%, train loss is: 8.894734751044764e-05\n",
      "Batch: 22%, train loss is: 7.162911744544326e-05\n",
      "Batch: 23%, train loss is: 8.863023435919454e-05\n",
      "Batch: 24%, train loss is: 0.0002016229733294365\n",
      "Batch: 25%, train loss is: 0.00014353857631758702\n",
      "Batch: 26%, train loss is: 0.00014411835317047495\n",
      "Batch: 27%, train loss is: 4.5482519204873874e-05\n",
      "Batch: 28%, train loss is: 0.00013221241035726178\n",
      "Batch: 29%, train loss is: 0.00016841094584255203\n",
      "Batch: 30%, train loss is: 0.0001518149711796139\n",
      "Batch: 31%, train loss is: 5.707489927966849e-05\n",
      "Batch: 32%, train loss is: 0.0003424622306252109\n",
      "Batch: 33%, train loss is: 0.00016623554389615813\n",
      "Batch: 34%, train loss is: 7.495087681011614e-05\n",
      "Batch: 35%, train loss is: 6.912034316399054e-05\n",
      "Batch: 36%, train loss is: 0.00024141412744766987\n",
      "Batch: 37%, train loss is: 0.00012730365312796217\n",
      "Batch: 38%, train loss is: 6.778205649009484e-05\n",
      "Batch: 39%, train loss is: 0.00020589069208583409\n",
      "Batch: 40%, train loss is: 0.00024999133928198746\n",
      "Batch: 41%, train loss is: 0.00020009194629519238\n",
      "Batch: 42%, train loss is: 4.173773774096522e-05\n",
      "Batch: 43%, train loss is: 9.481998431554636e-05\n",
      "Batch: 44%, train loss is: 0.00010824024196683998\n",
      "Batch: 45%, train loss is: 0.00012889718648969204\n",
      "Batch: 46%, train loss is: 0.00041061483616747826\n",
      "Batch: 47%, train loss is: 7.741282745746969e-05\n",
      "Batch: 48%, train loss is: 9.928081835163893e-05\n",
      "Batch: 49%, train loss is: 0.0019015442992593044\n",
      "Batch: 50%, train loss is: 0.00017506650687847952\n",
      "Batch: 51%, train loss is: 9.351244953677676e-05\n",
      "Batch: 52%, train loss is: 0.00010840416418750158\n",
      "Batch: 53%, train loss is: 0.00015944126722674607\n",
      "Batch: 54%, train loss is: 8.370850778014208e-05\n",
      "Batch: 55%, train loss is: 0.00014626295157091826\n",
      "Batch: 56%, train loss is: 9.693123186770915e-05\n",
      "Batch: 57%, train loss is: 9.352879165207972e-05\n",
      "Batch: 58%, train loss is: 0.00010070086078621201\n",
      "Batch: 59%, train loss is: 7.434128628811501e-05\n",
      "Batch: 60%, train loss is: 8.92937041869214e-05\n",
      "Batch: 61%, train loss is: 9.540920152101168e-05\n",
      "Batch: 62%, train loss is: 9.0361029251592e-05\n",
      "Batch: 63%, train loss is: 0.00010638198646407949\n",
      "Batch: 64%, train loss is: 0.00025021814876554944\n",
      "Batch: 65%, train loss is: 8.79886591987943e-05\n",
      "Batch: 66%, train loss is: 7.121267093106119e-05\n",
      "Batch: 67%, train loss is: 0.00011533361144836887\n",
      "Batch: 68%, train loss is: 0.00016938850002466184\n",
      "Batch: 69%, train loss is: 0.0002194435512913415\n",
      "Batch: 70%, train loss is: 0.0004840046675051386\n",
      "Batch: 71%, train loss is: 0.00033854058268867267\n",
      "Batch: 72%, train loss is: 0.00015647828448584513\n",
      "Batch: 73%, train loss is: 0.00023508521445659902\n",
      "Batch: 74%, train loss is: 0.001285698066456883\n",
      "Batch: 75%, train loss is: 0.00010702303924516325\n",
      "Batch: 76%, train loss is: 0.0002959997332177509\n",
      "Batch: 77%, train loss is: 0.00018369944618419864\n",
      "Batch: 78%, train loss is: 0.00027948475573718104\n",
      "Batch: 79%, train loss is: 0.0007241644824723593\n",
      "Batch: 80%, train loss is: 7.934097206017146e-05\n",
      "Batch: 81%, train loss is: 0.0001530496115348141\n",
      "Batch: 82%, train loss is: 0.00016490535596463534\n",
      "Batch: 83%, train loss is: 0.00011953809526626502\n",
      "Batch: 84%, train loss is: 0.00012162263385663487\n",
      "Batch: 85%, train loss is: 0.00013063719210660758\n",
      "Batch: 86%, train loss is: 4.639346167340212e-05\n",
      "Batch: 87%, train loss is: 0.00013354337267826327\n",
      "Batch: 88%, train loss is: 6.0491534934921284e-05\n",
      "Batch: 89%, train loss is: 0.00019286384798089236\n",
      "Batch: 90%, train loss is: 0.00017470813561291004\n",
      "Batch: 91%, train loss is: 6.088545223921076e-05\n",
      "Batch: 92%, train loss is: 0.00017579937635221394\n",
      "Batch: 93%, train loss is: 6.731997497067734e-05\n",
      "Batch: 94%, train loss is: 0.00045859080205064057\n",
      "Batch: 95%, train loss is: 0.0006016123581321243\n",
      "Batch: 96%, train loss is: 5.973808489782613e-05\n",
      "Batch: 97%, train loss is: 7.871960843434811e-05\n",
      "Batch: 98%, train loss is: 0.0001959291522071109\n",
      "Batch: 99%, train loss is: 0.0002509700246225348\n"
     ]
    }
   ],
   "source": [
    "for param_group in optim_Adam.param_groups:\n",
    "    param_group['lr'] = 0.0001\n",
    "\n",
    "train_model(loss_MSE,optim_Adam,model,data_loader,train_data,test_data,7,test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'../restruct_data_results/rflatBergomi_pointwise88_KAN.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "initial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
