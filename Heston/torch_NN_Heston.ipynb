{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 5)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"..\\Data\")\n",
    "\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "f = gzip.GzipFile('HestonTrainSet.txt.gz', \"r\")\n",
    "dat=np.load(f)\n",
    "xx=dat[:,:5]\n",
    "yy=dat[:,5:]\n",
    "print(xx.shape)\n",
    "strikes=np.array([0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4,1.5 ])\n",
    "maturities=np.array([0.1,0.3,0.6,0.9,1.2,1.5,1.8,2.0 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(\\n    yy, xx, test_size=0.15, random_state=42)\\nfrom sklearn.preprocessing import StandardScaler\\n\\nscale = StandardScaler()\\nscale2=  StandardScaler()\\ny_train_transform = scale.fit_transform(y_train)\\ny_test_transform = scale.transform(y_test)\\nx_train_transform = scale2.fit_transform(X_train)\\nx_test_transform = scale2.transform(X_test)\\n\\ndef xtransform(X_train,X_test):\\n    return [scale2.transform(X_train),scale2.transform(X_test)]\\n\\n    \\n[x_train_transform,x_test_transform]=xtransform(X_train,X_test)\\n\\ndef xinversetransform(x):\\n    return scale2.inverse_transform(x)\\n\\n\\nub=[0.04,-0.1,1.0,0.2,10.0]\\nlb=[0.0001,-0.95,0.01,0.01,1]\\ndef myscale(x):\\n    res=np.zeros(5)\\n    for i in range(5):\\n        res[i]=(x[i] - (ub[i] + lb[i])*0.5) * 2 / (ub[i] - lb[i])\\n        \\n    return res\\ndef myinverse(x):\\n    res=np.zeros(5)\\n    for i in range(5):\\n        res[i]=x[i]*(ub[i] - lb[i]) *0.5 + (ub[i] + lb[i])*0.5\\n        \\n    return res\\n\\n\\ny_train_transform = np.array([myscale(y) for y in y_train])\\ny_test_transform = np.array([myscale(y) for y in y_test])\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    yy, xx, test_size=0.15, random_state=42)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scale = StandardScaler()\n",
    "scale2=  StandardScaler()\n",
    "y_train_transform = scale.fit_transform(y_train)\n",
    "y_test_transform = scale.transform(y_test)\n",
    "x_train_transform = scale2.fit_transform(X_train)\n",
    "x_test_transform = scale2.transform(X_test)\n",
    "\n",
    "def xtransform(X_train,X_test):\n",
    "    return [scale2.transform(X_train),scale2.transform(X_test)]\n",
    "\n",
    "    \n",
    "[x_train_transform,x_test_transform]=xtransform(X_train,X_test)\n",
    "\n",
    "def xinversetransform(x):\n",
    "    return scale2.inverse_transform(x)\n",
    "\n",
    "\n",
    "ub=[0.04,-0.1,1.0,0.2,10.0]\n",
    "lb=[0.0001,-0.95,0.01,0.01,1]\n",
    "def myscale(x):\n",
    "    res=np.zeros(5)\n",
    "    for i in range(5):\n",
    "        res[i]=(x[i] - (ub[i] + lb[i])*0.5) * 2 / (ub[i] - lb[i])\n",
    "        \n",
    "    return res\n",
    "def myinverse(x):\n",
    "    res=np.zeros(5)\n",
    "    for i in range(5):\n",
    "        res[i]=x[i]*(ub[i] - lb[i]) *0.5 + (ub[i] + lb[i])*0.5\n",
    "        \n",
    "    return res\n",
    "\n",
    "\n",
    "y_train_transform = np.array([myscale(y) for y in y_train])\n",
    "y_test_transform = np.array([myscale(y) for y in y_test])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use pytorch instead of keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    xx, yy, test_size=0.15, random_state=42)\n",
    "scale_x = StandardScaler()\n",
    "scale_y=  StandardScaler()\n",
    "\n",
    "def xtransform(x_train, x_test):\n",
    "    return [scale_x.fit_transform(x_train),scale_x.transform(x_test)]\n",
    "\n",
    "def xinversetransform(x):\n",
    "    return scale_x.inverse_transform(x)\n",
    "\n",
    "def ytransform(y_train,y_test):\n",
    "    return [scale_y.fit_transform(y_train),scale_y.transform(y_test)]\n",
    "\n",
    "def yinversetransform(y):\n",
    "    return scale_y.inverse_transform(y)\n",
    "\n",
    "\n",
    "\n",
    "ub=np.array([0.04,-0.1,1.0,0.2,10.0])\n",
    "lb=np.array([0.0001,-0.95,0.01,0.01,1])\n",
    "def myscale(x):\n",
    "    return (x - (ub+lb)*0.5)*2/(ub-lb)\n",
    "def myinverse(x):\n",
    "    return x*(ub-lb)*0.5+(ub+lb)*0.5\n",
    "\n",
    "x_train_transform = myscale(x_train)\n",
    "x_test_transform = myscale(x_test)\n",
    "[y_train_transform,y_test_transform] = ytransform(y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load to torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#use gpu if you can\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"device is {device}\")\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(x_train_transform).to(device=device),\n",
    "                                               torch.from_numpy(y_train_transform).to(device=device))\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.from_numpy(x_test_transform).to(device=device),\n",
    "                                              torch.from_numpy(y_test_transform).to(device=device))\n",
    "\n",
    "test_data = (torch.from_numpy(x_test_transform).to(device=device),torch.from_numpy(y_test_transform).to(device=device))\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(train_dataset,batch_size =32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1800, 88])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define nn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "hyperparas = {'input_dim':5,'hidden_dim':30,'hidden_nums':3,'output_dim':88,'block_layer_nums':3}\n",
    "\n",
    "class NN_pricing(nn.Module):\n",
    "    def __init__(self,hyperparas):\n",
    "        super().__init__()\n",
    "        self.input_dim = hyperparas['input_dim']\n",
    "        self.hidden_dim = hyperparas['hidden_dim']\n",
    "        self.hidden_nums = hyperparas['hidden_nums']\n",
    "        self.output_dim = hyperparas['output_dim']\n",
    "\n",
    "        self.layer_list = []\n",
    "        self.layer_list.append(nn.Sequential(nn.Linear(self.input_dim,self.hidden_dim),nn.ELU() ) )\n",
    "\n",
    "        for _ in range(self.hidden_dim-1):\n",
    "            self.layer_list.append(nn.Sequential(nn.Linear(self.hidden_dim,self.hidden_dim),nn.ELU()))\n",
    "\n",
    "        self.layer_list.append(nn.Linear(self.hidden_dim,self.output_dim))\n",
    "\n",
    "        self.linear_stock = nn.Sequential(*self.layer_list)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        \n",
    "        return self.linear_stock(inputs)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize model, choose loss function and opim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_pricing = NN_pricing(hyperparas).to(device=device,dtype=torch.float64)\n",
    "loss_MSE = nn.MSELoss()\n",
    "optim_Adam = torch.optim.Adam(nn_pricing.parameters(),lr= 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load from pretrained keras model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(loss_function, optimizer, model, loader,test_data):\n",
    "  for(i, (x, y)) in enumerate(loader):\n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Run a forward pass\n",
    "    outputs = model.forward(x)\n",
    "    # Compute the batch loss\n",
    "    loss = loss_function(outputs,y)\n",
    "    # Calculate the gradients\n",
    "    loss.backward()\n",
    "    # Update the parameteres\n",
    "    optimizer.step()\n",
    "\n",
    "    if i%100 == 0:\n",
    "      print(f\"Batch: {i},train loss is: {loss}\")\n",
    "      with torch.no_grad():\n",
    "        test_outputs = model.forward(test_data[0])\n",
    "        test_loss = loss_function(test_outputs,test_data[1])\n",
    "        print(f\"test loss is {test_loss}\")\n",
    "\n",
    "\n",
    "\n",
    "def train_model(loss_function, optimizer, model, loader,test_data,epochs=25):\n",
    "  for i in range(epochs):\n",
    "    print(f\"-----------------------Epoch: {i}----------------------------------\")\n",
    "\n",
    "    train_epoch(loss_function, optimizer, model, loader,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Epoch: 0----------------------------------\n",
      "Batch: 0,train loss is: 1.1413268548630635\n",
      "test loss is 1.0363871573073755\n",
      "Batch: 100,train loss is: 0.9395603677946557\n",
      "test loss is 1.0284868055776266\n",
      "Batch: 200,train loss is: 0.7784195783929618\n",
      "test loss is 1.0250820677322559\n",
      "Batch: 300,train loss is: 0.7574236227937926\n",
      "test loss is 1.0085473606035116\n",
      "-----------------------Epoch: 1----------------------------------\n",
      "Batch: 0,train loss is: 1.2118412090438786\n",
      "test loss is 0.9445960523826002\n",
      "Batch: 100,train loss is: 0.5007043218665171\n",
      "test loss is 0.49386122889853445\n",
      "Batch: 200,train loss is: 0.623575587405397\n",
      "test loss is 0.44547819494439944\n",
      "Batch: 300,train loss is: 0.3762123622138977\n",
      "test loss is 0.4049358182450921\n",
      "-----------------------Epoch: 2----------------------------------\n",
      "Batch: 0,train loss is: 0.35481166748108406\n",
      "test loss is 0.40154221547385177\n",
      "Batch: 100,train loss is: 0.2893049831102349\n",
      "test loss is 0.3047669287711538\n",
      "Batch: 200,train loss is: 0.22348400460330772\n",
      "test loss is 0.23592043856696032\n",
      "Batch: 300,train loss is: 0.27586000790897514\n",
      "test loss is 0.21668842842528377\n",
      "-----------------------Epoch: 3----------------------------------\n",
      "Batch: 0,train loss is: 0.15132290050633018\n",
      "test loss is 0.20584338081844597\n",
      "Batch: 100,train loss is: 0.24568533981129395\n",
      "test loss is 0.19840831096897316\n",
      "Batch: 200,train loss is: 0.15474900610482037\n",
      "test loss is 0.18973279889953945\n",
      "Batch: 300,train loss is: 0.15242042679380186\n",
      "test loss is 0.18415945529351366\n",
      "-----------------------Epoch: 4----------------------------------\n",
      "Batch: 0,train loss is: 0.14118691885642637\n",
      "test loss is 0.1913736539387207\n",
      "Batch: 100,train loss is: 0.24710966776143556\n",
      "test loss is 0.18119703309450555\n",
      "Batch: 200,train loss is: 0.18986115063436085\n",
      "test loss is 0.17606273932625552\n",
      "Batch: 300,train loss is: 0.16919715773982064\n",
      "test loss is 0.17005317288286403\n",
      "-----------------------Epoch: 5----------------------------------\n",
      "Batch: 0,train loss is: 0.16313936537024076\n",
      "test loss is 0.17029145322477326\n",
      "Batch: 100,train loss is: 0.23240061523416564\n",
      "test loss is 0.1658200485982916\n",
      "Batch: 200,train loss is: 0.16035628549113876\n",
      "test loss is 0.16325451173139588\n",
      "Batch: 300,train loss is: 0.13737851938893772\n",
      "test loss is 0.16104079224806503\n",
      "-----------------------Epoch: 6----------------------------------\n",
      "Batch: 0,train loss is: 0.14003607354306008\n",
      "test loss is 0.16157030819502108\n",
      "Batch: 100,train loss is: 0.15075012147289663\n",
      "test loss is 0.16113965705005887\n",
      "Batch: 200,train loss is: 0.10685498384323684\n",
      "test loss is 0.1566386732326991\n",
      "Batch: 300,train loss is: 0.1829294285208249\n",
      "test loss is 0.15517302806097744\n",
      "-----------------------Epoch: 7----------------------------------\n",
      "Batch: 0,train loss is: 0.13982516173493328\n",
      "test loss is 0.15472007318269448\n",
      "Batch: 100,train loss is: 0.17569181188093974\n",
      "test loss is 0.1537432492276138\n",
      "Batch: 200,train loss is: 0.1703478427096428\n",
      "test loss is 0.15257279123280762\n",
      "Batch: 300,train loss is: 0.1513594108947947\n",
      "test loss is 0.15033868770800243\n",
      "-----------------------Epoch: 8----------------------------------\n",
      "Batch: 0,train loss is: 0.14594106152642175\n",
      "test loss is 0.14981995949644944\n",
      "Batch: 100,train loss is: 0.1265275549359516\n",
      "test loss is 0.14920113599468435\n",
      "Batch: 200,train loss is: 0.16176664952114442\n",
      "test loss is 0.14808985635882313\n",
      "Batch: 300,train loss is: 0.08433183499696052\n",
      "test loss is 0.14822952902850825\n",
      "-----------------------Epoch: 9----------------------------------\n",
      "Batch: 0,train loss is: 0.16144741828012069\n",
      "test loss is 0.1469499531156461\n",
      "Batch: 100,train loss is: 0.2223412289809861\n",
      "test loss is 0.14686572290068575\n",
      "Batch: 200,train loss is: 0.1851657294527231\n",
      "test loss is 0.1466076181737361\n",
      "Batch: 300,train loss is: 0.11411462270156687\n",
      "test loss is 0.14570988056051895\n",
      "-----------------------Epoch: 10----------------------------------\n",
      "Batch: 0,train loss is: 0.19762933098365137\n",
      "test loss is 0.14645722678123255\n",
      "Batch: 100,train loss is: 0.1367285742086568\n",
      "test loss is 0.14549182961212923\n",
      "Batch: 200,train loss is: 0.11532862733608139\n",
      "test loss is 0.14475629934549558\n",
      "Batch: 300,train loss is: 0.18236951331790818\n",
      "test loss is 0.14459297794815668\n",
      "-----------------------Epoch: 11----------------------------------\n",
      "Batch: 0,train loss is: 0.18024130883084014\n",
      "test loss is 0.14443473783843241\n",
      "Batch: 100,train loss is: 0.15412534806434033\n",
      "test loss is 0.14456038621000192\n",
      "Batch: 200,train loss is: 0.1412026630011243\n",
      "test loss is 0.14416044425793426\n",
      "Batch: 300,train loss is: 0.1568739911055658\n",
      "test loss is 0.1437638882247746\n",
      "-----------------------Epoch: 12----------------------------------\n",
      "Batch: 0,train loss is: 0.15883144294447404\n",
      "test loss is 0.1435969736600301\n",
      "Batch: 100,train loss is: 0.1299711740414072\n",
      "test loss is 0.14354021598320027\n",
      "Batch: 200,train loss is: 0.14355368888144074\n",
      "test loss is 0.14346043025096866\n",
      "Batch: 300,train loss is: 0.1308715389743512\n",
      "test loss is 0.14360814223647983\n",
      "-----------------------Epoch: 13----------------------------------\n",
      "Batch: 0,train loss is: 0.182612640539147\n",
      "test loss is 0.14413664613878052\n",
      "Batch: 100,train loss is: 0.11725629263232298\n",
      "test loss is 0.1433797599027121\n",
      "Batch: 200,train loss is: 0.1261166650037895\n",
      "test loss is 0.14353128352688438\n",
      "Batch: 300,train loss is: 0.1142936271936704\n",
      "test loss is 0.14316354587167965\n",
      "-----------------------Epoch: 14----------------------------------\n",
      "Batch: 0,train loss is: 0.11344665382738912\n",
      "test loss is 0.14291600068350727\n",
      "Batch: 100,train loss is: 0.16486812011740654\n",
      "test loss is 0.14320197488109093\n",
      "Batch: 200,train loss is: 0.1297040468835152\n",
      "test loss is 0.1428816279546202\n",
      "Batch: 300,train loss is: 0.12614013197822185\n",
      "test loss is 0.1431382902213015\n",
      "-----------------------Epoch: 15----------------------------------\n",
      "Batch: 0,train loss is: 0.13010178581869758\n",
      "test loss is 0.14294137856409492\n",
      "Batch: 100,train loss is: 0.10017978531739057\n",
      "test loss is 0.14288831547513883\n",
      "Batch: 200,train loss is: 0.1387831717159419\n",
      "test loss is 0.14365778738620721\n",
      "Batch: 300,train loss is: 0.11416234865058089\n",
      "test loss is 0.14339616089142254\n",
      "-----------------------Epoch: 16----------------------------------\n",
      "Batch: 0,train loss is: 0.08291264785429078\n",
      "test loss is 0.14305614262304572\n",
      "Batch: 100,train loss is: 0.15302188772164949\n",
      "test loss is 0.1426596414911855\n",
      "Batch: 200,train loss is: 0.14132322281939097\n",
      "test loss is 0.14268560410359724\n",
      "Batch: 300,train loss is: 0.11147477957146837\n",
      "test loss is 0.14284566312431152\n",
      "-----------------------Epoch: 17----------------------------------\n",
      "Batch: 0,train loss is: 0.13440755310972566\n",
      "test loss is 0.14260231320431446\n",
      "Batch: 100,train loss is: 0.15530103931647\n",
      "test loss is 0.14254166650012606\n",
      "Batch: 200,train loss is: 0.1502535826386444\n",
      "test loss is 0.14239397148458202\n",
      "Batch: 300,train loss is: 0.14770313395992704\n",
      "test loss is 0.14262812728250515\n",
      "-----------------------Epoch: 18----------------------------------\n",
      "Batch: 0,train loss is: 0.130507002178062\n",
      "test loss is 0.14273908312809505\n",
      "Batch: 100,train loss is: 0.18971866838853851\n",
      "test loss is 0.14311576846094792\n",
      "Batch: 200,train loss is: 0.12992896846115692\n",
      "test loss is 0.14257504635749071\n",
      "Batch: 300,train loss is: 0.11089870463166224\n",
      "test loss is 0.14228769962796167\n",
      "-----------------------Epoch: 19----------------------------------\n",
      "Batch: 0,train loss is: 0.13176980133553992\n",
      "test loss is 0.14220946034521778\n",
      "Batch: 100,train loss is: 0.1351968072750086\n",
      "test loss is 0.14284660666547977\n",
      "Batch: 200,train loss is: 0.11429424055750018\n",
      "test loss is 0.1430919272060363\n",
      "Batch: 300,train loss is: 0.14950291314516084\n",
      "test loss is 0.14209904475768334\n",
      "-----------------------Epoch: 20----------------------------------\n",
      "Batch: 0,train loss is: 0.10840360609745872\n",
      "test loss is 0.14235070022571097\n",
      "Batch: 100,train loss is: 0.14480029084576485\n",
      "test loss is 0.1421433815630511\n",
      "Batch: 200,train loss is: 0.12886110822326252\n",
      "test loss is 0.14249289147651667\n",
      "Batch: 300,train loss is: 0.177932617917346\n",
      "test loss is 0.1441698329048079\n",
      "-----------------------Epoch: 21----------------------------------\n",
      "Batch: 0,train loss is: 0.15207948323042345\n",
      "test loss is 0.14220772472046778\n",
      "Batch: 100,train loss is: 0.13596522530796257\n",
      "test loss is 0.1428425121298423\n",
      "Batch: 200,train loss is: 0.13700836845909936\n",
      "test loss is 0.14196922430807096\n",
      "Batch: 300,train loss is: 0.13417308411559153\n",
      "test loss is 0.14204923893899984\n",
      "-----------------------Epoch: 22----------------------------------\n",
      "Batch: 0,train loss is: 0.11100145881964017\n",
      "test loss is 0.14209235878355558\n",
      "Batch: 100,train loss is: 0.20640777001628974\n",
      "test loss is 0.14192032304993535\n",
      "Batch: 200,train loss is: 0.15690794619908385\n",
      "test loss is 0.14216008926203286\n",
      "Batch: 300,train loss is: 0.11848193371887375\n",
      "test loss is 0.14213032530394362\n",
      "-----------------------Epoch: 23----------------------------------\n",
      "Batch: 0,train loss is: 0.11009678188545954\n",
      "test loss is 0.14272588487199062\n",
      "Batch: 100,train loss is: 0.13606327429699225\n",
      "test loss is 0.14190363694537458\n",
      "Batch: 200,train loss is: 0.12262910501423953\n",
      "test loss is 0.14184170241831323\n",
      "Batch: 300,train loss is: 0.21854145031240185\n",
      "test loss is 0.1419423353674335\n",
      "-----------------------Epoch: 24----------------------------------\n",
      "Batch: 0,train loss is: 0.13537160601489237\n",
      "test loss is 0.1419604461520552\n",
      "Batch: 100,train loss is: 0.11953494136862171\n",
      "test loss is 0.14192268063071614\n",
      "Batch: 200,train loss is: 0.1506138871214265\n",
      "test loss is 0.14201726531506104\n",
      "Batch: 300,train loss is: 0.11864727786677434\n",
      "test loss is 0.14176265279277564\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "train_model(loss_MSE,optim_Adam,nn_pricing,data_loader,test_data,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean volatility divergency = 0.011157600375670288\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = yinversetransform(nn_pricing(torch.from_numpy(x_test_transform).to(device=device)).cpu().detach().numpy())\n",
    "\n",
    "div = y_test-y_test_pred\n",
    "\n",
    "abs_div =np.abs(div)\n",
    "\n",
    "print(f'mean volatility divergency = {abs_div.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use deep Layernormalization & Resnet network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, hyperparas):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hyperparas['hidden_dim']\n",
    "        self.block_layer_nums =hyperparas['block_layer_nums']\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        # Define layers for the function f (MLP)\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(self.block_layer_nums - 1):  # -2 because we already added one layer and last layer is already defined\n",
    "            self.layers.append(nn.Linear(self.hidden_dim,self.hidden_dim ))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layernorms = nn.ModuleList()\n",
    "        for _ in range(self.block_layer_nums - 1):  # -1 because layer normalization is not applied to the last layer\n",
    "            self.layernorms.append(nn.LayerNorm(self.hidden_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the function f (MLP)\n",
    "        out = x\n",
    "        for i in range(self.block_layer_nums - 1):  # -1 because last layer is already applied outside the loop\n",
    "            out = self.layers[i](out)\n",
    "            out = self.layernorms[i](out)\n",
    "            out = torch.relu(out)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Element-wise addition of input x and output of function f(x)\n",
    "        out = x + out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "class ResNN_pricing(nn.Module):\n",
    "    def __init__(self,hyperparas):\n",
    "        super().__init__()\n",
    "        self.input_dim = hyperparas['input_dim']\n",
    "        self.hidden_dim = hyperparas['hidden_dim']\n",
    "        self.hidden_nums = hyperparas['hidden_nums']\n",
    "        self.output_dim = hyperparas['output_dim']\n",
    "        self.block_layer_nums = hyperparas['block_layer_nums']\n",
    "\n",
    "        self.layer_list = []\n",
    "        self.layer_list.append(nn.Sequential(nn.Linear(self.input_dim,self.hidden_dim),nn.ReLU() ) )\n",
    "\n",
    "        for _ in range(self.hidden_nums-1):\n",
    "            self.layer_list.append(ResNetBlock(hyperparas)\n",
    "                                   )\n",
    "\n",
    "        self.layer_list.append(nn.Linear(self.hidden_dim,self.output_dim))\n",
    "\n",
    "        self.linear_stock = nn.Sequential(*self.layer_list)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        \n",
    "        return self.linear_stock(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparas = {'input_dim':5,'hidden_dim':64,'hidden_nums':10,'output_dim':88,'block_layer_nums':3}\n",
    "\n",
    "res_pricing = ResNN_pricing(hyperparas).to(device=device,dtype=torch.float64)\n",
    "loss_MSE = nn.MSELoss()\n",
    "optim_Adam = torch.optim.Adam(res_pricing.parameters(),lr= 0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Epoch: 0----------------------------------\n",
      "Batch: 0,train loss is: 5.139003106586036\n",
      "test loss is 4.520331972460173\n",
      "Batch: 100,train loss is: 0.7633071433567397\n",
      "test loss is 0.7723634582914265\n",
      "Batch: 200,train loss is: 0.2868414586561597\n",
      "test loss is 0.3146973447192724\n",
      "Batch: 300,train loss is: 0.16219731549888558\n",
      "test loss is 0.16251347347227765\n",
      "-----------------------Epoch: 1----------------------------------\n",
      "Batch: 0,train loss is: 0.15975359057811547\n",
      "test loss is 0.14948438930873842\n",
      "Batch: 100,train loss is: 0.1162937502593498\n",
      "test loss is 0.10681298089026733\n",
      "Batch: 200,train loss is: 0.07377534001917141\n",
      "test loss is 0.08682997125976542\n",
      "Batch: 300,train loss is: 0.07934477968063293\n",
      "test loss is 0.07043182217307149\n",
      "-----------------------Epoch: 2----------------------------------\n",
      "Batch: 0,train loss is: 0.05751288308702213\n",
      "test loss is 0.06816677209990359\n",
      "Batch: 100,train loss is: 0.08614609907536822\n",
      "test loss is 0.05727501317737035\n",
      "Batch: 200,train loss is: 0.038158131217379467\n",
      "test loss is 0.04757876199336847\n",
      "Batch: 300,train loss is: 0.03659121188810345\n",
      "test loss is 0.040443075447536225\n",
      "-----------------------Epoch: 3----------------------------------\n",
      "Batch: 0,train loss is: 0.03773472394001584\n",
      "test loss is 0.0394502800100343\n",
      "Batch: 100,train loss is: 0.028904163135214187\n",
      "test loss is 0.033998311587935245\n",
      "Batch: 200,train loss is: 0.02758944282359096\n",
      "test loss is 0.03005201027473229\n",
      "Batch: 300,train loss is: 0.02574425350122955\n",
      "test loss is 0.02684297188436713\n",
      "-----------------------Epoch: 4----------------------------------\n",
      "Batch: 0,train loss is: 0.027414065551054596\n",
      "test loss is 0.026082275067816914\n",
      "Batch: 100,train loss is: 0.023416163193939316\n",
      "test loss is 0.023921220179278824\n",
      "Batch: 200,train loss is: 0.015278068354286295\n",
      "test loss is 0.02200568833879199\n",
      "Batch: 300,train loss is: 0.018051259879950692\n",
      "test loss is 0.020148272668979405\n",
      "-----------------------Epoch: 5----------------------------------\n",
      "Batch: 0,train loss is: 0.03287861982332675\n",
      "test loss is 0.022173811395987827\n",
      "Batch: 100,train loss is: 0.01637010399510014\n",
      "test loss is 0.01869203611184493\n",
      "Batch: 200,train loss is: 0.01055043829021983\n",
      "test loss is 0.017472318045068946\n",
      "Batch: 300,train loss is: 0.014009812224696971\n",
      "test loss is 0.016381808619977727\n",
      "-----------------------Epoch: 6----------------------------------\n",
      "Batch: 0,train loss is: 0.022239991389521086\n",
      "test loss is 0.016400354140323467\n",
      "Batch: 100,train loss is: 0.013527090943103868\n",
      "test loss is 0.015273268970403932\n",
      "Batch: 200,train loss is: 0.01276592278240683\n",
      "test loss is 0.014372785739818492\n",
      "Batch: 300,train loss is: 0.012638635370550332\n",
      "test loss is 0.013713282927134562\n",
      "-----------------------Epoch: 7----------------------------------\n",
      "Batch: 0,train loss is: 0.011076368010385055\n",
      "test loss is 0.013128136653170636\n",
      "Batch: 100,train loss is: 0.014816368222923613\n",
      "test loss is 0.012495142297047708\n",
      "Batch: 200,train loss is: 0.013575062717635443\n",
      "test loss is 0.011917360323124333\n",
      "Batch: 300,train loss is: 0.013040316145490859\n",
      "test loss is 0.011261525742237207\n",
      "-----------------------Epoch: 8----------------------------------\n",
      "Batch: 0,train loss is: 0.0135037109692906\n",
      "test loss is 0.011027687464863727\n",
      "Batch: 100,train loss is: 0.007663175049555484\n",
      "test loss is 0.010495796329599535\n",
      "Batch: 200,train loss is: 0.010934442452786334\n",
      "test loss is 0.009872645962729085\n",
      "Batch: 300,train loss is: 0.006547305477029273\n",
      "test loss is 0.009251276365960886\n",
      "-----------------------Epoch: 9----------------------------------\n",
      "Batch: 0,train loss is: 0.010124164488527347\n",
      "test loss is 0.009368365182405412\n",
      "Batch: 100,train loss is: 0.01117184628238446\n",
      "test loss is 0.008805103949462986\n",
      "Batch: 200,train loss is: 0.009056972983767264\n",
      "test loss is 0.00835295130619554\n",
      "Batch: 300,train loss is: 0.0066615445762537055\n",
      "test loss is 0.008355656675922443\n",
      "-----------------------Epoch: 10----------------------------------\n",
      "Batch: 0,train loss is: 0.007104811843017566\n",
      "test loss is 0.00820731501069117\n",
      "Batch: 100,train loss is: 0.007284725133958847\n",
      "test loss is 0.007494722952948226\n",
      "Batch: 200,train loss is: 0.006510958518031897\n",
      "test loss is 0.007322192863073166\n",
      "Batch: 300,train loss is: 0.005698379215027241\n",
      "test loss is 0.007151221156827866\n",
      "-----------------------Epoch: 11----------------------------------\n",
      "Batch: 0,train loss is: 0.007069534770873079\n",
      "test loss is 0.006826158718484328\n",
      "Batch: 100,train loss is: 0.005682136331628835\n",
      "test loss is 0.006990259917882948\n",
      "Batch: 200,train loss is: 0.0055975727774882815\n",
      "test loss is 0.006314885773567461\n",
      "Batch: 300,train loss is: 0.00539126409710651\n",
      "test loss is 0.006952793759703154\n",
      "-----------------------Epoch: 12----------------------------------\n",
      "Batch: 0,train loss is: 0.0075092545290168814\n",
      "test loss is 0.006849911694030787\n",
      "Batch: 100,train loss is: 0.004634783390679606\n",
      "test loss is 0.006344647435841568\n",
      "Batch: 200,train loss is: 0.006495829567806606\n",
      "test loss is 0.005752478183111909\n",
      "Batch: 300,train loss is: 0.005938166063587701\n",
      "test loss is 0.0054785589270521075\n",
      "-----------------------Epoch: 13----------------------------------\n",
      "Batch: 0,train loss is: 0.00710682316810893\n",
      "test loss is 0.005351227713108972\n",
      "Batch: 100,train loss is: 0.005070547391478052\n",
      "test loss is 0.006040164391328541\n",
      "Batch: 200,train loss is: 0.005459548964227487\n",
      "test loss is 0.0051588789501800635\n",
      "Batch: 300,train loss is: 0.004016709278758269\n",
      "test loss is 0.005718159370220062\n",
      "-----------------------Epoch: 14----------------------------------\n",
      "Batch: 0,train loss is: 0.005996245792568207\n",
      "test loss is 0.005355372804200041\n",
      "Batch: 100,train loss is: 0.0046386929882538215\n",
      "test loss is 0.004724210603506829\n",
      "Batch: 200,train loss is: 0.003791770898163617\n",
      "test loss is 0.0047471531907634125\n",
      "Batch: 300,train loss is: 0.006271087325417193\n",
      "test loss is 0.00501359378693145\n",
      "-----------------------Epoch: 15----------------------------------\n",
      "Batch: 0,train loss is: 0.004172695051643024\n",
      "test loss is 0.00543926291249246\n",
      "Batch: 100,train loss is: 0.005400421774307011\n",
      "test loss is 0.004553433446723816\n",
      "Batch: 200,train loss is: 0.0040618535229071115\n",
      "test loss is 0.004384169909783513\n",
      "Batch: 300,train loss is: 0.0030222316003226763\n",
      "test loss is 0.004493812088422643\n",
      "-----------------------Epoch: 16----------------------------------\n",
      "Batch: 0,train loss is: 0.004256438259418354\n",
      "test loss is 0.004257879618437151\n",
      "Batch: 100,train loss is: 0.003263189532457775\n",
      "test loss is 0.004246305977947944\n",
      "Batch: 200,train loss is: 0.003168038390838977\n",
      "test loss is 0.004329476342900415\n",
      "Batch: 300,train loss is: 0.0036166046284021335\n",
      "test loss is 0.004384008023398307\n",
      "-----------------------Epoch: 17----------------------------------\n",
      "Batch: 0,train loss is: 0.004328870672976705\n",
      "test loss is 0.00414266688580368\n",
      "Batch: 100,train loss is: 0.00429395386387419\n",
      "test loss is 0.004463786652791246\n",
      "Batch: 200,train loss is: 0.004093900532902078\n",
      "test loss is 0.003817906365699755\n",
      "Batch: 300,train loss is: 0.0035328148626788897\n",
      "test loss is 0.0037876441135428224\n",
      "-----------------------Epoch: 18----------------------------------\n",
      "Batch: 0,train loss is: 0.0036463139114490473\n",
      "test loss is 0.0038666295461330506\n",
      "Batch: 100,train loss is: 0.0036115390621607583\n",
      "test loss is 0.003770859579998041\n",
      "Batch: 200,train loss is: 0.0034964961422202577\n",
      "test loss is 0.0037725492102066173\n",
      "Batch: 300,train loss is: 0.0025646151184474685\n",
      "test loss is 0.0036939027266927587\n",
      "-----------------------Epoch: 19----------------------------------\n",
      "Batch: 0,train loss is: 0.0024409569313216515\n",
      "test loss is 0.003658560091945694\n",
      "Batch: 100,train loss is: 0.0029807950421346465\n",
      "test loss is 0.003634909480401437\n",
      "Batch: 200,train loss is: 0.0032928736544062445\n",
      "test loss is 0.0035749800051519446\n",
      "Batch: 300,train loss is: 0.0023202371071534525\n",
      "test loss is 0.0034673774102105723\n",
      "-----------------------Epoch: 20----------------------------------\n",
      "Batch: 0,train loss is: 0.003553241733530658\n",
      "test loss is 0.0035225555602784895\n",
      "Batch: 100,train loss is: 0.0033017535896268254\n",
      "test loss is 0.003886376689204612\n",
      "Batch: 200,train loss is: 0.003541340643978011\n",
      "test loss is 0.0033381779876155817\n",
      "Batch: 300,train loss is: 0.002349323724181797\n",
      "test loss is 0.0037687383221208844\n",
      "-----------------------Epoch: 21----------------------------------\n",
      "Batch: 0,train loss is: 0.0027250272889037808\n",
      "test loss is 0.00352422057988733\n",
      "Batch: 100,train loss is: 0.004258174447282093\n",
      "test loss is 0.0031987464457935645\n",
      "Batch: 200,train loss is: 0.0029873561956748986\n",
      "test loss is 0.003419467928460281\n",
      "Batch: 300,train loss is: 0.001980755990376714\n",
      "test loss is 0.003438995369999176\n",
      "-----------------------Epoch: 22----------------------------------\n",
      "Batch: 0,train loss is: 0.002583922590941509\n",
      "test loss is 0.003159381666847448\n",
      "Batch: 100,train loss is: 0.005070368172916275\n",
      "test loss is 0.0033919767859190196\n",
      "Batch: 200,train loss is: 0.0036719412331521727\n",
      "test loss is 0.0030823848703717266\n",
      "Batch: 300,train loss is: 0.0031560139305336354\n",
      "test loss is 0.003046530573557477\n",
      "-----------------------Epoch: 23----------------------------------\n",
      "Batch: 0,train loss is: 0.0025708723113430807\n",
      "test loss is 0.0032262752119957346\n",
      "Batch: 100,train loss is: 0.0026345191806315576\n",
      "test loss is 0.0030647566733277777\n",
      "Batch: 200,train loss is: 0.0028516913466600565\n",
      "test loss is 0.003390868225315361\n",
      "Batch: 300,train loss is: 0.0026744596016713943\n",
      "test loss is 0.0029828183162329976\n",
      "-----------------------Epoch: 24----------------------------------\n",
      "Batch: 0,train loss is: 0.003348967047131892\n",
      "test loss is 0.0030789466445774456\n",
      "Batch: 100,train loss is: 0.002732066397052988\n",
      "test loss is 0.0030039258188001086\n",
      "Batch: 200,train loss is: 0.0025191785484089428\n",
      "test loss is 0.003196843208457872\n",
      "Batch: 300,train loss is: 0.0028141488659855886\n",
      "test loss is 0.00289107559047809\n"
     ]
    }
   ],
   "source": [
    "epochs =25\n",
    "train_model(loss_MSE,optim_Adam,res_pricing,data_loader,test_data,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean volatility divergency is: 0.0017638782581835153\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = yinversetransform(res_pricing(torch.from_numpy(x_test_transform).to(device=device)).cpu().detach().numpy())\n",
    "\n",
    "div = y_test-y_test_pred\n",
    "\n",
    "abs_div =np.abs(div)\n",
    "\n",
    "print(f\"mean volatility divergency is: {abs_div.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(nn_pricing.state_dict(),'./YU/initialnn_Heston.pth')\n",
    "torch.save(res_pricing.state_dict(),'./YU/res_layernorm_Heston.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
